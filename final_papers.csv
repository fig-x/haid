id,Database,Title,Abstract,DOI,Link,Venue,Authors,Year,Citation,Bibtex,Contribution Types,Application Domains,Decision Types,AI Roles,Human Roles,AI Influence,Human Influence,AI Input,Human Input,Modality,Code Abstract,Code Full Paper
2-10005,elsevier,Employees recruitment: a prescriptive analytics approach via machine learning and mathematical programming,"In this paper, we propose a comprehensive analytics framework that can serve as a decision support tool for HR recruiters in real-world settings in order to improve hiring and placement decisions. The proposed framework follows two main phases: a local prediction scheme for recruitments' success at the level of a single job placement, and a mathematical model that provides a global recruitment optimization scheme for the organization, taking into account multilevel considerations. In the first phase, a key property of the proposed prediction approach is the interpretability of the machine learning (ML) model, which in this case is obtained by applying the Variable-Order Bayesian Network (VOBN) model to the recruitment data. Specifically, we used a uniquely large dataset that contains recruitment records of hundreds of thousands of employees over a decade and represents a wide range of heterogeneous populations. Our analysis shows that the VOBN model can provide both high accuracy and interpretability insights to HR professionals. Moreover, we show that using the interpretable VOBN can lead to unexpected and sometimes counter-intuitive insights that might otherwise be overlooked by recruiters who rely on conventional methods. We demonstrate that it is feasible to predict the successful placement of a candidate in a specific position at a pre-hire stage and utilize predictions to devise a global optimization model. Our results show that in comparison to actual recruitment decisions, the devised framework is capable of providing a balanced recruitment plan while improving both diversity and recruitment success rates, despite the inherent trade-off between the two.",https://doi.org/10.1016/j.dss.2020.113290,https://www.sciencedirect.com/science/article/pii/S0167923620300452,Decision Support Systems,Dana Pessach;Gonen Singer;Dan Avrahami;Hila {Chalutz Ben-Gal};Erez Shmueli;Irad Ben-Gal,2020,402,"@article{2-10005,
  title     = {Employees recruitment: a prescriptive analytics approach via machine learning and mathematical programming},
  author    = {Dana Pessach and Gonen Singer and Dan Avrahami and Hila {Chalutz Ben-Gal} and Erez Shmueli and Irad Ben-Gal},
  year      = {2020},
  journal   = {Decision Support Systems},
  doi       = {10.1016/j.dss.2020.113290}
}",Algorithmic contributions,Everyday / Employment / Public Service,Operational,"Explaining, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-10053,elsevier,Enhancing accuracy and interpretability in eeg-based medical decision making using an explainable ensemble learning framework application for stroke prediction,"Medical decision making increasingly relies on machine learning algorithms to analyze complex patient data and provide recommendations. However, the lack of interpretability in “black box” models has limited their adoption in clinical practice, which demands transparency and justification. Echo State Networks (ESNs), a recurrent neural network architecture, have shown promise for medical Decision Support Systems (DSS) applications due to their accuracy in modeling time series data, such as physiological signals. However, conventional ESNs suffer from a lack of interpretability, which limits their usefulness for clinical decisions. This study proposes a multi-level framework to optimize both accuracy and interpretability for improved medical decision support. The framework is specifically designed for EEG data, which represents a non-invasive and continuous recording of brain activity. EEG data is ideal for recording information about the internal state of the brain, which is not always translated by perceptible external manifestations. The framework consists of four components: (1) data preprocessing and optimized feature selection utilizing a filter-based feature importance algorithm, (2) ensemble learning via Ensemble ESNs (E-ESNs) that combines the predictions of multiple ESNs to reduce variance and improve accuracy, (3) Global and Local model explanation techniques to allow users to understand the decision-making process and identify the most influential features for each prediction, and (4) decision-making for presenting diagnoses and recommendations. The framework was evaluated on an EEG dataset for stroke prediction, a valuable use case for informed clinical decisions and resource allocation. The results showed that the framework significantly outperformed baseline related works with an accuracy of 96.5% and provides insights into the E-ESN model's predictions. This transparency enhances trust and comprehension among healthcare practitioners. These findings suggest that the proposed framework can be a valuable tool for medical DSS applications.",https://doi.org/10.1016/j.dss.2023.114126,https://www.sciencedirect.com/science/article/pii/S0167923623002014,Decision Support Systems,Samar Bouazizi;Hela Ltifi,2024,14,"@article{2-10053,
  title={Enhancing accuracy and interpretability in EEG-based medical decision making using an explainable ensemble learning framework application for stroke prediction},
  author={Bouazizi, Samar and Ltifi, Hela},
  year={2024},
  journal={Decision Support Systems},
  volume={xxx},
  pages={xxx--xxx},
  doi={10.1016/j.dss.2023.114126}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Explaining, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-10177,elsevier,Evaluating artificial intelligence systems to guide purchasing decisions,"Many radiologists are considering investments in artificial intelligence (AI) to improve the quality of care for our patients. This article outlines considerations for the purchasing process beginning with performance evaluation. Practices should decide whether there is a need to independently verify performance or accept vendor-provided data. Successful implementations will consider who will receive AI results, how results will be presented, and the impact on efficiency. The article provides education on infrastructure considerations including the benefits and drawbacks of best-of-breed and platform approaches in addition to highly specialized server requirements like graphical processing unit availability. Finally, the article presents financial and quality and safety considerations, some of which are unique to AI. Examples include whether additional revenue could be obtained, as in the case of mammography, and whether an AI model unintentionally leads to reinforcing healthcare disparities.",https://doi.org/10.1016/j.jacr.2020.09.045,https://www.sciencedirect.com/science/article/pii/S154614402031005X,Journal of the American College of Radiology,Ross W. Filice;John Mongan;Marc D. Kohli,2020,1,"@article{2-10177,
  title = {Evaluating artificial intelligence systems to guide purchasing decisions},
  author = {Ross W. Filice and John Mongan and Marc D. Kohli},
  year = {2020},
  doi = {10.1016/j.jacr.2020.09.045},
  journal = {Journal of the American College of Radiology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Organizational,"Analyzing, Advising, Monitoring",Decision-maker,no such info,Update AI competence,visual explanations,"corrective feedback, evaluation, domain knowledge, public safety","corrective feedback, evaluation, domain knowledge, public safety",Yes,Yes
2-10215,elsevier,Evaluation of a machine-learning model based on laboratory parameters for the prediction of acute leukaemia subtypes: a multicentre model development and validation study in france,"Summary Background Acute leukaemias are life-threatening haematological cancers characterised by the infiltration of transformed immature haematopoietic cells in the blood and bone marrow. Prompt and accurate diagnosis of the three main acute leukaemia subtypes (ie acute lymphocytic leukaemia [ALL], acute myeloid leukaemia [AML], and acute promyelocytic leukaemia [APL]) is of utmost importance to guide initial treatment and prevent early mortality but requires cytological expertise that is not always available. We aimed to benchmark different machine-learning strategies using a custom variable selection algorithm to propose an extreme gradient boosting model to predict leukaemia subtypes on the basis of routine laboratory parameters. Methods This multicentre model development and validation study was conducted with data from six independent French university hospital databases. Patients aged 18 years or older diagnosed with AML, APL, or ALL in any one of these six hospital databases between March 1, 2012, and Dec 31, 2021, were recruited. 22 routine parameters were collected at the time of initial disease evaluation; variables with more than 25% of missing values in two datasets were not used for model training, leading to the final inclusion of 19 parameters. The performances of the final model were evaluated on internal testing and external validation sets with area under the receiver operating characteristic curves (AUCs), and clinically relevant cutoffs were chosen to guide clinical decision making. The final tool, Artificial Intelligence Prediction of Acute Leukemia (AI-PAL), was developed from this model. Findings 1410 patients diagnosed with AML, APL, or ALL were included. Data quality control showed few missing values for each cohort, with the exception of uric acid and lactate dehydrogenase for the cohort from Hôpital Cochin. 679 patients from Hôpital Lyon Sud and Centre Hospitalier Universitaire de Clermont-Ferrand were split into the training (n=477) and internal testing (n=202) sets. 731 patients from the four other cohorts were used for external validation. Overall AUCs across all validation cohorts were 0·97 (95% CI 0·95–0·99) for APL, 0·90 (0·83–0·97) for ALL, and 0·89 (0·82–0·95) for AML. Cutoffs were then established on the overall cohort of 1410 patients to guide clinical decisions. Confident cutoffs showed two (0·14%) wrong predictions for ALL, four (0·28%) wrong predictions for APL, and three (0·21%) wrong predictions for AML. Use of the overall cutoff greatly reduced the number of missing predictions; diagnosis was proposed for 1375 (97·5%) of 1410 patients for each category, with only a slight increase in wrong predictions. The final model evaluation across both the internal testing and external validation sets showed accuracy of 99·5% for ALL diagnosis, 98·8% for AML diagnosis, and 99·7% for APL diagnosis in the confident model and accuracy of 87·9% for ALL diagnosis, 86·3% for AML diagnosis, and 96·1% for APL diagnosis in the overall model. Interpretation AI-PAL allowed for accurate diagnosis of the three main acute leukaemia subtypes. Based on ten simple laboratory parameters, its broad availability could help guide initial therapies in a context where cytological expertise is lacking, such as in low-income countries. Funding None.",https://doi.org/10.1016/S2589-7500(24)00044-X,https://www.sciencedirect.com/science/article/pii/S258975002400044X,The Lancet Digital Health,Vincent Alcazer;Grégoire {Le Meur};Marie Roccon;Sabrina Barriere;Baptiste {Le Calvez};Bouchra Badaoui;Agathe Spaeth;Olivier Kosmider;Nicolas Freynet;Marion Eveillard;Carolyne Croizier;Simon Chevalier;Pierre Sujobert,2024,2,"@article{2-10215,
  title = {Evaluation of a machine-learning model based on laboratory parameters for the prediction of acute leukaemia subtypes: a multicentre model development and validation study in France},
  author = {Vincent Alcazer and Grégoire {Le Meur} and Marie Roccon and Sabrina Barriere and Baptiste {Le Calvez} and Bouchra Badaoui and Agathe Spaeth and Olivier Kosmider and Nicolas Freynet and Marion Eveillard and Carolyne Croizier and Simon Chevalier and Pierre Sujobert},
  year = {2024},
  doi = {https://doi.org/10.1016/S2589-7500(24)00044-X},
  journal = {The Lancet Digital Health}
}","Empirical contributions, System/Artifact contributions",Healthcare / Medicine / Surgery,Operational,Forecasting,"Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-10251,elsevier,Evidence-based managerial decision-making with machine learning: the case of bayesian inference in aviation incidents,"Understanding the factors behind aviation incidents is essential, not only because of the lethality of the accidents but also the incidents' direct and indirect economic impact. Even minor incidents trigger significant economic damage and create disruptions to aviation operations. It is crucial to investigate these incidents to understand the underlying reasons and hence, reduce the risk associated with physical and financial safety in a precarious industry like aviation. The findings may provide decision-makers with a causally accurate means of investigating the topic while untangling the difficulties concerning the statistical associations and causal effects. This research aims to identify the significant variables and their probabilistic dependencies/relationships determining the degree of aircraft damage. The value and the contribution of this study include (1) developing a fully automatic ML prediction-based DSS for aircraft damage severity, (2) conducting a deep network analysis of affinity between predicting variables using probabilistic graphical modeling (PGM), and (3) implementing a user-friendly dashboard to interpret the business insight coming from the design and development of the Bayesian Belief Network (BBN). By leveraging a large, real-world dataset, the proposed methodology captures the probability-based interrelations among air terminal, flight, flight crew, and air-vehicle-related characteristics as explanatory variables, thereby revealing the underlying, complex interactions in accident severity. This research contributes significantly to the current body of knowledge by defining and proving a methodology for automatically categorizing aircraft damage severity based on flight, aircraft, and PIC (pilot in command) information. Moreover, the study combines the findings of the Bayesian Belief Networks with decades of aviation expertise of the subject matter expert, drawing and explaining the association map to find the root causes of the problems and accident relayed variables.",https://doi.org/10.1016/j.omega.2023.102906,https://www.sciencedirect.com/science/article/pii/S0305048323000701,Omega - The International Journal of Management Science,Burak Cankaya;Kazim Topuz;Dursun Delen;Aaron Glassman,2023,29,"@article{2-10251,
  title = {Evidence-based managerial decision-making with machine learning: the case of bayesian inference in aviation incidents},
  author = {Burak Cankaya and Kazim Topuz and Dursun Delen and Aaron Glassman},
  year = {2023},
  doi = {10.1016/j.omega.2023.102906},
  journal = {Omega - The International Journal of Management Science}
}",System/Artifact contributions,Transportation / Mobility / Planning,Organizational,"Analyzing, Forecasting, Advising","Developer, Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-10296,elsevier,Expert-demonstration-augmented reinforcement learning for lane-change-aware eco-driving traversing consecutive traffic lights,"Eco-driving methods incorporating lateral motion exhibit enhanced energy-saving prospects in multi-lane traffic contexts, yet the randomly distributed obstructing vehicles and sparse traffic lights pose challenges in assessing the long-term value of instantaneous actions, impeding further improvement in energy efficiency. In response to this issue, a deep reinforcement learning (DRL)-based eco-driving method is proposed and augmented with the expert demonstration mechanism. Specifically, a Markov decision process matching with the target eco-driving scenario is systematically constructed, with which, the formulated DRL algorithm, parametrized soft actor-critic (PSAC), is trained to realize the integrated optimization of speed planning and lane-changing maneuver. To promote the training performance of PSAC under sparse rewards concerning traffic lights, an expert eco-driving model and an adaptive sampling approach are incorporated to constitute the expert demonstration mechanism. Simulation results highlight the superior performance of the proposed DRL-based eco-driving method and its training mechanism. Compared with the performance of the PSAC with a pure exploration-based training mechanism, the expert demonstration mechanism promotes the training efficiency and cumulated rewards of PSAC by about 60 % and 21.89 % respectively in the training phase, while in the test phase, a further reduction of 4.23 % benchmarked on a rule-based method is achieved in fuel consumption.",https://doi.org/10.1016/j.energy.2023.129472,https://www.sciencedirect.com/science/article/pii/S0360544223028669,Energy,Chuntao Zhang;Wenhui Huang;Xingyu Zhou;Chen Lv;Chao Sun,2024,18,"@article{2-10296,
  title = {Expert-demonstration-augmented reinforcement learning for lane-change-aware eco-driving traversing consecutive traffic lights},
  author = {Chuntao Zhang and Wenhui Huang and Xingyu Zhou and Chen Lv and Chao Sun},
  year = {2024},
  journal = {Energy},
  doi = {https://doi.org/10.1016/j.energy.2023.129472}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,"Knowledge provider, Stakeholder, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-10310,elsevier,Explainable ai tools for legal reasoning about cases: a study on the european court of human rights,"In this paper we report on a significant research project undertaken to design, implement and evaluate explainable decision-support tools for deciding legal cases. We provide a model of a legal domain, Article 6 of the European Convention on Human Rights, constructed using a methodology from the field of computational models of argument. We describe how the formal model has been developed, extended and transformed into practical tools, which were then used in evaluation exercises to determine the effectiveness and usability of the tools. The underpinning AI techniques used yield a level of explanation that is firmly grounded in legal reasoning and is also digestible by the target end users, as demonstrated through our evaluation activities. The results of our experimental evaluation show that on the first pass, our tool achieved an accuracy rate of 97% in matching the actual decisions of the cases and the user studies conducted gave highly encouraging results with respect to usability. As such, our project demonstrates how trustworthy AI tools can be built for a real world legal domain where critical needs of the end users are accounted for.",https://doi.org/10.1016/j.artint.2023.103861,https://www.sciencedirect.com/science/article/pii/S0004370223000073,Artificial Intelligence,Joe Collenette;Katie Atkinson;Trevor Bench-Capon,2023,101,"@article{2-10310,
  title = {Explainable AI Tools for Legal Reasoning about Cases: A Study on the European Court of Human Rights},
  author = {Collenette, Joe and Atkinson, Katie and Bench-Capon, Trevor},
  year = {2023},
  doi = {10.1016/j.artint.2023.103861},
  journal = {Artificial Intelligence},
  note = {https://doi.org/10.1016/j.artint.2023.103861}
}",System/Artifact contributions,Law / Policy / Governance,Operational,"Explaining, Advising","Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-10318,elsevier,Explainable artificial intelligence and agile decision-making in supply chain cyber resilience,"Although artificial intelligence can contribute to decision-making processes, many industry players lag behind pioneering companies in utilizing artificial intelligence-driven technologies, which is a significant problem. Explainable artificial intelligence can be a viable solution to mitigate this problem. This paper proposes a research model to address how explainable artificial intelligence can impact decision-making processes. Using an experimental design, empirical data is collected to test the research model. This paper is one of the pioneer papers providing empirical evidence about the impact of explainable artificial intelligence on supply chain decision-making processes. We propose a serial mediation path, which includes transparency and agile decision-making. Findings reveal that explainable artificial intelligence enhances transparency, thereby significantly contributing to agile decision-making for improving cyber resilience during supply chain cyberattacks. Moreover, we conduct a post hoc analysis using text analysis to explore the themes present in tweets discussing explainable artificial intelligence in decision support systems. The results indicate a predominantly positive attitude towards explainable artificial intelligence within these systems. Furthermore, the text analysis reveals two main themes that emphasize the importance of transparency, explainability, and interpretability in explainable artificial intelligence.",https://doi.org/10.1016/j.dss.2024.114194,https://www.sciencedirect.com/science/article/pii/S0167923624000277,Decision Support Systems,Kiarash {Sadeghi R.};Divesh Ojha;Puneet Kaur;Raj V. Mahto;Amandeep Dhir,2024,109,"@article{2-10318,
  title = {Explainable artificial intelligence and agile decision-making in supply chain cyber resilience},
  author = {Kiarash {Sadeghi R.} and Divesh Ojha and Puneet Kaur and Raj V. Mahto and Amandeep Dhir},
  year = {2024},
  doi = {10.1016/j.dss.2024.114194},
  journal = {Decision Support Systems}
}","Methodological contributions, Empirical contributions",Manufacturing / Industry / Automation,Organizational,"Advising, Explaining",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-10327,elsevier,Explainable bayesian networks applied to transport vulnerability,"To deal with increasing amounts of data, decision and policymakers frequently turn to advances in machine learning and artificial intelligence to capitalise on the potential reward. But there is also a reluctance to trust black-box models, especially when such models are used to support decisions and policies that affect people directly, like those associated with transport and people’s mobility. Recent developments focus on explainable artificial intelligence to bolster models’ trustworthiness. In this paper, we demonstrate the use of an explainable-by-design model, Bayesian Networks, on travel behaviour. The model incorporates various demographic and socioeconomic variables to describe full day activity chains: activity and mode choice, as well as the activity and trip durations. More importantly, this paper shows how the model can be used to provide the most relevant explanation for people’s observed travel behaviour. The overall goal is to show that model explanations can be quantified and, therefore, assist policymakers to truly make evidence-based decisions. This goal is achieved through two case studies to explain people’s vulnerability as it pertains to their total trip duration.",https://doi.org/10.1016/j.eswa.2022.118348,https://www.sciencedirect.com/science/article/pii/S0957417422014671,Expert Systems with Applications,Alta {de Waal};Johan W. Joubert,2022,33,"@article{2-10327,
  title = {Explainable Bayesian Networks Applied to Transport Vulnerability},
  author = {Alta {de Waal} and Johan W. Joubert},
  year = {2022},
  doi = {10.1016/j.eswa.2022.118348},
  journal = {Expert Systems with Applications}
}",Methodological contributions,Transportation / Mobility / Planning,Operational,"Explaining, Advising","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-10331,elsevier,Explainable classifier for improving the accountability in decision-making for colorectal cancer diagnosis from histopathological images,"Pathologists are responsible for cancer type diagnoses from histopathological cancer tissues. However, it is known that microscopic examination is tedious and time-consuming. In recent years, a long list of machine learning approaches to image classification and whole-slide segmentation has been developed to support pathologists. Although many showed exceptional performances, the majority of them are not able to rationalize their decisions. In this study, we developed an explainable classifier to support decision making for medical diagnoses. The proposed model does not provide an explanation about the causality between the input and the decisions, but offers a human-friendly explanation about the plausibility of the decision. Cumulative Fuzzy Class Membership Criterion (CFCMC) explains its decisions in three ways: through a semantical explanation about the possibilities of misclassification, showing the training sample responsible for a certain prediction and showing training samples from conflicting classes. In this paper, we explain about the mathematical structure of the classifier, which is not designed to be used as a fully automated diagnosis tool but as a support system for medical experts. We also report on the accuracy of the classifier against real world histopathological data for colorectal cancer. We also tested the acceptability of the system through clinical trials by 14 pathologists. We show that the proposed classifier is comparable to state of the art neural networks in accuracy, but more importantly it is more acceptable to be used by human experts as a diagnosis tool in the medical domain.",https://doi.org/10.1016/j.jbi.2020.103523,https://www.sciencedirect.com/science/article/pii/S1532046420301519,Journal of Biomedical Informatics,Patrik Sabol;Peter Sinčák;Pitoyo Hartono;Pavel Kočan;Zuzana Benetinová;Alžbeta Blichárová;Ľudmila Verbóová;Erika Štammová;Antónia Sabolová-Fabianová;Anna Jašková,2020,113,"@article{2-10331,
  title={Explainable classifier for improving the accountability in decision-making for colorectal cancer diagnosis from histopathological images},
  author={Sabol, Patrik and Sinčák, Peter and Hartono, Pitoyo and Kočan, Pavel and Benetinová, Zuzana and Blichárová, Alžbeta and Verbóová, Ľudmila and Štammová, Erika and Sabolová-Fabianová, Antónia and Jašková, Anna},
  year={2020},
  journal={Journal of Biomedical Informatics},
  doi={10.1016/j.jbi.2020.103523}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising, Forecasting","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-10334,elsevier,Explainable decision support through the learning and visualization of preferences from a formal ontology of antibiotic treatments,"The aim of eXplainable Artificial Intelligence (XAI) is to design intelligent systems that can explain their predictions or recommendations to humans. Such systems are particularly desirable for therapeutic decision support, because physicians need to understand rcommendations to have confidence in their application and to adapt them if required, e.g. in case of patient contraindication. We propose here an explainable and visual approach for decision support in antibiotic treatment, based on an ontology. There were three steps to our method. We first generated a tabular dataset from the ontology, containing features defined on various domains and n-ary features. A preference model was then learned from patient profiles, antibiotic features and expert recommendations found in clinical practice guidelines. This model made the implicit rationale of the expert explicit, including the way in which missing data was treated. We then visualized the preference model and its application to all antibiotics available on the market for a given clinical situation, using rainbow boxes, a recently developed technique for set visualization. The resulting preference model had an error rate of 3.5% on the learning data, and 5.2% on test data (10-fold validation). These findings suggest that our system can help physicians to prescribe antibiotics correctly, even for clinical situations not present in the guidelines (e.g. due to allergies or contraindications for the recommended treatment).",https://doi.org/10.1016/j.jbi.2020.103407,https://www.sciencedirect.com/science/article/pii/S1532046420300356,Journal of Biomedical Informatics,Jean-Baptiste Lamy;Karima Sedki;Rosy Tsopra,2020,6,"@article{2-10334,
  title={Explainable decision support through the learning and visualization of preferences from a formal ontology of antibiotic treatments},
  author={Lamy, Jean-Baptiste and Sedki, Karima and Tsopra, Rosy},
  year={2020},
  journal={Journal of Biomedical Informatics},
  doi={10.1016/j.jbi.2020.103407}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-10363,elsevier,Explaining machine learning models in sales predictions,"A complexity of business dynamics often forces decision-makers to make decisions based on subjective mental models, reflecting their experience. However, research has shown that companies perform better when they apply data-driven decision-making. This creates an incentive to introduce intelligent, data-based decision models, which are comprehensive and support the interactive evaluation of decision options necessary for the business environment. Recently, a new general explanation methodology has been proposed, which supports the explanation of state-of-the-art black-box prediction models. Uniform explanations are generated on the level of model/individual instance and support what-if analysis. We present a novel use of this methodology inside an intelligent system in a real-world case of business-to-business (B2B) sales forecasting, a complex task frequently done judgmentally. Users can validate their assumptions with the presented explanations and test their hypotheses using the presented what-if parallel graph representation. The results demonstrate effectiveness and usability of the methodology. A significant advantage of the presented method is the possibility to evaluate seller’s actions and to outline general recommendations in sales strategy. This flexibility of the approach and easy-to-follow explanations are suitable for many different applications. Our well-documented real-world case shows how to solve a decision support problem, namely that the best performing black-box models are inaccessible to human interaction and analysis. This could extend the use of the intelligent systems to areas where they were so far neglected due to their insistence on comprehensible models. A separation of the machine learning model selection from model explanation is another significant benefit for expert and intelligent systems. Explanations unconnected to a particular prediction model positively influence acceptance of new and complex models in the business environment through their easy assessment and switching.",https://doi.org/10.1016/j.eswa.2016.11.010,https://www.sciencedirect.com/science/article/pii/S0957417416306327,Expert Systems with Applications,Marko Bohanec;Mirjana {Kljajić Borštnar};Marko Robnik-Šikonja,2017,252,"@article{2-10363,
  title={Explaining machine learning models in sales predictions},
  author={Bohanec, Marko and Kljaji{\'c} Bor{\v{s}}tnar, Mirjana and Robnik-{\v{S}}ikonja, Marko},
  year={2017},
  journal={Expert Systems with Applications},
  doi={10.1016/j.eswa.2016.11.010}
}",System/Artifact contributions,Finance / Business / Economy,Operational,"Advising, Explaining, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-10366,elsevier,Explaining the black-box smoothly—a counterfactual approach,"We propose a BlackBox Counterfactual Explainer, designed to explain image classification models for medical applications. Classical approaches (e.g., , saliency maps) that assess feature importance do not explain how imaging features in important anatomical regions are relevant to the classification decision. Such reasoning is crucial for transparent decision-making in healthcare applications. Our framework explains the decision for a target class by gradually exaggerating the semantic effect of the class in a query image. We adopted a Generative Adversarial Network (GAN) to generate a progressive set of perturbations to a query image, such that the classification decision changes from its original class to its negation. Our proposed loss function preserves essential details (e.g., support devices) in the generated images. We used counterfactual explanations from our framework to audit a classifier trained on a chest X-ray dataset with multiple labels. Clinical evaluation of model explanations is a challenging task. We proposed clinically-relevant quantitative metrics such as cardiothoracic ratio and the score of a healthy costophrenic recess to evaluate our explanations. We used these metrics to quantify the counterfactual changes between the populations with negative and positive decisions for a diagnosis by the given classifier. We conducted a human-grounded experiment with diagnostic radiology residents to compare different styles of explanations (no explanation, saliency map, cycleGAN explanation, and our counterfactual explanation) by evaluating different aspects of explanations: (1) understandability, (2) classifier’s decision justification, (3) visual quality, (d) identity preservation, and (5) overall helpfulness of an explanation to the users. Our results show that our counterfactual explanation was the only explanation method that significantly improved the users’ understanding of the classifier’s decision compared to the no-explanation baseline. Our metrics established a benchmark for evaluating model explanation methods in medical images. Our explanations revealed that the classifier relied on clinically relevant radiographic features for its diagnostic decisions, thus making its decision-making process more transparent to the end-user.",https://doi.org/10.1016/j.media.2022.102721,https://www.sciencedirect.com/science/article/pii/S1361841522003498,Medical Image Analysis,Sumedha Singla;Motahhare Eslami;Brian Pollack;Stephen Wallace;Kayhan Batmanghelich,2023,109,"@article{2-10366,
  title={Explaining the black-box smoothly---a counterfactual approach},
  author={Singla, Sumedha and Eslami, Motahhare and Pollack, Brian and Wallace, Stephen and Batmanghelich, Kayhan},
  year={2023},
  journal={Medical Image Analysis},
  doi={10.1016/j.media.2022.102721}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Explaining, Auditing","Decision-maker, Knowledge provider, Guardian","Change cognitive demands, Change affective-perceptual",Update AI competence,"visual explanations, cycleGAN explanation, counterfactual explanations",NA,Visual,Yes,Yes
2-104,aaai,An Interactive Explanatory AI System for Industrial Quality Control,"Machine learning based image classification algorithms, such as deep neural network approaches, will be increasingly employed in critical settings such as quality control in industry, where transparency and comprehensibility of decisions are crucial. Therefore, we aim to extend the defect detection task towards an interactive human-in-the-loop approach that allows us to integrate rich background knowledge and the inference of complex relationships going beyond traditional purely data-driven approaches. We propose an approach for an interactive support system for classifications in an industrial quality control setting that combines the advantages of both( explainable) knowledge-driven and data-driven machine learning methods, in particular inductive logic programming and convolutional neural networks, with human expertise and control. The resulting system can assist domain experts with decisions, provide transparent explanations for results, and integrate feedback from users; thus reducing workload for humans while both respecting their expertise and without removing their agency or accountability.",10.1609/aaai.v36i11.21530,https://ojs.aaai.org/index.php/AAAI/article/view/21530,AAAI Conference on Artificial Intelligence,Dennis Müller;Michael März;Stephan Scheele;Ute Schmid,2022,18,"@inproceedings{2-104,
  title = {An Interactive Explanatory AI System for Industrial Quality Control},
  author = {Dennis Müller and Michael März and Stephan Scheele and Ute Schmid},
  year = {2022},
  doi = {10.1609/aaai.v36i11.21530},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence}
}",System/Artifact contributions,Manufacturing / Industry / Automation,Operational,"Explaining, Executing, Forecasting, Collaborating","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-10425,elsevier,Extrapolation-enhanced model for travel decision making: an ensemble machine learning approach considering behavioral theory,"Modeling individuals’ travel decision making in terms of choosing transport modes, route and departure time for daily activities is an indispensable component for transport system optimization and management. Conventional approaches of modeling travel decision making suffer from presumed model structures and parametric specifications. Emerging machine learning algorithms offer data-driven and non-parametric solutions for modeling travel decision making but encounter extrapolation issues (i.e., disability to predict scenarios beyond training samples) due to neglecting behavioral mechanisms in the framework. This study proposes an extrapolation-enhanced approach for modeling travel decision making, leveraging the complementary merits of ensemble machine learning algorithms (Random Forest in our study) and knowledge-based decision-making theory to enhance both predictive accuracy and model extrapolation. The proposed approach is examined using three datasets about travel decision making, including one estimation dataset (for cross-validation) and two test datasets (for model extrapolation tests). Especially, we use two test datasets containing extrapolated choice scenarios with features that exceed the ranges of training samples, to examine the predictive ability of proposed models in extrapolated choice scenarios, which have hardly been investigated by relevant literature. The results show that both proposed models and the direct application of Random Forest (RF) can give quite good predictive accuracy (around 80%) in the estimation dataset. However, RF has a deficient predictive ability in two test datasets with extrapolated choice scenarios. In contrast, the proposed models provide substantially superior predictive performances in the two test datasets, indicating much stronger extrapolation capacity. The model based on the proposed framework could improve the precision score by 274.93% than the direct application of RF in the first test dataset and by 21.9% in the second test dataset. The results indicate the merits of the proposed approach in terms of prediction power and extrapolation ability as compared to existing methods.",https://doi.org/10.1016/j.knosys.2021.106882,https://www.sciencedirect.com/science/article/pii/S0950705121001453,Knowledge-Based Systems,Kun Gao;Ying Yang;Tianshu Zhang;Aoyong Li;Xiaobo Qu,2021,42,"@article{2-10425,
  title={Extrapolation-enhanced model for travel decision making: an ensemble machine learning approach considering behavioral theory},
  author={Gao, Kun and Yang, Ying and Zhang, Tianshu and Li, Aoyong and Qu, Xiaobo},
  year={2021},
  journal={Knowledge-Based Systems},
  doi={10.1016/j.knosys.2021.106882}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Forecasting, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-10449,elsevier,Fairlens: auditing black-box clinical decision support systems,"The pervasive application of algorithmic decision-making is raising concerns on the risk of unintended bias in AI systems deployed in critical settings such as healthcare. The detection and mitigation of model bias is a very delicate task that should be tackled with care and involving domain experts in the loop. In this paper we introduce FairLens, a methodology for discovering and explaining biases. We show how this tool can audit a fictional commercial black-box model acting as a clinical decision support system (DSS). In this scenario, the healthcare facility experts can use FairLens on their historical data to discover the biases of the model before incorporating it into the clinical decision flow. FairLens first stratifies the available patient data according to demographic attributes such as age, ethnicity, gender and healthcare insurance; it then assesses the model performance on such groups highlighting the most common misclassifications. Finally, FairLens allows the expert to examine one misclassification of interest by explaining which elements of the affected patients’ clinical history drive the model error in the problematic group. We validate FairLens’ ability to highlight bias in multilabel clinical DSSs introducing a multilabel-appropriate metric of disparity and proving its efficacy against other standard metrics.",https://doi.org/10.1016/j.ipm.2021.102657,https://www.sciencedirect.com/science/article/pii/S030645732100145X,Information Processing & Management,Cecilia Panigutti;Alan Perotti;André Panisson;Paolo Bajardi;Dino Pedreschi,2021,105,"@article{2-10449,
  title = {Fairlens: Auditing Black-box Clinical Decision Support Systems},
  author = {Cecilia Panigutti and Alan Perotti and André Panisson and Paolo Bajardi and Dino Pedreschi},
  year = {2021},
  doi = {10.1016/j.ipm.2021.102657},
  journal = {Information Processing \& Management}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Auditing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-10464,elsevier,Fat-cat—explainability and augmentation for an ai system: a case study on ai recruitment-system adoption,"Because artificial intelligence (AI) recruitment systems exhibited discriminatory decisions in recent applications, the adoption of such systems in industry has raised doubts. As equity has been emphasized in AI decision-making frameworks, the non-explainability issue regarding the high performance of AI methods has become prominent. Therefore, scholars have focused on human–AI augmentation in which humans consider equity and AI supports the consideration. As a result, explainability is highlighted as a new capability of AI methods for an ideal decision. In this regard, this study proposes the so-called fairness, accountability, and transparency (FAT)-complexity, anxiety, and trust (CAT) model that describes the path from explainability to AI system adoption considering augmentation, assuming that the capability of the AI decision maker to explain the basis of its decision and interact with the human decision maker is crucial for AI recruitment system adoption. We found that explainability and augmentation are two key factors in AI recruitment system adoption and assessed that their importance will gradually increase as recruiters will be asked to use such AI systems more commonly. Moreover, this study conceptualized the role of an augmented relationship between humans and AI in decision-making, in which they complement each other's limitations.",https://doi.org/10.1016/j.ijhcs.2022.102976,https://www.sciencedirect.com/science/article/pii/S107158192200194X,International Journal of Human-Computer Studies,ChangHyun Lee;KyungJin Cha,2023,1,"@article{2-10464,
  title = {Fat-cat—explainability and augmentation for an AI system: a case study on AI recruitment-system adoption},
  author = {Lee, ChangHyun and Cha, KyungJin},
  year = {2023},
  doi = {10.1016/j.ijhcs.2022.102976},
  journal = {International Journal of Human-Computer Studies}
}",Theoretical contributions,Everyday / Employment / Public Service,Institutional,"Explaining, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-10681,elsevier,Generating a decision support system for states in the usa via machine learning,"In literature, many studies try to analyze healthcare usage and generated decision support systems. In this paper, the aim is to generate a decision support system for insurance companies in the United States according to the federal dataset. The data set contains variables from the United States Healthcare Administration formal data group and they are; group of age, group of healthcare insurance, group of nationality, Corporate Social Responsibility (CSR) usage, and Advance Premium Tax Credit (APTC) usage. The best model for each state in the United States is determined with the dataset from 2014 November to 2015 February. In this study, various statistical models are attempted to generate independent and optimal models for each state. In the model selection process, three different metrics are used. Accuracy rate, f1score and precision score are used in the model selection process and models are compared to each other according to these three metrics for each state uniquely. Consequently, the proposed model is used with a decision support scheme to support customer service workers in healthcare insurance companies. The decision support system under consideration can facilitate a substantial decrease in the duration of interactions between customer service representatives and clients. Additionally, this system can generate more logically sound and efficient customer offers.",https://doi.org/10.1016/j.eswa.2024.123259,https://www.sciencedirect.com/science/article/pii/S0957417424001246,Expert Systems with Applications,Hüseyin Ünözkan,2024,2,"@article{2-10681,
  title = {Generating a Decision Support System for States in the USA via Machine Learning},
  author = {H\""useyin \""Un\""ozkan},
  year = {2024},
  journal = {Expert Systems with Applications},
  doi = {10.1016/j.eswa.2024.123259}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-10690,elsevier,Generative pre-trained transformer 4 analysis of cardiovascular magnetic resonance reports in suspected myocarditis: a multicenter study,"ABSTRACT Background Diagnosing myocarditis relies on multimodal data, including cardiovascular magnetic resonance (CMR), clinical symptoms, and blood values. The correct interpretation and integration of CMR findings require radiological expertise and knowledge. We aimed to investigate the performance of Generative Pre-trained Transformer 4 (GPT-4), a large language model, for report-based medical decision-making in the context of cardiac MRI for suspected myocarditis. Methods This retrospective study includes CMR reports from 396 patients with suspected myocarditis and eight centers, respectively. CMR reports and patient data including blood values, age, and further clinical information were provided to GPT-4 and radiologists with 1 (resident 1), 2 (resident 2), and 4 years (resident 3) of experience in CMR and knowledge of the 2018 Lake Louise Criteria. The final impression of the report regarding the radiological assessment of whether myocarditis is present or not was not provided. The performance of Generative pre-trained transformer 4 (GPT-4) and the human readers were compared to a consensus reading (two board-certified radiologists with 8 and 10 years of experience in CMR). Sensitivity, specificity, and accuracy were calculated. Results GPT-4 yielded an accuracy of 83%, sensitivity of 90%, and specificity of 78%, which was comparable to the physician with 1 year of experience (R1: 86%, 90%, 84%, p = 0.14) and lower than that of more experienced physicians (R2: 89%, 86%, 91%, p = 0.007 and R3: 91%, 85%, 96%, p < 0.001). GPT-4 and human readers showed a higher diagnostic performance when results from T1- and T2-mapping sequences were part of the reports, for residents 1 and 3 with statistical significance (p = 0.004 and p = 0.02, respectively). Conclusion GPT-4 yielded good accuracy for diagnosing myocarditis based on CMR reports in a large dataset from multiple centers and therefore holds the potential to serve as a diagnostic decision-supporting tool in this capacity, particularly for less experienced physicians. Further studies are required to explore the full potential and elucidate educational aspects of the integration of large language models in medical decision-making.",https://doi.org/10.1016/j.jocmr.2024.101068,https://www.sciencedirect.com/science/article/pii/S1097664724010950,Journal of Cardiovascular Magnetic Resonance,Kenan Kaya;Carsten Gietzen;Robert Hahnfeldt;Maher Zoubi;Tilman Emrich;Moritz C. Halfmann;Malte Maria Sieren;Yannic Elser;Patrick Krumm;Jan M. Brendel;Konstantin Nikolaou;Nina Haag;Jan Borggrefe;Ricarda von Krüchten;Katharina Müller-Peltzer;Constantin Ehrengut;Timm Denecke;Andreas Hagendorff;Lukas Goertz;Roman J. Gertz;Alexander Christian Bunck;David Maintz;Thorsten Persigehl;Simon Lennartz;Julian A. Luetkens;Astha Jaiswal;Andra Iza Iuga;Lenhard Pennig;Jonathan Kottlors,2024,13,"@article{2-10690,
  title = {Generative Pre-trained Transformer 4 Analysis of Cardiovascular Magnetic Resonance Reports in Suspected Myocarditis: A Multicenter Study},
  author = {Kenan Kaya and Carsten Gietzen and Robert Hahnfeldt and Maher Zoubi and Tilman Emrich and Moritz C. Halfmann and Malte Maria Sieren and Yannic Elser and Patrick Krumm and Jan M. Brendel and Konstantin Nikolaou and Nina Haag and Jan Borggrefe and Ricarda von Krüchten and Katharina Müller-Peltzer and Constantin Ehrengut and Timm Denecke and Andreas Hagendorff and Lukas Goertz and Roman J. Gertz and Alexander Christian Bunck and David Maintz and Thorsten Persigehl and Simon Lennartz and Julian A. Luetkens and Astha Jaiswal and Andra Iza Iuga and Lenhard Pennig and Jonathan Kottlors},
  year = {2024},
  doi = {10.1016/j.jocmr.2024.101068},
  journal = {Journal of Cardiovascular Magnetic Resonance}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-10728,elsevier,Gpm: a graph convolutional network based reinforcement learning framework for portfolio management,"Portfolio management is a decision-making process of periodically reallocating a certain amount of funds into a portfolio of assets, with the objective of maximizing the profits constrained to a given risk level. Due to its nature of learning from dynamic interactions and planning for long-run performance, reinforcement learning (RL) recently has received much attention in portfolio management. However, most of existing RL-based PM approaches in general only consider price changes of portfolio assets and the implicit correlations between price changes, while ignoring the rich relations between companies in the market, such as two assets in the same sector or two companies have a supplier-customer relation, which are very important for trading decision making. To address these limitations, in this paper, we propose GPM, a novel graph convolutional network-based reinforcement learning framework for portfolio management, which first employs Relational Graph Convolutional Network (R-GCN) to extract asset relational features, and then combines relational features with multi-scale temporal features to make trading decisions for better performance. We also introduce softmax with temperature to increase portfolio diversity, which leads to further increase in profits. Experimental results on two real-world datasets: NASDAQ and NYSE, validate the effectiveness of GPM over state-of-the-art PM methods. Further, more experiments are conducted to evaluate GPM with different graph neural networks and different number of network layers, to explore proper settings for different markets.",https://doi.org/10.1016/j.neucom.2022.04.105,https://www.sciencedirect.com/science/article/pii/S0925231222005021,Neurocomputing,Si Shi;Jianjun Li;Guohui Li;Peng Pan;Qi Chen;Qing Sun,2022,34,"@article{2-10728,
  title={GPM: A Graph Convolutional Network Based Reinforcement Learning Framework for Portfolio Management},
  author={Shi, Si and Li, Jianjun and Li, Guohui and Pan, Peng and Chen, Qi and Sun, Qing},
  year={2022},
  journal={Neurocomputing},
  doi={10.1016/j.neucom.2022.04.105}
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Analyzing, Forecasting, Executing","Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-1073,aaai,Sequential Attacks on Kalman Filter-based Forward Collision Warning Systems,"Kalman Filter( KF) is widely used in various domains to perform sequential learning or variable estimation. In the context of autonomous vehicles, KF constitutes the core component of many Advanced Driver Assistance Systems( ADAS) , such as Forward Collision Warning( FCW). It tracks the states( distance, velocity etc. ) of relevant traffic objects based on sensor measurements. The tracking output of KF is often fed into downstream logic to produce alerts, which will then be used by human drivers to make driving decisions in near-collision scenarios. In this paper, we study adversarial attacks on KF as part of the more complex machine-human hybrid system of Forward Collision Warning. Our attack goal is to negatively affect human braking decisions by causing KF to output incorrect state estimations that lead to false or delayed alerts. We accomplish this by sequentially manipulating measure ments fed into the KF, and propose a novel Model Predictive Control( MPC) approach to compute the optimal manipulation. Via experiments conducted in a simulated driving environment, we show that the attacker is able to successfully change FCW alert signals through planned manipulation over measurements prior to the desired target time. These results demonstrate that our attack can stealthily mislead a distracted human driver and cause vehicle collisions.",10.1609/aaai.v35i10.17073,https://ojs.aaai.org/index.php/AAAI/article/view/17073,AAAI Conference on Artificial Intelligence,Yuzhe Ma;Jon A Sharp;Ruizhe Wang;Earlence Fernandes;Xiaojin Zhu,2021,19,"@inproceedings{2-1073,
  title     = {Sequential Attacks on Kalman Filter-based Forward Collision Warning Systems},
  author    = {Yuzhe Ma and Jon A Sharp and Ruizhe Wang and Earlence Fernandes and Xiaojin Zhu},
  year      = {2021},
  doi       = {10.1609/aaai.v35i10.17073},
  booktitle = {AAAI Conference on Artificial Intelligence}
}",Methodological contributions,Transportation / Mobility / Planning,Individual,"Analyzing, Executing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-10763,elsevier,Guidelines and evaluation of clinical explainable ai in medical image analysis,"Explainable artificial intelligence (XAI) is essential for enabling clinical users to get informed decision support from AI and comply with evidence-based medical practice. Applying XAI in clinical settings requires proper evaluation criteria to ensure the explanation technique is both technically sound and clinically useful, but specific support is lacking to achieve this goal. To bridge the research gap, we propose the Clinical XAI Guidelines that consist of five criteria a clinical XAI needs to be optimized for. The guidelines recommend choosing an explanation form based on Guideline 1 (G1) Understandability and G2 Clinical relevance. For the chosen explanation form, its specific XAI technique should be optimized for G3 Truthfulness, G4 Informative plausibility, and G5 Computational efficiency. Following the guidelines, we conducted a systematic evaluation on a novel problem of multi-modal medical image explanation with two clinical tasks, and proposed new evaluation metrics accordingly. Sixteen commonly-used heatmap XAI techniques were evaluated and found to be insufficient for clinical use due to their failure in G3 and G4. Our evaluation demonstrated the use of Clinical XAI Guidelines to support the design and evaluation of clinically viable XAI.",https://doi.org/10.1016/j.media.2022.102684,https://www.sciencedirect.com/science/article/pii/S1361841522003127,Medical Image Analysis,Weina Jin;Xiaoxiao Li;Mostafa Fatehi;Ghassan Hamarneh,2023,179,"@article{2-10763,
  title = {Guidelines and Evaluation of Clinical Explainable AI in Medical Image Analysis},
  author = {Jin, Weina and Li, Xiaoxiao and Fatehi, Mostafa and Hamarneh, Ghassan},
  year = {2023},
  doi = {10.1016/j.media.2022.102684},
  journal = {Medical Image Analysis}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-10765,elsevier,Guiding supervisors in artificial intelligence-enabled forecasting: understanding the impacts of salience and detail on decision-making,"In many real-world situations, multiple humans are involved in decision-making when interacting with machine recommendations. We investigated a setting where an artificial intelligence system creates demand forecasts that a human planner can either accept or revise, and a supervisor then makes the final decision about which forecast to select. We designed and conducted two experimental studies to understand decision-making by a supervisor. First, we provided the improvement probabilities of adjustments at an aggregated level and found evidence for overoptimism bias and mean anchoring. Second, we provided decomposed guidance based on two adjustment attributes, direction and magnitude, to investigate the role of salience based on the distance between the improvement probabilities and level of detail in guidance effectiveness. We found no significant difference in using less and more salient guidance provided that the detail level was fixed. However, revealing more details when the guidance was more salient increased the use of guidance.",https://doi.org/10.1016/j.ijforecast.2024.08.001,https://www.sciencedirect.com/science/article/pii/S0169207024000803,International Journal of Forecasting,Naghmeh Khosrowabadi;Kai Hoberg;Yun Shin Lee,2024,3,"@article{2-10765,
  title={Guiding supervisors in artificial intelligence-enabled forecasting: understanding the impacts of salience and detail on decision-making},
  author={Naghmeh Khosrowabadi and Kai Hoberg and Yun Shin Lee},
  year={2024},
  journal={International Journal of Forecasting},
  doi={10.1016/j.ijforecast.2024.08.001}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,no such info,"Advising, Forecasting","Decision-maker, Guardian","Alter decision outcomes, Change cognitive demands, Change affective-perceptual",Change AI responses,"prediction of alternative, decomposed guidance with different levels of salience and detail",human-adjusted forecast,Interactive interface,Yes,Yes
2-10798,elsevier,Healthcare system: moving forward with artificial intelligence,"Artificial intelligence (AI) in healthcare is becoming increasingly important, given its potential to generate and analyse healthcare data to improve patient care and reduce costs and clinical risk while enhancing administrative processes within organisations. AI can introduce new sources of growth, change how people work and improve the effectiveness of their work. Consequently, implementing AI systems in healthcare can enable the optimisation of healthcare resources, facilitate a better patient experience, improve population health, reduce per capita costs, and improve the satisfaction of health professionals. Nowadays, most studies have focused on the potential benefits and barriers to implementing AI in healthcare, while only a few have explained the rational decision-making process for deploying new technologies in the healthcare system. In this study, we aim to fill this gap by investigating how AI supports the effective and efficient management of the healthcare system by examining the Humber River Hospital in Toronto using the case study methodology. To achieve the desired benefits from the process of implementing technology in healthcare, our key findings show that hospitals need to undergo a business transformation that exploits technology. Finally, we conclude that only effective knowledge of technology will enable hospitals to effectively become technological and digital.",https://doi.org/10.1016/j.technovation.2022.102510,https://www.sciencedirect.com/science/article/pii/S0166497222000578,"Technovation: The International Journal of Technological Innovation, Entrepreneurship and Technology Management",Grazia Dicuonzo;Francesca Donofrio;Antonio Fusco;Matilda Shini,2023,22,"@article{2-10798,
  title = {Healthcare system: moving forward with artificial intelligence},
  author = {Dicuonzo, Grazia and Donofrio, Francesca and Fusco, Antonio and Shini, Matilda},
  year = {2023},
  doi = {10.1016/j.technovation.2022.102510},
  journal = {Technovation: The International Journal of Technological Innovation, Entrepreneurship and Technology Management}
}",Empirical contributions,Healthcare / Medicine / Surgery,Organizational,"Analyzing, Advising","Decision-maker, Knowledge provider",Alter decision outcomes,"Shape AI for accountability, Update AI competence",automated workflows,lack of in-house expertise,NA,Yes,Yes
2-10800,elsevier,Heartman dss: a decision support system for self-management of congestive heart failure,"Congestive heart failure is a chronic medical condition that affects about 2% of the adult population. Even though it cannot be cured, it can be relieved by a proper, long-term, complex and personalized disease management. In this paper we present the HeartMan Decision Support System (DSS), aimed at supporting individual patients in their uptake of well-established clinical guidelines (i.e., both medication and behaviour based) for disease management. The HeartMan DSS is a central component of the wider HeartMan mobile-health platform that employs mobile phones, wristband sensors and a web application for communication with patients, their physicians and caregivers. The DSS itself provides recommendations for (1) managing patient’s physical health in terms of exercise, nutrition, medications and self-monitoring, (2) psychological support, and (3) managing environmental parameters. The DSS employs a variety of methods: rule-based decision models and adaptable workflows developed using literature and in collaboration with medical experts, classification models developed by machine learning from data, and optimization algorithms. Taken together, they provide a comprehensive, personalized and user-friendly disease management platform. The system was evaluated in a clinical proof-or-concept trial, involving 56 patients in four hospitals. The results confirmed that the system was successful in improving self-care behaviour, decreased patients' levels of depression and anxiety, and improved the overall predicted 1-year mortality risk.",https://doi.org/10.1016/j.eswa.2021.115688,https://www.sciencedirect.com/science/article/pii/S0957417421010733,Expert Systems with Applications,Marko Bohanec;Gennaro Tartarisco;Flavia Marino;Giovanni Pioggia;Paolo Emilio Puddu;Michele Salvatore Schiariti;Anneleen Baert;Sofie Pardaens;Els Clays;Aljoša Vodopija;Mitja Luštrek,2021,31,"@article{2-10800,
  title     = {Heartman DSS: A Decision Support System for Self-Management of Congestive Heart Failure},
  author    = {Marko Bohanec and Gennaro Tartarisco and Flavia Marino and Giovanni Pioggia and Paolo Emilio Puddu and Michele Salvatore Schiariti and Anneleen Baert and Sofie Pardaens and Els Clays and Aljoša Vodopija and Mitja Luštrek},
  year      = {2021},
  journal   = {Expert Systems with Applications},
  doi       = {10.1016/j.eswa.2021.115688}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Individual,"Advising, Forecasting","Decision-subject, Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-10817,elsevier,Hex: human-in-the-loop explainability via deep reinforcement learning,"The use of machine learning (ML) models in decision-making contexts, particularly those used in high-stakes decision-making, are fraught with issue and peril since a person – not a machine – must ultimately be held accountable for the consequences of decisions made using such systems. Machine learning explainability (MLX) promises to provide decision-makers with prediction-specific rationale, assuring them that the model-elicited predictions are made for the right reasons and are thus reliable. Few works explicitly consider this key human-in-the-loop (HITL) component, however. In this work we propose HEX, a human-in-the-loop deep reinforcement learning approach to MLX. HEX incorporates 0-distrust projection to synthesize decider-specific explainers that produce explanations strictly in terms of a decider’s preferred explanatory features using any classification model. Our formulation explicitly considers the decision boundary of the ML model in question using a proposed explanatory point mode of explanation, thus ensuring explanations are specific to the ML model in question. We empirically evaluate HEX against other competing methods, finding that HEX is competitive with the state-of-the-art and outperforms other methods in human-in-the-loop scenarios. We conduct a randomized, controlled laboratory experiment utilizing actual explanations elicited from both HEX and competing methods. We causally establish that our method increases decider’s trust and tendency to rely on trusted features.",https://doi.org/10.1016/j.dss.2024.114304,https://www.sciencedirect.com/science/article/pii/S0167923624001374,Decision Support Systems,Michael T. Lash,2024,5,"@article{2-10817,
  title = {Hex: human-in-the-loop explainability via deep reinforcement learning},
  author = {Michael T. Lash},
  year = {2024},
  doi = {10.1016/j.dss.2024.114304},
  journal = {Decision Support Systems}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Operational,"Explaining, Advising","Decision-maker, Knowledge provider","Change trust, Change cognitive demands",Update AI competence,explanations,NA,Textual,Yes,Yes
2-10873,elsevier,How do citizens perceive the use of artificial intelligence in public sector decisions?,"Artificial Intelligence (AI) has become increasingly prevalent in almost every aspect of our lives. At the same time, a debate about its applications, safety, and privacy is raging. In three studies, we explored how UK respondents perceive the usage of AI in various public sector decisions. Our results are fourfold. First, we found that people prefer AI to have considerably less decisional weight than various human decision-makers; those being: politicians, citizens, and (human) experts. Secondly, our findings revealed that people prefer AI to provide input and advice to these human decision-makers, rather than letting AI make decisions by itself. Thirdly, although AI is seen as contributing less to perceived legitimacy than these human decision-makers, similar to (human) experts, its contribution is seen more in terms of output legitimacy than in terms of input and throughput legitimacy. Finally, our results suggest that the involvement of AI is perceived more suitable for decisions that are low (instead of high) ideologically-charged. Overall, our findings thus show that people are rather skeptical towards using AI in the public domain, but this does not imply that they want to exclude AI entirely from the decision-making process.",https://doi.org/10.1016/j.giq.2023.101906,https://www.sciencedirect.com/science/article/pii/S0740624X23001065,Government Information Quarterly,Tessa Haesevoets;Bram Verschuere;Ruben {Van Severen};Arne Roets,2024,52,"@article{2-10873,
  title   = {How do citizens perceive the use of artificial intelligence in public sector decisions?},
  author  = {Tessa Haesevoets and Bram Verschuere and Ruben {Van Severen} and Arne Roets},
  year    = {2024},
  doi     = {https://doi.org/10.1016/j.giq.2023.101906},
  journal = {Government Information Quarterly}
}",Empirical contributions,Everyday / Employment / Public Service,Institutional,Advising,"Decision-maker, Stakeholder","Change affective-perceptual, Change trust",no such info,recommendations,"less decisional weight, less contribution to perceived legitimacy, more suitable for decisions that are low (instead of high) ideologically-charged, more in terms of output legitimacy, prefer hybrid decision-making over fully human-decision making",NA,Yes,Yes
2-10890,elsevier,How the different explanation classes impact trust calibration: the case of clinical decision support systems,"Machine learning has made rapid advances in safety-critical applications, such as traffic control, finance, and healthcare. With the criticality of decisions they support and the potential consequences of following their recommendations, it also became critical to provide users with explanations to interpret machine learning models in general, and black-box models in particular. However, despite the agreement on explainability as a necessity, there is little evidence on how recent advances in eXplainable Artificial Intelligence literature (XAI) can be applied in collaborative decision-making tasks, i.e., human decision-maker and an AI system working together, to contribute to the process of trust calibration effectively. This research conducts an empirical study to evaluate four XAI classes for their impact on trust calibration. We take clinical decision support systems as a case study and adopt a within-subject design followed by semi-structured interviews. We gave participants clinical scenarios and XAI interfaces as a basis for decision-making and rating tasks. Our study involved 41 medical practitioners who use clinical decision support systems frequently. We found that users perceive the contribution of explanations to trust calibration differently according to the XAI class and to whether XAI interface design fits their job constraints and scope. We revealed additional requirements on how explanations shall be instantiated and designed to help a better trust calibration. Finally, we build on our findings and present guidelines for designing XAI interfaces.",https://doi.org/10.1016/j.ijhcs.2022.102941,https://www.sciencedirect.com/science/article/pii/S1071581922001616,International Journal of Human-Computer Studies,Mohammad Naiseh;Dena Al-Thani;Nan Jiang;Raian Ali,2023,148,"@article{2-10890,
  title = {How the different explanation classes impact trust calibration: the case of clinical decision support systems},
  author = {Mohammad Naiseh and Dena Al-Thani and Nan Jiang and Raian Ali},
  year = {2023},
  doi = {10.1016/j.ijhcs.2022.102941},
  journal = {International Journal of Human-Computer Studies}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising",Decision-maker,"Alter decision outcomes, Change trust, Change cognitive demands",no such info,"local explanations, global explanations, example-based explanations, counterfactual explanations, recommendations, wrong recommendations",trust calibration,"Interactive interface, Autonomous System",Yes,Yes
2-10891,elsevier,How the terminator might affect the car manufacturing industry: examining the role of pre-announcement bias for ai-based is adoptions,"The steep development of artificial intelligence (AI) is accompanied by a completely new set of challenges for information systems (IS) research and practice, especially in the area of individual-level technology adoption. In this article, we elaborate on the important role that biases play regarding the adoption of AI-based IS by individuals in a work environment and for which, in addition, an alarmingly early occurrence within the overall process can be reported. Based on an exploratory case study within three different German car manufacturers, we credit this work with a valuable threefold contribution. First, it introduces and substantiates knowledge and theory on the pre-announcement phase of individual-level IS adoption. Second, it is one of the very few works that thoroughly conceptualize and analyze the role of biases in user decision making resulting from the individuals’ cognitive limitations. And third, it indicates a notable spillover effect of experiences and opinions on AI from the users’ private lives to their professional ones, which stands in clear contrast to what usually is reported in IS research. The article thereby discusses general implications for IS theory but also elaborates on the AI-specific elements regarding IS implementations in a professional environment.",https://doi.org/10.1016/j.im.2023.103881,https://www.sciencedirect.com/science/article/pii/S0378720623001295,Information & Management,Quirin Demlehner;Sven Laumer,2024,8,"@article{2-10891,
  title = {How the Terminator Might Affect the Car Manufacturing Industry: Examining the Role of Pre-Announcement Bias for AI-Based IS Adoptions},
  author = {Quirin Demlehner and Sven Laumer},
  year = {2024},
  doi = {10.1016/j.im.2023.103881},
  journal = {Information \& Management}
}",Empirical contributions,Manufacturing / Industry / Automation,Operational,"Forecasting, Advising, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-10910,elsevier,Human confidence in artificial intelligence and in themselves: the evolution and impact of confidence on adoption of ai advice,"Artificial intelligence (AI) has shown its promise in assisting human decision-making. However, humans' inappropriate decision to accept or reject suggestions from AI can lead to severe consequences in high-stakes AI-assisted decision-making scenarios. This problem persists due to insufficient understanding of human trust in AI. Therefore, this research studies how two types of human confidence that affect trust, their confidence in AI and confidence in themselves, evolve and affect humans’ decisions. A cognitive study and a quantitative model together examine how changing positive and negative experiences affect these confidences and ultimate decisions. Results show that human self-confidence, not their confidence in AI, directs the decision to accept or reject AI suggestions. Furthermore, this work finds that humans often misattribute blame to themselves and enter a vicious cycle of relying on a poorly performing AI. Findings reveal the need and provide insights to effectively calibrate human self-confidence for successful AI-assisted decision-making.",https://doi.org/10.1016/j.chb.2021.107018,https://www.sciencedirect.com/science/article/pii/S0747563221003411,Computers in Human Behavior,Leah Chong;Guanglu Zhang;Kosa Goucher-Lambert;Kenneth Kotovsky;Jonathan Cagan,2022,293,"@article{2-10910,
  title   = {Human confidence in artificial intelligence and in themselves: the evolution and impact of confidence on adoption of AI advice},
  author  = {Leah Chong and Guanglu Zhang and Kosa Goucher-Lambert and Kenneth Kotovsky and Jonathan Cagan},
  year    = {2022},
  journal = {Computers in Human Behavior},
  doi     = {10.1016/j.chb.2021.107018}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Media / Communication / Entertainment",Operational,Advising,Decision-maker,"Alter decision outcomes, Change affective-perceptual, Change trust",no such info,AI suggestions,NA,"Textual, Conversational/Natural Language, Visual",Yes,Yes
2-10913,elsevier,Human performance consequences of normative and contrastive explanations: an experiment in machine learning for reliability maintenance,"Decision aids based on artificial intelligence and machine learning can benefit human decisions and system performance, but can also provide incorrect advice, and invite operators to inappropriately rely on automation. This paper examined the extent to which example-based explanations could improve reliance on a decision aid that is based on machine learning. Participants engaged in a preventive maintenance task by providing their diagnosis of the conditions of three components of a hydraulic system. A decision aid based on machine learning provided advice but was not always reliable. Three explanation displays (baseline, normative, normative plus contrastive) were manipulated within-participants. With the normative explanation display, we found improvements in participants' decision time and subjective workload. With the addition of contrastive explanations, we found improvements in participants' hit rate and sensitivity in discriminating between correct and incorrect ML advice. Implications for the design of explainable interfaces to support human-AI interaction in data intensive environments are discussed.",https://doi.org/10.1016/j.artint.2023.103945,https://www.sciencedirect.com/science/article/pii/S0004370223000917,Artificial Intelligence,Davide Gentile;Birsen Donmez;Greg A. Jamieson,2023,13,"@article{2-10913,
  title = {Human performance consequences of normative and contrastive explanations: an experiment in machine learning for reliability maintenance},
  author = {Davide Gentile and Birsen Donmez and Greg A. Jamieson},
  year = {2023},
  doi = {10.1016/j.artint.2023.103945},
  journal = {Artificial Intelligence}
}",Empirical contributions,Manufacturing / Industry / Automation,Operational,"Explaining, Advising, Executing",Decision-maker,"Alter decision outcomes, Change trust, Change cognitive demands, Change affective-perceptual",no such info,"""an explanation for why a certain ML output (i.e., best estimation) was generated"", ""an explanation for why the alternative ML output (i.e., second-best estimation) was not provided.""",NA,"Textual, Visual",Yes,Yes
2-10915,elsevier,Human understandable thyroid ultrasound imaging ai report system — a bridge between ai and clinicians,"Summary Artificial intelligence (AI) enables accurate diagnosis of thyroid cancer; however, the lack of explanation limits its application. In this study, we collected 10,021 ultrasound images from 8,079 patients across four independent institutions to develop and validate a human understandable AI report system named TiNet for thyroid cancer prediction. TiNet can extract thyroid nodule features such as texture, margin, echogenicity, shape, and location using a deep learning method conforming to the clinical diagnosis standard. Moreover, it offers excellent prediction performance (AUC 0.88) and provides quantitative explanations for the predictions. We conducted a reverse cognitive test in which clinicians matched the correct ultrasound images according to TiNet and clinical reports. The results indicated that TiNet reports (87.1% accuracy) were significantly easier to understand than clinical reports (81.6% accuracy; p < 0.001). TiNet can serve as a bridge between AI-based diagnosis and clinicians, enhancing human–AI cooperative medical decision-making.",https://doi.org/10.1016/j.isci.2023.106530,https://www.sciencedirect.com/science/article/pii/S2589004223006077,iScience,Siqiong Yao;Pengcheng Shen;Tongwei Dai;Fang Dai;Yun Wang;Weituo Zhang;Hui Lu,2023,11,"@article{2-10915,
  title={Human understandable thyroid ultrasound imaging AI report system---a bridge between AI and clinicians},
  author={Yao, Siqiong and Shen, Pengcheng and Dai, Tongwei and Dai, Fang and Wang, Yun and Zhang, Weituo and Lu, Hui},
  year={2023},
  journal={iScience},
  doi={10.1016/j.isci.2023.106530}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Forecasting, Advising","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-10920,elsevier,Human-centered xai: developing design patterns for explanations of clinical decision support systems,"Much of the research on eXplainable Artificial Intelligence (XAI) has centered on providing transparency of machine learning models. More recently, the focus on human-centered approaches to XAI has increased. Yet, there is a lack of practical methods and examples on the integration of human factors into the development processes of AI-generated explanations that humans prove to uptake for better performance. This paper presents a case study of an application of a human-centered design approach for AI-generated explanations. The approach consists of three components: Domain analysis to define the concept & context of explanations, Requirements elicitation & assessment to derive the use cases & explanation requirements, and the consequential Multi-modal interaction design & evaluation to create a library of design patterns for explanations. In a case study, we adopt the DoReMi-approach to design explanations for a Clinical Decision Support System (CDSS) for child health. In the requirements elicitation & assessment, a user study with experienced paediatricians uncovered what explanations the CDSS should provide. In the interaction design & evaluation, a second user study tested the consequential interaction design patterns. This case study provided a first set of user requirements and design patterns for an explainable decision support system in medical diagnosis, showing how to involve expert end users in the development process and how to develop, more or less, generic solutions for general design problems in XAI.",https://doi.org/10.1016/j.ijhcs.2021.102684,https://www.sciencedirect.com/science/article/pii/S1071581921001026,International Journal of Human-Computer Studies,Tjeerd A.J. Schoonderwoerd;Wiard Jorritsma;Mark A. Neerincx;Karel {van den Bosch},2021,271,"@article{2-10920,
  title = {Human-centered XAI: Developing Design Patterns for Explanations of Clinical Decision Support Systems},
  author = {Tjeerd A.J. Schoonderwoerd and Wiard Jorritsma and Mark A. Neerincx and Karel {van den Bosch}},
  year = {2021},
  journal = {International Journal of Human-Computer Studies},
  doi = {10.1016/j.ijhcs.2021.102684}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising","Decision-maker, Knowledge provider","Change trust, Alter decision outcomes, Change affective-perceptual",Update AI competence,"textual explanations, counterfactual explanations, explanations with personalization based on their information preferences, the ability of the interfaces",NA,Interactive interface,Yes,Yes
2-10923,elsevier,Human-like decision making for lane change based on the cognitive map and hierarchical reinforcement learning,"Abstract Human-like decision making is crucial to developing an autonomous driving system (ADS) with high acceptance. Inspired by the cognitive map, this paper proposes a hierarchical reinforcement learning (HRL)-based framework with sound biological plausibility named Cog-MP, which combines the cognitive map and motion primitive (MP) in human-like decision making. In the proposed Cog-MP, three general levels involved in ADS are integrated in a top–bottom way, including operational, decision-making, and cognitive levels. The proposed Cog-MP is used to make human-like decisions in lane-changing scenarios, focusing on three aspects: human-like lane decision, human-like path decision, and decision optimization. The proposed framework is validated on two groups of realistic lane-change data, of which one group is used to train cognitions towards different styles of driving behaviors, and the other group is to provide validation scenarios. Experimental results show that the proposed framework can generate human-like decisions and perform soundly regarding the three considered aspects, demonstrating a promising prospect in developing a brain-inspired human-like ADS.",https://doi.org/10.1016/j.trc.2023.104328,https://www.sciencedirect.com/science/article/pii/S0968090X23003170,Transportation Research Part C: Emerging Technologies,Chao Lu;Hongliang Lu;Danni Chen;Haoyang Wang;Penghui Li;Jianwei Gong,2023,21,"@article{2-10923,
  title = {Human-like decision making for lane change based on the cognitive map and hierarchical reinforcement learning},
  author = {Chao Lu and Hongliang Lu and Danni Chen and Haoyang Wang and Penghui Li and Jianwei Gong},
  year = {2023},
  journal = {Transportation Research Part C: Emerging Technologies},
  doi = {10.1016/j.trc.2023.104328}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Analyzing, Executing","Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-10924,elsevier,Human-machine collaboration for feature selection and integration to improve congestive heart failure risk prediction,"The issue of harnessing machine learning (ML) algorithms for the prediction of adverse medical events is important considering the prevalence of vast repositories of patient-level medical data, and the quest to reduce healthcare costs while elevating the quality of care. Of particular interest is identifying a set of features that can better predict the risk of unplanned readmissions of chronically ill patients within 30 days post discharge, among them patients with Congestive Heart Failure (CHF). While numerous studies have employed ML algorithms to identify sets of significant features, only a handful have compared features extracted by various methods, and even fewer have compared the predictive power of feature sets selected by human experts to those selected by ML. Hence, this research aimed to elicit a comprehensive feature set selected by four ML algorithms and compare its predictive performance to that of a feature set selected by experts. We then evaluated a merged list of the two feature sets, constructing a human-machine collaborative set to predict the likelihood of 30-day unplanned readmission of CHF patients using data on 10,763 CHF patients hospitalized during the years 2010–2017. Our models achieved Area under the ROC Curve (AUC) above 0.8 and an Accuracy of ∼0.89, comparable to the best models in the extant literature. Moreover, the Human-Machine Collaborative model significantly outperformed (best AUC 0.836) other human-only selected models as well as the machine-selected set. This study contributes to a better understanding of the power of using ML for patient risk stratification with special attention to the benefits of human-machine collaboration, even when these two entities work separately or in parallel, for improved clinical decision making.",https://doi.org/10.1016/j.dss.2023.113982,https://www.sciencedirect.com/science/article/pii/S016792362300057X,Decision Support Systems,Ofir Ben-Assuli;Tsipi Heart;Robert Klempfner;Rema Padman,2023,39,"@article{2-10924,
  title = {Human-machine collaboration for feature selection and integration to improve congestive heart failure risk prediction},
  author = {Ben-Assuli, Ofir and Heart, Tsipi and Klempfner, Robert and Padman, Rema},
  year = {2023},
  journal = {Decision Support Systems},
  doi = {10.1016/j.dss.2023.113982}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Collaborating, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-11007,elsevier,Iils: intelligent imaging layout system for automatic imaging report standardization and intra-interdisciplinary clinical workflow optimization,"Background To achieve imaging report standardization and improve the quality and efficiency of the intra-interdisciplinary clinical workflow, we proposed an intelligent imaging layout system (IILS) for a clinical decision support system-based ubiquitous healthcare service, which is a lung nodule management system using medical images. Methods We created a lung IILS based on deep learning for imaging report standardization and workflow optimization for the identification of nodules. Our IILS utilized a deep learning plus adaptive auto layout tool, which trained and tested a neural network with imaging data from all the main CT manufacturers from 11,205 patients. Model performance was evaluated by the receiver operating characteristic curve (ROC) and calculating the corresponding area under the curve (AUC). The clinical application value for our IILS was assessed by a comprehensive comparison of multiple aspects. Findings Our IILS is clinically applicable due to the consistency with nodules detected by IILS, with its highest consistency of 0·94 and an AUC of 90·6% for malignant pulmonary nodules versus benign nodules with a sensitivity of 76·5% and specificity of 89·1%. Applying this IILS to a dataset of chest CT images, we demonstrate performance comparable to that of human experts in providing a better layout and aiding in diagnosis in 100% valid images and nodule display. The IILS was superior to the traditional manual system in performance, such as reducing the number of clicks from 14·45 ± 0·38 to 2, time consumed from 16·87 ± 0·38 s to 6·92 ± 0·10 s, number of invalid images from 7·06 ± 0·24 to 0, and missing lung nodules from 46·8% to 0%. Interpretation This IILS might achieve imaging report standardization, and improve the clinical workflow therefore opening a new window for clinical application of artificial intelligence. Fund The National Natural Science Foundation of China.",https://doi.org/10.1016/j.ebiom.2019.05.040,https://www.sciencedirect.com/science/article/pii/S2352396419303482,eBioMedicine,Yang Wang;Fangrong Yan;Xiaofan Lu;Guanming Zheng;Xin Zhang;Chen Wang;Kefeng Zhou;Yingwei Zhang;Hui Li;Qi Zhao;Hu Zhu;Fei Chen;Cailiang Gao;Zhao Qing;Jing Ye;Aijing Li;Xiaoyan Xin;Danyan Li;Han Wang;Hongming Yu;Lu Cao;Chaowei Zhao;Rui Deng;Libo Tan;Yong Chen;Lihua Yuan;Zhuping Zhou;Wen Yang;Mingran Shao;Xin Dou;Nan Zhou;Fei Zhou;Yue Zhu;Guangming Lu;Bing Zhang,2019,45,"@article{2-11007,
  title = {Iils: Intelligent Imaging Layout System for Automatic Imaging Report Standardization and Intra-Interdisciplinary Clinical Workflow Optimization},
  author = {Yang Wang and Fangrong Yan and Xiaofan Lu and Guanming Zheng and Xin Zhang and Chen Wang and Kefeng Zhou and Yingwei Zhang and Hui Li and Qi Zhao and Hu Zhu and Fei Chen and Cailiang Gao and Zhao Qing and Jing Ye and Aijing Li and Xiaoyan Xin and Danyan Li and Han Wang and Hongming Yu and Lu Cao and Chaowei Zhao and Rui Deng and Libo Tan and Yong Chen and Lihua Yuan and Zhuping Zhou and Wen Yang and Mingran Shao and Xin Dou and Nan Zhou and Fei Zhou and Yue Zhu and Guangming Lu and Bing Zhang},
  year = {2019},
  doi = {10.1016/j.ebiom.2019.05.040},
  journal = {eBioMedicine}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Executing","Guardian, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-11027,elsevier,Impact of artificial intelligence–assisted indication selection on appropriateness order scoring for imaging clinical decision support,"Purpose The aim of this study was to assess appropriateness scoring and structured order entry after the implementation of an artificial intelligence (AI) tool for analysis of free-text indications. Methods Advanced outpatient imaging orders in a multicenter health care system were recorded 7 months before (March 1, 2020, to September 21, 2020) and after (October 20, 2020, to May 13, 2021) the implementation of an AI tool targeting free-text indications. Clinical decision support score (not appropriate, may be appropriate, appropriate, or unscored) and indication type (structured, free-text, both, or none) were assessed. The χ2 and multivariate logistic regression adjusting for covariables with bootstrapping were used. Results In total, 115,079 orders before and 150,950 orders after AI tool deployment were analyzed. The mean patient age was 59.3 ± 15.5 years, and 146,035 (54.9%) were women; 49.9% of orders were for CT, 38.8% for MR, 5.9% for nuclear medicine, and 5.4% for PET. After deployment, scored orders increased to 52% from 30% (P < .001). Orders with structured indications increased to 67.3% from 34.6% (P < .001). On multivariate analysis, orders were more likely to be scored after tool deployment (odds ratio [OR], 2.7, 95% CI, 2.63-2.78; P < .001). Compared with physicians, orders placed by nonphysician providers were less likely to be scored (OR, 0.80; 95% CI, 0.78-0.83; P < .001). MR (OR, 0.84; 95% CI, 0.82-0.87) and PET (OR, 0.12; 95% CI, 0.10-0.13) were less likely to be scored than CT (; P < .001). After AI tool deployment, 72,083 orders (47.8%) remained unscored, 45,186 (62.7%) with free-text-only indications. Conclusions Embedding AI assistance within imaging clinical decision support was associated with increased structured indication orders and independently predicted a higher likelihood of scored orders. However, 48% of orders remained unscored, driven by both provider behavior and infrastructure-related barriers.",https://doi.org/10.1016/j.jacr.2023.04.016,https://www.sciencedirect.com/science/article/pii/S154614402300412X,Journal of the American College of Radiology,Lauren A. Shreve;Jessica G. Fried;Fang Liu;Quy Cao;Jina Pakpoor;Charles E. Kahn;Hanna M. Zafar,2023,0,"@article{2-11027,
  title = {Impact of artificial intelligence--assisted indication selection on appropriateness order scoring for imaging clinical decision support},
  author = {Shreve, Lauren A. and Fried, Jessica G. and Liu, Fang and Cao, Quy and Pakpoor, Jina and Kahn, Charles E. and Zafar, Hanna M.},
  year = {2023},
  journal = {Journal of the American College of Radiology},
  doi = {10.1016/j.jacr.2023.04.016}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Knowledge provider",Alter decision outcomes,Update AI competence,AI suggestions,NA,Textual,Yes,Yes
2-11062,elsevier,Improved decision-making through life event prediction: a case study in the financial services industry,"Life event prediction is an important tool for customer relationship management (CRM), because life events shift customers’ preferences towards different products and services. Existing life event research mainly uses cross-sectional data, whereas in the CRM field, incorporating longitudinal data is increasingly common. Because longitudinal data can capture the dynamics of customer behavior, opportunities arise to benchmark the power of longitudinal customer data for predictions of cross-sectional versus longitudinal life events. Therefore, this study compares statistical and machine learning (SaML) classifiers, such as logistic regression, random forest, and XGBoost, with long- and short-term memory networks (LSTM), using data represented in both cross-sectional and longitudinal setups for life event prediction. Through a real-life longitudinal customer data set from a European bank, the authors represent the longitudinal data in a cross-sectional data format, using featurization in the form of aggregation. The available data cover 42 end-of-month snapshots for 760,438 unique customers. For marketing decision-making literature, this article (1) introduces three novel life events (i.e., primary, secondary, and rental residence purchases) to life event predictions; (2) offers guidance for how to leverage longitudinal customer data, according to the comparison of various featurization approaches and benchmarking SaML classifiers against LSTM; and (3) clarifies the importance of features and timing for improving marketing decision-making dynamically. The results show that aggregating features over time is preferable as a featurization approach for cross-sectional modeling using SaML classifiers. Furthermore, LSTM can capture behavioral changes over time, unlike SaML classifiers. It also performs significantly better than SaML classifiers on the area under curve and F1 metrics. Insights into the uses of integrated gradients reveal that feature importance changes over time. An integrated gradients method can assist decision-makers in their efforts to plan effective communication with customers in advance, such as by allocating more resources to customers who exhibit high probabilities of a particular life event occurrence.",https://doi.org/10.1016/j.dss.2024.114342,https://www.sciencedirect.com/science/article/pii/S0167923624001751,Decision Support Systems,Stephanie Beyer Diaz;Kristof Coussement;Arno {De Caigny},2024,3,"@article{2-11062,
  title={Improved decision-making through life event prediction: a case study in the financial services industry},
  author={Beyer Diaz, Stephanie and Coussement, Kristof and {De Caigny}, Arno},
  year={2024},
  doi={10.1016/j.dss.2024.114342},
  journal={Decision Support Systems}
}",Empirical contributions,Finance / Business / Economy,Operational,"Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-11103,elsevier,Improving healthcare access management by predicting patient no-show behaviour,"Low attendance levels in medical appointments have been associated with poor health outcomes and efficiency problems for service providers. To address this problem, healthcare managers could aim at improving attendance levels or minimizing the operational impact of no-shows by adapting resource allocation policies. However, given the uncertainty of patient behaviour, generating relevant information regarding no-show probabilities could support the decision-making process for both approaches. In this context many researchers have used multiple regression models to identify patient and appointment characteristics than can be used as good predictors for no-show probabilities. This work develops a Decision Support System (DSS) to support the implementation of strategies to encourage attendance, for a preventive care program targeted at underserved communities in Bogotá, Colombia. Our contribution to literature is threefold. Firstly, we assess the effectiveness of different machine learning approaches to improve the accuracy of regression models. In particular, Random Forest and Neural Networks are used to model the problem accounting for non-linearity and variable interactions. Secondly, we propose a novel use of Layer-wise Relevance Propagation in order to improve the explainability of neural network predictions and obtain insights from the modelling step. Thirdly, we identify variables explaining no-show probabilities in a developing context and study its policy implications and potential for improving healthcare access. In addition to quantifying relationships reported in previous studies, we find that income and neighbourhood crime statistics affect no-show probabilities. Our results will support patient prioritization in a pilot behavioural intervention and will inform appointment planning decisions.",https://doi.org/10.1016/j.dss.2020.113398,https://www.sciencedirect.com/science/article/pii/S0167923620301536,Decision Support Systems,David {Barrera Ferro};Sally Brailsford;Cristián Bravo;Honora Smith,2020,69,"@article{2-11103,
  title = {Improving healthcare access management by predicting patient no-show behaviour},
  author = {David Barrera Ferro and Sally Brailsford and Cristián Bravo and Honora Smith},
  year = {2020},
  journal = {Decision Support Systems},
  doi = {https://doi.org/10.1016/j.dss.2020.113398}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting, Explaining","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-11120,elsevier,Improving the safety of atrial fibrillation monitoring systems through human verification,"In this paper we propose a hybrid decision-making process for medical diagnosis. The hypothesis tested is that a deep learning system can provide real-time monitoring of Atrial Fibrillation (AF), a prevalent heart arrhythmia, and a human cardiologist will then verify the results and reach a diagnosis. The verification step adds the necessary checks and balances to increase the safety of the computer-based diagnostic process. In order to test hybrid-decision making, we created a prototype AF monitoring service. The service is based on Heart Rate (HR) sensors for signal acquisition as well as Internet of Things (IoT) technology for data communication and storage. These technologies enable transfer of HR data from patient to central cloud server. A deep learning system is used to analyze the data, which is then presented to a cardiologist when a dangerous condition is detected. This human specialist then works to verify the deep learning results based on the HR data and additional knowledge obtained through patient records or by personal interaction with the patient. A prerequisite for safety in any computer expert system is the clarity of purpose for the decision-making process. Health-care providers are considered customers who register patients with the AF monitoring service. The service delivers real-time diagnostic support by providing timely alarm messages and HR analysis. The safety critical decision then lies with the human practitioner.",https://doi.org/10.1016/j.ssci.2019.05.013,https://www.sciencedirect.com/science/article/pii/S0925753519306630,Safety Science,Oliver Faust;Edward J. Ciaccio;Arshad Majid;U. Rajendra Acharya,2019,65,"@article{2-11120,
  title = {Improving the Safety of Atrial Fibrillation Monitoring Systems through Human Verification},
  author = {Oliver Faust and Edward J. Ciaccio and Arshad Majid and U. Rajendra Acharya},
  year = {2019},
  journal = {Safety Science},
  doi = {https://doi.org/10.1016/j.ssci.2019.05.013}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising, Monitoring","Decision-maker, Guardian, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-11124,elsevier,"In consilium apparatus: artificial intelligence, stakeholder reciprocity, and firm performance","Firms are increasingly using forms of AI to serve stakeholders across various business functions, resulting in both positive and negative outcomes. Stakeholder theory explains how firms create and destroy value via their stakeholder encounters, making it an ideal foundation for understanding AI deployment on firm-level performance. As AI continues to evolve, both when it comes to the activities and roles it takes and the stakeholders it affects, the AI-stakeholder framework developed herein identifies and situates key managerial decisions related to the adoption and deployment of AI that drive the firm’s likelihood of creating or destroying value through stakeholder encounters. The AI–stakeholder framework focuses on stakeholder justice and is supported by testable propositions about the conditions most likely to affect the outcomes of incorporating AI into business processes. The framework also supports future research and practical managerial guidance by articulating the challenges and potential of AI for managing stakeholder encounters.",https://doi.org/10.1016/j.jbusres.2022.113402,https://www.sciencedirect.com/science/article/pii/S0148296322008670,Journal of Business Research,Douglas Bosse;Steven Thompson;Peter Ekman,2023,13,"@article{2-11124,
  title = {In consilium apparatus: artificial intelligence, stakeholder reciprocity, and firm performance},
  author = {Douglas Bosse and Steven Thompson and Peter Ekman},
  year = {2023},
  doi = {10.1016/j.jbusres.2022.113402},
  journal = {Journal of Business Research}
}",Theoretical contributions,Finance / Business / Economy,Institutional,"Executing, Advising, Collaborating","Decision-maker, Guardian, Stakeholder",NA,NA,NA,NA,Autonomous System,Yes,No
2-11195,elsevier,Information that matters: exploring information needs of people affected by algorithmic decisions,"Every AI system that makes decisions about people has a group of stakeholders that are personally affected by these decisions. However, explanations of AI systems rarely address the information needs of this stakeholder group, who often are AI novices. This creates a gap between conveyed information and information that matters to those who are impacted by the system’s decisions, such as domain experts and decision subjects. To address this, we present the “XAI Novice Question Bank”, an extension of the XAI Question Bank (Liao et al., 2020) containing a catalog of information needs from AI novices in two use cases: employment prediction and health monitoring. The catalog covers the categories of data, system context, system usage, and system specifications. We gathered information needs through task based interviews where participants asked questions about two AI systems to decide on their adoption and received verbal explanations in response. Our analysis showed that participants’ confidence increased after receiving explanations but that their understanding faced challenges. These included difficulties in locating information and in assessing their own understanding, as well as attempts to outsource understanding. Additionally, participants’ prior perceptions of the systems’ risks and benefits influenced their information needs. Participants who perceived high risks sought explanations about the intentions behind a system’s deployment, while those who perceived low risks rather asked about the system’s operation. Our work aims to support the inclusion of AI novices in explainability efforts by highlighting their information needs, aims, and challenges. We summarize our findings as five key implications that can inform the design of future explanations for lay stakeholder audiences.",https://doi.org/10.1016/j.ijhcs.2024.103380,https://www.sciencedirect.com/science/article/pii/S1071581924001630,International Journal of Human-Computer Studies,Timothée Schmude;Laura Koesten;Torsten Möller;Sebastian Tschiatschek,2025,13,"@article{2-11195,
  title={Information that matters: exploring information needs of people affected by algorithmic decisions},
  author={Schmude, Timothée and Koesten, Laura and Möller, Torsten and Tschiatschek, Sebastian},
  year={2025},
  journal={International Journal of Human-Computer Studies},
  doi={10.1016/j.ijhcs.2024.103380}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service, Healthcare / Medicine / Surgery",Operational,"Explaining, Advising","Decision-maker, Knowledge provider, Stakeholder","Change cognitive demands, Change affective-perceptual, Alter decision outcomes",no such info,"verbal explanations, question-driven explanations",NA,Conversational/Natural Language,Yes,Yes
2-11252,elsevier,Integrating human knowledge into artificial intelligence for complex and ill-structured problems: informed artificial intelligence,"Artificial intelligence (AI) has been gaining significant attention in various fields to reduce costs, increase revenue, and improve customer satisfaction. AI can be particularly beneficial in enhancing decision-making processes for complex and ill-structured problems that lack transparency and have unclear goals. Most AI algorithms require labeled datasets to learn the problem characteristics, draw decision boundaries, and generalize. However, most datasets collected to solve complex and ill-structured problems do not have labels. Additionally, most AI algorithms are opaque and not easily interpretable, making it hard for decision-makers to obtain model insights for developing effective solution strategies. To this end, we examine existing AI paradigms, mainly symbolic AI (SAI) guided by human domain knowledge and data-driven AI (DAI) guided by data. We propose an approach called informed AI (IAI) by integrating human domain knowledge into AI to develop effective and reliable data labeling and model explainability processes. We demonstrate and validate the use of IAI by applying it to a social media dataset comprised of conversations between customers and customer support agents to construct a solution – IAI defect explorer (I-AIDE). I-AIDE is utilized to identify product defects and extract the voice of customers to help managers make decisions to improve quality and enhance customer satisfaction.",https://doi.org/10.1016/j.ijinfomgt.2022.102479,https://www.sciencedirect.com/science/article/pii/S026840122200010X,International Journal of Information Management,Marina Johnson;Abdullah Albizri;Antoine Harfouche;Samuel Fosso-Wamba,2022,140,"@article{2-11252,
  title={Integrating human knowledge into artificial intelligence for complex and ill-structured problems: informed artificial intelligence},
  author={Johnson, Marina and Albizri, Abdullah and Harfouche, Antoine and Fosso-Wamba, Samuel},
  year={2022},
  journal={International Journal of Information Management},
  doi={10.1016/j.ijinfomgt.2022.102479}
}",Algorithmic contributions,"Finance / Business / Economy, Generic / Abstract / Domain-agnostic",Operational,"Advising, Explaining, Analyzing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-11260,elsevier,Integrating machine learning predictions for perioperative risk management: towards an empirical design of a flexible-standardized risk assessment tool,"Background Surgical patients are complex, vulnerable, and prone to postoperative complications that can potentially be mitigated with quality perioperative risk assessment and management. Several institutions have incorporated machine learning (ML) into their patient care to improve awareness and support clinician decision-making along the perioperative spectrum. Recent research suggests that ML risk prediction can support perioperative patient risk monitoring and management across several situations, including the operating room (OR) to intensive care unit (ICU) handoffs. Objectives Our study objectives were threefold: (1) evaluate whether ML-generated postoperative predictions are concordant with clinician-generated risk rankings for acute kidney injury, delirium, pneumonia, deep vein thrombosis, and pulmonary embolism, and establish their associated risk factors; (2) ascertain clinician end-user suggestions to improve adoption of ML-generated risks and their integration into the perioperative workflow; and (3) develop a user-friendly visualization format for a tool to display ML-generated risks and risk factors to support postoperative care planning, for example, within the context of OR-ICU handoffs. Methods Graphical user interfaces for postoperative risk prediction models were assessed for end-user usability through cognitive walkthroughs and interviews with anesthesiologists, surgeons, certified registered nurse anesthetists, registered nurses, and critical care physicians. Thematic analysis relying on an explanation design framework was used to identify feedback and suggestions for improvement. Results 17 clinicians participated in the evaluation. ML estimates of complication risks aligned with clinicians' independent rankings, and related displays were perceived as valuable for decision-making and care planning for postoperative care. During OR-ICU handoffs, the tool could speed up report preparation and remind clinicians to address patient-specific complications, thus providing more tailored care information. Suggestions for improvement centered on electronic tool delivery; methods to build trust in ML models; modifiable risks and risk mitigation strategies; and additional patient information based on individual preferences (e.g., surgical procedure). Conclusions ML estimates of postoperative complication risks can provide anticipatory guidance, potentially increasing the efficiency of care planning. We have offered an ML visualization framework for designing future ML-augmented tools and anticipate the development of tools that recommend specific actions to the user based on ML model output. Statement of Significance.•Problem: Implementation of machine-learning (ML) output within clinical workflows is not trivial.•What is already known: Barriers to ML implementation and appropriate use within clinical workflows include clinicians’ limited understanding of ML, wariness of ML output, and lack of ML output actionability.•What this paper adds: Qualitative end-user evaluation of postoperative risk predictions found high-levels of agreement between rankings of clinicians' manual risk rankings and ML-augmented risk estimates. The evaluation also highlighted design (functional and visual) enhancements for translational implementation of ML models within clinical workflows.",https://doi.org/10.1016/j.jbi.2022.104270,https://www.sciencedirect.com/science/article/pii/S1532046422002751,Journal of Biomedical Informatics,Joanna Abraham;Brian Bartek;Alicia Meng;Christopher {Ryan King};Bing Xue;Chenyang Lu;Michael S. Avidan,2023,35,"@article{2-11260,
  title = {Integrating machine learning predictions for perioperative risk management: towards an empirical design of a flexible-standardized risk assessment tool},
  author = {Joanna Abraham and Brian Bartek and Alicia Meng and Christopher {Ryan King} and Bing Xue and Chenyang Lu and Michael S. Avidan},
  year = {2023},
  journal = {Journal of Biomedical Informatics},
  doi = {https://doi.org/10.1016/j.jbi.2022.104270}
}","Empirical contributions, System/Artifact contributions",Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising",Decision-maker,"Alter decision outcomes, Change affective-perceptual","Update AI competence, Change AI responses",ML estimates of postoperative complication risks,NA,"Visual, Textual, Conversational/Natural Language",Yes,Yes
2-11343,elsevier,Interpretable ai-assisted clinical decision making (cdm) for dose prescription in radiosurgery of brain metastases,"Purpose AI modeling physicians’ clinical decision-making (CDM) can improve the efficiency and accuracy of clinical practice or serve as a surrogate to provide initial consultations to patients seeking secondary opinions. In this study, we developed an interpretable AI model that predicts dose fractionation for patients receiving radiation therapy for brain metastases with an interpretation of its decision-making process. Materials/Methods 152 patients with brain metastases treated by radiosurgery from 2017 to 2021 were obtained. CT images and target and organ-at-risk (OAR) contours were extracted. Eight non-image clinical parameters were also extracted and digitized, including age, the number of brain metastasis, ECOG performance status, presence of symptoms, sequencing with surgery (pre- or post-operative radiation therapy), de novo vs. re-treatment, primary cancer type, and metastasis to other sites. 3D convolutional neural networks (CNN) architectures with encoding paths were built based on the CT data and clinical parameters to capture three inputs: (1) Tumor size, shape, and location; (2) The spatial relationship between tumors and OARs; (3) The clinical parameters. The models fuse the features extracted from these three inputs at the decision-making level to learn the input independently to predict dose prescription. Models with different independent paths were developed, including models combining two independent paths (IM-2), three independent paths (IM-3), and ten independent paths (IM-10) at the decision-making level. A class activation score and relative weighting were calculated for each input path during the model prediction to represent the role of each input in the decision-making process, providing an interpretation of the model prediction. The actual prescription in the record was used as ground truth for model training. The model performance was assessed by 19-fold cross-validation, with each fold consisting of randomly selected 128 training, 16 validation, and 8 testing subjects. Result The dose prescriptions of 152 patient cases included 48 cases with 1 × 24 Gy, 48 cases with 1 × 20–22 Gy, 32 cases with 3 × 9 Gy, and 24 cases with 5 × 6 Gy prescribed by 8 physicians. IM-2 achieved slightly superior performance than IM-3 and IM-10, with 131 (86%) patients classified correctly and 21 (14%) patients misclassified. IM-10 provided the most interpretability with a relative weighting for each input: target (34%), the relationship between target and OAR (35%), ECOG (6%), re-treatment (6%), metastasis to other sites (6%), number of brain metastases (3%), symptomatic (3%), pre/post-surgery (3%), primary cancer type (2%), age (2%), reflecting the importance of the inputs in decision making. The importance ranking of inputs interpreted from the model also matched closely with a physician’s own ranking in the decision process. Conclusion Interpretable CNN models were successfully developed to use CT images and non-image clinical parameters to predict dose prescriptions for brain metastases patients treated by radiosurgery. Models showed high prediction accuracy while providing an interpretation of the decision process, which was validated by the physician. Such interpretability makes the model more transparent, which is crucial for the future clinical adoption of the models in routine practice for CDM assistance.",https://doi.org/10.1016/j.radonc.2023.109842,https://www.sciencedirect.com/science/article/pii/S0167814023897365,Radiotherapy and Oncology,Yufeng Cao;Dan Kunaprayoon;Lei Ren,2023,11,"@article{2-11343,
  title={Interpretable AI-assisted clinical decision making (CDM) for dose prescription in radiosurgery of brain metastases},
  author={Cao, Yufeng and Kunaprayoon, Dan and Ren, Lei},
  year={2023},
  journal={Radiotherapy and Oncology},
  doi={10.1016/j.radonc.2023.109842}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Forecasting, Explaining","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-11441,elsevier,Justice at the forefront: cultivating felt accountability towards artificial intelligence among healthcare professionals,"The advent of AI has ushered in a new era of patient care, but with it emerges a contentious debate surrounding accountability for algorithmic medical decisions. Within this discourse, a spectrum of views prevails, ranging from placing accountability on AI solution providers to laying it squarely on the shoulders of healthcare professionals. In response to this debate, this study, grounded in the mutualistic partner choice (MPC) model of the evolution of morality, seeks to establish a configurational framework for cultivating felt accountability towards AI among healthcare professionals. This framework underscores two pivotal conditions: AI ethics enactment and trusting belief in AI and considers the influence of organizational complexity in the implementation of this framework. Drawing on Fuzzy-set Qualitative Comparative Analysis (fsQCA) of a sample of 401 healthcare professionals, this study reveals that a) focusing justice and autonomy in AI ethics enactment along with building trusting belief in AI reliability and functionality reinforces healthcare professionals’ sense of felt accountability towards AI, b) fostering felt accountability towards AI necessitates ensuring the establishment of trust in its functionality for high complexity hospitals, and c) prioritizing justice in AI ethics enactment and trust in AI reliability is essential for low complexity hospitals.",https://doi.org/10.1016/j.socscimed.2024.116717,https://www.sciencedirect.com/science/article/pii/S0277953624001618,Social Science & Medicine,Weisha Wang;Yichuan Wang;Long Chen;Rui Ma;Minhao Zhang,2024,17,"@article{2-11441,
  title = {Justice at the forefront: cultivating felt accountability towards artificial intelligence among healthcare professionals},
  author = {Weisha Wang and Yichuan Wang and Long Chen and Rui Ma and Minhao Zhang},
  year = {2024},
  doi = {10.1016/j.socscimed.2024.116717},
  journal = {Social Science \& Medicine}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Guardian","Change trust, Shift responsibility",Shape AI for accountability,"AI ethics, AI accountability",NA,NA,Yes,Yes
2-11454,elsevier,Knowledge discovery for course choice decision in massive open online courses using machine learning approaches,"Massive Open Online Courses (MOOCs) provide learners with high-quality and flexible online courses with no limitations regarding time and location. Detecting users’ behavior in MOOCs is an important task for course recommendations. Collaborative Filtering (CF) is considered the widely approach in recommender systems to provide a online learner courses according to similar learners’ preferences in an e-learning environment. The current research provides a novel framework through machine learning techniques to propose course recommendations in MOOCs according to the uses’ preferences and behavior. The method is developed using multi-criteria ratings extracted from users’ online reviews. We use Latent Dirichlet Allocation (LDA) for text mining, Decision Trees for decision rule generation, Self-Organizing Map (SOM) for users’ reviews on courses and the fuzzy rule-based system for users’ preferences prediction. We also adopt a feature selection method to select the most important criteria for users’ preferences prediction. The method is evaluated using the data collected from an online learning platform, Udemy. The results showed that the method is able to accurately provide relevant courses to the users tailored to their preferences. The method has the potential to be implemented as a recommendation agent in the MOOC websites for course recommendations.",https://doi.org/10.1016/j.eswa.2022.117092,https://www.sciencedirect.com/science/article/pii/S0957417422004985,Expert Systems with Applications,Mehrbakhsh Nilashi;Behrouz Minaei-Bidgoli;Abdullah Alghamdi;Mesfer Alrizq;Omar Alghamdi;Fatima {Khan Nayer};Nojood O Aljehane;Arash Khosravi;Saidatulakmal Mohd,2022,59,"@article{2-11454,
  title={Knowledge discovery for course choice decision in massive open online courses using machine learning approaches},
  author={Nilashi, Mehrbakhsh and Minaei-Bidgoli, Behrouz and Alghamdi, Abdullah and Alrizq, Mesfer and Alghamdi, Omar and {Khan Nayer}, Fatima and Aljehane, Nojood O and Khosravi, Arash and Mohd, Saidatulakmal},
  year={2022},
  doi={10.1016/j.eswa.2022.117092},
  journal={Expert Systems with Applications}
}",Methodological contributions,Education / Teaching / Research,Individual,"Advising, Analyzing, Forecasting","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-11486,elsevier,Lanescannet: a deep-learning approach for simultaneous detection of obstacle-lane states for autonomous driving systems,"Autonomous driving is the future of the automotive industry across the globe. Many challenges must be resolved in designing and developing successful Autonomous Driving Systems (ADS), especially in developing countries with poor road infrastructure. Detecting obstacles in lanes is essential for robust ADS to decide whether to change lanes, slow down, or even stop. Developing countries like India present challenges, including poor or no lane line demarcation, poor traffic management, and diverse driving behaviors. That brings many challenges to developing robust lane-keeping or lane-changing decision systems in an ADS. Publicly available driving datasets have an inherent bias towards the road infrastructure of developed countries and hence cannot cater to the needs of the Indian requirements. That makes the existing obstacle-lane segment detection models unsuitable for road infrastructures in developing countries. Due to these factors, the rate of advancement of ADS and Advanced Driver Assistance Systems (ADAS) in developing countries is meager compared to other developed countries. This makes it even more critical for future research to focus on these developing countries. This paper proposes a deep-learning-based novel decision-making network named LaneScanNET to assist the ADS in lane-changing or lane-keeping decision-making. The proposed system assists ADS in detecting obstacles, localization of the Ego Vehicle (EGV) on roads, and estimating lane status in its Field of View (FOV). The proposed LaneScanNET uses a parallel pipeline with an Obstacle Detection Network (ODN) and a Lane Detection Network (LDN) to simultaneously process the incoming image frames for detecting obstacles and segmenting lane lines, respectively. Further, the Obstacle-Lane Fusion Network (OLFN) fuses these results to predict the status of the obstacle lane in the FOV of autonomous vehicles. Vellore Institute of Technology’s (VIT’s) real-time driving dataset on indigenous roads has been collected to train the proposed LaneScanNET with 2464 obstacle-lane images obtained by driving over 60 km. The dataset generated for obstacle detection and its corresponding lane detection has also been made publicly available to promote research work for these developing countries. The proposed system outperforms all existing networks on Indian roads with an accuracy of 75.28% in obstacle detection and 91.36% in lane detection. Additionally, the proposed system can analyze the lane status with an accuracy of 92.54%. The proposed network performs exceptionally well in unforeseen circumstances like shadows, fog, dust, and occlusions making LaneScanNET a robust network that can be an add-on for ADS to make lane-keeping or lane-changing decisions. The LaneScanNET can be integrated into real-time vehicles to assist drivers or ADS.",https://doi.org/10.1016/j.eswa.2023.120970,https://www.sciencedirect.com/science/article/pii/S0957417423014720,Expert Systems with Applications,P. {Shunmuga Perumal};Yong Wang;M. Sujasree;Shobhit Tulshain;Saksham Bhutani;M. Kiruthik Suriyah;V. Umesh {Kumar Raju},2023,30,"@article{2-11486,
  title = {Lanescannet: a deep-learning approach for simultaneous detection of obstacle-lane states for autonomous driving systems},
  author = {P. {Shunmuga Perumal} and Yong Wang and M. Sujasree and Shobhit Tulshain and Saksham Bhutani and M. Kiruthik Suriyah and V. Umesh {Kumar Raju}},
  year = {2023},
  doi = {10.1016/j.eswa.2023.120970},
  journal = {Expert Systems with Applications}
}",System/Artifact contributions,Transportation / Mobility / Planning,Individual,"Executing, Analyzing",Developer,NA,NA,NA,NA,NA,Yes,No
2-11526,elsevier,Learning from success: a machine learning approach to guiding solar building envelope applications in non-domestic market,"Solar building envelopes, also known as Building Integrated PV (BIPV) show significant growth in Asia and Europe, although other regions such as Australia are still lagging. The decision to uptake BIPV is complex due to the heterogeneous interest of adopters and multi-dimensional features. Instead of redesigning BIPV in hypothetical buildings, we built a machine learning model using a database of real BIPV and building-attached PV (BAPV) applications, for the purpose of learning and predicting a BIPV adoption decision-making in non-domestic buildings in western countries. We used Australia as a case study to execute the support vector machine (SVM) prediction model. It was revealed that the combination of project determinants such as geographical conditions, equivalent building materials, interest rates and capital cost influenced the decision of BIPV. The prediction model provides pieces of information for stakeholders across the BIPV ecosystem to take their decision on investment, policymaking, and research directions. The current global industry transformation and innovations in technology are favourable to politically promoting and investing in BIPV. Such promotion and investment would help both expand the current market and reach the greenhouse targets.",https://doi.org/10.1016/j.jclepro.2022.133997,https://www.sciencedirect.com/science/article/pii/S0959652622035697,Journal of Cleaner Production,Nilmini Pradeepika Weerasinghe;Rebecca Jing Yang;Chen Wang,2022,19,"@article{2-11526,
  title     = {Learning from success: a machine learning approach to guiding solar building envelope applications in non-domestic market},
  author    = {Weerasinghe, Nilmini Pradeepika and Yang, Rebecca Jing and Wang, Chen},
  year      = {2022},
  journal   = {Journal of Cleaner Production},
  doi       = {10.1016/j.jclepro.2022.133997}
}","Algorithmic contributions, Empirical contributions",Environment / Resources / Energy,Institutional,"Advising, Forecasting","Decision-maker, Guardian, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-1153,aaai,The Benefit in Free Information Disclosure When Selling Information to People,"This paper studies the benefit for information providers in free public information disclosure in settings where the prospective information buyers are people. The underlying model, which applies to numerous real-life situations, considers a standard decision making setting where the decision maker is uncertain about the outcomes of her decision. The information provider can fully disambiguate this uncertainty and wish to maximize her profit from selling such information. We use a series of AMT-based experiments with people to test the benefit for the information provider from reducing some of the uncertainty associated with the decision makers problem, for free. Free information disclosure of this kind can be proved to be ineffective when the buyer is a fully-rational agent. Yet, when it comes to people we manage to demonstrate that a substantial improvement in the information providers profit can be achieved with such an approach. The analysis of the results reveals that the primary reason for this phenomena is peoples failure to consider the strategic nature of the interaction with the information provider. Peoples inability to properly calculate the value of information is found to be secondary in its influence.",10.1609/aaai.v31i1.10630,https://ojs.aaai.org/index.php/AAAI/article/view/10630,AAAI Conference on Artificial Intelligence,Shani Alkoby;David Sarne,2017,15,"@inproceedings{2-1153,
  title = {The Benefit in Free Information Disclosure When Selling Information to People},
  author = {Shani Alkoby and David Sarne},
  year = {2017},
  doi = {10.1609/aaai.v31i1.10630},
  booktitle = {AAAI Conference on Artificial Intelligence}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Individual,"Auditing, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-11555,elsevier,Legal and ethical considerations of artificial intelligence for residents in post-acute and long-term care,"This article proposes a framework for examining the ethical and legal concerns for using artificial intelligence (AI) in post-acute and long-term care (PA-LTC). It argues that established frameworks on health, AI, and the law should be adapted to specific care contexts. For residents in PA-LTC, their social, psychological, and mobility needs should act as a gauge for examining the benefits and risks of integrating AI into their care. Using those needs as a gauge, 4 areas of particular concern are identified. First, the threat that AI poses to the autonomy of residents can undermine their core needs. Second, how discrimination and bias in algorithmic decision-making can undermine Medicare coverage for PA-LTC, causing doctors' recommendations to be ignored and denying residents the care they are entitled to. Third, privacy rules concerning data use may undermine developers’ ability to train accurate AI systems, limiting their usefulness in PA-LTC contexts. Fourth, the importance of obtaining consent before AI is used and discussions about how that care should continue if there are concerns about an ongoing decline in cognition. Together, these considerations elevate existing frameworks and adapt them to the context-specific case of PA-LTC. It is hoped that future research will examine the legal implications of these matters in each of these specific cases.",https://doi.org/10.1016/j.jamda.2024.105105,https://www.sciencedirect.com/science/article/pii/S1525861024005279,Journal of the American Medical Directors Association,Barry Solaiman,2024,0,"@article{2-11555,
  title={Legal and ethical considerations of artificial intelligence for residents in post-acute and long-term care},
  author={Barry Solaiman},
  year={2024},
  doi={10.1016/j.jamda.2024.105105},
  journal={Journal of the American Medical Directors Association}
}","Vision contributions, Theoretical contributions",Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing, Executing","Decision-maker, Stakeholder, Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-11601,elsevier,Llm-tsfd: an industrial time series human-in-the-loop fault diagnosis method based on a large language model,"Industrial time series data provides real-time information about the operational status of equipment and helps identify anomalies. Data-driven and knowledge-guided methods have become predominant in this field. However, these methods depend on industrial domain knowledge and high-quality industrial data which can lead to issues such as unclear diagnostic results and lengthy development cycles. This paper introduces a novel human-in-the-loop task-driven approach to reduce reliance on manually annotated data and improve the interpretability of diagnostic outcomes. This approach utilises a large language model for fault detection, fostering process autonomy and enhancing human-machine collaboration. Furthermore, this paper explores four key roles of the large language model: managing the data pipeline, correcting causality, controlling model management, and making decisions about diagnostic results. Additionally, it presents a prompt structure designed for fault diagnosis of time series data, enabling the large language model to realize task-driven. Finally, the paper validates the proposed framework through a case study in the context of steel metallurgy.",https://doi.org/10.1016/j.eswa.2024.125861,https://www.sciencedirect.com/science/article/pii/S0957417424027283,Expert Systems with Applications,Qi Zhang;Chao Xu;Jie Li;Yicheng Sun;Jinsong Bao;Dan Zhang,2024,28,"@article{2-11601,
  title={Llm-tsfd: an industrial time series human-in-the-loop fault diagnosis method based on a large language model},
  author={Zhang, Qi and Xu, Chao and Li, Jie and Sun, Yicheng and Bao, Jinsong and Zhang, Dan},
  year={2024},
  journal={Expert Systems with Applications},
  doi={10.1016/j.eswa.2024.125861}
}",Methodological contributions,Manufacturing / Industry / Automation,Operational,"Analyzing, Executing, Monitoring, Collaborating","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-11606,elsevier,Local public services and the ethical deployment of artificial intelligence,"Responding to growing criticism that the use of artificial intelligence in public services reinforces unethical activities such as discrimination, the paper presents two new cases from the cities in Finland, both self-describing as centres for the ethical use of AI. Structured by an ethical AI foresighting framework we explore how and why AI is being used in local public services and its outcomes, the degree to which current AI-enabled public services are ethically evaluated and whether ethical evaluation features in trends for future AI use. The research objectives are to demonstrate how AI is being deployed in cities claiming to be European centres for ethical AI use, to innovate new service models and to present a new framework, based on social learning to help analysis of ethics in AI-related innovation processes, in particular those enhancing accountability to citizens. In doing so, we show in practical terms how ethical decision-making processes are identified and responded to addressing explainability and understandability issues. We suggest that negative ethical results from AI use can be avoided, however this requires an ethos of citizen involvement in innovation processes and significant investment in times and attention to distribute learning and opinions between providers, technical partners and service users include an acknowledgment that technical partners learn from users as well as users learning from technical partners.",https://doi.org/10.1016/j.giq.2023.101865,https://www.sciencedirect.com/science/article/pii/S0740624X23000655,Government Information Quarterly,T. Kinder;J. Stenvall;E. Koskimies;H. Webb;S. Janenova,2023,62,"@article{2-11606,
  title = {Local public services and the ethical deployment of artificial intelligence},
  author = {Kinder, T. and Stenvall, J. and Koskimies, E. and Webb, H. and Janenova, S.},
  year = {2023},
  doi = {10.1016/j.giq.2023.101865},
  journal = {Government Information Quarterly}
}",Methodological contributions,"Everyday / Employment / Public Service, Law / Policy / Governance",Institutional,"Advising, Explaining","Decision-maker, Stakeholder, Guardian",NA,NA,NA,NA,NA,Yes,No
2-11649,elsevier,Machine assistance in energy-efficient building design: a predictive framework toward dynamic interaction with human decision-making under uncertainty,"At the energy-efficient buildings design stage, architects suffer from multi-discipline requirements and insufficient information to make proper decisions during the process. Inspired by the human nervous system's estimation mechanism, we proposed a data-driven process-based framework for decision-making support. This framework achieves the performance-oriented decision aid under uncertainties based on a general component design, consisting of three parts: the probabilistic surrogate modeling, ensemble modeling, and the model interpretation method. With the characterization of uncertainties into aleatory or epistemic based on the possibility for minimization, the component’s design enables the framework to achieve dynamic interaction with users and inference toward higher intelligence to “make assumptions” in potential design space. Subsequently, it maps possible consequences of output scenarios to input variants’ causes by generating informative feedback and ensures a robust prediction under certain flexibility of incomplete inputs. We utilized the framework as an assistance system to conduct the strategic feedback of energy efficiency for building designers in different early design stages: The framework is tested on the Energy Performance Certificate (EPC) data from England and Wales (19,725,379 buildings). The result achieves a comparable forecasting performance as the SOTA machine learning and provides coherent input variants' interpretation. More importantly, during the design process, the framework enables to interactively provide building designers with expected building energy efficiency range in on-going possible design space with intervention consequences and input causes interpretation. Eventually, it drives users to operate toward higher energy-efficient building designs.",https://doi.org/10.1016/j.apenergy.2021.118240,https://www.sciencedirect.com/science/article/pii/S0306261921015038,Applied Energy,Xia Chen;Philipp Geyer,2022,33,"@article{2-11649,
  title = {Machine assistance in energy-efficient building design: a predictive framework toward dynamic interaction with human decision-making under uncertainty},
  author = {Xia Chen and Philipp Geyer},
  year = {2022},
  doi = {https://doi.org/10.1016/j.apenergy.2021.118240},
  journal = {Applied Energy}
}","Methodological contributions, System/Artifact contributions",Environment / Resources / Energy,Operational,"Forecasting, Analyzing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-11651,elsevier,Machine learning advice in managerial decision-making: the overlooked role of decision makers’ advice utilization,"Machine learning (ML) analyses offer great potential to craft profound advice for augmenting managerial decision-making. Yet, even the most promising ML advice cannot improve decision-making if it is not utilized by decision makers. We therefore investigate how ML analyses influence decision makers’ utilization of advice and resulting decision-making performance. By analyzing data from 239 ML-supported decisions in real-world organizational scenarios, we demonstrate that decision makers’ utilization of ML advice depends on the information quality and transparency of ML advice as well as decision makers’ trust in data scientists’ competence. Furthermore, we find that decision makers’ utilization of ML advice can lead to improved decision-making performance, which is, however, moderated by the decision makers’ management level. The study’s results can help organizations leverage ML advice to improve decision-making and promote the mutual consideration of technical and social aspects behind ML advice in research and practice as a basic requirement.",https://doi.org/10.1016/j.jsis.2023.101790,https://www.sciencedirect.com/science/article/pii/S0963868723000367,The Journal of Strategic Information Systems,Timo Sturm;Luisa Pumplun;Jin P. Gerlach;Martin Kowalczyk;Peter Buxmann,2023,35,"@article{2-11651,
  title = {Machine learning advice in managerial decision-making: the overlooked role of decision makers' advice utilization},
  author = {Timo Sturm and Luisa Pumplun and Jin P. Gerlach and Martin Kowalczyk and Peter Buxmann},
  year = {2023},
  journal = {The Journal of Strategic Information Systems},
  doi = {10.1016/j.jsis.2023.101790}
}",Empirical contributions,Finance / Business / Economy,Organizational,Advising,Decision-maker,"Alter decision outcomes, Change trust, Change cognitive demands",no such info,recommendations,NA,NA,Yes,Yes
2-11660,elsevier,Machine learning analysis of multispectral imaging and clinical risk factors to predict amputation wound healing,"Objective Prediction of amputation wound healing is challenging due to the multifactorial nature of critical limb ischemia and lack of objective assessment tools. Up to one-third of amputations require revision to a more proximal level within 1 year. We tested a novel wound imaging system to predict amputation wound healing at initial evaluation. Methods Patients planned to undergo amputation due to critical limb ischemia were prospectively enrolled. Clinicians evaluated the patients in traditional fashion, and all clinical decisions for amputation level were determined by the clinician's judgement. Multispectral images of the lower extremity were obtained preoperatively using a novel wound imaging system. Clinicians were blinded to the machine analysis. A standardized wound healing assessment was performed on postoperative day 30 by physical exam to determine whether the amputation site achieved complete healing. If operative revision or higher level of amputation was required, this was undertaken based solely upon the provider's clinical judgement. A machine learning algorithm combining the multispectral imaging data with patient clinical risk factors was trained and tested using cross-validation to measure the wound imaging system's accuracy of predicting amputation wound healing. Results A total of 22 patients undergoing 25 amputations (10 toe, five transmetatarsal, eight below-knee, and two above-knee amputations) were enrolled. Eleven amputations (44%) were non-healing after 30 days. The machine learning algorithm had 91% sensitivity and 86% specificity for prediction of non-healing amputation sites (area under curve, 0.89). Conclusions This pilot study suggests that a machine learning algorithm combining multispectral wound imaging with patient clinical risk factors may improve prediction of amputation wound healing and therefore decrease the need for reoperation and incidence of delayed healing. We propose that this, in turn, may offer significant cost savings to the patient and health system in addition to decreasing length of stay for patients.",https://doi.org/10.1016/j.jvs.2021.06.478,https://www.sciencedirect.com/science/article/pii/S0741521421016864,Journal of Vascular Surgery,John J. Squiers;Jeffrey E. Thatcher;David S. Bastawros;Andrew J. Applewhite;Ronald D. Baxter;Faliu Yi;Peiran Quan;Shuai Yu;J. Michael DiMaio;Dennis R. Gable,2022,17,"@article{2-11660,
  title={Machine learning analysis of multispectral imaging and clinical risk factors to predict amputation wound healing},
  author={John J. Squiers and Jeffrey E. Thatcher and David S. Bastawros and Andrew J. Applewhite and Ronald D. Baxter and Faliu Yi and Peiran Quan and Shuai Yu and J. Michael DiMaio and Dennis R. Gable},
  year={2022},
  journal={Journal of Vascular Surgery},
  doi={10.1016/j.jvs.2021.06.478}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-11779,elsevier,Machine learning modelling for predicting non-domestic buildings energy performance: a model to support deep energy retrofit decision-making,"Non-domestic buildings contribute 20% of the UK’s annual carbon emissions. A contribution exacerbated by its ageing stock of which only 7% is considered new-build. Consequently, the government has set regulations to decrease the amount of energy take-up by buildings which currently favour deep energy retrofitting analysis for decision-making and demonstrating compliance. Due to the size and complexity of non-domestic buildings, identifying optimal retrofit packages can be very challenging. The need for effective decision-making has led to the wide adoption of artificial intelligence in the retrofit strategy design process. However, the vast retrofit solution space and high time-complexity of energy simulations inhibit artificial intelligence’s application. This paper presents an energy performance prediction model for non-domestic buildings supported by machine learning. The aim of the model is to provide a rapid energy performance estimation engine for assisting multi-objective optimisation of non-domestic buildings energy retrofit planning. The study lays out the process of model development from the investigation of requirements and feature extraction to the application on a case study. It employs sensitivity analysis methods to evaluate the effectiveness of the feature set in covering retrofit technologies. The machine learning model which is optimised using advanced evolutionary algorithms provide a robust and reliable tool for building analysts enabling them to meaningfully explore the expanding solution space. The model is evaluated by assessing three thousand retrofit variations of a case study building, achieving a root mean square error of 1.02 kgCO2∕m2×year equal to 1.7% of error.",https://doi.org/10.1016/j.apenergy.2020.115908,https://www.sciencedirect.com/science/article/pii/S0306261920313702,Applied Energy,Saleh Seyedzadeh;Farzad {Pour Rahimian};Stephen Oliver;Sergio Rodriguez;Ivan Glesk,2020,171,"@article{2-11779,
  title={Machine learning modelling for predicting non-domestic buildings energy performance: a model to support deep energy retrofit decision-making},
  author={Saleh Seyedzadeh and Farzad {Pour Rahimian} and Stephen Oliver and Sergio Rodriguez and Ivan Glesk},
  year={2020},
  doi={10.1016/j.apenergy.2020.115908},
  journal={Applied Energy}
}",Algorithmic contributions,Environment / Resources / Energy,Organizational,"Analyzing, Advising, Executing, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-1182,aaai,Towards Hybrid Automation by Bootstrapping Conversational Interfaces for IT Operation Tasks,"Process automation has evolved from end-to-end automation of repetitive process branches to hybrid automation where bots perform some activities and humans serve other activities. In the context of knowledge-intensive processes such as IT operations, implementing hybrid automation is a natural choice where robots can perform certain mundane functions, with humans taking over the decision of when and which IT systems need to act. Recently, ChatOps, which refers to conversation-driven collaboration for IT operations, has rapidly accelerated efficiency by providing a cross-organization and cross-domain platform to resolve and manage issues as soon as possible. Hence, providing a natural language interface to bots is a logical progression to enable collaboration between humans and bots. This work presents a no-code approach to provide a conversational interface that enables human workers to collaborate with bots executing automation scripts. The bots identify the intent of users requests and automatically orchestrate one or more relevant automation tasks to serve the request. We further detail our process of mining the conversations between humans and bots to monitor performance and identify the scope for improvement in service quality.",10.1609/aaai.v37i13.26856,https://ojs.aaai.org/index.php/AAAI/article/view/26856,AAAI Conference on Artificial Intelligence,Jayachandu Bandlamudi;Kushal Mukherjee;Prerna Agarwal;Sampath Dechu;Siyu Huo;Vatche Isahagian;Vinod Muthusamy;Naveen Purushothaman;Renuka Sindhgatta,2024,8,"@article{2-1182,
        author = {Bandlamudi, Jayachandu and Mukherjee, Kushal and Agarwal, Prerna and Dechu, Sampath and Huo, Siyu and Isahagian, Vatche and Muthusamy, Vinod and Purushothaman, Naveen and Sindhgatta, Renuka},
        journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
        month = {Jul.},
        number = {13},
        pages = {15654-15660},
        title = {Towards Hybrid Automation by Bootstrapping Conversational Interfaces for IT Operation Tasks},
        volume = {37},
        doi={10.1609/aaai.v37i13.26856},
        year = {2024}
}",System/Artifact contributions,Software / Systems / Security,Operational,"Collaborating, Executing",Decision-maker,NA,NA,AI response,requests,Conversational/Natural Language,Yes,No
2-11834,elsevier,Machine learning-based design support system for the prediction of heterogeneous machine parameters in industry 4.0,"In the engineering practice, it frequently occurs that designers, final or intermediate users have to roughly estimate some basic performance or specification data on the basis of input data available at the moment, which can be time-consuming. There is the need for a tool that will fill the missing gap in the optimization problems in engineering design processes, by making use of the advances in the artificial intelligence field. This paper aims to fill this gap by introducing an innovative Design Support System (DesSS), originated from the Decision Support System, for the prediction and estimation of machine specification data such as machine geometry and machine design on the basis of heterogeneous input parameters. As the main core of the developed DesSS, we introduced different machine learning (ML) approaches based on Decision/Regression Tree, k-Nearest Neighbors, and Neighborhood Component Features Selection. Experimental results obtained on a real use case and using two different real datasets demonstrated the reliability and the effectiveness of the proposed approach. The innovative machine learning-based DesSS meant for supporting the designing choice, can bring various benefits such as the easier decision-making, conservation of the company’s knowledge, savings in man-hours, higher computational speed and accuracy.",https://doi.org/10.1016/j.eswa.2019.112869,https://www.sciencedirect.com/science/article/pii/S0957417419305792,Expert Systems with Applications,Luca Romeo;Jelena Loncarski;Marina Paolanti;Gianluca Bocchini;Adriano Mancini;Emanuele Frontoni,2020,33,"@article{2-11834,
  title={Machine learning-based design support system for the prediction of heterogeneous machine parameters in industry 4.0},
  author={Romeo, Luca and Loncarski, Jelena and Paolanti, Marina and Bocchini, Gianluca and Mancini, Adriano and Frontoni, Emanuele},
  year={2020},
  journal={Expert Systems with Applications},
  volume={},
  pages={},
  doi={10.1016/j.eswa.2019.112869}
}",System/Artifact contributions,Manufacturing / Industry / Automation,Operational,"Executing, Advising, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-11908,elsevier,Majority voting of doctors improves appropriateness of ai reliance in pathology,"As Artificial Intelligence (AI) making advancements in medical decision-making, there is a growing need to ensure doctors develop appropriate reliance on AI to avoid adverse outcomes. However, existing methods in enabling appropriate AI reliance might encounter challenges while being applied in the medical domain. With this regard, this work employs and provides the validation of an alternative approach – majority voting – to facilitate appropriate reliance on AI in medical decision-making. This is achieved by a multi-institutional user study involving 32 medical professionals with various backgrounds, focusing on the pathology task of visually detecting a pattern, mitoses, in tumor images. Here, the majority voting process was conducted by synthesizing decisions under AI assistance from a group of pathology doctors (pathologists). Two metrics were used to evaluate the appropriateness of AI reliance: Relative AI Reliance (RAIR) and Relative Self-Reliance (RSR). Results showed that even with groups of three pathologists, majority-voted decisions significantly increased both RAIR and RSR – by approximately 9% and 31%, respectively – compared to decisions made by one pathologist collaborating with AI. This increased appropriateness resulted in better precision and recall in the detection of mitoses. While our study is centered on pathology, we believe these insights can be extended to general high-stakes decision-making processes involving similar visual tasks.",https://doi.org/10.1016/j.ijhcs.2024.103315,https://www.sciencedirect.com/science/article/pii/S1071581924000995,International Journal of Human-Computer Studies,Hongyan Gu;Chunxu Yang;Shino Magaki;Neda Zarrin-Khameh;Nelli S. Lakis;Inma Cobos;Negar Khanlou;Xinhai R. Zhang;Jasmeet Assi;Joshua T. Byers;Ameer Hamza;Karam Han;Anders Meyer;Hilda Mirbaha;Carrie A. Mohila;Todd M. Stevens;Sara L. Stone;Wenzhong Yan;Mohammad Haeri;Xiang ‘Anthony’ Chen,2024,4,"@article{2-11908,
  title = {Majority Voting of Doctors Improves Appropriateness of AI Reliance in Pathology},
  author = {Hongyan Gu and Chunxu Yang and Shino Magaki and Neda Zarrin-Khameh and Nelli S. Lakis and Inma Cobos and Negar Khanlou and Xinhai R. Zhang and Jasmeet Assi and Joshua T. Byers and Ameer Hamza and Karam Han and Anders Meyer and Hilda Mirbaha and Carrie A. Mohila and Todd M. Stevens and Sara L. Stone and Wenzhong Yan and Mohammad Haeri and Xiang 'Anthony' Chen},
  year = {2024},
  doi = {10.1016/j.ijhcs.2024.103315},
  journal = {International Journal of Human-Computer Studies}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Forecasting, Advising, Collaborating","Decision-maker, Knowledge provider",Alter decision outcomes,Change AI responses,"recommendations, confidence score, visual explanations, example-based explanations, subclass",NA,Interactive interface,Yes,Yes
2-11912,elsevier,Man against machine reloaded: performance of a market-approved convolutional neural network in classifying a broad spectrum of skin lesions in comparison with 96 dermatologists working under less artificial conditions,"Background Convolutional neural networks (CNNs) efficiently differentiate skin lesions by image analysis. Studies comparing a market-approved CNN in a broad range of diagnoses to dermatologists working under less artificial conditions are lacking. Materials and methods One hundred cases of pigmented/non-pigmented skin cancers and benign lesions were used for a two-level reader study in 96 dermatologists (level I: dermoscopy only; level II: clinical close-up images, dermoscopy, and textual information). Additionally, dermoscopic images were classified by a CNN approved for the European market as a medical device (Moleanalyzer Pro, FotoFinder Systems, Bad Birnbach, Germany). Primary endpoints were the sensitivity and specificity of the CNN’s dichotomous classification in comparison with the dermatologists’ management decisions. Secondary endpoints included the dermatologists’ diagnostic decisions, their performance according to their level of experience, and the CNN’s area under the curve (AUC) of receiver operating characteristics (ROC). Results The CNN revealed a sensitivity, specificity, and ROC AUC with corresponding 95% confidence intervals (CI) of 95.0% (95% CI 83.5% to 98.6%), 76.7% (95% CI 64.6% to 85.6%), and 0.918 (95% CI 0.866–0.970), respectively. In level I, the dermatologists’ management decisions showed a mean sensitivity and specificity of 89.0% (95% CI 87.4% to 90.6%) and 80.7% (95% CI 78.8% to 82.6%). With level II information, the sensitivity significantly improved to 94.1% (95% CI 93.1% to 95.1%; P < 0.001), while the specificity remained unchanged at 80.4% (95% CI 78.4% to 82.4%; P = 0.97). When fixing the CNN’s specificity at the mean specificity of the dermatologists’ management decision in level II (80.4%), the CNN’s sensitivity was almost equal to that of human raters, at 95% (95% CI 83.5% to 98.6%) versus 94.1% (95% CI 93.1% to 95.1%); P = 0.1. In contrast, dermatologists were outperformed by the CNN in their level I management decisions and level I and II diagnostic decisions. More experienced dermatologists frequently surpassed the CNN’s performance. Conclusions Under less artificial conditions and in a broader spectrum of diagnoses, the CNN and most dermatologists performed on the same level. Dermatologists are trained to integrate information from a range of sources rendering comparative studies that are solely based on one single case image inadequate.",https://doi.org/10.1016/j.annonc.2019.10.013,https://www.sciencedirect.com/science/article/pii/S0923753419354687,Annals of Oncology,H.A. Haenssle;C. Fink;F. Toberer;J. Winkler;W. Stolz;T. Deinlein;R. Hofmann-Wellenhof;A. Lallas;S. Emmert;T. Buhl;M. Zutt;A. Blum;M.S. Abassi;L. Thomas;I. Tromme;P. Tschandl;A. Enk;A. Rosenberger;Christina Alt;Marie Bachelerie;Sonali Bajaj;Alise Balcere;Sophie Baricault;Clément Barthaux;Yvonne Beckenbauer;Ines Bertlich;Andreas Blum;Marie-France Bouthenet;Sophie Brassat;Philipp {Marcel Buck};Kristina Buder-Bakhaya;Maria-Letizia Cappelletti;Cécile Chabbert;Julie {De Labarthe};Eveline DeCoster;Teresa Deinlein;Michèle Dobler;Daphnée Dumon;Steffen Emmert;Julie Gachon-Buffet;Mikhail Gusarov;Franziska Hartmann;Julia Hartmann;Anke Herrmann;Isabelle Hoorens;Eva Hulstaert;Raimonds Karls;Andreea Kolonte;Christian Kromer;Aimilios Lallas;Céline {Le Blanc Vasseux};Annabelle Levy-Roy;Pawel Majenka;Marine Marc;Veronique Martin Bourret;Nadège Michelet-Brunacci;Christina Mitteldorf;Jean Paroissien;Camille Picard;Diana Plise;Valérie Reymann;Fabrice Ribeaudeau;Pauline Richez;Hélène {Roche Plaine};Deborah Salik;Elke Sattler;Sarah Schäfer;Roland Schneiderbauer;Thierry Secchi;Karen Talour;Lukas Trennheuser;Alexander Wald;Priscila Wölbing;Pascale Zukervar,2020,243,"@article{2-11912,
  title={Man against machine reloaded: performance of a market-approved convolutional neural network in classifying a broad spectrum of skin lesions in comparison with 96 dermatologists working under less artificial conditions},
  author={Haenssle, H.A. and Fink, C. and Toberer, F. and Winkler, J. and Stolz, W. and Deinlein, T. and Hofmann-Wellenhof, R. and Lallas, A. and Emmert, S. and Buhl, T. and Zutt, M. and Blum, A. and Abassi, M.S. and Thomas, L. and Tromme, I. and Tschandl, P. and Enk, A. and Rosenberger, A. and Alt, Christina and Bachelerie, Marie and Bajaj, Sonali and Balcere, Alise and Baricault, Sophie and Barthaux, Clément and Beckenbauer, Yvonne and Bertlich, Ines and Blum, Andreas and Bouthenet, Marie-France and Brassat, Sophie and Buck, Philipp Marcel and Buder-Bakhaya, Kristina and Cappelletti, Maria-Letizia and Chabbert, Cécile and De Labarthe, Julie and DeCoster, Eveline and Deinlein, Teresa and Dobler, Michèle and Dumon, Daphnée and Emmert, Steffen and Gachon-Buffet, Julie and Gusarov, Mikhail and Hartmann, Franziska and Hartmann, Julia and Herrmann, Anke and Hoorens, Isabelle and Hulstaert, Eva and Karls, Raimonds and Kolonte, Andreea and Kromer, Christian and Lallas, Aimilios and Le Blanc Vasseux, Céline and Levy-Roy, Annabelle and Majenka, Pawel and Marc, Marine and Martin Bourret, Veronique and Michelet-Brunacci, Nadège and Mitteldorf, Christina and Paroissien, Jean and Picard, Camille and Plise, Diana and Reymann, Valérie and Ribeaudeau, Fabrice and Richez, Pauline and Roche Plaine, Hélène and Salik, Deborah and Sattler, Elke and Schäfer, Sarah and Schneiderbauer, Roland and Secchi, Thierry and Talour, Karen and Trennheuser, Lukas and Wald, Alexander and Wölbing, Priscila and Zukervar, Pascale},
  year={2020},
  doi={10.1016/j.annonc.2019.10.013},
  journal={Annals of Oncology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Knowledge provider",Alter decision outcomes,Update AI competence,NA,NA,"Textual, Visual, Interactive interface",Yes,Yes
2-11913,elsevier,Man against machine: diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists,"Background Deep learning convolutional neural networks (CNN) may facilitate melanoma detection, but data comparing a CNN’s diagnostic performance to larger groups of dermatologists are lacking. Methods Google’s Inception v4 CNN architecture was trained and validated using dermoscopic images and corresponding diagnoses. In a comparative cross-sectional reader study a 100-image test-set was used (level-I: dermoscopy only; level-II: dermoscopy plus clinical information and images). Main outcome measures were sensitivity, specificity and area under the curve (AUC) of receiver operating characteristics (ROC) for diagnostic classification (dichotomous) of lesions by the CNN versus an international group of 58 dermatologists during level-I or -II of the reader study. Secondary end points included the dermatologists’ diagnostic performance in their management decisions and differences in the diagnostic performance of dermatologists during level-I and -II of the reader study. Additionally, the CNN’s performance was compared with the top-five algorithms of the 2016 International Symposium on Biomedical Imaging (ISBI) challenge. Results In level-I dermatologists achieved a mean (±standard deviation) sensitivity and specificity for lesion classification of 86.6% (±9.3%) and 71.3% (±11.2%), respectively. More clinical information (level-II) improved the sensitivity to 88.9% (±9.6%, P=0.19) and specificity to 75.7% (±11.7%, P<0.05). The CNN ROC curve revealed a higher specificity of 82.5% when compared with dermatologists in level-I (71.3%, P<0.01) and level-II (75.7%, P<0.01) at their sensitivities of 86.6% and 88.9%, respectively. The CNN ROC AUC was greater than the mean ROC area of dermatologists (0.86 versus 0.79, P<0.01). The CNN scored results close to the top three algorithms of the ISBI 2016 challenge. Conclusions For the first time we compared a CNN’s diagnostic performance with a large international group of 58 dermatologists, including 30 experts. Most dermatologists were outperformed by the CNN. Irrespective of any physicians’ experience, they may benefit from assistance by a CNN’s image classification. Clinical trial number This study was registered at the German Clinical Trial Register (DRKS-Study-ID: DRKS00013570; https://www.drks.de/drks_web/).",https://doi.org/10.1093/annonc/mdy166,https://www.sciencedirect.com/science/article/pii/S0923753419341055,Annals of Oncology,H.A. Haenssle;C. Fink;R. Schneiderbauer;F. Toberer;T. Buhl;A. Blum;A. Kalloo;A. Ben Hadj Hassen;L. Thomas;A. Enk;L. Uhlmann;Christina Alt;Monika Arenbergerova;Renato Bakos;Anne Baltzer;Ines Bertlich;Andreas Blum;Therezia Bokor-Billmann;Jonathan Bowling;Naira Braghiroli;Ralph Braun;Kristina Buder-Bakhaya;Timo Buhl;Horacio Cabo;Leo Cabrijan;Naciye Cevic;Anna Classen;David Deltgen;Christine Fink;Ivelina Georgieva;Lara-Elena Hakim-Meibodi;Susanne Hanner;Franziska Hartmann;Julia Hartmann;Georg Haus;Elti Hoxha;Raimonds Karls;Hiroshi Koga;Jürgen Kreusch;Aimilios Lallas;Pawel Majenka;Ash Marghoob;Cesare Massone;Lali Mekokishvili;Dominik Mestel;Volker Meyer;Anna Neuberger;Kari Nielsen;Margaret Oliviero;Riccardo Pampena;John Paoli;Erika Pawlik;Barbar Rao;Adriana Rendon;Teresa Russo;Ahmed Sadek;Kinga Samhaber;Roland Schneiderbauer;Anissa Schweizer;Ferdinand Toberer;Lukas Trennheuser;Lyobomira Vlahova;Alexander Wald;Julia Winkler;Priscila Wölbing;Iris Zalaudek,2018,1857,"@article{2-11913,
  title = {Man against machine: diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists},
  author = {H.A. Haenssle and C. Fink and R. Schneiderbauer and F. Toberer and T. Buhl and A. Blum and A. Kalloo and A. Ben Hadj Hassen and L. Thomas and A. Enk and L. Uhlmann and Christina Alt and Monika Arenbergerova and Renato Bakos and Anne Baltzer and Ines Bertlich and Andreas Blum and Therezia Bokor-Billmann and Jonathan Bowling and Naira Braghiroli and Ralph Braun and Kristina Buder-Bakhaya and Timo Buhl and Horacio Cabo and Leo Cabrijan and Naciye Cevic and Anna Classen and David Deltgen and Christine Fink and Ivelina Georgieva and Lara-Elena Hakim-Meibodi and Susanne Hanner and Franziska Hartmann and Julia Hartmann and Georg Haus and Elti Hoxha and Raimonds Karls and Hiroshi Koga and Jürgen Kreusch and Aimilios Lallas and Pawel Majenka and Ash Marghoob and Cesare Massone and Lali Mekokishvili and Dominik Mestel and Volker Meyer and Anna Neuberger and Kari Nielsen and Margaret Oliviero and Riccardo Pampena and John Paoli and Erika Pawlik and Barbar Rao and Adriana Rendon and Teresa Russo and Ahmed Sadek and Kinga Samhaber and Roland Schneiderbauer and Anissa Schweizer and Ferdinand Toberer and Lukas Trennheuser and Lyobomira Vlahova and Alexander Wald and Julia Winkler and Priscila Wölbing and Iris Zalaudek},
  year = {2018},
  doi = {10.1093/annonc/mdy166},
  journal = {Annals of Oncology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Forecasting, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-1195,aaai,Towards Safe Mechanical Ventilation Treatment Using Deep Offline Reinforcement Learning,"Mechanical ventilation is a key form of life support for patients with pulmonary impairment. Healthcare workers are required to continuously adjust ventilator settings for each patient, a challenging and time consuming task. Hence, it would be beneficial to develop an automated decision support tool to optimize ventilation treatment. We present DeepVent, a Conservative Q-Learning( CQL) based offline Deep Reinforcement Learning( DRL) agent that learns to predict the optimal ventilator parameters for a patient to promote 90 day survival. We design a clinically relevant intermediate reward that encourages continuous improvement of the patient vitals as well as addresses the challenge of sparse reward in RL. We find that DeepVent recommends ventilation parameters within safe ranges, as outlined in recent clinical trials. The CQL algorithm offers additional safety by mitigating the overestimation of the value estimates of out-of-distribution states/actions. We evaluate our agent using Fitted Q Evaluation( FQE) and demonstrate that it outperforms physicians from the MIMIC-III dataset.",10.1609/aaai.v37i13.26862,https://ojs.aaai.org/index.php/AAAI/article/view/26862,AAAI Conference on Artificial Intelligence,Flemming Kondrup;Thomas Jiralerspong;Elaine Lau;Nathan de Lara;Jacob Shkrob;My Duc Tran;Doina Precup;Sumana Basu,2024,1,"@inproceedings{2-1195,
  title = {Towards Safe Mechanical Ventilation Treatment Using Deep Offline Reinforcement Learning},
  author = {Flemming Kondrup and Thomas Jiralerspong and Elaine Lau and Nathan de Lara and Jacob Shkrob and My Duc Tran and Doina Precup and Sumana Basu},
  year = {2024},
  doi = {10.1609/aaai.v37i13.26862},
  booktitle = {AAAI Conference on Artificial Intelligence}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Executing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-11992,elsevier,Medical ai and human dignity: contrasting perceptions of human and artificially intelligent (ai) decision making in diagnostic and medical resource allocation contexts,"Forms of Artificial Intelligence (AI) are already being deployed into clinical settings and research into its future healthcare uses is accelerating. Despite this trajectory, more research is needed regarding the impacts on patients of increasing AI decision making. In particular, the impersonal nature of AI means that its deployment in highly sensitive contexts-of-use, such as in healthcare, raises issues associated with patients' perceptions of (un)dignified treatment. We explore this issue through an experimental vignette study comparing individuals’ perceptions of being treated in a dignified and respectful way in various healthcare decision contexts. Participants were subject to a 2 (human or AI decision maker) x 2 (positive or negative decision outcome) x 2 (diagnostic or resource allocation healthcare scenario) factorial design. We found evidence of a “human bias” (i.e., a preference for human over AI decision makers) and an “outcome bias” (i.e., a preference for positive over negative outcomes). However, we found that for perceptions of respectful and dignified interpersonal treatment, it matters more who makes the decisions in diagnostic cases and it matters more what the outcomes are for resource allocation cases. We also found that humans were consistently viewed as appropriate decision makers and AI was viewed as dehumanizing, and that participants perceived they were treated better when subject to diagnostic as opposed to resource allocation decisions. Thematic coding of open-ended text responses supported these results. We also outline the theoretical and practical implications of these findings.",https://doi.org/10.1016/j.chb.2022.107296,https://www.sciencedirect.com/science/article/pii/S0747563222001182,Computers in Human Behavior,Paul Formosa;Wendy Rogers;Yannick Griep;Sarah Bankins;Deborah Richards,2022,93,"@article{2-11992,
  title={Medical AI and Human Dignity: Contrasting Perceptions of Human and Artificially Intelligent (AI) Decision Making in Diagnostic and Medical Resource Allocation Contexts},
  author={Formosa, Paul and Rogers, Wendy and Griep, Yannick and Bankins, Sarah and Richards, Deborah},
  year={2022},
  doi={10.1016/j.chb.2022.107296},
  journal={Computers in Human Behavior}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Executing, Advising","Decision-subject, Decision-maker","Change trust, Change affective-perceptual, Shape ethical norms",no such info,NA,NA,Interactive interface,Yes,Yes
2-11994,elsevier,Medical decision support system for cancer treatment in precision medicine in developing countries,"In many developing countries and regions, there are medical problems such as dense populations, lack of medical resources, and shortage of doctors, making it impossible to provide patients with more convenient full-cycle services. Non-small cell lung cancer is a malignant tumor with the highest morbidity and mortality in the world. Research on the auxiliary treatment of intelligent systems is helpful in understanding and evaluate the disease. The system can help doctors provide patients with effective drug treatments and personalized medical services by actively learning the experience of outstanding experts. According to expert knowledge, through quantitative efficacy scores and specific analysis of evaluation indicators, based on the efficacy evaluation matrix and the extraction of key features of patients and drugs, a predictive model of drug efficacy evaluation for adjuvant therapy is established. The model divides into latent feature extraction and curative effect collaborative prediction modules. In the feature extraction module, adding noise to the original data in model training process helps reduce the impact of the sparseness of the patient's medication data. By considering the uncertainty of experts in drug efficacy evaluation modeling, based on probability analysis and efficacy prediction, the proposed method demonstrates the potential options in the face of hesitating choices. According to the predicted efficacy score, candidate drugs are selected to assist doctors in disease analysis and secondary diagnosis. Experiments have shown that drug efficacy prediction methods can provide adjuvant treatments for diseases and quantify the therapeutic effects of targeted drugs. The efficacy information and detection information of patient-drug pairs are helpful to improve decision-making ability, and the proposed medical decision support system framework is superior to other deep learning methods. By adding data, the performance can be significantly improved.",https://doi.org/10.1016/j.eswa.2021.115725,https://www.sciencedirect.com/science/article/pii/S0957417421011064,Expert Systems with Applications,Genghua Yu;Zhigang Chen;Jia Wu;Yanlin Tan,2021,50,"@article{2-11994,
  title = {Medical decision support system for cancer treatment in precision medicine in developing countries},
  author = {Yu, Genghua and Chen, Zhigang and Wu, Jia and Tan, Yanlin},
  year = {2021},
  doi = {10.1016/j.eswa.2021.115725},
  journal = {Expert Systems with Applications}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-11999,elsevier,Medical practitioner's adoption of intelligent clinical diagnostic decision support systems: a mixed-methods study,"Artificial intelligence-based clinical diagnostic decision support systems promise transformational improvements in doctors’ efficiency and accuracy. Nevertheless, low adoption rates suggest that this innovation could fail without adequate uptake. This study uses a mixed-methods approach to develop and test a model based on theories of Unified Theory of Acceptance and Use of Technology, status quo bias, and technology trust. The results show that performance expectancy, effort expectancy, social influence, initial trust, and resistance to change predict intention to use. Further, inertia, perceived threat, and risks (medico-legal and performance) determine resistance to change. Measures for alleviating resistance and improving adoption are proposed.",https://doi.org/10.1016/j.im.2021.103524,https://www.sciencedirect.com/science/article/pii/S0378720621000987,Information & Management,Ashish Viswanath Prakash;Saini Das,2021,138,"@article{2-11999,
  title={Medical practitioner's adoption of intelligent clinical diagnostic decision support systems: a mixed-methods study},
  author={Prakash, Ashish Viswanath and Das, Saini},
  year={2021},
  journal={Information \& Management},
  doi={10.1016/j.im.2021.103524}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing",Decision-maker,"Alter decision outcomes, Change trust, Change cognitive demands, Change affective-perceptual, Restrict human agency",no such info,NA,behavioral intention,"Textual, Visual, Interactive interface",Yes,Yes
2-12009,elsevier,Men's sociotechnical imaginaries of artificial intelligence for prostate cancer diagnostics – a focus group study,"Artificial intelligence (AI) is increasingly used for diagnostic purposes in cancer care. Prostate cancer is one of the most prevalent cancers affecting men worldwide, but current diagnostic approaches have limitations in terms of specificity and sensitivity. Using AI to interpret MR images in prostate cancer diagnostics shows promising results, but raises questions about implementation, user acceptance, trust, and doctor-patient communication. Drawing on approaches from the sociology of expectations and theories about sociotechnical imaginaries, we explore men's expectations of artificial intelligence for prostate cancer diagnostics. We conducted ten focus groups with 48 men aged 54–85 in Norway with various experiences of prostate cancer diagnostics. Five groups of men had been treated for prostate cancer, one group was on active surveillance, two groups had been through prostate cancer diagnostics without having a diagnosis, and two groups of men had no experience with prostate cancer diagnostics or treatment. Data was subject to reflexive thematic analysis. Our analysis suggests that men's expectations of AI for prostate cancer diagnostics come from two perspectives: Technology-centered expectations that build on their conceptions of AI's form and agency, and human-centered expectations of AI that build on their perceptions of patient-professional relationships and decision-making processes. These two perspectives are intertwined in three imaginaries of AI: The tool imaginary, the advanced machine imaginary, and the intelligence imaginary – each carrying distinct expectations and ideas of technologies and humans' role in decision-making processes. These expectations are multifaceted and simultaneously optimistic and pessimistic; while AI is expected to improve the accuracy of cancer diagnoses and facilitate more personalized medicine, AI is also expected to threaten interpersonal and communicational relationships between patients and healthcare professionals, and the maintenance of trust in these relationships. This emphasizes how AI cannot be implemented without caution about maintaining human healthcare relationships.",https://doi.org/10.1016/j.socscimed.2024.116771,https://www.sciencedirect.com/science/article/pii/S0277953624002156,Social Science & Medicine,Emilie Hybertsen Lysø;Maria Bårdsen Hesjedal;John-Arne Skolbekken;Marit Solbjør,2024,19,"@article{2-12009,
  title = {Men's sociotechnical imaginaries of artificial intelligence for prostate cancer diagnostics – a focus group study},
  author = {Emilie Hybertsen Lysø and Maria Bårdsen Hesjedal and John-Arne Skolbekken and Marit Solbjør},
  year = {2024},
  doi = {10.1016/j.socscimed.2024.116771},
  journal = {Social Science \& Medicine}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting, Executing","Decision-maker, Decision-subject",Alter decision outcomes,Change AI responses,relevant data,"evaluation, supervision",NA,Yes,Yes
2-12057,elsevier,Mitigating knowledge imbalance in ai-advised decision-making through collaborative user involvement,"Integrating artificial intelligence (AI) systems into decision-making tasks attempts to assist people by augmenting or complementing their abilities and ultimately improve task performance. However, when considering recommendations from modern “black box” intelligent systems, users are confronted with the decision of accepting or overriding AI’s recommendations. These decisions are even more challenging to make when there exists a significant knowledge imbalance between the users and the AI system—namely, when people lack necessary task knowledge and are therefore unable to accurately complete the task on their own. In this work, we aim to understand people’s behavior in AI-assisted decision-making tasks when faced with the challenge of knowledge imbalance and explore whether involving users in an AI’s prediction generation process makes them more willing to follow the AI’s recommendations and enhances their perception of collaboration. Our empirical study reveals that the involvement of users in generating AI recommendations during a task with notable knowledge imbalance causes them to be more willing to agree with the AI’s suggestions and to perceive the AI agent and their collaboration as a team more positively.",https://doi.org/10.1016/j.ijhcs.2022.102977,https://www.sciencedirect.com/science/article/pii/S1071581922001951,International Journal of Human-Computer Studies,Catalina Gomez;Mathias Unberath;Chien-Ming Huang,2023,43,"@article{2-12057,
  title={Mitigating knowledge imbalance in AI-advised decision-making through collaborative user involvement},
  author={Gomez, Catalina and Unberath, Mathias and Huang, Chien-Ming},
  year={2023},
  journal={International Journal of Human-Computer Studies},
  doi={10.1016/j.ijhcs.2022.102977}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Forecasting, Advising, Collaborating",Decision-maker,"Alter decision outcomes, Change trust, Change affective-perceptual",Update AI competence,"recommendations, AI knowledge",domain knowledge,Interactive interface,Yes,Yes
2-12085,elsevier,Modeling adoption of intelligent agents in medical imaging,"Artificial intelligence has the potential to transform many application domains fundamentally. One notable example is clinical radiology. A growing number of decision-making support systems are available for lesion detection and segmentation, two fundamental steps to accomplish diagnosis and treatment planning. This paper proposes a model based on the unified theory of acceptance and use of technology to study the determinants for the adoption of intelligent agents across the medical imaging workflow. We tested the model via confirmatory factor analysis and structural equation modeling using clinicians’ data from an international evaluation of healthcare practitioners. Results show an increased understanding of the vital role of security, risk, and trust in the usage intention of intelligent agents. These empirical findings provide valuable theoretical contributions to researchers by explaining the reasons behind the adoption and usage of intelligent agents in the medical imaging workflow.",https://doi.org/10.1016/j.ijhcs.2022.102922,https://www.sciencedirect.com/science/article/pii/S1071581922001422,International Journal of Human-Computer Studies,Francisco Maria Calisto;Nuno Nunes;Jacinto C. Nascimento,2022,116,"@article{2-12085,
  title={Modeling adoption of intelligent agents in medical imaging},
  author={Calisto, Francisco Maria and Nunes, Nuno and Nascimento, Jacinto C.},
  year={2022},
  journal={International Journal of Human-Computer Studies},
  doi={10.1016/j.ijhcs.2022.102922}
}","Theoretical contributions, Empirical contributions",Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-12163,elsevier,Mri spine request form enhancement and auto protocoling using a secure institutional large language model,"BACKGROUND CONTEXT Secure institutional large language models (LLM) could reduce the burden of non-interpretative tasks for radiologists. PURPOSE Assess the utility of a secure institutional LLM for MRI spine request form enhancement and auto-protocoling. STUDY DESIGN/SETTING Retrospective study conducted from December 2023 to February 2024, including patients with clinical entries accessible on the electronic medical record (EMR). PATIENT SAMPLE Overall, 250 spine MRI request forms were analyzed from 218 patients (mean age = 55.9 years ± 18.9 [SD]; 108 women) across the cervical (n=56/250, 22.4%), thoracic (n=13/250, 5.2%), lumbar (n=166/250, 66.4%), and whole (n=15/250, 6.0%) spine. Of these, 60/250 (24.0%) required contrast and 41/250 (16.4%) had prior spine surgery/instrumentation. OUTCOME MEASURES Primary–Adequacy of clinical information on clinician and LLM-augmented request forms were rated using a four-point scale. Secondary–Correct MRI protocol suggestion by LLM and first-year board-certified radiologists (Rad4 and Rad5) compared to a consensus reference standard. METHODS A secured institutional LLM (Claude 2.0) used a majority decision prompt (out of six runs) to enhance clinical information on clinician request forms using the EMR, and suggest the appropriate MRI protocol. The adequacy of clinical information on the clinician and LLM-augmented request forms was rated by three musculoskeletal radiologists independently (Rad1:10-years-experience; Rad2:12-years-experience; Rad3:10-years-experience). The same radiologists provided a consensus reference standard for the correct protocol, which was compared to the protocol suggested by the LLM and two first-year board-certified radiologists (Rad4 and Rad5). Overall agreement (Fleiss kappas for inter-rater agreement or % agreement with the reference standard and respective 95%CIs) were provided where appropriate. RESULTS LLM-augmented forms were rated by Rads 1–3 as having adequate clinical information in 93.6-96.0% of cases compared to 46.8-58.8% of the clinician request forms (p<0.01). Substantial interobserver agreement was observed with kappas of 0.71 (95%CI:0.67–0.76) for original forms and 0.66 (95%CI:0.61–0.72) for LLM-enhanced requests. Rads 1–3 showed almost perfect agreement on protocol decisions, with kappas of 0.99 (95%CI:0.94–1.0) for spine region selection, 0.93 (95%CI:0.86–1.0) for contrast necessity, and 0.93 (95%CI:0.86–0.99) for recognition of prior spine surgery. Compared to the consensus reference standard, the LLM suggested the correct protocol in 78.4% (196/250, p<0.01) of cases, albeit inferior to Rad4 (90.0%, p<0.01) and Rad5 (89.2%, p<0.01). The secure LLM did best in identifying spinal instrumentation in 39/41 (95.1%) cases, improved compared to Rad4 (61.0%) and Rad5 (41.5%) (both p<0.01). The secure LLM had high consistency with 227/250 cases (90.8%) having 100% (6/6 runs) agreement. CONCLUSIONS Enhancing spine MRI request forms with a secure institutional LLM improved the adequacy of clinical information. The LLM also accurately suggested the correct protocol in 78.4% of cases which could optimize the MRI workflow.",https://doi.org/10.1016/j.spinee.2024.10.021,https://www.sciencedirect.com/science/article/pii/S1529943024011112,The Spine Journal,James Thomas Patrick Decourcy Hallinan;Naomi Wenxin Leow;Wilson Ong;Aric Lee;Yi Xian Low;Matthew Ding Zhou Chan;Ganakirthana Kalpenya Devi;Daniel De-Liang Loh;Stephanie Shengjie He;Faimee Erwan Muhamat Nor;Desmond Shi Wei Lim;Ee Chin Teo;Xi Zhen Low;Shaheryar Mohammad Furqan;Wilson Wei Yang Tham;Jiong Hao Tan;Naresh Kumar;Andrew Makmur;Yonghan Ting,2024,10,"@article{2-12163,
  title = {Mri spine request form enhancement and auto protocoling using a secure institutional large language model},
  author = {James Thomas Patrick Decourcy Hallinan and Naomi Wenxin Leow and Wilson Ong and Aric Lee and Yi Xian Low and Matthew Ding Zhou Chan and Ganakirthana Kalpenya Devi and Daniel De-Liang Loh and Stephanie Shengjie He and Faimee Erwan Muhamat Nor and Desmond Shi Wei Lim and Ee Chin Teo and Xi Zhen Low and Shaheryar Mohammad Furqan and Wilson Wei Yang Tham and Jiong Hao Tan and Naresh Kumar and Andrew Makmur and Yonghan Ting},
  year = {2024},
  doi = {https://doi.org/10.1016/j.spinee.2024.10.021},
  journal = {The Spine Journal}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising, Analyzing","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-1222,aaai,Unfooling Perturbation-Based Post Hoc Explainers,"Monumental advancements in artificial intelligence( AI) have lured the interest of doctors, lenders, judges, and other professionals. While these high-stakes decision-makers are optimistic about the technology, those familiar with AI systems are wary about the lack of transparency of its decision-making processes. Perturbation-based post hoc explainers offer a model agnostic means of interpreting these systems while only requiring query-level access. However, recent work demonstrates that these explainers can be fooled adversarially. This discovery has adverse implications for auditors, regulators, and other sentinels. With this in mind, several natural questions arise - how can we audit these black box systems? And how can we ascertain that the auditee is complying with the audit in good faith? In this work, we rigorously formalize this problem and devise a defense against adversarial attacks on perturbation-based explainers. We propose algorithms for the detection( CAD-Detect) and defense( CAD-Defend) of these attacks, which are aided by our novel conditional anomaly detection approach, KNN-CAD. We demonstrate that our approach successfully detects whether a black box system adversarially conceals its decision-making process and mitigates the adversarial attack on real-world data for the prevalent explainers, LIME and SHAP. The code for this work is available at https://github. com/craymichael/unfooling.",10.1609/aaai.v37i6.25847,https://ojs.aaai.org/index.php/AAAI/article/view/25847,AAAI Conference on Artificial Intelligence,Zachariah Carmichael;Walter J. Scheirer,2023,21,"@inproceedings{2-1222,
  title = {Unfooling Perturbation-Based Post Hoc Explainers},
  author = {Zachariah Carmichael and Walter J. Scheirer},
  year = {2023},
  doi = {10.1609/aaai.v37i6.25847},
  booktitle = {AAAI Conference on Artificial Intelligence}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance, Finance / Business / Economy, Everyday / Employment / Public Service",Operational,"Auditing, Explaining","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-12220,elsevier,Multi-modal and multi-criteria conflict analysis model based on deep learning and dominance-based rough sets: application to clinical non-parallel decision problems,"The non-parallel disease progression and curative effect are the difficulties of clinical diagnosis and treatment decisions. Experts (doctors) constantly summarize these non-parallel phenomena for more accurate diagnosis and treatment. In order to discover the mechanism of clinical non-parallel decision-making, this paper constructs a multi-modal and multi-criteria conflict analysis method based on deep learning (DL) and dominance-based rough sets (DRSA). First, for multi-modal attribute information, we adopted a deep learning based visual attention distribution to focus on the priority areas of images, a deep residual network is used for a feature extractor. The dominant characteristics of the attributes are considered, and the dominant similarity relationship based on cosine similarity is constructed using DRSA. Second, conditional attributes are used to classify objects and predict clinical progression (outcome). At the same time, the objects are classified according to decision attributes based on DRSA. Third, the Pawlak conflict analysis is introduced to analyze the consistency between the predicted results of conditional attributes and the practical results generated by decision attributes. Finally, four clinically non-parallel decision datasets are used, including colorectal cancer (CRC), membranous nephropathy (MN), rheumatoid arthritis (RA) diagnosis and MN efficacy evaluation, to verify the applicability and validity of the proposed model and discover the non-parallel decision mechanism of different diseases. This paper constructs a data-driven clinical decision research paradigm, and provides a research approach to a wide range of non-parallel decision-making problems.",https://doi.org/10.1016/j.inffus.2024.102636,https://www.sciencedirect.com/science/article/pii/S1566253524004147,Information Fusion,Xiaoli Chu;Bingzhen Sun;Xiaodong Chu;Lu Wang;Kun Bao;Nanguan Chen,2025,12,"@article{2-12220,
  title={Multi-modal and multi-criteria conflict analysis model based on deep learning and dominance-based rough sets: application to clinical non-parallel decision problems},
  author={Chu, Xiaoli and Sun, Bingzhen and Chu, Xiaodong and Wang, Lu and Bao, Kun and Chen, Nanguan},
  year={2025},
  journal={Information Fusion},
  doi={10.1016/j.inffus.2024.102636}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Forecasting","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-12475,elsevier,Observational study investigating the level of support from a convolutional neural network in face and scalp lesions deemed diagnostically ‘unclear’ by dermatologists,"Background The clinical diagnosis of face and scalp lesions (FSL) is challenging due to overlapping features. Dermatologists encountering diagnostically ‘unclear’ lesions may benefit from artificial intelligence support via convolutional neural networks (CNN). Methods In a web-based classification task, dermatologists (n = 64) diagnosed a convenience sample of 100 FSL as ‘benign’, ‘malignant’, or ‘unclear’ and indicated their management decisions (‘no action’, ‘follow-up’, ‘treatment/excision’). A market-approved CNN (Moleanalyzer-Pro®, FotoFinder Systems, Germany) was applied for binary classifications (benign/malignant) of dermoscopic images. Results After reviewing one dermoscopic image per case, dermatologists labelled 562 of 6400 diagnoses (8.8%) as ‘unclear’ and mostly managed these by follow-up examinations (57.3%, n = 322) or excisions (42.5%, n = 239). Management was incorrect in 58.8% of 291 truly malignant cases (171 ‘follow-up’ or ‘no action’) and 43.9% of 271 truly benign cases (119 ‘excision’). Accepting CNN classifications in unclear cases would have reduced false management decisions to 4.1% in truly malignant and 31.7% in truly benign lesions (both p < 0.01). After receiving full case information 239 diagnoses (3.7%) remained ‘unclear’ to dermatologists, now triggering more excisions (72.0%) than follow-up examinations (28.0%). These management decisions were incorrect in 32.8% of 116 truly malignant cases and 76.4% of 123 truly benign cases. Accepting CNN classifications would have reduced false management decisions to 6.9% in truly malignant lesions and to 38.2% in truly benign cases (both p < 0.01). Conclusions Dermatologists mostly managed diagnostically ‘unclear’ FSL by treatment/excision or follow-up examination. Following CNN classifications as guidance in unclear cases seems suitable to significantly reduce incorrect decisions.",https://doi.org/10.1016/j.ejca.2023.02.025,https://www.sciencedirect.com/science/article/pii/S0959804923001181,European Journal of Cancer,Katharina S. Kommoss;Julia K. Winkler;Christine Mueller-Christmann;Felicitas Bardehle;Ferdinand Toberer;Wilhelm Stolz;Teresa Kraenke;Rainer Hofmann-Wellenhof;Andreas Blum;Alexander Enk;Albert Rosenberger;Holger A. Haenssle,2023,68,"@article{2-12475,
  title={Observational study investigating the level of support from a convolutional neural network in face and scalp lesions deemed diagnostically ‘unclear’ by dermatologists},
  author={Kommoss, Katharina S. and Winkler, Julia K. and Mueller-Christmann, Christine and Bardehle, Felicitas and Toberer, Ferdinand and Stolz, Wilhelm and Kraenke, Teresa and Hofmann-Wellenhof, Rainer and Blum, Andreas and Enk, Alexander and Rosenberger, Albert and Haenssle, Holger A.},
  year={2023},
  journal={European Journal of Cancer},
  doi={10.1016/j.ejca.2023.02.025}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Knowledge provider","Alter decision outcomes, Change affective-perceptual",no such info,"classification, recommendations, prediction of alternative",NA,Interactive interface,Yes,Yes
2-12516,elsevier,On-line strategy selection for reducing overcrowding in an emergency department,"Overcrowding is a well-known major issue affecting the behavior of an Emergency Department (ED), as it is responsible for patients’ dissatisfaction and has a negative impact on the quality of workers’ performance. Dealing with overcrowding in an ED is complicated by lack of its precise definition and by exogenous and stochastic nature of requests to be served. In this paper, we present a Decision Support System (DSS) based on the integration of a Deep Neural Network for dealing with the sources of uncertainty and a simulation tool to evaluate how specific management policies affect the ED behavior. The DSS is designed to be run on-line, dynamically suggesting the most suitable policy to be implemented in the ED. We evaluate the performance of the DSS on a specific major ED located in northern Italy. Numerical results show that overcrowding can be considerably reduced by allowing a dynamic selection among a limited set of simple policies for queue management.",https://doi.org/10.1016/j.omega.2024.103098,https://www.sciencedirect.com/science/article/pii/S0305048324000641,Omega - The International Journal of Management Science,Cristiano Fabbri;Michele Lombardi;Enrico Malaguti;Michele Monaci,2024,7,"@article{2-12516,
  title={On-line strategy selection for reducing overcrowding in an emergency department},
  author={Fabbri, Cristiano and Lombardi, Michele and Malaguti, Enrico and Monaci, Michele},
  year={2024},
  journal={Omega - The International Journal of Management Science},
  doi={10.1016/j.omega.2024.103098}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Organizational,"Analyzing, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-1254,aaai,When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-Making,"Historically, much of machine learning research has focused on the performance of the algorithm alone, but recently more attention has been focused on optimizing joint human-algorithm performance. Here, we analyze a specific type of human-algorithm collaboration where the algorithm has access to a set of n items, and presents a subset of size k to the human, who selects a final item from among those k. This scenario could model content recommendation, route planning, or any type of labeling task. Because both the human and algorithm have imperfect, noisy information about the true ordering of items, the key question is: which value of k maximizes the probability that the best item will be ultimately selected? For k=1, performance is optimized by the algorithm acting alone, and for k=n it is optimized by the human acting alone. Surprisingly, we show that for multiple of noise models, it is optimal to set k in [2, n-1] - that is, there are strict benefits to collaborating, even when the human and algorithm have equal accuracy separately. We demonstrate this theoretically for the Mallows model and experimentally for the Random Utilities models of noisy permutations. However, we show this pattern is *reversed* when the human is anchored on the algorithms presented ordering - the joint system always has strictly worse performance. We extend these results to the case where the human and algorithm differ in their accuracy levels, showing that there always exist regimes where a more accurate agent would strictly benefit from collaborating with a less accurate one, but these regimes are asymmetric between the human and the algorithms accuracy.",10.1609/aaai.v38i9.28866,https://ojs.aaai.org/index.php/AAAI/article/view/28866,AAAI Conference on Artificial Intelligence,Kate Donahue;Sreenivas Gollapudi;Kostas Kollias,2024,0,"@inproceedings{2-1254,
  title = {When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-Making},
  author = {Donahue, Kate and Gollapudi, Sreenivas and Kollias, Kostas},
  year = {2024},
  doi = {10.1609/aaai.v38i9.28866},
  booktitle = {AAAI Conference on Artificial Intelligence}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Operational,"Advising, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-12593,elsevier,Optimal use of β-lactams in neonates: machine learning-based clinical decision support system,"Summary Background Accurate prediction of the optimal dose for β-lactam antibiotics in neonatal sepsis is challenging. We aimed to evaluate whether a reliable clinical decision support system (CDSS) based on machine learning (ML) can assist clinicians in making optimal dose selections. Methods Five β-lactam antibiotics (amoxicillin, ceftazidime, cefotaxime, meropenem and latamoxef), commonly used to treat neonatal sepsis, were selected. The CDSS was constructed by incorporating the drug, patient, dosage, pharmacodynamic, and microbiological factors. The CatBoost ML algorithm was used to build the CDSS. Real-world studies were used to evaluate the CDSS performance. Virtual trials were used to compare the CDSS-optimized doses with guideline-recommended doses. Findings For a specific drug, by entering the patient characteristics and pharmacodynamic (PD) target (50%/70%/100% fraction of time that the free drug concentration is above the minimal inhibitory concentration [fT > MIC]), the CDSS can determine whether the planned dosing regimen will achieve the PD target and suggest an optimal dose. The prediction accuracy of all five drugs was >80.0% in the real-world validation. Compared with the PopPK model, the overall accuracy, precision, recall, and F1-Score improved by 10.7%, 22.1%, 64.2%, and 43.1%, respectively. Using the CDSS-optimized doses, the average probability of target concentration attainment increased by 58.2% compared to the guideline-recommended doses. Interpretation An ML-based CDSS was successfully constructed to assist clinicians in selecting optimal β-lactam antibiotic doses. Funding This work was supported by the National Natural Science Foundation of China; Distinguished Young and Middle-aged Scholar of Shandong University; National Key Research and Development Program of China.",https://doi.org/10.1016/j.ebiom.2024.105221,https://www.sciencedirect.com/science/article/pii/S2352396424002561,eBioMedicine,Bo-Hao Tang;Bu-Fan Yao;Wei Zhang;Xin-Fang Zhang;Shu-Meng Fu;Guo-Xiang Hao;Yue Zhou;De-Qing Sun;Gang Liu;John {van den Anker};Yue-E Wu;Yi Zheng;Wei Zhao,2024,9,"@article{2-12593,
  title = {Optimal use of β-lactams in neonates: machine learning-based clinical decision support system},
  author = {Tang, Bo-Hao and Yao, Bu-Fan and Zhang, Wei and Zhang, Xin-Fang and Fu, Shu-Meng and Hao, Guo-Xiang and Zhou, Yue and Sun, De-Qing and Liu, Gang and van den Anker, John and Wu, Yue-E and Zheng, Yi and Zhao, Wei},
  year = {2024},
  doi = {10.1016/j.ebiom.2024.105221},
  journal = {eBioMedicine}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-12600,elsevier,Optimization of building demand flexibility using reinforcement learning and rule-based expert systems,"The increasing use of renewable energy in buildings requires optimization of building demand flexibility to reduce energy costs and carbon emissions. Nevertheless, the optimization process is generally challenging that needs to consider the on-site intermittent energy supply, dynamic building energy demand, and proper utilization of energy storage systems. Leveraging the growing availability of operational data in buildings, data-driven strategies such as reinforcement learning (RL) have emerged as effective approaches to optimizing building demand flexibility. However, training a reliable RL agent is practically data-demanding and time-consuming, limiting its practical applicability. This study proposes a new strategy that integrates a rule-based expert system (RBES) and RL agents into the decision-making process to jointly reduce building energy costs, minimize the peak-to-average ratio (PAR) of grid power, and maximize PV self-consumption. In this strategy, the RBES determines system operation directly in less complex decision-making scenarios, while, in more intricate decision-making environments, it provides a reference decision for RL to explore optimal solutions further. This integration empowers RL agents to avoid unnecessary exploration and significantly enhance learning efficiency. The proposed strategy was tested using PV generation data and energy consumption data of a low energy office building. The results demonstrated an 85.7% improvement in RL learning efficiency and this strategy can successfully avoid sub-optimal convergence during policy learning. Compared to relying solely on the RBES, the proposed strategy led to 5.4% and 19.2% reductions in the electricity costs and daily PAR of grid power at peak hours, respectively. The strategy also achieved a satisfying PV self-consumption ratio of 62.4%, which is merely 0.4% lower than the optimal value determined by the RBES strategy that prioritized maximizing PV self-consumption. Additionally, compared with a model predictive control method developed for cost reduction, the strategy achieved similar cost savings while significantly reducing the decision time.",https://doi.org/10.1016/j.apenergy.2023.121792,https://www.sciencedirect.com/science/article/pii/S030626192301156X,Applied Energy,Xinlei Zhou;Shan Xue;Han Du;Zhenjun Ma,2023,25,"@article{2-12600,
  title={Optimization of building demand flexibility using reinforcement learning and rule-based expert systems},
  author={Zhou, Xinlei and Xue, Shan and Du, Han and Ma, Zhenjun},
  year={2023},
  doi={10.1016/j.apenergy.2023.121792},
  journal={Applied Energy}
}",Methodological contributions,Environment / Resources / Energy,Operational,Executing,"Knowledge provider, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-12703,elsevier,Patented intelligence: cloning human decision models for industry 4.0,"Industry 4.0 is a trend related to smart factories, which are cyber-physical spaces populated and controlled by the collective intelligence for the autonomous and highly flexible manufacturing purposes. Artificial Intelligence (AI) embedded into various planning, production, and management processes in Industry 4.0 must take the initiative and responsibility for making necessary real-time decisions in many cases. In this paper, we suggest the Pi-Mind technology as a compromise between completely human-expert-driven decision-making and AI-driven decision-making. Pi-Mind enables capturing, cloning and patenting essential parameters of the decision models from a particular human expert making these models transparent, proactive and capable of autonomic and fast decision-making simultaneously in many places. The technology facilitates the human impact (due to ubiquitous presence) in smart manufacturing processes and enables human-AI shared responsibility for the consequences of the decisions made. It also benefits from capturing and utilization of the traditionally human creative cognitive capabilities (sometimes intuitive and emotional), which in many cases outperform the rational decision-making. Pi-Mind technology is a set of models, techniques, and tools built on principles of value-based biased decision-making and creative cognitive computing to augment the axioms of decision rationality in industry.",https://doi.org/10.1016/j.jmsy.2018.04.019,https://www.sciencedirect.com/science/article/pii/S0278612518300608,Journal of Manufacturing Systems,Vagan Terziyan;Svitlana Gryshko;Mariia Golovianko,2018,6,"@article{2-12703,
  title = {Patented intelligence: cloning human decision models for industry 4.0},
  author = {Vagan Terziyan and Svitlana Gryshko and Mariia Golovianko},
  year = {2018},
  journal = {Journal of Manufacturing Systems},
  doi = {10.1016/j.jmsy.2018.04.019}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Institutional,"Analyzing, Collaborating","Decision-maker, Knowledge provider","Change trust, Change cognitive demands, Change affective-perceptual, Shift responsibility, Shape ethical norms, Alter decision outcomes","Update AI competence, Change AI responses, Shape AI for accountability",NA,traditionally human creative cognitive capabilities,"Physical / Embodiment, Autonomous System",Yes,Yes
2-12731,elsevier,Performance and attitudes toward real-time computer-aided polyp detection during colonoscopy in a large tertiary referral center in the united states,"Background and Aims Computer-aided detection (CADe) has been shown to improve polyp detection in clinical trials. Limited data exist on the impact, utilization, and attitudes toward artificial intelligence (AI)-assisted colonoscopy in daily clinical practice. We aimed to evaluate the effectiveness of the first U.S. Food and Drug Administration–approved CADe device for polyp detection in the United States and the attitudes toward its implementation. Methods We performed a retrospective analysis of a prospectively maintained database of patients undergoing colonoscopy at a tertiary center in the United States before and after a real-time CADe system was made available. The decision to activate the CADe system was at the discretion of the endoscopist. An anonymous survey was circulated among endoscopy physicians and staff at the beginning and conclusion of the study period regarding their attitudes toward AI-assisted colonoscopy. Results CADe was activated in 52.1% of cases. Compared with historical control subjects, there was no statistically significant difference in adenomas detected per colonoscopy (1.08 vs 1.04, P = .65), even after excluding diagnostic and therapeutic indications and cases where CADe was not activated (1.27 vs 1.17, P = .45). In addition, there was no statistically significant difference in adenoma detection rate (ADR), median procedure, and withdrawal times. Survey results demonstrated mixed attitudes toward AI-assisted colonoscopy, of which main concerns were high number of false-positive signals (82.4%), high level of distraction (58.8%), and impression it prolonged procedure time (47.1%). Conclusions CADe did not improve adenoma detection in daily practice among endoscopists with high baseline ADRs. Despite its availability, AI-assisted colonoscopy was only activated in half of the cases, and multiple concerns were raised by staff and endoscopists. Future studies will help elucidate the patients and endoscopists that would benefit most from AI-assisted colonoscopy.",https://doi.org/10.1016/j.gie.2023.02.016,https://www.sciencedirect.com/science/article/pii/S0016510723002638,Gastrointestinal Endoscopy,Fredy Nehme;Emmanuel Coronel;Denise A. Barringer;Laura G. Romero;Mehnaz A. Shafi;William A. Ross;Phillip S. Ge,2023,62,"@article{2-12731,
  title = {Performance and attitudes toward real-time computer-aided polyp detection during colonoscopy in a large tertiary referral center in the United States},
  author = {Fredy Nehme and Emmanuel Coronel and Denise A. Barringer and Laura G. Romero and Mehnaz A. Shafi and William A. Ross and Phillip S. Ge},
  year = {2023},
  doi = {10.1016/j.gie.2023.02.016},
  journal = {Gastrointestinal Endoscopy}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising, Analyzing","Decision-maker, Decision-subject, Knowledge provider","Alter decision outcomes, Change affective-perceptual",no such info,"recommendations, information prompts",NA,"Visual, Auditory",Yes,Yes
2-12808,elsevier,Post hoc explanations improve consumer responses to algorithmic decisions,"Algorithms are capable of assisting with, or making, critical decisions in many areas of consumers’ lives. Algorithms have consistently outperformed human decision-makers in multiple domains, and the list of cases where algorithms can make superior decisions will only grow as the technology evolves. Nevertheless, many people distrust algorithmic decisions. One concern is their lack of transparency. For instance, it is often unclear how a machine learning algorithm produces a given prediction. To address the problem, organizations have started providing post-hoc explanations of the logic behind their algorithmic decisions. However, it remains unclear to what extent explanations can improve consumer attitudes and intentions. Five experiments demonstrate that algorithmic explanations can improve perceptions of transparency, attitudes, and behavioral intentions – or they can backfire, depending on the explanation method used. The most effective explanations highlight concrete and feasible steps consumers can take to positively influence their future decision outcomes.",https://doi.org/10.1016/j.jbusres.2024.114981,https://www.sciencedirect.com/science/article/pii/S0148296324004855,Journal of Business Research,Mehdi Mourali;Dallas Novakowski;Ruth Pogacar;Neil Brigden,2025,9,"@article{2-12808,
  title = {Post hoc explanations improve consumer responses to algorithmic decisions},
  author = {Mehdi Mourali and Dallas Novakowski and Ruth Pogacar and Neil Brigden},
  year = {2025},
  doi = {10.1016/j.jbusres.2024.114981},
  journal = {Journal of Business Research}
}",Empirical contributions,"Finance / Business / Economy, Everyday / Employment / Public Service",Individual,"Forecasting, Explaining","Decision-maker, Decision-subject","Alter decision outcomes, Change affective-perceptual",no such info,"post hoc explanations, sensitivity explanations, actionable explanations, actionable sensitivity explanations",NA,Autonomous System,Yes,Yes
2-1285,aaai,Heterogeneous Graph Learning for Multi-Modal Medical Data Analysis,"Routine clinical visits of a patient produce not only image data, but also non-image data containing clinical information regarding the patient, i. e. , medical data is multi-modal in nature. Such heterogeneous modalities offer different and complementary perspectives on the same patient, resulting in more accurate clinical decisions when they are properly combined. However, despite its significance, how to effectively fuse the multi-modal medical data into a unified framework has received relatively little attention. In this paper, we propose an effective graph-based framework called HetMed( Heterogeneous Graph Learning for Multi-modal Medical Data Analysis) for fusing the multi-modal medical data. Specifically, we construct a multiplex network that incorporates multiple types of non-image features of patients to capture the complex relationship between patients in a systematic way, which leads to more accurate clinical decisions. Extensive experiments on various real-world datasets demonstrate the superiority and practicality of HetMed. The source code for HetMed is available at https://github. com/Sein-Kim/Multimodal-Medical.",10.1609/aaai.v37i4.25643,https://ojs.aaai.org/index.php/AAAI/article/view/25643,AAAI Conference on Artificial Intelligence,Sein Kim;Namkyeong Lee;Junseok Lee;Dongmin Hyun;Chanyoung Park,2023,51,"@inproceedings{2-1285,
  title = {Heterogeneous Graph Learning for Multi-Modal Medical Data Analysis},
  author = {Kim, Sein and Lee, Namkyeong and Lee, Junseok and Hyun, Dongmin and Park, Chanyoung},
  year = {2023},
  doi = {10.1609/aaai.v37i4.25643},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Analyzing","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-1286,aaai,i-Rebalance: Personalized Vehicle Repositioning for Supply Demand Balance,"Ride-hailing platforms have been facing the challenge of balancing demand and supply. Existing vehicle reposition techniques often treat drivers as homogeneous agents and relocate them deterministically, assuming compliance with the reposition. In this paper, we consider a more realistic and driver-centric scenario where drivers have unique cruising preferences and can decide whether to take the recommendation or not on their own. We propose i-Rebalance, a personalized vehicle reposition technique with deep reinforcement learning( DRL). i-Rebalance estimates drivers decisions on accepting reposition recommendations through an on-field user study involving 99 real drivers. To optimize supply-demand balance and enhance preference satisfaction simultaneously, i-Rebalance has a sequential reposition strategy with dual DRL agents: Grid Agent to determine the reposition order of idle vehicles, and Vehicle Agent to provide personalized recommendations to each vehicle in the pre-defined order. This sequential learning strategy facilitates more effective policy training within a smaller action space compared to traditional joint-action methods. Evaluation of real-world trajectory data shows that i-Rebalance improves driver acceptance rate by 38. 07% and total driver income by 9. 97%.",10.1609/aaai.v38i1.27754,https://ojs.aaai.org/index.php/AAAI/article/view/27754,AAAI Conference on Artificial Intelligence,Haoyang Chen;Peiyan Sun;Qiyuan Song;Wanyuan Wang;Weiwei Wu;Wencan Zhang;Guanyu Gao;Yan Lyu,2024,10,"@inproceedings{2-1286,
  title={i-Rebalance: Personalized Vehicle Repositioning for Supply Demand Balance},
  author={Chen, Haoyang and Sun, Peiyan and Song, Qiyuan and Wang, Wanyuan and Wu, Weiwei and Zhang, Wencan and Gao, Guanyu and Lyu, Yan},
  year={2024},
  doi={10.1609/aaai.v38i1.27754},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,"Advising, Executing","Decision-maker, Decision-subject",Alter decision outcomes,Update AI competence,recommendations,personalized settings,"Interactive interface, Textual, Visual",Yes,Yes
2-12875,elsevier,Predicting future state for adaptive clinical pathway management,"Clinical decision support systems are assisting physicians in providing care to patients. However, in the context of clinical pathway management such systems are rather limited as they only take the current state of the patient into account and ignore the possible evolvement of that state in the future. In the past decade, the availability of big data in the healthcare domain did open a new era for clinical decision support. Machine learning technologies are now widely used in the clinical domain, nevertheless, mostly as a tool for disease prediction. A tool that not only predicts future states, but also enables adaptive clinical pathway management based on these predictions is still in need. This paper introduces weighted state transition logic, a logic to model state changes based on actions planned in clinical pathways. Weighted state transition logic extends linear logic by taking weights – numerical values indicating the quality of an action or an entire clinical pathway – into account. It allows us to predict the future states of a patient and it enables adaptive clinical pathway management based on these predictions. We provide an implementation of weighted state transition logic using semantic web technologies, which makes it easy to integrate semantic data and rules as background knowledge. Executed by a semantic reasoner, it is possible to generate a clinical pathway towards a target state, as well as to detect potential conflicts in the future when multiple pathways are coexisting. The transitions from the current state to the predicted future state are traceable, which builds trust from human users on the generated pathway.",https://doi.org/10.1016/j.jbi.2021.103750,https://www.sciencedirect.com/science/article/pii/S1532046421000794,Journal of Biomedical Informatics,Hong Sun;Dörthe Arndt;Jos {De Roo};Erik Mannens,2021,14,"@article{2-12875,
  title = {Predicting future state for adaptive clinical pathway management},
  author = {Hong Sun and Dörthe Arndt and Jos {De Roo} and Erik Mannens},
  year = {2021},
  doi = {10.1016/j.jbi.2021.103750},
  journal = {Journal of Biomedical Informatics}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-12926,elsevier,Predicting solutions of large-scale optimization problems via machine learning: a case study in blood supply chain management,"Practical constrained optimization models are often large, and solving them in a reasonable time is a challenge in many applications. Further, many industries have limited access to professional commercial optimization solvers or computational power for use in their day-to-day operational decisions. In this paper, we propose a novel approach to deal with the issue of solving large operational stochastic optimization problems (SOPs) by using machine learning models. We assume that decision makers have access to facilities to optimally solve their large-scale optimization model for some initial and limited period and for some test instances. This might be through a collaborative project with research institutes or through short-term use of high-performance computing facilities. We propose that longer term support can be provided by utilizing the solutions (i.e., the optimal value of the actionable decision variables) of the stochastic optimization model from this initial period to train a machine learning model to learn optimal operational decisions in the future. In this study, the proposed approach is employed to make decisions on transshipment of blood units in a network of hospitals. We compare the decisions learned by several machine learning models with the optimal results obtained if the hospitals had access to commercial optimization solvers and computational power, and with the hospital network’s current empirical heuristic policy. The results show that using a trained neural network model reduces the average daily cost by about 29% compared with current policy, while the exact optimal policy reduces the average daily cost by 37%. Although optimization models cannot be fully replaced by machine learning, our proposed approach while not guaranteed to be optimal can improve operational decisions when optimization models are computationally expensive and infeasible for daily operational decisions in organizations such as not-for-profit and small and medium-sized enterprises.",https://doi.org/10.1016/j.cor.2020.104941,https://www.sciencedirect.com/science/article/pii/S0305054820300587,Computers & Operations Research,Babak Abbasi;Toktam Babaei;Zahra Hosseinifard;Kate Smith-Miles;Maryam Dehghani,2020,179,"@article{2-12926,
  title = {Predicting solutions of large-scale optimization problems via machine learning: a case study in blood supply chain management},
  author = {Babak Abbasi and Toktam Babaei and Zahra Hosseinifard and Kate Smith-Miles and Maryam Dehghani},
  year = {2020},
  journal = {Computers \& Operations Research},
  doi = {https://doi.org/10.1016/j.cor.2020.104941}
}",Methodological contributions,Healthcare / Medicine / Surgery,Organizational,"Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-12928,elsevier,Predicting supply chain risks using machine learning: the trade-off between performance and interpretability,"Managing supply chain risks has received increased attention in recent years, aiming to shield supply chains from disruptions by predicting their occurrence and mitigating their adverse effects. At the same time, the resurgence of Artificial Intelligence (AI) has led to the investigation of machine learning techniques and their applicability in supply chain risk management. However, most works focus on prediction performance and neglect the importance of interpretability so that results can be understood by supply chain practitioners, helping them make decisions that can mitigate or prevent risks from occurring. In this work, we first propose a supply chain risk prediction framework using data-driven AI techniques and relying on the synergy between AI and supply chain experts. We then explore the trade-off between prediction performance and interpretability by implementing and applying the framework on the case of predicting delivery delays in a real-world multi-tier manufacturing supply chain. Experiment results show that prioritising interpretability over performance may require a level of compromise, especially with regard to average precision scores.",https://doi.org/10.1016/j.future.2019.07.059,https://www.sciencedirect.com/science/article/pii/S0167739X19308003,Future Generation Computer Systems,George Baryannis;Samir Dani;Grigoris Antoniou,2019,451,"@article{2-12928,
  title={Predicting supply chain risks using machine learning: the trade-off between performance and interpretability},
  author={Baryannis, George and Dani, Samir and Antoniou, Grigoris},
  year={2019},
  journal={Future Generation Computer Systems},
  doi={10.1016/j.future.2019.07.059}
}",Methodological contributions,Manufacturing / Industry / Automation,Organizational,"Forecasting, Explaining","Decision-maker, Knowledge provider, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-12961,elsevier,"Predicting wastewater treatment plant influent in mixed, separate, and combined sewers using nearby surface water discharge for better wastewater-based epidemiology sampling design","For wastewater sample collection approaches supporting public health applications, few high hydrologic activity normalizing guidelines currently consider readily available environmental flow data that may earlier capture information regarding periods of influent mixing and dilution of wastewater with groundwater and runoff. This study aimed to identify wastewater sampling rules for high hydrological activity events, allowing for an earlier decision point in the control of dilution before sample collection. We defined the sampling rules via data-driven models (Random Forest and linear regression) using environmental data (i.e., wastewater treatment facility influent rates, nearby stream discharge flow, and precipitation). These models were applied to five treatment plants in Jefferson County, Kentucky (USA) in mixed, separate, and combined sewers with different population sizes. We proposed cutoffs of 10 %, 25 %, and 50 % flow conditions for orientation towards public health samples. The results showed a strong nonlinear relationship between nearby stream discharge and treatment facility flow rates, which was used to infer the hydrological conditions that produce high volumes of diluted wastewater in the sewer system. Accumulated Local Effects and SHapley Additive exPlanations aided in deciphering the relationship between the predictors and response variables of the Random Forest models. The influent rate to the treatment plant from the previous day and two USGS stream gages were needed to adequately predict the degree of infiltration and inflow mixing on a given day. Surface water discharge data can be used to provide an earlier workflow decision point during wet weather periods to improve understanding of flow conditions for wastewater-based epidemiological studies to inform laboratory analysis and data interpretation. Not only total flow, but also the specific proportions of infiltration and inflow to wastewater volume in influent should be considered when analyzing data for normalization purposes, and our method provides a starting point for doing so rapidly and at low cost.",https://doi.org/10.1016/j.scitotenv.2023.167375,https://www.sciencedirect.com/science/article/pii/S0048969723060023,Science of The Total Environment,Arlex Marin-Ramirez;Tyler Mahoney;Ted Smith;Rochelle H. Holm,2024,5,"@article{2-12961,
  title = {Predicting wastewater treatment plant influent in mixed, separate, and combined sewers using nearby surface water discharge for better wastewater-based epidemiology sampling design},
  author = {Arlex Marin-Ramirez and Tyler Mahoney and Ted Smith and Rochelle H. Holm},
  year = {2024},
  doi = {10.1016/j.scitotenv.2023.167375},
  journal = {Science of The Total Environment}
}","Empirical contributions, Methodological contributions",Environment / Resources / Energy,Operational,"Forecasting, Advising","Decision-maker, Developer, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-12984,elsevier,Prediction of brake pedal aperture for automatic wheel loader based on deep learning,"Complex and changing driving environments not only affect the operating requirements of automatic wheel loader but also threaten its driving safety. Therefore, the automatic wheel loader must adopt appropriate braking strategies to realize accurate control of the brake pedal aperture under certain operating conditions. For the V-shaped operation mode of wheel loader, the operator's operation specification is evaluated using three characteristics: operation time, driving distance and friction work. By combining the driving data of experienced drivers in different driving environments with deep learning, a deep long short-term memory network was constructed to predict the brake pedal aperture for different braking types. The proposed anthropomorphic control method that combines driving data and deep learning can be used to predict the aperture value of the wheel loader brake pedal in complex driving environments. This would enable the braking process to conform to the braking decisions of experienced drivers and thereby meet the operational requirements while ensuring driving safety.",https://doi.org/10.1016/j.autcon.2020.103313,https://www.sciencedirect.com/science/article/pii/S0926580520300595,Automation in Construction,Junren Shi;Dongye Sun;Minghui Hu;Sheng Liu;Yingzhe Kan;Ruibo Chen;Ke Ma,2020,0,"@article{2-12984,
  title = {Prediction of brake pedal aperture for automatic wheel loader based on deep learning},
  author = {Shi, Junren and Sun, Dongye and Hu, Minghui and Liu, Sheng and Kan, Yingzhe and Chen, Ruibo and Ma, Ke},
  year = {2020},
  doi = {10.1016/j.autcon.2020.103313},
  journal = {Automation in Construction}
}",Algorithmic contributions,Manufacturing / Industry / Automation,Operational,"Forecasting, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-13,aaai,A Distillation Approach to Data Efficient Individual Treatment Effect Estimation,"The potential for using machine learning algorithms as a tool for suggesting optimal interventions has fueled significant interest in developing methods for estimating heterogeneous or individual treatment effects( ITEs) from observational data. While several methods for estimating ITEs have been recently suggested, these methods assume no constraints on the availability of data at the time of deployment or test time. This assumption is unrealistic in settings where data acquisition is a significant part of the analysis pipeline, meaning data about a test case has to be collected in order to predict the ITE. In this work, we present Data Efficient Individual Treatment Effect Estimation( DEITEE) , a method which exploits the idea that adjusting for confounding, and hence collecting information about confounders, is not necessary at test time. DEITEE allows the development of rich models that exploit all variables at train time but identifies a minimal set of variables required to estimate the ITE at test time. Using 77 semi-synthetic datasets with varying data generating processes, we show that DEITEE achieves significant reductions in the number of variables required at test time with little to no loss in accuracy. Using real data, we demonstrate the utility of our approach in helping soon-to-be mothers make planning and lifestyle decisions that will impact newborn health.",10.1609/aaai.v33i01.33014544,https://ojs.aaai.org/index.php/AAAI/article/view/4375,AAAI Conference on Artificial Intelligence,Maggie Makar;Adith Swaminathan;Emre Kıcıman,2019,17,"@inproceedings{2-13,
  title={A Distillation Approach to Data Efficient Individual Treatment Effect Estimation},
  author={Makar, Maggie and Swaminathan, Adith and Kıcıman, Emre},
  year={2019},
  doi={10.1609/aaai.v33i01.33014544},
  booktitle={AAAI Conference on Artificial Intelligence}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Executing, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-1301,acl,Bridging The Gap: Entailment Fused-T5 for Open-retrieval Conversational Machine Reading Comprehension,"Open-retrieval conversational machine reading comprehension (OCMRC) simulates real-life conversational interaction scenes. Machines are required to make a decision of “Yes/No/Inquire” or generate a follow-up question when the decision is “Inquire” based on retrieved rule texts, user scenario, user question and dialogue history. Recent studies try to reduce the information gap between decision-making and question generation, in order to improve the performance of generation. However, the information gap still persists because these methods are still limited in pipeline framework, where decision-making and question generation are performed separately, making it hard to share the entailment reasoning used in decision-making across all stages. To tackle the above problem, we propose a novel one-stage end-to-end framework, called Entailment Fused-T5 (EFT), to bridge the information gap between decision-making and question generation in a global understanding manner. The extensive experimental results demonstrate that our proposed framework achieves new state-of-the-art performance on the OR-ShARC benchmark. Our model and code are publicly available at an anonymous link.",10.18653/v1/2023.acl-long.857,https://aclanthology.org/2023.acl-long.857,Annual Meeting of the Association for Computational Linguistics,"Zhang, Xiao; Huang, Heyan; Chi, Zewen; Mao, Xian-Ling",2023,2,"@inproceedings{2-1301,
  title={Bridging The Gap: Entailment Fused-T5 for Open-retrieval Conversational Machine Reading Comprehension},
  author={Zhang, Xiao and Huang, Heyan and Chi, Zewen and Mao, Xian-Ling},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2023.acl-long.857}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Operational,"Executing, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-13059,elsevier,Predictive and prescriptive analytics for esg performance evaluation: a case of fortune 500 companies,"Given the growing importance of organizations’ environmental, social, and governance (ESG) performance, studies employing AI-based techniques to generate insights from ESG data for investors and managers are limited. To bridge this gap, this study proposes an AI-based multi-stage ESG performance prediction system consolidating clustering for identifying patterns within ESG data, association rule mining for uncovering meaningful relationships, deep learning for predictive accuracy, and prescriptive analytics for actionable insights. This study is grounded in the big data analytics capability view that has emerged from the dynamic capabilities theory. The model is validated using an ESG dataset of 470 Fortune listed 500 companies obtained from the Refinitiv database. The model offers practical guidance for decision-makers to maintain or enhance their ESG scores, crucial in a business landscape where ESG metrics significantly affect investor choices and public image.",https://doi.org/10.1016/j.jbusres.2024.114742,https://www.sciencedirect.com/science/article/pii/S0148296324002467,Journal of Business Research,Gorkem Sariyer;Sachin {Kumar Mangla};Soumyadeb Chowdhury;Mert {Erkan Sozen};Yigit Kazancoglu,2024,31,"@article{2-13059,
  title={Predictive and prescriptive analytics for ESG performance evaluation: a case of Fortune 500 companies},
  author={Sariyer, Gorkem and Kumar Mangla, Sachin and Chowdhury, Soumyadeb and Erkan Sozen, Mert and Kazancoglu, Yigit},
  year={2024},
  doi={10.1016/j.jbusres.2024.114742},
  journal={Journal of Business Research}
}",System/Artifact contributions,Finance / Business / Economy,Organizational,"Advising, Forecasting, Analyzing","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-1310,acl,Decision-Oriented Dialogue for Human-AI Collaboration,"We describe a class of tasks called decision-oriented dialogues, in which AI assistants such as large language models (LMs) must collaborate with one or more humans via natural language to help them make complex decisions. We formalize three domains in which users face everyday decisions: (1) choosing an assignment of reviewers to conference papers, (2) planning a multi-step itinerary in a city, and (3) negotiating travel plans for a group of friends. In each of these settings, AI assistants and users have disparate abilities that they must combine to arrive at the best decision: Assistants can access and process large amounts of information, while users have preferences and constraints external to the system. For each task, we build a dialogue environment where agents receive a reward based on the quality of the final decision they reach. We evaluate LMs in self-play and in collaboration with humans and find that they fall short compared to human assistants, achieving much lower rewards despite engaging in longer dialogues. We highlight a number of challenges models face in decision-oriented dialogues, ranging from goal-directed behavior to reasoning and optimization, and release our environments as a testbed for future work.",10.1162/tacl_a_00679,https://aclanthology.org/2024.tacl-1.50,Transactions of the Association for Computational Linguistics,"Lin, Jessy; Tomlin, Nicholas; Andreas, Jacob; Eisner, Jason",2024,0,"@article{2-1310,
  title={Decision-Oriented Dialogue for Human-AI Collaboration},
  author={Lin, Jessy and Tomlin, Nicholas and Andreas, Jacob and Eisner, Jason},
  year={2024},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={to appear},
  doi={10.1162/tacl_a_00679}
}",System/Artifact contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research, Transportation / Mobility / Planning",Operational,"Collaborating, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-1312,acl,Explainable Clinical Decision Support from Text,"Clinical prediction models often use structured variables and provide outcomes that are not readily interpretable by clinicians. Further, free-text medical notes may contain information not immediately available in structured variables. We propose a hierarchical CNN-transformer model with explicit attention as an interpretable, multi-task clinical language model, which achieves an AUROC of 0.75 and 0.78 on sepsis and mortality prediction, respectively. We also explore the relationships between learned features from structured and unstructured variables using projection-weighted canonical correlation analysis. Finally, we outline a protocol to evaluate model usability in a clinical decision support context. From domain-expert evaluations, our model generates informative rationales that have promising real-life applications.",10.18653/v1/2020.emnlp-main.115,https://aclanthology.org/2020.emnlp-main.115,Empirical Methods in Natural Language Processing,"Feng, Jinyue; Shaib, Chantal; Rudzicz, Frank",2020,44,"@inproceedings{2-1312,
  title = {Explainable Clinical Decision Support from Text},
  author = {Feng, Jinyue and Shaib, Chantal and Rudzicz, Frank},
  year = {2020},
  doi = {10.18653/v1/2020.emnlp-main.115},
  booktitle = {Empirical Methods in Natural Language Processing}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Explaining",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-1315,acl,Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical Decision-Support Setting,"The growing capabilities of AI models are leading to their wider use, including in safety-critical domains. Explainable AI (XAI) aims to make these models safer to use by making their inference process more transparent. However, current explainability methods are seldom evaluated in the way they are intended to be used: by real-world end users. To address this, we conducted a large-scale user study with 85 healthcare practitioners in the context of human-AI collaborative chest X-ray analysis. We evaluated three types of explanations: visual explanations (saliency maps), natural language explanations, and a combination of both modalities. We specifically examined how different explanation types influence users depending on whether the AI advice and explanations are factually correct. We find that text-based explanations lead to significant over-reliance, which is alleviated by combining them with saliency maps. We also observe that the quality of explanations, that is, how much factually correct information they entail, and how much this aligns with AI correctness, significantly impacts the usefulness of the different explanation types.",10.18653/v1/2024.emnlp-main.1051,https://aclanthology.org/2024.emnlp-main.1051,Empirical Methods in Natural Language Processing,"Kayser, Maxime Guillaume; Menzat, Bayar; Emde, Cornelius; Bercean, Bogdan Alexandru; Novak, Alex; Morgado, Abdalá Trinidad Espinosa; Papiez, Bartlomiej; Gaube, Susanne; Lukasiewicz, Thomas; Camburu, Oana-Maria",2024,6,"@inproceedings{2-1315,
  title = {Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical Decision-Support Setting},
  author = {Kayser, Maxime Guillaume and Menzat, Bayar and Emde, Cornelius and Bercean, Bogdan Alexandru and Novak, Alex and Morgado, Abdal{\'a} Trinidad Espinosa and Papiez, Bartlomiej and Gaube, Susanne and Lukasiewicz, Thomas and Camburu, Oana-Maria},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-main.1051},
  booktitle = {Empirical Methods in Natural Language Processing}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,"Forecasting, Explaining",Decision-maker,"Change cognitive demands, Change trust, Change affective-perceptual",no such info,"visual explanations, textual explanations",NA,"Textual, Conversational/Natural Language, Visual",Yes,Yes
2-13160,elsevier,Processcarbonagent: a large language models-empowered autonomous agent for decision-making in manufacturing carbon emission management,"Knowledge-intensive production represents a primary trend in industrial manufacturing, which heavily relies on the production logs of large-scale, historically similar orders for enhancing production efficiency and process quality. These logs are essential for predicting resource allocation and identifying bottlenecks in throughput. As a result, root cause analysis of the production process state is crucial for supporting decision-making in these settings. However, current methodologies heavily depend on expert knowledge, making the analysis time-consuming and inefficient for large-scale, multivariable processes. Although the development of large language models and autonomous agents presents a potential solution, these models are limited in their direct interaction with event logs due to inadequate data representation, token constraints, and insufficient accuracy. Therefore, enabling the interactive capabilities of large language models to overcome these specific limitations in process event data and industrial domain illusions poses a significant challenge. To address these issues, this paper introduces the ProcessCarbonAgent framework, an autonomous agent empowered by large language models, designed to enhance decision-making within industrial processes. Initially, a process data agent combines predefined semantic text representation methods with process template prompting strategies to improve interaction capabilities. Subsequently, an intention agent utilizing self-information and large language models is developed to address context length limitations by identifying and eliminating redundancies. Finally, a two-stage confidence estimation method is implemented to refine the precision of decision-making assistance, thereby improving the accuracy of decisions supported by large language models. Experiments with textile industry carbon emission data reveal that the assisted decision-making scores employing a compression ratio of 0.5, closely align with scores from manually labeled evaluations, with a 98% overlap across scoring intervals. Moreover, in contrast to relying solely on the original evaluation method, the two-stage confidence estimation method has led to a 20% increase in accuracy performance. The ProcessCarbonAgent achieved scores of 16.64, 55.13, 26.32, and 34.17 on METEOR, BERTScore, NUBIA, and BLEURT, respectively. The results demonstrate that the ProcessCarbonAgent framework significantly enhances the decision-making process for high-carbon emission states in industrial production, providing technical support for the low-carbon transformation and intelligent upgrading of these processes.",https://doi.org/10.1016/j.jmsy.2024.08.008,https://www.sciencedirect.com/science/article/pii/S0278612524001729,Journal of Manufacturing Systems,Tao Wu;Jie Li;Jinsong Bao;Qiang Liu,2024,17,"@article{2-13160,
  title = {Processcarbonagent: A Large Language Models-Empowered Autonomous Agent for Decision-Making in Manufacturing Carbon Emission Management},
  author = {Tao Wu and Jie Li and Jinsong Bao and Qiang Liu},
  year = {2024},
  journal = {Journal of Manufacturing Systems},
  doi = {10.1016/j.jmsy.2024.08.008}
}",System/Artifact contributions,Manufacturing / Industry / Automation,Operational,"Analyzing, Executing, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-13193,elsevier,Proof of witness presence: blockchain consensus for augmented democracy in smart cities,"Smart Cities evolve into complex and pervasive urban environments with a citizens’ mandate to meet sustainable development goals. Repositioning democratic values of citizens’ choices in these complex ecosystems has turned out to be imperative in an era of social media filter bubbles, fake news and opportunities for manipulating electoral results with such means. This paper introduces a new paradigm of augmented democracy that promises actively engaging citizens in a more informed decision-making augmented into public urban space. The proposed concept is inspired by a digital revive of the Ancient Agora of Athens, an arena of public discourse, a Polis where citizens assemble to actively deliberate and collectively decide about public matters. The core contribution of the proposed paradigm is the concept of proving witness presence: making decision-making subject of providing secure evidence and testifying for choices made in the physical space. This paper shows how the challenge of proving witness presence can be tackled with blockchain consensus to empower citizens’ trust and overcome security vulnerabilities of GPS localization. Moreover, a novel platform for collective decision-making and crowd-sensing in urban space is introduced: Smart Agora. It is shown how real-time collective measurements over citizens’ choices can be made in a fully decentralized and privacy-preserving way. Witness presence is tested by deploying a decentralized system for crowd-sensing the sustainable use of transport means. Furthermore, witness presence of cycling risk is validated using official accident data from public authorities, which are compared against wisdom of the crowd. The paramount role of dynamic consensus, self-governance and ethically aligned artificial intelligence in the augmented democracy paradigm is outlined.",https://doi.org/10.1016/j.jpdc.2020.06.015,https://www.sciencedirect.com/science/article/pii/S0743731520303282,Journal of Parallel and Distributed Computing,Evangelos Pournaras,2020,102,"@article{2-13193,
  title={Proof of witness presence: blockchain consensus for augmented democracy in smart cities},
  author={Pournaras, Evangelos},
  year={2020},
  journal={Journal of Parallel and Distributed Computing},
  doi={10.1016/j.jpdc.2020.06.015}
}",Theoretical contributions,"Software / Systems / Security, Transportation / Mobility / Planning, Law / Policy / Governance",Institutional,"Analyzing, Executing","Decision-maker, Guardian, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-132,aaai,Deep Reinforcement Learning for Early Diagnosis of Lung Cancer,"Lung cancer remains the leading cause of cancer-related death worldwide, and early diagnosis of lung cancer is critical for improving the survival rate of patients. Performing annual low-dose computed tomography( LDCT) screening among high-risk populations is the primary approach for early diagnosis. However, after each screening, whether to continue monitoring( with follow-up screenings) or to order a biopsy for diagnosis remains a challenging decision to make. Continuing with follow-up screenings may lead to delayed diagnosis but ordering a biopsy without sufficient evidence incurs unnecessary risk and cost. In this paper, we tackle the problem by an optimal stopping approach. Our proposed algorithm, called EarlyStop-RL, utilizes the structure of the Snell envelope for optimal stopping, and model-free deep reinforcement learning for making diagnosis decisions. Through evaluating our algorithm on a commonly used clinical trial dataset( the National Lung Screening Trial) , we demonstrate that EarlyStop-RL has the potential to greatly enhance risk assessment and early diagnosis of lung cancer, surpassing the performance of two widely adopted clinical models, namely the Lung-RADS and the Brock model.",10.1609/aaai.v38i20.30248,https://ojs.aaai.org/index.php/AAAI/article/view/30248,AAAI Conference on Artificial Intelligence,Yifan Wang;Qining Zhang;Lei Ying;Chuan Zhou,2024,12,"@inproceedings{2-132,
  title={Deep Reinforcement Learning for Early Diagnosis of Lung Cancer},
  author={Wang, Yifan and Zhang, Qining and Ying, Lei and Zhou, Chuan},
  year={2024},
  booktitle={AAAI Conference on Artificial Intelligence},
  doi={10.1609/aaai.v38i20.30248}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Executing, Collaborating, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-13207,elsevier,Proteus study: a prospective randomized controlled trial evaluating the use of artificial intelligence in stress echocardiography,"Background Stress echocardiography (SE) is one of the most commonly used diagnostic imaging tests for coronary artery disease (CAD) but requires clinicians to visually assess scans to identify patients who may benefit from invasive investigation and treatment. EchoGo Pro provides an automated interpretation of SE based on artificial intelligence (AI) image analysis. In reader studies, use of EchoGo Pro when making clinical decisions improves diagnostic accuracy and confidence. Prospective evaluation in real world practice is now important to understand the impact of EchoGo Pro on the patient pathway and outcome. Methods PROTEUS is a randomized, multicenter, 2-armed, noninferiority study aiming to recruit 2,500 participants from National Health Service (NHS) hospitals in the UK referred to SE clinics for investigation of suspected CAD. All participants will undergo a stress echocardiogram protocol as per local hospital policy. Participants will be randomized 1:1 to a control group, representing current practice, or an intervention group, in which clinicians will receive an AI image analysis report (EchoGo Pro, Ultromics Ltd, Oxford, UK) to use during image interpretation, indicating the likelihood of severe CAD. The primary outcome will be appropriateness of clinician decision to refer for coronary angiography. Secondary outcomes will assess other health impacts including appropriate use of other clinical management approaches, impact on variability in decision making, patient and clinician qualitative experience and a health economic analysis. Discussion This will be the first study to assess the impact of introducing an AI medical diagnostic aid into the standard care pathway of patients with suspected CAD being investigated with SE. Trial Registration Clinicaltrials.gov registration number NCT05028179, registered on 31 August 2021; ISRCTN: ISRCTN15113915; IRAS ref: 293515; REC ref: 21/NW/0199.",https://doi.org/10.1016/j.ahj.2023.05.003,https://www.sciencedirect.com/science/article/pii/S000287032300114X,American Heart Journal,Gary Woodward;Mamta Bajre;Sanjeev Bhattacharyya;Maria Breen;Virginia Chiocchia;Helen Dawes;Hakim-Moulay Dehbi;Tine Descamps;Elena Frangou;Carol-Ann Fazakarley;Victoria Harris;Will Hawkes;Oliver Hewer;Casey L Johnson;Samuel Krasner;Lynn Laidlaw;Jonathan Lau;Tom Marwick;Steffen E Petersen;Hania Piotrowska;Ged Ridgeway;David P Ripley;Emily Sanderson;Natalie Savage;Rizwan Sarwar;Louise Tetlow;Ben Thompson;Samantha Thulborn;Victoria Williamson;William Woodward;Ross Upton;Paul Leeson,2023,21,"@article{2-13207,
  title={Proteus study: a prospective randomized controlled trial evaluating the use of artificial intelligence in stress echocardiography},
  author={Woodward, Gary and Bajre, Mamta and Bhattacharyya, Sanjeev and Breen, Maria and Chiocchia, Virginia and Dawes, Helen and Dehbi, Hakim-Moulay and Descamps, Tine and Frangou, Elena and Fazakarley, Carol-Ann and Harris, Victoria and Hawkes, Will and Hewer, Oliver and Johnson, Casey L and Krasner, Samuel and Laidlaw, Lynn and Lau, Jonathan and Marwick, Tom and Petersen, Steffen E and Piotrowska, Hania and Ridgeway, Ged and Ripley, David P and Sanderson, Emily and Savage, Natalie and Sarwar, Rizwan and Tetlow, Louise and Thompson, Ben and Thulborn, Samantha and Williamson, Victoria and Woodward, William and Upton, Ross and Leeson, Paul},
  year={2023},
  doi={10.1016/j.ahj.2023.05.003},
  journal={American Heart Journal}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Decision-maker, Decision-subject",NA,NA,recommendations,NA,NA,Yes,No
2-13210,elsevier,Proving unfairness of decision making systems without model access,"The problem of guaranteeing the fairness of automatic decision making systems has become a topic of considerable interest. Many competing definitions of fairness have been proposed, as well as methods aiming to achieve or approximate them while maintaining the ability to train useful models. The complimentary question of testing the fairness of an existing predictor is important both to the creators of machine learning systems, and to users. More specifically, it is important for users to be able to prove that an unfair system that affects them is indeed unfair, even when full and direct access to the system internals is denied. In this paper, we propose a framework that enables us to prove the unfairness of predictors which have known accuracy properties, without direct access to the model, the features it is based on, or even individual predictions. To do so, we analyze the fairness-accuracy trade-off under the definition of demographic parity. We develop an information-theoretic method that uses only an external dataset containing the protected attributes and the targets and provides a bound on the accuracy of any fair model that predicts the same targets, regardless of the features it is based on. The result is an algorithm that enables proof of unfairness, with absolutely no cooperation from the system owners.",https://doi.org/10.1016/j.eswa.2022.118987,https://www.sciencedirect.com/science/article/pii/S095741742202005X,Expert Systems with Applications,Yehezkel S. Resheff;Yair Horesh;Moni Shahar,2023,0,"@article{2-13210,
  title = {Proving unfairness of decision making systems without model access},
  author = {Yehezkel S. Resheff and Yair Horesh and Moni Shahar},
  year = {2023},
  journal = {Expert Systems with Applications},
  doi = {10.1016/j.eswa.2022.118987}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Institutional,"Forecasting, Auditing","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-1322,acl,Language Models are Alignable Decision-Makers: Dataset and Application to the Medical Triage Domain,"In difficult decision-making scenarios, it is common to have conflicting opinions among expert human decision-makers as there may not be a single right answer. Such decisions may be guided by different attributes that can be used to characterize an individual's decision. We introduce a novel dataset for medical triage decision-making, labeled with a set of decision-maker attributes (DMAs). This dataset consists of 62 scenarios, covering six different DMAs, including ethical principles such as fairness and moral desert. We present a novel software framework for human-aligned decision-making by utilizing these DMAs, paving the way for trustworthy AI with better guardrails. Specifically, we demonstrate how large language models (LLMs) can serve as ethical decision-makers, and how their decisions can be aligned to different DMAs using zero-shot prompting. Our experiments focus on different open-source models with varying sizes and training techniques, such as Falcon, Mistral, and Llama 2. Finally, we also introduce a new form of weighted self-consistency that improves the overall quantified performance. Our results provide new research directions in the use of LLMs as alignable decision-makers. The dataset and open-source software are publicly available at: https://github.com/ITM-Kitware/llm-alignable-dm.",10.18653/v1/2024.naacl-industry.18,https://aclanthology.org/2024.naacl-industry.18,NAACL HLT (Industry Track),"Hu, Brian; Ray, Bill; Leung, Alice; Summerville, Amy; Joy, David; Funk, Christopher; Basharat, Arslan",2024,13,"@inproceedings{2-1322,
  title = {Language Models are Alignable Decision-Makers: Dataset and Application to the Medical Triage Domain},
  author = {Hu, Brian and Ray, Bill and Leung, Alice and Summerville, Amy and Joy, David and Funk, Christopher and Basharat, Arslan},
  year = {2024},
  doi = {10.18653/v1/2024.naacl-industry.18},
  booktitle = {NAACL HLT (Industry Track)}
}","Dataset/Benchmark contributions, Methodological contributions",Healthcare / Medicine / Surgery,Operational,"Executing, Advising, Explaining","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-1323,acl,Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark,"The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question-answering (QA) task with answer options for evaluation. However, many clinical decisions involve answering open-ended questions without pre-set options. To better understand LLMs in the clinic, we construct a benchmark ClinicBench. We first collect eleven existing datasets covering diverse clinical language generation, understanding, and reasoning tasks. Furthermore, we construct six novel datasets and clinical tasks that are complex but common in real-world practice, e.g., open-ended decision-making, long document processing, and emerging drug analysis. We conduct an extensive evaluation of twenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite medical experts to evaluate the clinical usefulness of LLMs",10.18653/v1/2024.emnlp-main.759,https://aclanthology.org/2024.emnlp-main.759,Empirical Methods in Natural Language Processing,"Liu, Fenglin; Li, Zheng; Zhou, Hongjian; Yin, Qingyu; Yang, Jingfeng; Tang, Xianfeng; Luo, Chen; Zeng, Ming; Jiang, Haoming; Gao, Yifan; Nigam, Priyanka; Nag, Sreyashi; Yin, Bing; Hua, Yining; Zhou, Xuan; Rohanian, Omid; Thakur, Anshul; Clifton, Lei; Clifton, David A.",2024,41,"@inproceedings{2-1323,
  title = {Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark},
  author = {Liu, Fenglin and Li, Zheng and Zhou, Hongjian and Yin, Qingyu and Yang, Jingfeng and Tang, Xianfeng and Luo, Chen and Zeng, Ming and Jiang, Haoming and Gao, Yifan and Nigam, Priyanka and Nag, Sreyashi and Yin, Bing and Hua, Yining and Zhou, Xuan and Rohanian, Omid and Thakur, Anshul and Clifton, Lei and Clifton, David A.},
  year = {2024},
  booktitle = {Empirical Methods in Natural Language Processing},
  doi = {10.18653/v1/2024.emnlp-main.759}
}",Dataset/Benchmark contributions,Healthcare / Medicine / Surgery,Operational,"Executing, Forecasting","Knowledge provider, Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-1324,acl,Learning to Explain Selectively: A Case Study on Question Answering,"Explanations promise to bridge the gap between humans and AI, yet it remains difficult to achieve consistent improvement in AI-augmented human decision making. The usefulness of AI explanations depends on many factors, and always showing the same type of explanation in all cases is suboptimal—so is relying on heuristics to adapt explanations for each scenario. We propose learning to explain”selectively”: for each decision that the user makes, we use a model to choose the best explanation from a set of candidates and update this model with feedback to optimize human performance. We experiment on a question answering task, Quizbowl, and show that selective explanations improve human performance for both experts and crowdworkers.",10.18653/v1/2022.emnlp-main.573,https://aclanthology.org/2022.emnlp-main.573,Empirical Methods in Natural Language Processing,"Feng, Shi; Boyd-Graber, Jordan",2022,6,"@inproceedings{2-1324,
  title = {Learning to Explain Selectively: A Case Study on Question Answering},
  author = {Feng, Shi and Boyd-Graber, Jordan},
  year = {2022},
  booktitle = {Empirical Methods in Natural Language Processing},
  doi = {10.18653/v1/2022.emnlp-main.573}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Environment / Resources / Energy",Individual,"Explaining, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-1327,acl,PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers,"In this paper, we conduct a study to utilize LLMs as a solution for decision making that requires complex data analysis. We define **Decision QA** as the task of answering the best decision, d<sub>best</sub>, for a decision-making question Q, business rules R and a database D. Since there is no benchmark that can examine Decision QA, we propose Decision QA benchmark, **DQA**. It has two scenarios, Locating and Building, constructed from two video games (Europa Universalis IV and Victoria 3) that have almost the same goal as Decision QA. To address Decision QA effectively, we also propose a new RAG technique called the *iterative plan-then-retrieval augmented generation* (**PlanRAG**). Our PlanRAG-based LM generates the plan for decision making as the first step, and the retriever generates the queries for data analysis as the second step. The proposed method outperforms the state-of-the-art iterative RAG method by 15.8% in the Locating scenario and by 7.4% in the Building scenario, respectively. We release our code and benchmark at https://github.com/myeon9h/PlanRAG.",10.18653/v1/2024.naacl-long.364,https://aclanthology.org/2024.naacl-long.364,NAACL HLT,"Lee, Myeonghwa; An, Seonho; Kim, Min-Soo",2024,34,"@inproceedings{2-1327,
  title={PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers},
  author={Lee, Myeonghwa and An, Seonho and Kim, Min-Soo},
  year={2024},
  doi={10.18653/v1/2024.naacl-long.364},
  booktitle={Proceedings of the NAACL HLT},
}",Dataset/Benchmark contributions,"Generic / Abstract / Domain-agnostic, Transportation / Mobility / Planning",Individual,"Analyzing, Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-133,aaai,Demo Alleviate: Demonstrating Artificial Intelligence Enabled Virtual Assistance for Telehealth: The Mental Health Case,"After the pandemic, artificial intelligence( AI) powered support for mental health care has become increasingly important. The breadth and complexity of significant challenges required to provide adequate care involve:( a) Personalized patient understanding, ( b) Safety-constrained and medically validated chatbot patient interactions, and( c) Support for continued feedback-based refinements in design using chatbot-patient interactions. We propose Alleviate, a chatbot designed to assist patients suffering from mental health challenges with personalized care and assist clinicians with understanding their patients better. Alleviate draws from an array of publicly available clinically valid mental-health texts and databases, allowing Alleviate to make medically sound and informed decisions. In addition, Alleviates modular design and explainable decision-making lends itself to robust and continued feedback-based refinements to its design. In this paper, we explain the different modules of Alleviate and submit a short video demonstrating Alleviates capabilities to help patients and clinicians understand each other better to facilitate optimal care strategies.",10.1609/aaai.v37i13.27085,https://ojs.aaai.org/index.php/AAAI/article/view/27085,AAAI Conference on Artificial Intelligence,Kaushik Roy;Vedant Khandelwal;Raxit Goswami;Nathan Dolbir;Jinendra Malekar;Amit Sheth,2024,26,"@inproceedings{2-133,
  title     = {Demo Alleviate: Demonstrating Artificial Intelligence Enabled Virtual Assistance for Telehealth: The Mental Health Case},
  author    = {Kaushik Roy and Vedant Khandelwal and Raxit Goswami and Nathan Dolbir and Jinendra Malekar and Amit Sheth},
  year      = {2024},
  doi       = {10.1609/aaai.v37i13.27085},
  booktitle = {AAAI Conference on Artificial Intelligence}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Explaining, Collaborating","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-1331,acl,Supporting Complaints Investigation for Nursing and Midwifery Regulatory Agencies,"Health professional regulators aim to protect the health and well-being of patients and the public by setting standards for scrutinising and overseeing the training and conduct of health and care professionals. A major task of such regulators is the investigation of complaints against practitioners. However, processing a complaint often lasts several months and is particularly costly. Hence, we worked with international regulators from different countries (the UK, US and Australia), to develop the first decision support tool that aims to help such regulators process complaints more efficiently. Our system uses state-of-the-art machine learning and natural language processing techniques to process complaints and predict their risk level. Our tool also provides additional useful information including explanations, to help the regulatory staff interpret the prediction results, and similar past cases as well as non-compliance to regulations, to support the decision making.",10.18653/v1/2021.acl-demo.10,https://aclanthology.org/2021.acl-demo.10,ACL-IJCNLP System Demonstrations,"Lertvittayakumjorn, Piyawat; Petej, Ivan; Gao, Yang; Krishnamurthy, Yamuna; Van Der Gaag, Anna; Jago, Robert; Stathis, Kostas",2021,21,"@inproceedings{2-1331,
  title = {Supporting Complaints Investigation for Nursing and Midwifery Regulatory Agencies},
  author = {Lertvittayakumjorn, Piyawat and Petej, Ivan and Gao, Yang and Krishnamurthy, Yamuna and Van Der Gaag, Anna and Jago, Robert and Stathis, Kostas},
  year = {2021},
  doi = {10.18653/v1/2021.acl-demo.10},
  booktitle = {ACL-IJCNLP System Demonstrations}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Organizational,"Advising, Forecasting","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-13310,elsevier,Real-time decision support for human–machine interaction in digital railway control rooms,"This study proposes a real-time Decision Support System (DSS) using machine learning to enhance proactive management of Human–Machine Interaction (HMI) in safety–critical digital control rooms. The DSS provides explainable predictions and recommendations regarding near-future automation usage, customized for the railway control room management, who supervise the operations of traffic controllers (TCs). In this setting, TCs decide on the spot whether to manually or automatically open signals to regulate railway traffic, a critical aspect of ensuring punctuality and safety. This time-setting specific HMI differs across TCs and is not yet supported by a data-driven tool. The proposed DSS includes agreement levels for predictions among different modeling paradigms: linear models, tree-based models, and deep neural networks. SHAP (SHapley Additive exPlanations) values are deployed to assess the agreement level in explainability between these different modeling paradigms. The prescriptions are based on the HMI of well-performing peers. We implement the DSS as proof of concept at the Belgian railway infrastructure company and report end-user feedback on the perception, the operational impact, and the inclusion of agreement levels.",https://doi.org/10.1016/j.dss.2024.114216,https://www.sciencedirect.com/science/article/pii/S0167923624000496,Decision Support Systems,Léon Sobrie;Marijn Verschelde,2024,0,"@article{2-13310,
  title={Real-time decision support for human--machine interaction in digital railway control rooms},
  author={Sobrie, L{\'e}on and Verschelde, Marijn},
  year={2024},
  journal={Decision Support Systems},
  doi={10.1016/j.dss.2024.114216}
}",System/Artifact contributions,Transportation / Mobility / Planning,Operational,"Explaining, Advising, Forecasting","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-1339,acl,"“You Gotta be a Doctor, Lin” : An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations","Social science research has shown that candidates with names indicative of certain races or genders often face discrimination in employment practices. Similarly, Large Language Models (LLMs) have demonstrated racial and gender biases in various applications. In this study, we utilize GPT-3.5-Turbo and Llama 3-70B-Instruct to simulate hiring decisions and salary recommendations for candidates with 320 first names that strongly signal their race and gender, across over 750,000 prompts. Our empirical results indicate a preference among these models for hiring candidates with White female-sounding names over other demographic groups across 40 occupations. Additionally, even among candidates with identical qualifications, salary recommendations vary by as much as 5% between different subgroups. A comparison with real-world labor data reveals inconsistent alignment with U.S. labor market characteristics, underscoring the necessity of risk investigation of LLM-powered systems.",10.18653/v1/2024.emnlp-main.413,https://aclanthology.org/2024.emnlp-main.413,Empirical Methods in Natural Language Processing,"Nghiem, Huy; Prindle, John; Zhao, Jieyu; Daumé Iii, Hal",2024,3,"@inproceedings{2-1339,
  title = {``You Gotta be a Doctor, Lin'' : An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations},
  author = {Nghiem, Huy and Prindle, John and Zhao, Jieyu and Daum{\'e} III, Hal},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-main.413},
  booktitle = {Empirical Methods in Natural Language Processing}
}",Empirical contributions,Everyday / Employment / Public Service,Operational,"Advising, Auditing","Decision-maker, Stakeholder, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-13416,elsevier,Reinforcement learning for facilitating human-robot-interaction in manufacturing,"For many contemporary manufacturing processes, autonomous robotic operators have become ubiquitous. Despite this, the number of human operators within these processes remains high, and as a consequence, the number of interactions between humans and robots has increased in this context. This is a problem, as human beings introduce a source of disturbance and unpredictability into these processes in the form of performance variation. Despite the natural human aptitude for flexibility, their presence remains a source of disturbance within the system and make modelling and optimization of these systems considerably more challenging, and in many cases impossible. Improving the ability of robotic operators to adapt their behaviour to variations in human task performance is, therefore, a significant challenge to be overcome to enable many ideas in the larger intelligent manufacturing paradigm to be realised. This work presents the development of a methodology to effectively model these systems and a reinforcement learning agent capable of autonomous decision-making. This decision-making provides the robotic operators with greater adaptability, by enabling its behaviour to change based on observed information, both of its environment and human colleagues. The work extends theoretical knowledge on how learning methods can be implemented for robotic control, and how the capabilities that they enable may be leveraged to improve the interaction between robots and their human counterparts. The work further presents a novel methodology for the implementation of a reinforcement learning-based intelligent agent which enables a change in behavioural policy in robotic operators in response to performance variation in their human colleagues. The development and evaluation are supported by a generalized simulation model, which is parameterized to enable appropriate variation in human performance. The evaluation demonstrates that the reinforcement agent can effectively learn to make adjustments to its behaviour based on the knowledge extracted from observed information, and balance the task demands to optimise these adjustments.",https://doi.org/10.1016/j.jmsy.2020.06.018,https://www.sciencedirect.com/science/article/pii/S0278612520301084,Journal of Manufacturing Systems,Harley Oliff;Ying Liu;Maneesh Kumar;Michael Williams;Michael Ryan,2020,154,"@article{2-13416,
  title = {Reinforcement learning for facilitating human-robot-interaction in manufacturing},
  author = {Harley Oliff and Ying Liu and Maneesh Kumar and Michael Williams and Michael Ryan},
  year = {2020},
  doi = {10.1016/j.jmsy.2020.06.018},
  journal = {Journal of Manufacturing Systems}
}",Algorithmic contributions,Manufacturing / Industry / Automation,Operational,"Executing, Collaborating","Developer, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-13461,elsevier,Removing order effects from human-classified datasets: a machine learning method to improve decision making systems,"Although recent developments in Artificial Intelligence (AI) and machine learning (ML) aim to enhance the fairness and transparency of decision-making systems, research has found that neural networks (or other similar AI techniques) are still effected by human cognitive biases due to the training datasets. In this study, we focus on order effects, i.e., when the input of information impacts human perception and the decisions resulting from this information. We propose the Order Effect Removal Method (OERM) for handling the order effect which leads to bias and for helping organizations remove these biases from their training datasets and, therefore, from automated decision-making systems. Using design science principles to theoretically create, test, and validate the method, we can eliminate the order bias even in basic classification systems. Furthermore, the method can be applied in a multidisciplinary context, where an AI-based algorithm substitutes for manual work.",https://doi.org/10.1016/j.dss.2022.113891,https://www.sciencedirect.com/science/article/pii/S0167923622001622,Decision Support Systems,Dmitry Romanov;Valentin Molokanov;Nikolai Kazantsev;Ashish Kumar Jha,2023,18,"@article{2-13461,
  title={Removing order effects from human-classified datasets: a machine learning method to improve decision making systems},
  author={Romanov, Dmitry and Molokanov, Valentin and Kazantsev, Nikolai and Jha, Ashish Kumar},
  year={2023},
  journal={Decision Support Systems},
  doi={10.1016/j.dss.2022.113891}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Institutional,"Forecasting, Advising, Auditing","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-13464,elsevier,Repelled at first sight? Expectations and intentions of job-seekers reading about ai selection in job advertisements,"Artificial intelligence (AI) is increasingly used in personnel selection to automate decision-making. Initial evidence points to negative effects of automating these procedures on applicant experiences. However, the effect of the prospect of automated procedures on job-seekers’ pre-process perceptions (e.g., organizational attractiveness) and intentions (to apply for the advertised job) is still unclear. We conducted three experiments (Study 1 and Study 2 as within-subjects designs, Study 3 as a between-subjects design; N1 = 36, N2 = 44, N3 = 172) systematically varying the information in job advertisements on the automation of different stages of the selection process (Study 1: screening stage conducted by a human vs. a non-specified agent vs. an AI; Study 2 and Study 3: human screening and human interview vs. AI screening and human interview vs. AI screening and AI interview). Results showed small negative effects of screening conducted by an AI vs. a human (Study 1, Study 2, Study 3), but stronger negative effects when also interviews were conducted by an AI vs. a human (Study 2, Study3) on job-seekers pre-process expectations, perceptions, and intentions. Possible reasons for these effects are discussed with special consideration of the different stages of the recruiting and selection process and explored with a qualitative approach in Study 2.",https://doi.org/10.1016/j.chb.2021.106931,https://www.sciencedirect.com/science/article/pii/S0747563221002545,Computers in Human Behavior,Jenny S. Wesche;Andreas Sonderegger,2021,80,"@article{2-13464,
  title={Repelled at first sight? Expectations and intentions of job-seekers reading about AI selection in job advertisements},
  author={Wesche, Jenny S. and Sonderegger, Andreas},
  year={2021},
  journal={Computers in Human Behavior},
  doi={10.1016/j.chb.2021.106931}
}",Empirical contributions,Everyday / Employment / Public Service,no such info,Executing,"Decision-subject, Decision-maker","Alter decision outcomes, Restrict human agency, Shape ethical norms",no such info,NA,NA,Autonomous System,Yes,Yes
2-13498,elsevier,Response transformation and profit decomposition for revenue uplift modeling,"Uplift models support decision-making in marketing campaign planning. Estimating the causal effect of a marketing treatment, an uplift model facilitates targeting marketing actions to responsive customers and efficient allocation of marketing budget. Research into uplift models focuses on conversion models to maximize incremental sales. The paper introduces uplift models for maximizing incremental revenues. If customers differ in their spending behavior, revenue maximization is a more plausible business objective compared to maximizing conversions. The proposed methodology entails a transformation of the prediction target, customer-level revenues, that facilitates implementing a causal uplift model using standard machine learning algorithms. The distribution of campaign revenues is typically zero-inflated because of many non-buyers. Remedies to this modeling challenge are incorporated in the proposed revenue uplift strategies in the form of two-stage models. Empirical experiments using real-world e-commerce data confirm the merits of the proposed revenue uplift strategy over relevant alternatives, including uplift models for conversion and recently developed causal machine learning algorithms. To quantify the degree to which improved targeting decisions raise return on marketing, the paper develops a decomposition of campaign profit. Applying the decomposition to a digital coupon targeting campaign, the paper provides evidence that revenue uplift modeling, as well as causal machine learning, can improve campaign profit substantially.",https://doi.org/10.1016/j.ejor.2019.11.030,https://www.sciencedirect.com/science/article/pii/S0377221719309415,European Journal of Operational Research,Robin M. Gubela;Stefan Lessmann;Szymon Jaroszewicz,2020,5,"@article{2-13498,
  title={Response transformation and profit decomposition for revenue uplift modeling},
  author={Gubela, Robin M. and Lessmann, Stefan and Jaroszewicz, Szymon},
  year={2020},
  doi={10.1016/j.ejor.2019.11.030},
  journal={European Journal of Operational Research}
}",Methodological contributions,Finance / Business / Economy,Organizational,"Advising, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-13500,elsevier,Responsible machine learning for united states air force pilot candidate selection,"The United States Air Force (USAF) continues to be plagued by a chronic pilot shortage, one that could be exacerbated by an accompanying shortfall in the commercial airlines. As a result, efforts have increased to alleviate this shortage by finding methods to reduce pilot training attrition. We contribute to these efforts by setting forth a decision support system (DSS) for pilot candidate selection using modern machine learning techniques. In view of the recent Responsible Artificial Intelligence Strategy published by the United States Department of Defense, this research leverages interpretable and explainable machine learning methods to create traceable and equitable models that may be responsibly and reliably governed. These models are used to regress candidates’ average merit assignment selection system scores based on information available for selection and prior to training. More specifically, using data provided by the USAF from 2010 to 2018, this paper develops and analyzes multiple interpretable models based on Gaussian Bayesian networks, as well as multiple black-box models rendered explainable by SHAP values and conformal prediction. A preferred pair of interpretable and explainable models is selected and embedded within a DSS for USAF pilot candidate selection boards: the Air Force Pilot Applicant Selection System. The utilization of this DSS is explored, the analyses it enables are discussed, and relevant USAF policymaking issues are examined.",https://doi.org/10.1016/j.dss.2024.114198,https://www.sciencedirect.com/science/article/pii/S0167923624000319,Decision Support Systems,Devin Wasilefsky;William N. Caballero;Chancellor Johnstone;Nathan Gaw;Phillip R. Jenkins,2024,13,"@article{2-13500,
  title={Responsible machine learning for United States Air Force pilot candidate selection},
  author={Wasilefsky, Devin and Caballero, William N. and Johnstone, Chancellor and Gaw, Nathan and Jenkins, Phillip R.},
  year={2024},
  journal={Decision Support Systems},
  doi={10.1016/j.dss.2024.114198}
}",System/Artifact contributions,Everyday / Employment / Public Service,Operational,"Explaining, Forecasting, Analyzing","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-13518,elsevier,Retrospective evaluation of artificial intelligence leveraging free-text imaging order entry to facilitate federally required clinical decision support,"Objective The Protecting Access to Medicare Act mandates clinical decision support (CDS) at imaging order entry, necessitating the use of structured indications to map CDS scores. We evaluated the performance of a commercially available artificial intelligence (AI) tool leveraging free-text order entry to facilitate provider selection of the necessary structured indications. Methods Our institution implemented an AI tool offering predicted structured indications based upon the ordering provider’s entry of a free-text reason for examination. Providers remained able to order via the traditional direct search for structured indications. Alternatively, they could take the new free-text–AI approach allowing them to select from AI-predicted indications, perform additional direct searches, indicate no matching indication, or exit CDS workflow. We hypothesized the free-text–AI approach would be elected more often and the AI tool would be successful in facilitating selection of structured indications. We reviewed advanced imaging orders (n = 40,053) for the first 3 months (February to May 2020) since implementation. Results Providers were more likely (P < .001) to choose the free-text–AI approach (23,580; 58.9%) to order entry over direct search for structured indications (16,473; 41.1%). The AI tool yielded alerts with predicted indications in 91.7% (n = 21,631) of orders with free text. Ultimately, providers chose AI-predicted indications in 57.7% (n = 12,490) of cases in which they were offered by the tool. Discussion Providers significantly more often elected the new free-text–AI approach to order entry for CDS, suggesting provider preference over the traditional approach. The AI tool commonly predicted indications acceptable to ordering providers.",https://doi.org/10.1016/j.jacr.2021.08.021,https://www.sciencedirect.com/science/article/pii/S1546144021007390,Journal of the American College of Radiology,David S. Gish;Amy L. Ellenbogen;James T. Patrie;Cree M. Gaskin,2021,5,"@article{2-13518,
  title = {Retrospective evaluation of artificial intelligence leveraging free-text imaging order entry to facilitate federally required clinical decision support},
  author = {David S. Gish and Amy L. Ellenbogen and James T. Patrie and Cree M. Gaskin},
  year = {2021},
  doi = {10.1016/j.jacr.2021.08.021},
  journal = {Journal of the American College of Radiology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Knowledge provider","Change cognitive demands, Alter decision outcomes","Update AI competence, Shape AI for accountability",predicted structured indications,NA,"Textual, Interactive interface",Yes,Yes
2-13548,elsevier,Rise of the machines: delegating decisions to autonomous ai,"Delegation is an important part of organizational success and can be used to overcome personal shortcomings and draw upon the expertise and abilities of others. However, delegation comes with risks and uncertainties, as it entails a transfer of power and loss of control. Indeed, research has documented that people tend to under-delegate to other humans, often leading to poor decisions and ultimately negative economic consequences. Today, however, people are faced with a new delegation choice: Artificial Intelligence (AI). Fueled by Big Data, AI is rapidly becoming more intelligent and frequently outperforming human forecasters and decision-makers. Given this evolution of computational autonomy, researchers need to revisit the hows and whys of decision delegation and clarify not only whether people are willing to cede control to AI agents but also whether AI can reduce the under-delegation that is especially pronounced when people are faced with decisions that spur a high desire for control. By linking research on decision delegation, social risk, and control premium to the emerging field of trust in AI, we propose and find that people prefer to delegate decisions to AI as compared to human agents, especially when decisions entail losses (Studies 1–3). Results further illuminate the underlying psychological process involved (Study 1 and 2) and show that process transparency increases delegation to humans but not to AI (Study 3). These findings have important implications for research on trust in AI and the applicability of autonomous AI systems for managers and decision makers.",https://doi.org/10.1016/j.chb.2022.107308,https://www.sciencedirect.com/science/article/pii/S0747563222001303,Computers in Human Behavior,Cindy Candrian;Anne Scherer,2022,2,"@article{2-13548,
  title = {Rise of the machines: Delegating decisions to autonomous AI},
  author = {Cindy Candrian and Anne Scherer},
  year = {2022},
  doi = {10.1016/j.chb.2022.107308},
  journal = {Computers in Human Behavior}
}",Empirical contributions,"Everyday / Employment / Public Service, Generic / Abstract / Domain-agnostic",Institutional,"Executing, Forecasting",Decision-maker,"Alter decision outcomes, Restrict human agency, Change trust, Change affective-perceptual",no such info,NA,"delegation, desire for control, more willing to delegate decisions to AI especially in the domain of losses",Autonomous System,Yes,Yes
2-13734,elsevier,Sentiment analysis based multi-person multi-criteria decision making methodology using natural language processing and deep learning for smarter decision aid. Case study of restaurant choice using tripadvisor reviews,"Decision making models are constrained by taking the expert evaluations with pre-defined numerical or linguistic terms. We claim that the use of sentiment analysis will allow decision making models to consider expert evaluations in natural language. Accordingly, we propose the Sentiment Analysis based Multi-person Multi-criteria Decision Making (SA-MpMcDM) methodology for smarter decision aid, which builds the expert evaluations from their natural language reviews, and even from their numerical ratings if they are available. The SA-MpMcDM methodology incorporates an end-to-end multi-task deep learning model for aspect based sentiment analysis, named DOC-ABSADeepL model, able to identify the aspect categories mentioned in an expert review, and to distill their opinions and criteria. The individual evaluations are aggregated via the procedure named criteria weighting through the attention of the experts. We evaluate the methodology in a case study of restaurant choice using TripAdvisor reviews, hence we build, manually annotate, and release the TripR-2020 dataset of restaurant reviews. We analyze the SA-MpMcDM methodology in different scenarios using and not using natural language and numerical evaluations. The analysis shows that the combination of both sources of information results in a higher quality preference vector.",https://doi.org/10.1016/j.inffus.2020.10.019,https://www.sciencedirect.com/science/article/pii/S1566253520304000,Information Fusion,Cristina Zuheros;Eugenio Martínez-Cámara;Enrique Herrera-Viedma;Francisco Herrera,2021,115,"@article{2-13734,
  title = {Sentiment analysis based multi-person multi-criteria decision making methodology using natural language processing and deep learning for smarter decision aid. Case study of restaurant choice using tripadvisor reviews},
  author = {Cristina Zuheros and Eugenio Martínez-Cámara and Enrique Herrera-Viedma and Francisco Herrera},
  year = {2021},
  doi = {10.1016/j.inffus.2020.10.019},
  journal = {Information Fusion}
}",Methodological contributions,"Everyday / Employment / Public Service, Generic / Abstract / Domain-agnostic",Individual,"Analyzing, Advising","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-13829,elsevier,Skin lesions of face and scalp – classification by a market-approved convolutional neural network in comparison with 64 dermatologists,"Background The clinical differentiation of face and scalp lesions (FSLs) is challenging even for trained dermatologists. Studies comparing the diagnostic performance of a convolutional neural network (CNN) with dermatologists in FSL are lacking. Methods A market-approved CNN (Moleanalyzer-Pro, FotoFinder Systems) was used for binary classifications of 100 dermoscopic images of FSL. The same lesions were used in a two-level reader study including 64 dermatologists (level I: dermoscopy only; level II: dermoscopy, clinical close-up images, textual information). Primary endpoints were the CNN's sensitivity and specificity in comparison with the dermatologists' management decisions in level II. Generalizability of the CNN results was tested by using four additional external data sets. Results The CNN's sensitivity, specificity and ROC AUC were 96.2% [87.0%–98.9%], 68.8% [54.7%–80.1%] and 0.929 [0.880–0.978], respectively. In level II, the dermatologists' management decisions showed a mean sensitivity of 84.2% [82.2%–86.2%] and specificity of 69.4% [66.0%–72.8%]. When fixing the CNN's specificity at the dermatologists' mean specificity (69.4%), the CNN's sensitivity (96.2% [87.0%–98.9%]) was significantly higher than that of dermatologists (84.2% [82.2%–86.2%]; p < 0.001). Dermatologists of all training levels were outperformed by the CNN (all p < 0.001). In confirmation, the CNN's accuracy (83.0%) was significantly higher than dermatologists' accuracies in level II management decisions (all p < 0.001). The CNN's performance was largely confirmed in three additional external data sets but particularly showed a reduced specificity in one Australian data set including FSL on severely sun-damaged skin. Conclusions When applied as an assistant system, the CNN's higher sensitivity at an equivalent specificity may result in an improved early detection of face and scalp skin cancers.",https://doi.org/10.1016/j.ejca.2020.11.034,https://www.sciencedirect.com/science/article/pii/S095980492031371X,European Journal of Cancer,Holger Andreas Haenssle;Julia Katharina Winkler;Christine Fink;Ferdinand Toberer;Alexander Enk;Wilhelm Stolz;Teresa Deinlein;Rainer Hofmann-Wellenhof;Harald Kittler;Philipp Tschandl;Cliff Rosendahl;Aimilios Lallas;Andreas Blum;Mohamed Souhayel Abassi;Luc Thomas;Isabelle Tromme;Albert Rosenberger;Marie Bachelerie;Sonali Bajaj;Alise Balcere;Sophie Baricault;Clément Barthaux;Yvonne Beckenbauer;Ines Bertlich;Andreas Blum;Marie-France Bouthenet;Sophie Brassat;Philipp Marcel Buck;Kristina Buder-Bakhaya;Maria-Letizia Cappelletti;Cécile Chabbert;Julie {De Labarthe};Eveline DeCoster;Teresa Deinlein;Michèle Dobler;Daphnée Dumon;Steffen Emmert;Julie Gachon-Buffet;Mikhail Gusarov;Franziska Hartmann;Julia Hartmann;Anke Herrmann;Isabelle Hoorens;Eva Hulstaert;Raimonds Karls;Andreea Kolonte;Christian Kromer;Aimilios Lallas;Céline {Le Blanc Vasseux};Annabelle Levy-Roy;Pawel Majenka;Marine Marc;Veronique Martin Bourret;Nadège Michelet-Brunacci;Christina Mitteldorf;Jean Paroissien;Camille Picard;Diana Plise;Valérie Reymann;Fabrice Ribeaudeau;Pauline Richez;Hélène Roche Plaine;Deborah Salik;Elke Sattler;Sarah Schäfer;Roland Schneiderbauer;Thierry Secchi;Karen Talour;Lukas Trennheuser;Alexander Wald;Priscila Wölbing;Pascale Zukervar,2021,50,"@article{2-13829,
  title        = {Skin lesions of face and scalp -- classification by a market-approved convolutional neural network in comparison with 64 dermatologists},
  author       = {Holger Andreas Haenssle and Julia Katharina Winkler and Christine Fink and Ferdinand Toberer and Alexander Enk and Wilhelm Stolz and Teresa Deinlein and Rainer Hofmann-Wellenhof and Harald Kittler and Philipp Tschandl and Cliff Rosendahl and Aimilios Lallas and Andreas Blum and Mohamed Souhayel Abassi and Luc Thomas and Isabelle Tromme and Albert Rosenberger and Marie Bachelerie and Sonali Bajaj and Alise Balcere and Sophie Baricault and Clément Barthaux and Yvonne Beckenbauer and Ines Bertlich and Andreas Blum and Marie-France Bouthenet and Sophie Brassat and Philipp Marcel Buck and Kristina Buder-Bakhaya and Maria-Letizia Cappelletti and Cécile Chabbert and Julie {De Labarthe} and Eveline DeCoster and Teresa Deinlein and Michèle Dobler and Daphnée Dumon and Steffen Emmert and Julie Gachon-Buffet and Mikhail Gusarov and Franziska Hartmann and Julia Hartmann and Anke Herrmann and Isabelle Hoorens and Eva Hulstaert and Raimonds Karls and Andreea Kolonte and Christian Kromer and Aimilios Lallas and Céline {Le Blanc Vasseux} and Annabelle Levy-Roy and Pawel Majenka and Marine Marc and Veronique Martin Bourret and Nadège Michelet-Brunacci and Christina Mitteldorf and Jean Paroissien and Camille Picard and Diana Plise and Valérie Reymann and Fabrice Ribeaudeau and Pauline Richez and Hélène Roche Plaine and Deborah Salik and Elke Sattler and Sarah Schäfer and Roland Schneiderbauer and Thierry Secchi and Karen Talour and Lukas Trennheuser and Alexander Wald and Priscila Wölbing and Pascale Zukervar},
  year         = {2021},
  doi          = {10.1016/j.ejca.2020.11.034},
  journal      = {European Journal of Cancer}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Forecasting,"Decision-maker, Guardian, Knowledge provider",Alter decision outcomes,Update AI competence,NA,NA,Autonomous System,Yes,Yes
2-13990,elsevier,Stock market one-day ahead movement prediction using disparate data sources,"There are several commercial financial expert systems that can be used for trading on the stock exchange. However, their predictions are somewhat limited since they primarily rely on time-series analysis of the market. With the rise of the Internet, new forms of collective intelligence (e.g. Google and Wikipedia) have emerged, representing a new generation of “crowd-sourced” knowledge bases. They collate information on publicly traded companies, while capturing web traffic statistics that reflect the public’s collective interest. Google and Wikipedia have become important “knowledge bases” for investors. In this research, we hypothesize that combining disparate online data sources with traditional time-series and technical indicators for a stock can provide a more effective and intelligent daily trading expert system. Three machine learning models, decision trees, neural networks and support vector machines, serve as the basis for our “inference engine”. To evaluate the performance of our expert system, we present a case study based on the AAPL (Apple NASDAQ) stock. Our expert system had an 85% accuracy in predicting the next-day AAPL stock movement, which outperforms the reported rates in the literature. Our results suggest that: (a) the knowledge base of financial expert systems can benefit from data captured from nontraditional “experts” like Google and Wikipedia; (b) diversifying the knowledge base by combining data from disparate sources can help improve the performance of financial expert systems; and (c) the use of simple machine learning models for inference and rule generation is appropriate with our rich knowledge database. Finally, an intelligent decision making tool is provided to assist investors in making trading decisions on any stock, commodity or index.",https://doi.org/10.1016/j.eswa.2017.02.041,https://www.sciencedirect.com/science/article/pii/S0957417417301331,Expert Systems with Applications,Bin Weng;Mohamed A. Ahmed;Fadel M. Megahed,2017,316,"@article{2-13990,
  title={Stock market one-day ahead movement prediction using disparate data sources},
  author={Weng, Bin and Ahmed, Mohamed A. and Megahed, Fadel M.},
  year={2017},
  doi={10.1016/j.eswa.2017.02.041},
  journal={Expert Systems with Applications}
}",System/Artifact contributions,Finance / Business / Economy,"Individual, Operational","Forecasting, Analyzing, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-14079,elsevier,Swarm intelligence goal-oriented approach to data-driven innovation in customer churn management,"One type of data-driven innovations in management is data-driven decision making. Confronted with a big amount of data external and internal to their organization's managers strive for predictive data analysis that enables insight into the future, but even more for prescriptive ones that use algorithms to prepare recommendations for current and future actions. Most of the decision-making techniques use deterministic machine learning (ML) techniques but unfortunately, they do not take into account the variety and volatility of decision-making situations and do not allow for a more flexible approach, i.e., adjusted to changing environmental conditions or changing management priorities. A way to better adapt ML tools to the needs of decision-makers is to use swarm intelligence ML (SIML) methods that provide a set of alternative solutions that allow matching actions with the current decision-making situation. Thus, applying SIML methods in managerial decision-making is conceptualized as a company capability as it allows for systematic alignment of allocating resources decisions vis-à -vis changing decision-making conditions. The study focuses on the customer churn management as the area of applying SIML techniques to managerial decision-making. The objectives are twofold: to present the specific features and the role of SIML methods in customer churn management and to test if a modified SIML algorithm may increase the effectiveness of churn-related segmentation and improve decision-making process. The empirical study uses publicly available customer data related to digital markets to test if and how SIML methods facilitate managerial decision-making with regard to customers potentially leaving the company in the context of changing conditions. The research results are discussed with regard to prior studies on applying ML techniques to decision-making and customer churn management studies. We also discuss the place of presented analytical approach in the literature on dynamic capabilities, especially big data-driven capabilities.",https://doi.org/10.1016/j.ijinfomgt.2021.102357,https://www.sciencedirect.com/science/article/pii/S0268401221000505,International Journal of Information Management,Jan Kozak;Krzysztof Kania;Przemysław Juszczuk;Maciej Mitręga,2021,59,"@article{2-14079,
  title={Swarm intelligence goal-oriented approach to data-driven innovation in customer churn management},
  author={Kozak, Jan and Kania, Krzysztof and Juszczuk, Przemysław and Mitręga, Maciej},
  year={2021},
  doi={10.1016/j.ijinfomgt.2021.102357},
  journal={International Journal of Information Management}
}",Empirical contributions,Finance / Business / Economy,Operational,"Forecasting, Analyzing, Advising","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-14204,elsevier,The effects of domain knowledge on trust in explainable ai and task performance: a case of peer-to-peer lending,"Increasingly, artificial intelligence (AI) is being used to assist complex decision-making such as financial investing. However, there are concerns regarding the black-box nature of AI algorithms. The field of explainable AI (XAI) has emerged to address these concerns. XAI techniques can reveal how an AI decision is formed and can be used to understand and appropriately trust an AI system. However, XAI techniques still may not be human-centred and may not support human decision-making adequately. In this work, we explored how domain knowledge, identified by expert decision makers, can be used to achieve a more human-centred approach to AI. We measured the effect of domain knowledge on trust in AI, reliance on AI, and task performance in an AI-assisted complex decision-making environment. In a peer-to-peer lending simulator, non-expert participants made financial investments using an AI assistant. The presence or absence of domain knowledge was manipulated. The results showed that participants who had access to domain knowledge relied less on the AI assistant when the AI assistant was incorrect and indicated less trust in AI assistant. However, overall investing performance was not affected. These results suggest that providing domain knowledge can influence how non-expert users use AI and could be a powerful tool to help these users develop appropriate levels of trust and reliance.",https://doi.org/10.1016/j.ijhcs.2022.102792,https://www.sciencedirect.com/science/article/pii/S1071581922000210,International Journal of Human-Computer Studies,Murat Dikmen;Catherine Burns,2022,130,"@article{2-14204,
  title = {The effects of domain knowledge on trust in explainable AI and task performance: a case of peer-to-peer lending},
  author = {Murat Dikmen and Catherine Burns},
  year = {2022},
  journal = {International Journal of Human-Computer Studies},
  doi = {10.1016/j.ijhcs.2022.102792}
}",Empirical contributions,Finance / Business / Economy,Individual,"Explaining, Advising",Decision-maker,"Change trust, Alter decision outcomes, Shift responsibility",Update AI competence,prediction of alternative,domain knowledge,Interactive interface,Yes,Yes
2-14205,elsevier,"The effects of explainability and causability on perception, trust, and acceptance: implications for explainable ai","Artificial intelligence and algorithmic decision-making processes are increasingly criticized for their black-box nature. Explainable AI approaches to trace human-interpretable decision processes from algorithms have been explored. Yet, little is known about algorithmic explainability from a human factors’ perspective. From the perspective of user interpretability and understandability, this study examines the effect of explainability in AI on user trust and attitudes toward AI. It conceptualizes causability as an antecedent of explainability and as a key cue of an algorithm and examines them in relation to trust by testing how they affect user perceived performance of AI-driven services. The results show the dual roles of causability and explainability in terms of its underlying links to trust and subsequent user behaviors. Explanations of why certain news articles are recommended generate users trust whereas causability of to what extent they can understand the explanations affords users emotional confidence. Causability lends the justification for what and how should be explained as it determines the relative importance of the properties of explainability. The results have implications for the inclusion of causability and explanatory cues in AI systems, which help to increase trust and help users to assess the quality of explanations. Causable explainable AI will help people understand the decision-making process of AI algorithms by bringing transparency and accountability into AI systems.",https://doi.org/10.1016/j.ijhcs.2020.102551,https://www.sciencedirect.com/science/article/pii/S1071581920301531,International Journal of Human-Computer Studies,Donghee Shin,2021,1414,"@article{2-14205,
  title = {The effects of explainability and causability on perception, trust, and acceptance: implications for explainable AI},
  author = {Donghee Shin},
  year = {2021},
  journal = {International Journal of Human-Computer Studies},
  volume = {},
  number = {},
  pages = {},
  doi = {10.1016/j.ijhcs.2020.102551}
}",Empirical contributions,"Media / Communication / Entertainment, Generic / Abstract / Domain-agnostic",Individual,"Explaining, Advising",Decision-maker,"Change trust, Shape ethical norms",no such info,explanations,NA,"Textual, Visual, Conversational/Natural Language",Yes,Yes
2-14210,elsevier,The existence of manual mode increases human blame for ai mistakes,"People are offloading many tasks to artificial intelligence (AI)—including driving, investing decisions, and medical choices—but it is human nature to want to maintain ultimate control. So even when using autonomous machines, people want a “manual mode”, an option that shifts control back to themselves. Unfortunately, the mere existence of manual mode leads to more human blame when AI makes mistakes. When observers know that a human agent theoretically had the option to take control, the humans are assigned more responsibility, even when agents lack the time or ability to actually exert control, as with self-driving car crashes. Four experiments reveal that though people prefer having a manual mode, even if the AI mode is more efficient and adding the manual mode is more expensive (Study 1), the existence of a manual mode increases human blame (Studies 2a-3c). We examine two mediators for this effect: increased perceptions of causation and counterfactual cognition (Study 4). The results suggest that the human thirst for illusory control comes with real costs. Implications of AI decision-making are discussed.",https://doi.org/10.1016/j.cognition.2024.105931,https://www.sciencedirect.com/science/article/pii/S0010027724002178,Cognition,Mads N. Arnestad;Samuel Meyers;Kurt Gray;Yochanan E. Bigman,2024,11,"@article{2-14210,
  title = {The existence of manual mode increases human blame for AI mistakes},
  author = {Mads N. Arnestad and Samuel Meyers and Kurt Gray and Yochanan E. Bigman},
  year = {2024},
  journal = {Cognition},
  doi = {10.1016/j.cognition.2024.105931}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,Executing,"Developer, Decision-maker, Decision-subject","Shift responsibility, Change affective-perceptual, Restrict human agency",no such info,NA,delegation,Autonomous System,Yes,Yes
2-14239,elsevier,The importance of the assurance that “humans are still in the decision loop” for public trust in artificial intelligence: evidence from an online experiment,"This study investigates the public's initial trust in an artificial intelligence (AI) decision aid utilized in the delivery of public services. Amidst societal anxiety surrounding AI, the study posited that the information communicated to the public about the use of AI matters to the public's initial trust in AI. More specifically, the study hypothesized that an assurance that “humans are still in the decision loop” (HDL) makes a difference in the public's initial trust (H1), which might also depend on the stated purposes for using AI (H2). This article reports on the results from an online experiment testing these hypotheses in the context of Japan's long-term nursing care sector, based on the responses of care users and their families (N = 1542). The study did not find strong evidence to support H2. However, it found some support for H1: the proportion of those who trusted a care plan prepared with AI assistance more than a care plan not involving AI was higher by 8.95 percentage points with the HDL assurance than without. This highlights the importance of the HDL assurance and reveals respondents' reservations about a complete AI takeover in care planning.",https://doi.org/10.1016/j.chb.2020.106572,https://www.sciencedirect.com/science/article/pii/S0747563220303204,Computers in Human Behavior,Naomi Aoki,2021,122,"@article{2-14239,
  title = {The importance of the assurance that ""humans are still in the decision loop"" for public trust in artificial intelligence: evidence from an online experiment},
  author = {Naomi Aoki},
  year = {2021},
  journal = {Computers in Human Behavior},
  doi = {https://doi.org/10.1016/j.chb.2020.106572}
}",Empirical contributions,Everyday / Employment / Public Service,Operational,"Advising, Collaborating","Decision-maker, Decision-subject",Change trust,no such info,AI suggestions,NA,Textual,Yes,Yes
2-14277,elsevier,The precursors of ai adoption in business: towards an efficient decision-making and functional performance,"Artificial intelligence (AI) is a highly effective solution for enhancing decision-making efficiency and optimising the functional performance of organisations. However, there have been limited attempts to assess the consequences of implementing AI systems on the quality and efficiency of decision-making. This study proposes and empirically examines an extended model covering all aspects that would shape the successful adoption of AI by decision-makers while investigating how the successful adoption of AI enhances the efficiency of the decision-making process. This study also intends to test the validity of the integrated AI acceptance-avoidance model (IAAAM) proposed by Cao et al. (2021) using the Middle East context (i.e. Saudi Arabia). The extended model of the current study was based on the IAAAM and IS professional distinctiveness (ISPD). Two quantitative studies were conducted to achieve the research objectives. The first study was conducted to validate the IAAAM using a purposive sample of employees (non-adopters of AI applications). The second study tested the proposed model using a purposive sample of employees (actual adopters). The structural equation modelling (SEM) results of the first study (non-adopters) supported the validity of the IAAAM in Saudi Arabia. Factors (performance expectancy (PE), facilitating conditions (FC), personal well-being concern (PWC), perceived threat (PT), and attitudes (ATT)) had a significant impact on either ATT or the intention to use AI. The SEM results of actual adopters supported the impact of PE, EE, FC, PWC, and ATT on either ATT or the adoption of AI (AoAI). As an external factor, the ISPD was the most significant predictor of AoAI. The AoAI was confirmed to strongly predict decision-making efficiency, which, in turn, contributes to functional performance. This study enriches the current understanding of the main factors that contribute to the successful implementation of AI systems, offering an in-depth understanding of both AI adopters and non-adopters. It identifies factors important to non-users to enhance future adoption, whereas current AI users focus on improving decision-making quality with the AI assistance.",https://doi.org/10.1016/j.ijinfomgt.2023.102745,https://www.sciencedirect.com/science/article/pii/S0268401223001263,International Journal of Information Management,Abdullah M. Baabdullah,2024,76,"@article{2-14277,
  title={The precursors of AI adoption in business: Towards an efficient decision-making and functional performance},
  author={Baabdullah, Abdullah M.},
  year={2024},
  doi={10.1016/j.ijinfomgt.2023.102745},
  journal={International Journal of Information Management}
}",Empirical contributions,Finance / Business / Economy,Organizational,"Advising, Executing",Decision-maker,"Change trust, Change cognitive demands, Change affective-perceptual, Restrict human agency, Shift responsibility, Shape ethical norms","Update AI competence, Change AI responses, Shape AI for accountability",prediction of alternative,NA,"Textual, Visual, Interactive interface",Yes,Yes
2-14283,elsevier,The rationality of explanation or human capacity? Understanding the impact of explainable artificial intelligence on human-ai trust and decision performance,"Artificial intelligence models can process massive amounts of data and surpass human experts in predictions. However, the lack of trust in algorithms sealed in the “black box” is one of the most challenging barriers to taking advantage of AI in human decision-making. Improving algorithm transparency by presenting explanations is one of the most common approaches to curing this. Explainable artificial intelligence (XAI) has been a recent research focus, but most concentrate on explainable algorithm development rather than human factors. Thus, the objective of this study is twofold: (1) to explore whether or not XAI can improve human performance and trust in AI in the competitive tasks of sales prediction, and (2) to reveal the different impact routines XAI on individuals with different task-related capacities. Based on a quasi-experimental study, our results indicate that XAI can improve human decision accuracy in the scenario of sales prediction in cross-border e-commerce. XAI cannot improve self-report trust to AI but can improve behavioral trust. We also found the placebo effect of explanation for relatively low task-related capacity.",https://doi.org/10.1016/j.ipm.2024.103732,https://www.sciencedirect.com/science/article/pii/S030645732400092X,Information Processing & Management,Ping Wang;Heng Ding,2024,32,"@article{2-14283,
  title={The rationality of explanation or human capacity? Understanding the impact of explainable artificial intelligence on human-ai trust and decision performance},
  author={Wang, Ping and Ding, Heng},
  year={2024},
  journal={Information Processing \& Management},
  doi={10.1016/j.ipm.2024.103732}
}",Empirical contributions,Finance / Business / Economy,Operational,"Explaining, Forecasting, Advising","Decision-maker, Knowledge provider","Change trust, Alter decision outcomes",no such info,"prediction of alternative, visual explanations, randomly generated explanations",NA,Interactive interface,Yes,Yes
2-14300,elsevier,"The social consequences of machine allocation behavior: fairness, interpersonal perceptions and performance","Machines increasingly decide over the allocation of resources or tasks among people resulting in what we call Machine Allocation Behavior. People respond strongly to how other people or machines allocate resources. However, the implications for human relationships of algorithmic allocations of, for example, tasks among crowd workers, annual bonuses among employees, or a robot’s gaze among members of a group entering a store remains unclear. We leverage a novel research paradigm to study the impact of machine allocation behavior on fairness perceptions, interpersonal perceptions, and individual performance. In a 2 × 3 between-subject design that manipulates how the allocation agent is presented (human vs. artificial intelligent [AI] system) and the allocation type (receiving less vs. equal vs. more resources), we find that group members who receive more resources perceive their counterpart as less dominant when the allocation originates from an AI as opposed to a human. Our findings have implications on our understanding of the impact of machine allocation behavior on interpersonal dynamics and on the way in which we understand human responses towards this type of machine behavior.",https://doi.org/10.1016/j.chb.2022.107628,https://www.sciencedirect.com/science/article/pii/S0747563222004484,Computers in Human Behavior,Houston Claure;Seyun Kim;René F. Kizilcec;Malte Jung,2023,24,"@article{2-14300,
  title        = {The social consequences of machine allocation behavior: fairness, interpersonal perceptions and performance},
  author       = {Houston Claure and Seyun Kim and René F. Kizilcec and Malte Jung},
  year         = {2023},
  doi          = {10.1016/j.chb.2022.107628},
  journal      = {Computers in Human Behavior}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,Executing,"Knowledge provider, Decision-subject","Restrict human agency, Shape ethical norms, Alter decision outcomes, Change affective-perceptual",no such info,NA,NA,Interactive interface,Yes,Yes
2-1431,acl,Automatic Detection of Generated Text is Easiest when Humans are Fooled,"Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies—top-_k_, nucleus sampling, and untruncated random sampling—and show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems.",10.18653/v1/2020.acl-main.164,https://aclanthology.org/2020.acl-main.164,Annual Meeting of the Association for Computational Linguistics,"Ippolito, Daphne; Duckworth, Daniel; Callison-Burch, Chris; Eck, Douglas",2020,509,"@inproceedings{2-1431,
  title     = {Automatic Detection of Generated Text is Easiest when Humans are Fooled},
  author    = {Ippolito, Daphne and Duckworth, Daniel and Callison-Burch, Chris and Eck, Douglas},
  year      = {2020},
  booktitle = {Annual Meeting of the Association for Computational Linguistics},
  doi       = {10.18653/v1/2020.acl-main.164}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Individual,Executing,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-14342,elsevier,Threat of racial and economic inequality increases preference for algorithm decision-making,"Artificial intelligence (AI) algorithms hold promise to reduce inequalities across race and socioeconomic status. One of the most important domains of racial and economic inequalities is medical outcomes; Black and low-income people are more likely to die from many diseases. Algorithms can help reduce these inequalities because they are less likely than human doctors to make biased decisions. Unfortunately, people are generally averse to algorithms making important moral decisions—including in medicine—undermining the adoption of AI in healthcare. Here we use the COVID-19 pandemic to examine whether the threat of racial and economic inequality increases the preference for algorithm decision-making. Four studies (N = 2819) conducted in the United States and Singapore show that emphasizing inequality in medical outcomes increases the preference for algorithm decision-making for triage decisions. These studies suggest that one way to increase the acceptance of AI in healthcare is to emphasize the threat of inequality and its negative outcomes associated with human decision-making.",https://doi.org/10.1016/j.chb.2021.106859,https://www.sciencedirect.com/science/article/pii/S0747563221001825,Computers in Human Behavior,Yochanan E. Bigman;Kai Chi Yam;Déborah Marciano;Scott J. Reynolds;Kurt Gray,2021,2,"@article{2-14342,
  title = {Threat of Racial and Economic Inequality Increases Preference for Algorithm Decision-Making},
  author = {Yochanan E. Bigman and Kai Chi Yam and Déborah Marciano and Scott J. Reynolds and Kurt Gray},
  year = {2021},
  doi = {10.1016/j.chb.2021.106859},
  journal = {Computers in Human Behavior}
}",Empirical contributions,Healthcare / Medicine / Surgery,no such info,Executing,Decision-maker,"Change trust, Change affective-perceptual",no such info,NA,the threat of racial and economic inequality,"Textual, Conversational/Natural Language",Yes,Yes
2-14354,elsevier,Three-way decision support for diagnosis on focal liver lesions,"Malignant Focal Liver Lesion (FLL) is a main cause of primary liver cancer. In most existing Computer-Aided Diagnosis (CAD) systems of FLLs, machine learning and data mining methods have been widely applied to classify liver CT images for diagnostic decision making. However, these strategies of automatic decision support depend on data-driven classification methods and may lead to risky diagnosis on uncertain medical cases. To tackle the drawback, we expect to integrate the objective judgments from classification algorithms and the subjective judgments from human expert experiences, and propose a data-human-driven Three-way Decision Support for FLL diagnosis. The methodology of three-way decision support is motivated by Three-way Decision (3WD) theory. It tri-partitions the FLL medical records into certain benign, certain malignant and uncertain cases. The certain cases are automatically classified by decision rules and the challenging uncertain cases will be carefully diagnosed by human experts. Therefore, the method of three-way decision support can balance well the risk and efficiency of decision making. The workflow of three-way decision support for FLL diagnosis includes the stages of semantic feature extraction, three-way rule mining and decision cost optimization. Abundant experiments demonstrate that the proposed three-way decision support method is effective to handle the uncertain medical cases, and in the meantime achieves precise classification of FLLs to support liver cancer diagnosis.",https://doi.org/10.1016/j.knosys.2017.04.008,https://www.sciencedirect.com/science/article/pii/S0950705117301843,Knowledge-Based Systems,Yufei Chen;Xiaodong Yue;Hamido Fujita;Siyuan Fu,2017,102,"@article{2-14354,
  title     = {Three-way decision support for diagnosis on focal liver lesions},
  author    = {Yufei Chen and Xiaodong Yue and Hamido Fujita and Siyuan Fu},
  year      = {2017},
  doi       = {10.1016/j.knosys.2017.04.008},
  journal   = {Knowledge-Based Systems}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Analyzing, Advising","Knowledge provider, Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-14375,elsevier,To err is human: bias salience can help overcome resistance to medical ai,"Prior research has shown that many individuals exhibit an aversion to algorithms and are resistant to the use of artificial intelligence (AI) in healthcare. In the present research, we show that an intervention that increases the salience of bias in decision making—either in general or specifically with respect to gender or age—makes individuals relatively more receptive to medical AI. This increased receptiveness to AI occurs because bias is perceived to be a fundamentally human shortcoming. As such, when the prospect of bias is made salient, perceptions of AI integrity—defined as the perceived fairness and trustworthiness of an AI agent relative to a human counterpart—are enhanced.",https://doi.org/10.1016/j.chb.2024.108402,https://www.sciencedirect.com/science/article/pii/S074756322400270X,Computers in Human Behavior,Mathew S. Isaac;Rebecca {Jen-Hui Wang};Lucy E. Napper;Jessecae K. Marsh,2024,8,"@article{2-14375,
  title = {To err is human: bias salience can help overcome resistance to medical AI},
  author = {Mathew S. Isaac and Rebecca {Jen-Hui Wang} and Lucy E. Napper and Jessecae K. Marsh},
  year = {2024},
  doi = {10.1016/j.chb.2024.108402},
  journal = {Computers in Human Behavior}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Decision-subject","Alter decision outcomes, Shape ethical norms, Change trust",no such info,NA,bias-salience intervention,"Visual, Textual, Interactive interface",Yes,Yes
2-14389,elsevier,Toward a person-environment fit framework for artificial intelligence implementation in the public sector,"Using an embedded mixed method design, we compared a nationally representative sample of US adults and a sample of US-based emergency managers (EM) on their attitudes toward artificial intelligence (AI) and their intentions to rely on AI in a set of decision-making scenarios relevant to emergency management. Emergency managers reported significantly less positive attitudes toward AI and were less likely to rely on AI for decisions compared to the nationally representative sample. Our analysis of EMs' open-ended responses explaining their choices to use or not use AI-based solutions reflected specific concerns about implementation rather than wariness toward AI generally. These concerns included the complexity of the potential outcomes in the scenarios, the value they placed on human input and their own extensive experience, procedural concerns, collaborative decision-making, team-building, training, and the ethical implications of decisions, rather than a rejection of AI more generally. Managers' insights integrated with our quantitative findings led to a person-environment fit framework for AI implementation in the public sector. Our findings and framework have implications for how AI systems should be introduced and integrated in emergency managerial contexts and in public sector organizations more generally. Public managers' perceptions and intentions to use AI and organizational oversight processes are at least as important as technology design considerations when public sector organizations are considering the deployment of AI.",https://doi.org/10.1016/j.giq.2024.101962,https://www.sciencedirect.com/science/article/pii/S0740624X24000546,Government Information Quarterly,Shalini Misra;Benjamin Katz;Patrick Roberts;Mackenzie Carney;Isabel Valdivia,2024,29,"@article{2-14389,
  title={Toward a person-environment fit framework for artificial intelligence implementation in the public sector},
  author={Misra, Shalini and Katz, Benjamin and Roberts, Patrick and Carney, Mackenzie and Valdivia, Isabel},
  year={2024},
  journal={Government Information Quarterly},
  doi={10.1016/j.giq.2024.101962}
}","Empirical contributions, Methodological contributions",Defense / Military / Emergency,Organizational,Advising,Decision-maker,"Alter decision outcomes, Change trust, Change affective-perceptual, Change cognitive demands",no such info,NA,NA,NA,Yes,Yes
2-14396,elsevier,Toward personalized decision making for autonomous vehicles: a constrained multi-objective reinforcement learning technique,"Reinforcement learning promises to provide a state-of-the-art solution to the decision making problem of autonomous driving. Nonetheless, numerous real-world decision making problems involve balancing multiple conflicting or competing objectives. In addition, passengers may typically prefer to explore diversified driving modes through their specific preferences (i.e., relative importance of different objectives). Taking into account these demands, traditional reinforcement learning algorithms with applications in personalized self-driving vehicles remain challenging. Consequently, here we present a novel constrained multi-objective reinforcement learning technique for personalized decision making in autonomous driving, with the goal of learning a single model for Pareto optimal policies across the space of all possible user preferences. Specifically, a nonlinear constraint incorporating a user-specified preference and a vectorized action–value function is introduced to ensure both diversity in learned decision behaviors and efficient alignment between the user-specified preference and the corresponding optimal policy. Additionally, a constrained multi-objective actor–critic approach is advanced to approximate the Pareto optimal policies for any user-specified preferences while adhering to the nonlinear constraint. Finally, the proposed personalized decision making scheme for autonomous driving is assessed in a highway on-ramp merging scenario with dynamic traffic flows. The results demonstrate the effectiveness of our method by comparing it with classical and state-of-the-art baselines.",https://doi.org/10.1016/j.trc.2023.104352,https://www.sciencedirect.com/science/article/pii/S0968090X2300342X,Transportation Research Part C: Emerging Technologies,Xiangkun He;Chen Lv,2023,59,"@article{2-14396,
  title={Toward personalized decision making for autonomous vehicles: a constrained multi-objective reinforcement learning technique},
  author={He, Xiangkun and Lv, Chen},
  year={2023},
  journal={Transportation Research Part C: Emerging Technologies},
  doi={10.1016/j.trc.2023.104352}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-14439,elsevier,Towards personalized nutritional treatment for malnutrition using machine learning-based screening tools,"Summary Early identification of patients at risk of malnutrition or who are malnourished is crucial in order to start a timely and adequate nutritional therapy. Yet, despite the presence of many nutrition screening tools for use in the hospital setting, there is no consensus regarding the best tool as well as inadequate adherence to screening practices which impairs the achievement of effective nutritional therapy. In recent years, artificial intelligence and machine learning methods have been widely used, across multiple medical domains, to aid clinical decision making and to improve quality and efficiency of care. Therefore, Yin and colleagues propose a machine learning based individualized decision support system aimed to identify and grade malnutrition in cancer patients by applying unsupervised and supervised machine learning methods on nationwide cohort. This approach, demonstrate the ability of machine learning methods to create tools to recognize malnutrition. The machine learning based screening serves as a first layer in a nutritional therapy workflow and provides improved support for decision making of health professionals to fit individualized nutritional therapy in at-risk patients.",https://doi.org/10.1016/j.clnu.2021.08.013,https://www.sciencedirect.com/science/article/pii/S0261561421003939,Clinical Nutrition,Orit Raphaeli;Pierre Singer,2021,24,"@article{2-14439,
  title={Towards personalized nutritional treatment for malnutrition using machine learning-based screening tools},
  author={Raphaeli, Orit and Singer, Pierre},
  year={2021},
  journal={Clinical Nutrition},
  doi={10.1016/j.clnu.2021.08.013}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-14460,elsevier,Tracking and handling behavioral biases in active learning frameworks,"The computational capabilities of AI engines integrated with human knowledge and experience can help create intelligent human-in-the-loop (HITL) decision systems. Such synergistic frameworks improve the decision-making process and make AI-enabled systems more robust, accurate and smarter. In safety-critical applications that require a certain level of human supervision, human and AI engine errors can be costly. However, modeling human behavior in collaborative human-AI decision setup is not straightforward. Humans use cognitive mechanisms and decision heuristics to process information and make decisions under uncertainty. This paper, for the first time, presents a systematic framework for modeling, tracking and adaptation of behavioral biases in a collaborative decision environment within an “Active Learning” context. The proposed framework is validated using experiments conducted on a real-world pancreatic cancer dataset. We consider five different learning scenarios based on different grades of human experts and compare the performance of bias-aware decision models with naive models. It is observed that bias-aware models improve the classification accuracy of decision models by upto 16%.",https://doi.org/10.1016/j.ins.2023.119117,https://www.sciencedirect.com/science/article/pii/S0020025523007028,Information Sciences,Deepesh Agarwal;Balasubramaniam Natarajan,2023,7,"@article{2-14460,
  title={Tracking and handling behavioral biases in active learning frameworks},
  author={Agarwal, Deepesh and Natarajan, Balasubramaniam},
  year={2023},
  journal={Information Sciences},
  doi={https://doi.org/10.1016/j.ins.2023.119117}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",no such info,"Analyzing, Forecasting, Advising, Executing","Knowledge provider, Stakeholder, Guardian, Decision-maker",Alter decision outcomes,Update AI competence,bias-aware decision models,NA,Semi-Autonomous System,Yes,Yes
2-14499,elsevier,Transparency and accountability in ai decision support: explaining and visualizing convolutional neural networks for text information,"Proliferating applications of deep learning, along with the prevalence of large-scale text datasets, have revolutionized the natural language processing (NLP) field, thereby driving the recent explosive growth. Nevertheless, it is argued that state-of-the-art studies focus excessively on producing quantitative performances superior to existing models, by playing “the Kaggle game.” Hence, the field requires more effort in solving new problems and proposing novel approaches and architectures. We claim that one of the promising and constructive efforts would be to design transparent and accountable artificial intelligence (AI) systems for text analytics. By doing so, we can enhance the applicability and problem-solving capacity of the system for real-world decision support. It is widely accepted that deep learning models demonstrate remarkable performances compared to existing algorithms. However, they are often criticized for being less interpretable, i.e., the “black box.” In such cases, users tend to hesitate to utilize them for decision-making, especially in crucial tasks. Such complexity obstructs transparency and accountability of the overall system, potentially debilitating the deployment of decision support systems powered by AI. Furthermore, recent regulations are emphasizing fairness and transparency in algorithms to a greater extent, turning explanations more compulsory than voluntary. Thus, to enhance the transparency and accountability of the decision support system and preserve the capacity to model complex text data at the same time, we propose the Explaining and Visualizing Convolutional neural networks for Text information (EVCT) framework. By adopting and ameliorating cutting-edge methods in NLP and image processing, the EVCT framework provides a human-interpretable solution to the problem of text classification while minimizing information loss. Experimental results with large-scale, real-world datasets show that EVCT performs comparably to benchmark models, including widely used deep learning models. In addition, we provide instances of human-interpretable and relevant visualized explanations obtained from applying EVCT to the dataset and possible applications for real-world decision support.",https://doi.org/10.1016/j.dss.2020.113302,https://www.sciencedirect.com/science/article/pii/S0167923620300579,Decision Support Systems,Buomsoo Kim;Jinsoo Park;Jihae Suh,2020,255,"@article{2-14499,
  title={Transparency and accountability in AI decision support: explaining and visualizing convolutional neural networks for text information},
  author={Kim, Buomsoo and Park, Jinsoo and Suh, Jihae},
  year={2020},
  journal={Decision Support Systems},
  doi={10.1016/j.dss.2020.113302}
}",System/Artifact contributions,Generic / Abstract / Domain-agnostic,no such info,"Forecasting, Analyzing, Advising, Explaining","Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-1452,acl,BiasX: “Thinking Slow” in Toxic Content Moderation with Explanations of Implied Social Biases,"Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. We show that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content. The quality of explanations is critical: imperfect machine-generated explanations (+2.4% on hard toxic examples) help less compared to expert-written human explanations (+7.2%). Our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation.",10.18653/v1/2023.emnlp-main.300,https://aclanthology.org/2023.emnlp-main.300,Conference on Empirical Methods in Natural Language Processing,"Zhang, Yiming; Nanduri, Sravani; Jiang, Liwei; Wu, Tongshuang; Sap, Maarten",2023,16,"@inproceedings{2-1452,
  title = {BiasX: ``Thinking Slow'' in Toxic Content Moderation with Explanations of Implied Social Biases},
  author = {Zhang, Yiming and Nanduri, Sravani and Jiang, Liwei and Wu, Tongshuang and Sap, Maarten},
  year = {2023},
  doi = {10.18653/v1/2023.emnlp-main.300},
  booktitle = {Conference on Empirical Methods in Natural Language Processing}
}",System/Artifact contributions,Everyday / Employment / Public Service,Operational,"Explaining, Advising","Decision-maker, Knowledge provider","Change trust, Change cognitive demands, Shape ethical norms",Update AI competence,textual explanations,NA,"Textual, Conversational/Natural Language",Yes,Yes
2-14522,elsevier,Trust and reliance on ai — an experimental study on the extent and costs of overreliance on ai,"Decision-making is undergoing rapid changes due to the introduction of artificial intelligence (AI), as AI recommender systems can help mitigate human flaws and increase decision accuracy and efficiency. However, AI can also commit errors or suffer from algorithmic bias. Hence, blind trust in technologies carries risks, as users may follow detrimental advice resulting in undesired consequences. Building upon research on algorithm appreciation and trust in AI, the current study investigates whether users who receive AI advice in an uncertain situation overrely on this advice — to their own detriment and that of other parties. In a domain-independent, incentivized, and interactive behavioral experiment, we find that the mere knowledge of advice being generated by an AI causes people to overrely on it, that is, to follow AI advice even when it contradicts available contextual information as well as their own assessment. Frequently, this overreliance leads not only to inefficient outcomes for the advisee, but also to undesired effects regarding third parties. The results call into question how AI is being used in assisted decision making, emphasizing the importance of AI literacy and effective trust calibration for productive deployment of such systems.",https://doi.org/10.1016/j.chb.2024.108352,https://www.sciencedirect.com/science/article/pii/S0747563224002206,Computers in Human Behavior,Artur Klingbeil;Cassandra Grützner;Philipp Schreck,2024,210,"@article{2-14522,
  title = {Trust and Reliance on AI — An Experimental Study on the Extent and Costs of Overreliance on AI},
  author = {Artur Klingbeil and Cassandra Gr{\""u}tzner and Philipp Schreck},
  year = {2024},
  journal = {Computers in Human Behavior},
  doi = {10.1016/j.chb.2024.108352}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,Advising,"Decision-maker, Decision-subject","Alter decision outcomes, Change trust",no such info,AI advice,NA,"Textual, Conversational/Natural Language",Yes,Yes
2-14524,elsevier,Trust in an ai versus a human teammate: the effects of teammate identity and performance on human-ai cooperation,"Recent advances in artificial intelligence (AI) enable researchers to create more powerful AI agents that are becoming competent teammates for humans. However, human distrust of AI is a critical factor that may impede human-AI cooperation. Although AI agents have been endowed with anthropomorphic traits, such as a human-like appearance, in prior studies to improve human trust in AI, it is still an open question whether humans have more trust in an AI teammate and achieve better human-AI joint performance if they are deceived about the identity of their AI teammate as another human. This research assesses the effects of teammate identity (“human” vs. AI) and teammate performance (low-performing vs. high-performing AI) on human-AI cooperation through a human subjects study. The results of this study show that humans behaviorally trust the AI more than another human by accepting their AI teammate's decisions more often. In addition, teammate performance has a significant effect on human-AI joint performance in the study, while teammate identity does not. These results caution against deceiving humans about the identity of AI in future applications involving human-AI cooperation.",https://doi.org/10.1016/j.chb.2022.107536,https://www.sciencedirect.com/science/article/pii/S0747563222003569,Computers in Human Behavior,Guanglu Zhang;Leah Chong;Kenneth Kotovsky;Jonathan Cagan,2023,130,"@article{2-14524,
  title = {Trust in an AI versus a Human Teammate: The Effects of Teammate Identity and Performance on Human-AI Cooperation},
  author = {Guanglu Zhang and Leah Chong and Kenneth Kotovsky and Jonathan Cagan},
  year = {2023},
  doi = {10.1016/j.chb.2022.107536},
  journal = {Computers in Human Behavior}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Media / Communication / Entertainment",Individual,"Collaborating, Executing","Decision-maker, Decision-subject","Alter decision outcomes, Change trust, Change cognitive demands",no such info,"teammate performance, teammate identity",NA,"Textual, Conversational/Natural Language",Yes,Yes
2-14527,elsevier,Trusting under risk – comparing human to ai decision support agents,"The growing number of safety-critical technologized workplaces leads to enhanced support of complex human decision-making by artificial intelligence (AI), increasing the relevance of risk in the joint decision process. This online study examined participants' trust, attitude and behavior during a visual estimation task supported by either a human or an AI decision support agent. Throughout the online studyrisk levels were manipulated through different scenarios. Contrary to recent literature, no main effects were found in participants' trust attitude or trust behavior between support agent conditions or risk levels. However, participants using AI support exhibited increased trust behavior under higher risk, while participants with human support agents did not display behavioral differences. Self-confidence vs. trust and an increased feeling of responsibility may be possible reasons. Furthermore, participants reported the human support agent to be more responsible for possible negative outcomes of the joint task than the AI support agent. Hereby, risk did not influence perceived responsibility. However, the study's findings concerning trust behavior underscore the crucial importance of investigating the impact of risk in workplaces, shedding light on the under-researched effect of risk on trust attitude and behavior in AI-supported human decision-making.",https://doi.org/10.1016/j.chb.2023.108107,https://www.sciencedirect.com/science/article/pii/S0747563223004582,Computers in Human Behavior,Hannah Fahnenstich;Tobias Rieger;Eileen Roesler,2024,42,"@article{2-14527,
  title={Trusting under risk – comparing human to AI decision support agents},
  author={Fahnenstich, Hannah and Rieger, Tobias and Roesler, Eileen},
  year={2024},
  journal={Computers in Human Behavior},
  doi={10.1016/j.chb.2023.108107}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,"Executing, Advising","Decision-maker, Decision-subject","Change trust, Alter decision outcomes, Shift responsibility",no such info,estimation,NA,"Textual, Visual, Conversational/Natural Language",Yes,Yes
2-14571,elsevier,Uncertainty-aware multi-criteria decision analysis for evaluation of explainable artificial intelligence methods: a use case from the healthcare domain,"This study introduces a Z-numbers-based Weighted Sum Model (WSM) tailored to evaluate user satisfaction with explanations provided by Explainable Artificial Intelligence (XAI) methods in the healthcare domain. Focusing on the interpretability of XAI, we measure how users perceive the adequacy of explanations through the lens of SHapley Additive exPlanations (SHAP), Individual Conditional Expectation (ICE) plots, and Counterfactual Explanations (CFE). By conducting interviews with healthcare professionals, we integrate their qualitative feedback with quantitative analysis to assess the effectiveness of these methods. The results present a user-centric perspective on the clarity, relevance, and trustworthiness of the generated post-hoc explanations. This study advances the fields of information sciences and healthcare by offering a systematic approach for evaluating XAI, enhancing the transparency and reliability of AI in critical decision-making processes.",https://doi.org/10.1016/j.ins.2023.119987,https://www.sciencedirect.com/science/article/pii/S0020025523015724,Information Sciences,Kamala Aliyeva;Nijat Mehdiyev,2024,0,"@article{2-14571,
  title={Uncertainty-aware multi-criteria decision analysis for evaluation of explainable artificial intelligence methods: a use case from the healthcare domain},
  author={Aliyeva, Kamala and Mehdiyev, Nijat},
  year={2024},
  journal={Information Sciences},
  doi={10.1016/j.ins.2023.119987}
}","Empirical contributions, Methodological contributions",Healthcare / Medicine / Surgery,Institutional,"Explaining, Advising, Analyzing","Decision-maker, Guardian, Knowledge provider","Change cognitive demands, Change trust",no such info,"visual explanations, counterfactual explanations",NA,"Visual, Textual",Yes,Yes
2-14584,elsevier,Understand your decision rather than your model prescription: towards explainable deep learning approaches for commodity procurement,"Hedging against price increases is particularly important in times of significant market uncertainty and price volatility. For commodity procuring firms, futures contracts are a widespread means of financially hedging price risks. Recently, digital data-driven decision-support approaches have been developed, with deep learning-based methods achieving outstanding results in capturing non-linear relationships between external features and price trends. Digital procurement systems leverage targeted purchasing decisions of these optimization models, yet the decision-mechanisms are opaque. We employ a prescriptive deep-learning approach modeling hedging decisions as a multi-label time series classification problem. We backtest the performance in two evaluation periods, i.e., 2018–2020 and 2021–2023, for natural gas, crude oil, nickel, and copper. The approach performs well in the first evaluation period of the testing framework yet fails to capture market disruptions (pandemic, geopolitical developments) in the second evaluation period, yielding significant hedging losses or degenerating into a simple hand-to-mouth procurement policy. We employ explainable artificial intelligence to analyze the performance for both periods. The results show that the models cannot handle market regime switches and disruptive events within the considered feature set.",https://doi.org/10.1016/j.cor.2024.106905,https://www.sciencedirect.com/science/article/pii/S0305054824003770,Computers & Operations Research,Moritz Rettinger;Stefan Minner;Jenny Birzl,2024,3,"@article{2-14584,
  title={Understand your decision rather than your model prescription: towards explainable deep learning approaches for commodity procurement},
  author={Rettinger, Moritz and Minner, Stefan and Birzl, Jenny},
  year={2024},
  journal={Computers \& Operations Research},
  doi={10.1016/j.cor.2024.106905}
}",Empirical contributions,Finance / Business / Economy,Organizational,"Explaining, Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-14591,elsevier,Understanding managers’ attitudes and behavioral intentions towards using artificial intelligence for organizational decision-making,"While using artificial intelligence (AI) could improve organizational decision-making, it also creates challenges associated with the “dark side” of AI. However, there is a lack of research on managers' attitudes and intentions to use AI for decision making. To address this gap, we develop an integrated AI acceptance-avoidance model (IAAAM) to consider both the positive and negative factors that collectively influence managers' attitudes and behavioral intentions towards using AI. The research model is tested through a large-scale questionnaire survey of 269 UK business managers. Our findings suggest that IAAAM provides a more comprehensive model for explaining and predicting managers' attitudes and behavioral intentions towards using AI. Our research contributes conceptually and empirically to the emerging literature on using AI for organizational decision-making. Further, regarding the practical implications of using AI for organizational decision-making, we highlight the importance of developing favorable facilitating conditions, having an effective mechanism to alleviate managers’ personal concerns, and having a balanced consideration of both the benefits and the dark side associated with using AI.",https://doi.org/10.1016/j.technovation.2021.102312,https://www.sciencedirect.com/science/article/pii/S0166497221000936,"Technovation: The International Journal of Technological Innovation, Entrepreneurship and Technology Management",Guangming Cao;Yanqing Duan;John S. Edwards;Yogesh K. Dwivedi,2021,16,"@article{2-14591,
  title={Understanding managers' attitudes and behavioral intentions towards using artificial intelligence for organizational decision-making},
  author={Cao, Guangming and Duan, Yanqing and Edwards, John S. and Dwivedi, Yogesh K.},
  year={2021},
  journal={Technovation: The International Journal of Technological Innovation, Entrepreneurship and Technology Management},
  doi={10.1016/j.technovation.2021.102312}
}",Empirical contributions,Everyday / Employment / Public Service,Organizational,"Advising, Forecasting, Explaining","Stakeholder, Decision-maker","Change affective-perceptual, Change trust, Change cognitive demands",Shape AI for accountability,NA,NA,NA,Yes,Yes
2-1460,acl,"Buy Tesla, Sell Ford: Assessing Implicit Stock Market Preference in Pre-trained Language Models","Pretrained language models such as BERT have achieved remarkable success in several NLP tasks. With the wide adoption of BERT in real-world applications, researchers begin to investigate the implicit biases encoded in the BERT. In this paper, we assess the implicit stock market preferences in BERT and its finance domain-specific model FinBERT. We find some interesting patterns. For example, the language models are overall more positive towards the stock market, but there are significant differences in preferences between a pair of industry sectors, or even within a sector. Given the prevalence of NLP models in financial decision making systems, this work raises the awareness of their potential implicit preferences in the stock markets. Awareness of such problems can help practitioners improve robustness and accountability of their financial NLP pipelines .",10.18653/v1/2022.acl-short.12,https://aclanthology.org/2022.acl-short.12,Annual Meeting of the Association for Computational Linguistics (Short Papers),"Chuang, Chengyu; Yang, Yi",2022,20,"@inproceedings{2-1460,
  title = {Buy Tesla, Sell Ford: Assessing Implicit Stock Market Preference in Pre-trained Language Models},
  author = {Chuang, Chengyu and Yang, Yi},
  year = {2022},
  booktitle = {Annual Meeting of the Association for Computational Linguistics (Short Papers)},
  doi = {10.18653/v1/2022.acl-short.12}
}",Empirical contributions,Finance / Business / Economy,Operational,Advising,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-14682,elsevier,Using automatically extracted information from mammography reports for decision-support,"Objective To evaluate a system we developed that connects natural language processing (NLP) for information extraction from narrative text mammography reports with a Bayesian network for decision-support about breast cancer diagnosis. The ultimate goal of this system is to provide decision support as part of the workflow of producing the radiology report. Materials and methods We built a system that uses an NLP information extraction system (which extract BI-RADS descriptors and clinical information from mammography reports) to provide the necessary inputs to a Bayesian network (BN) decision support system (DSS) that estimates lesion malignancy from BI-RADS descriptors. We used this integrated system to predict diagnosis of breast cancer from radiology text reports and evaluated it with a reference standard of 300 mammography reports. We collected two different outputs from the DSS: (1) the probability of malignancy and (2) the BI-RADS final assessment category. Since NLP may produce imperfect inputs to the DSS, we compared the difference between using perfect (“reference standard”) structured inputs to the DSS (“RS-DSS”) vs NLP-derived inputs (“NLP-DSS”) on the output of the DSS using the concordance correlation coefficient. We measured the classification accuracy of the BI-RADS final assessment category when using NLP-DSS, compared with the ground truth category established by the radiologist. Results The NLP-DSS and RS-DSS had closely matched probabilities, with a mean paired difference of 0.004±0.025. The concordance correlation of these paired measures was 0.95. The accuracy of the NLP-DSS to predict the correct BI-RADS final assessment category was 97.58%. Conclusion The accuracy of the information extracted from mammography reports using the NLP system was sufficient to provide accurate DSS results. We believe our system could ultimately reduce the variation in practice in mammography related to assessment of malignant lesions and improve management decisions.",https://doi.org/10.1016/j.jbi.2016.07.001,https://www.sciencedirect.com/science/article/pii/S1532046416300557,Journal of Biomedical Informatics,Selen Bozkurt;Francisco Gimenez;Elizabeth S. Burnside;Kemal H. Gulkesen;Daniel L. Rubin,2016,1,"@article{2-14682,
  title={Using automatically extracted information from mammography reports for decision-support},
  author={Bozkurt, Selen and Gimenez, Francisco and Burnside, Elizabeth S. and Gulkesen, Kemal H. and Rubin, Daniel L.},
  year={2016},
  doi={10.1016/j.jbi.2016.07.001},
  journal={Journal of Biomedical Informatics}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising, Analyzing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-14701,elsevier,Using machine learning methods for predicting inhospital mortality in patients undergoing open repair of abdominal aortic aneurysm,"An abdominal aortic aneurysm is an abnormal dilatation of the aortic vessel at abdominal level. This disease presents high rate of mortality and complications causing a decrease in the quality of life and increasing the cost of treatment. To estimate the mortality risk of patients undergoing surgery is complex due to the variables associated. The use of clinical decision support systems based on machine learning could help medical staff to improve the results of surgery and get a better understanding of the disease. In this work, the authors present a predictive system of inhospital mortality in patients who were undergoing to open repair of abdominal aortic aneurysm. Different methods as multilayer perceptron, radial basis function and Bayesian networks are used. Results are measured in terms of accuracy, sensitivity and specificity of the classifiers, achieving an accuracy higher than 95%. The developing of a system based on the algorithms tested can be useful for medical staff in order to make a better planning of care and reducing undesirable surgery results and the cost of the post-surgical treatments.",https://doi.org/10.1016/j.jbi.2016.07.007,https://www.sciencedirect.com/science/article/pii/S1532046416300612,Journal of Biomedical Informatics,Ana Monsalve-Torra;Daniel Ruiz-Fernandez;Oscar Marin-Alonso;Antonio Soriano-Payá;Jaime Camacho-Mackenzie;Marisol Carreño-Jaimes,2016,92,"@article{2-14701,
  title={Using machine learning methods for predicting inhospital mortality in patients undergoing open repair of abdominal aortic aneurysm},
  author={Monsalve-Torra, Ana and Ruiz-Fernandez, Daniel and Marin-Alonso, Oscar and Soriano-Pay{\'a}, Antonio and Camacho-Mackenzie, Jaime and Carre{\~n}o-Jaimes, Marisol},
  year={2016},
  journal={Journal of Biomedical Informatics},
  doi={10.1016/j.jbi.2016.07.007}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-14783,elsevier,Virtsi: a novel trust dynamics model enhancing artificial intelligence collaboration with human users – insights from a chatgpt evaluation study,"The rapid integration of intelligent processes and methods into information systems in the Artificial Intelligence (AI) era has led to a substantial shift towards autonomous software decision-making. This evolution necessitates robust human oversight, especially in critical domains like Healthcare, Education, and Energy. Human trust in AI plays a vital role in influencing decision-making processes of users interacting with AI. This paper presents VIRTSI (Variability and Impact of Reciprocal Trust States towards Intelligent systems), a novel rigorous computational model for human-AI Interaction. VIRTSI simulates human trust states, spanning from overtrust to distrust, through user modelling. It comprises: 1. A trust dynamics representational model based on Deterministic Finite State Automata (DFAs), illustrating transitions among cognitive trust states in response to AI-generated replies. 2. A trust evaluation model based on Confusion Matrices, originating from machine learning and Accuracy Metrics, providing a quantitative framework for analysing human trust dynamics. As a result, this is the first time that trust dynamics have been thoroughly traced in a representational model and a method has been developed to assess the impact of possibly harmful states like overtrust and distrust. An empirical study on the recently launched Large Language Model of generative AI, ChatGPT (version 3.5), provides a radical underexplored AI-generated platform for evaluating the human-AI interaction through VIRTSI. The study involved 1200 interactions of real users as well as AI experts together with experts in two very different domains of evaluation, namely software engineering and poetry. This study traces trust dynamics and the emerging human-AI interaction, in concrete examples of real user synergies with generative AI. The research reveals the vital role of maintaining normal trust states for optimal human-AI interaction and that both AI and human users need further steps towards this goal. The real-world implications of this research can guide the creation and evaluation of user interfaces with AI and the incorporation of functionalities in the development of generative AI chatbots in terms of trust by providing a new rigorous DFA representational method of trust dynamics and a corresponding new perspective of confusion matrix evaluation method of the dynamics’ impact in the efficiency of human-AI dialogues.",https://doi.org/10.1016/j.ins.2024.120759,https://www.sciencedirect.com/science/article/pii/S002002552400673X,Information Sciences,Maria Virvou;George A. Tsihrintzis;Evangelia-Aikaterini Tsichrintzi,2024,4,"@article{2-14783,
  title={Virtsi: a novel trust dynamics model enhancing artificial intelligence collaboration with human users -- insights from a chatgpt evaluation study},
  author={Virvou, Maria and Tsihrintzis, George A. and Tsichrintzi, Evangelia-Aikaterini},
  year={2024},
  journal={Information Sciences},
  doi={10.1016/j.ins.2024.120759}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Individual,"Advising, Collaborating","Knowledge provider, Developer, Decision-maker","Alter decision outcomes, Change trust","Change AI responses, Update AI competence",AI response,NA,Textual,Yes,Yes
2-14825,elsevier,What drives ai-based risk information-seeking intent? Insufficiency of risk information versus (un)certainty of ai chatbots,"This study explored the factors influencing the U.S. public's intent to seek risk information via AI-powered channels, such as ChatGPT. It focused on cognitive and affective pathways that lead to uncertainty about both risk information and AI chatbots in the context of climate change risk. We conducted a comparative analysis to discern the impacts of risk perceptions related to climate change and AI-caused privacy risks on public uncertainty and decision-making regarding the use of AI chatbots. Specifically, we assessed how different risk-related perceptions and emotions contribute to subsequent uncertainty perceptions and decision-making regarding AI chatbot use for climate change risk information. We enlisted 1023 U.S. citizens aged 21–65 via CloudResearch in September 2023. The results reveal that high levels of perceived risk, strong negative emotions, and information insufficiency drive information-seeking behavior through AI chatbots. Perceived privacy concerns about AI technology significantly increase AI anxiety, which is positively associated with perceived uncertainty. Both AI anxiety and perceived uncertainty negatively affect the intent to seek information via AI chatbots. Conversely, perceived trust in AI chatbots significantly increases positive emotional responses, reduces perceived uncertainty, and enhances the intent to seek information via AI chatbots. We also investigated the mediation effects within each study model tested. The findings offer theoretical and practical implications for future studies on the public's adoption of AI services for risk information seeking, influenced by both risk-related and technology-based contexts.",https://doi.org/10.1016/j.chb.2024.108460,https://www.sciencedirect.com/science/article/pii/S0747563224003285,Computers in Human Behavior,Soo Jung Hong,2025,0,"@article{2-14825,
  title = {What drives AI-based risk information-seeking intent? Insufficiency of risk information versus (un)certainty of AI chatbots},
  author = {Soo Jung Hong},
  year = {2025},
  doi = {10.1016/j.chb.2024.108460},
  journal = {Computers in Human Behavior}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Environment / Resources / Energy","Individual, Institutional","Analyzing, Advising, Collaborating","Decision-maker, Decision-subject",Change affective-perceptual,no such info,NA,"emotion expression, privacy concern",Physical / Embodiment,Yes,Yes
2-14833,elsevier,"What shapes our attitudes towards algorithms in urban governance? The role of perceived friendliness and controllability of the city, and human-algorithm cooperation","Today, artificial intelligence (AI) algorithms are increasingly often implemented in urban management; however, so far, there has been no research on how their use can affect the perception of the city. Is a city employing algorithmic decision-making perceived as friendly and controllable? Will algorithms assisting city officials be more accepted than those making a decision without human supervision? And, is a city in which algorithms make decisions with city officials perceived as more friendly and controllable than a city in which algorithms make decisions unadvised? To address these questions, we have conducted two studies in Poland, representative for the population’s age and gender structure. The first study results (N = 753) showed that individuals who perceived cities using algorithms as resident-unfriendly, were also more reluctant to accept algorithmic governance. The perceived controllability of the city was not related to the acceptance of algorithms. The second, experimental study (N = 1047) demonstrated that the acceptance of algorithms increased when they were presented as collaborating with humans (vs. making decisions unadvised). Cooperation between algorithms and humans also increased the perceived friendliness and controllability of the city. The studies provide evidence that implementing algorithms into urban governance can lead to a sense of alienation and lower the city’s friendliness; however, collaboration between humans and algorithms may diminish this feeling and increase the overall acceptance of algorithmic decision making in the city.",https://doi.org/10.1016/j.chb.2023.107653,https://www.sciencedirect.com/science/article/pii/S0747563223000043,Computers in Human Behavior,Tomasz Oleksy;Anna Wnuk;Anna Domaradzka;Dominika Maison,2023,0,"@article{2-14833,
  title = {What shapes our attitudes towards algorithms in urban governance? The role of perceived friendliness and controllability of the city, and human-algorithm cooperation},
  author = {Tomasz Oleksy and Anna Wnuk and Anna Domaradzka and Dominika Maison},
  year = {2023},
  doi = {10.1016/j.chb.2023.107653},
  journal = {Computers in Human Behavior}
}",Empirical contributions,Law / Policy / Governance,Organizational,"Executing, Advising","Decision-maker, Stakeholder","Alter decision outcomes, Restrict human agency, Change affective-perceptual",no such info,NA,NA,Autonomous System,Yes,Yes
2-14836,elsevier,What to expect from opening up ‘black boxes’? Comparing perceptions of justice between human and automated agents,"Advances in artificial intelligence contribute to increasing automation of decisions. In a healthcare-scheduling context, this study compares effects of decision agents and explanations for decisions on decision-recipients’ perceptions of justice. In a 2 (decision agent: automated vs. human) × 3 (explanation: no explanation vs. equality-explanation vs. equity-explanation) between-subjects online study, 209 healthcare professionals were asked to put themselves in a situation where their vacation request was denied by either a human or an automated agent. Participants either received no explanation or an explanation based on equality or equity norms. Perceptions of interpersonal justice were stronger for the human agent. Additionally, participants perceived human agents as offering more voice and automated agents as being more consistent in decision-making. When given no explanation, perceptions of informational justice were impaired only for the human decision agent. In the study's second part, participants took the perspective of a decision-maker and were given the choice to delegate decision-making to an automated system. Participants who delegated an unpleasant decision to the system frequently externalized responsibility and showed different response patterns when confronted by a decision-recipient who asked for a rationale for the decision.",https://doi.org/10.1016/j.chb.2021.106837,https://www.sciencedirect.com/science/article/pii/S0747563221001606,Computers in Human Behavior,Nadine Schlicker;Markus Langer;Sonja K. Ötting;Kevin Baum;Cornelius J. König;Dieter Wallach,2021,129,"@article{2-14836,
  title={What to expect from opening up ‘black boxes’? Comparing perceptions of justice between human and automated agents},
  author={Schlicker, Nadine and Langer, Markus and Ötting, Sonja K. and Baum, Kevin and König, Cornelius J. and Wallach, Dieter},
  year={2021},
  journal={Computers in Human Behavior},
  doi={10.1016/j.chb.2021.106837}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Executing, Advising","Decision-maker, Decision-subject","Change trust, Change affective-perceptual, Shape ethical norms, Restrict human agency, Alter decision outcomes, Shift responsibility",no such info,"equality-explanation, equity-explanation",delegating unpleasant decisions leads humans to externalize responsibility to AI,Autonomous System,Yes,Yes
2-14837,elsevier,What type of algorithm is perceived as fairer and more acceptable? A comparative analysis of rule-driven versus data-driven algorithmic decision-making in public affairs,"Various types of algorithms are being increasingly used to support public decision-making, yet we do not know how these different algorithm types affect citizens' attitudes and behaviors in specific public affairs. Drawing on public value theory, this study uses a survey experiment to compare the effects of rule-driven versus data-driven algorithmic decision-making (ADM) on citizens' perceived fairness and acceptance. This study also examines the moderating role of familiarity with public affairs and the mediating role of perceived fairness on the relationship. The findings show that rule-driven ADM is generally perceived as fairer and more acceptable than data-driven ADM. Low familiarity with public affairs strengthens citizens' perceived fairness and acceptance of rule-driven ADM more than data-driven ADM, and citizens' perceived fairness plays a significant mediating role in the effect of rule-driven ADM on citizens' acceptance behaviors. These findings further imply that citizens' perceived fairness and acceptance of ADM is strongly shaped by how they perceive familiarity of the decision-making context. In high-familiarity AI application scenarios, the realization of public values may ultimately not be what matters for ADM acceptance among citizens.",https://doi.org/10.1016/j.giq.2023.101803,https://www.sciencedirect.com/science/article/pii/S0740624X23000035,Government Information Quarterly,Ge Wang;Yue Guo;Weimin Zhang;Shenghua Xie;Qiwei Chen,2023,0,"@article{2-14837,
  title = {What type of algorithm is perceived as fairer and more acceptable? A comparative analysis of rule-driven versus data-driven algorithmic decision-making in public affairs},
  author = {Ge Wang and Yue Guo and Weimin Zhang and Shenghua Xie and Qiwei Chen},
  year = {2023},
  journal = {Government Information Quarterly},
  doi = {10.1016/j.giq.2023.101803}
}",Empirical contributions,"Law / Policy / Governance, Everyday / Employment / Public Service",Institutional,Executing,"Decision-subject, Stakeholder","Shape ethical norms, Change trust",Shape AI for accountability,algorithm transparency,NA,Autonomous System,Yes,Yes
2-14857,elsevier,Who needs explanation and when? Juggling explainable ai and user epistemic uncertainty,"In recent years, AI explainability (XAI) has received wide attention. Although XAI is expected to play a positive role in decision-making and advice acceptance, various opposing effects have also been found. The opposing effects of XAI highlight the critical role of context, especially human factors, in understanding XAI's impacts. This study investigates the effects of providing three types of post-hoc explanations (alternative advice, prediction confidence scores, and prediction rationale) on two context-specific user decision-making outcomes (AI advice acceptance and advice adoption). Our field experiment results show that users’ epistemic uncertainty matters when understanding XAI's impacts. As users’ epistemic uncertainty increases, only providing prediction rationale is beneficial, whereas providing alternative advice and showing prediction confidence scores may hinder users’ advice acceptance. Our study contributes to the emerging literature on the human aspects of XAI by clarifying XAI and showing that XAI may not always be desirable. It also contributes by highlighting the importance of considering user profiles when predicting XAI's impacts, designing XAI, and providing professional services with AI.",https://doi.org/10.1016/j.ijhcs.2022.102839,https://www.sciencedirect.com/science/article/pii/S1071581922000660,International Journal of Human-Computer Studies,Jinglu Jiang;Surinder Kahai;Ming Yang,2022,119,"@article{2-14857,
  title={Who needs explanation and when? Juggling explainable AI and user epistemic uncertainty},
  author={Jiang, Jinglu and Kahai, Surinder and Yang, Ming},
  year={2022},
  journal={International Journal of Human-Computer Studies},
  doi={10.1016/j.ijhcs.2022.102839}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Individual,"Explaining, Advising, Analyzing",Decision-maker,"Change trust, Alter decision outcomes, Change affective-perceptual","Change AI responses, Update AI competence","alternative advice, prediction confidence scores, prediction rationale",user uncertainty,"Textual, Visual, Conversational/Natural Language",Yes,Yes
2-14863,elsevier,Why did the ai make that decision? Towards an explainable artificial intelligence (xai) for autonomous driving systems,"User trust has been identified as a critical issue that is pivotal to the success of autonomous vehicle (AV) operations where artificial intelligence (AI) is widely adopted. For such integrated AI-based driving systems, one promising way of building user trust is through the concept of explainable artificial intelligence (XAI) which requires the AI system to provide the user with the explanations behind each decision it makes. Motivated by both the need to enhance user trust and the promise of novel XAI technology in addressing such need, this paper seeks to enhance trustworthiness in autonomous driving systems through the development of explainable Deep Learning (DL) models. First, the paper casts the decision-making process of the AV system not as a classification task (which is the traditional process) but rather as an image-based language generation (image captioning) task. As such, the proposed approach makes driving decisions by first generating textual descriptions of the driving scenarios, which serve as explanations that humans can understand. To this end, a novel multi-modal DL architecture is proposed to jointly model the correlation between an image (driving scenario) and language (descriptions). It adopts a fully Transformer-based structure and therefore has the potential to perform global attention and imitate effectively, the learning processes of human drivers. The results suggest that the proposed model can and does generate legal and meaningful sentences to describe a given driving scenario, and subsequently to correctly generate appropriate driving decisions in autonomous vehicles (AVs). It is also observed that the proposed model significantly outperforms multiple baseline models in terms of generating both explanations and driving actions. From the end user’s perspective, the proposed model can be beneficial in enhancing user trust because it provides the rationale behind an AV’s actions. From the AV developer’s perspective, the explanations from this explainable system could serve as a “debugging” tool to detect potential weaknesses in the existing system and identify specific directions for improvement.",https://doi.org/10.1016/j.trc.2023.104358,https://www.sciencedirect.com/science/article/pii/S0968090X23003480,Transportation Research Part C: Emerging Technologies,Jiqian Dong;Sikai Chen;Mohammad Miralinaghi;Tiantian Chen;Pei Li;Samuel Labi,2023,97,"@article{2-14863,
  title={Why did the AI make that decision? Towards an explainable artificial intelligence (XAI) for autonomous driving systems},
  author={Dong, Jiqian and Chen, Sikai and Miralinaghi, Mohammad and Chen, Tiantian and Li, Pei and Labi, Samuel},
  year={2023},
  journal={Transportation Research Part C: Emerging Technologies},
  doi={10.1016/j.trc.2023.104358}
}","Algorithmic contributions, System/Artifact contributions",Transportation / Mobility / Planning,Individual,"Explaining, Executing","Decision-subject, Developer, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-14874,elsevier,Windowshap: an efficient framework for explaining time-series classifiers based on shapley values,"Unpacking and comprehending how black-box machine learning algorithms (such as deep learning models) make decisions has been a persistent challenge for researchers and end-users. Explaining time-series predictive models is useful for clinical applications with high stakes to understand the behavior of prediction models, e.g., to determine how different variables and time points influence the clinical outcome. However, existing approaches to explain such models are frequently unique to architectures and data where the features do not have a time-varying component. In this paper, we introduce WindowSHAP, a model-agnostic framework for explaining time-series classifiers using Shapley values. We intend for WindowSHAP to mitigate the computational complexity of calculating Shapley values for long time-series data as well as improve the quality of explanations. WindowSHAP is based on partitioning a sequence into time windows. Under this framework, we present three distinct algorithms of Stationary, Sliding and Dynamic WindowSHAP, each evaluated against baseline approaches, KernelSHAP and TimeSHAP, using perturbation and sequence analyses metrics. We applied our framework to clinical time-series data from both a specialized clinical domain (Traumatic Brain Injury - TBI) as well as a broad clinical domain (critical care medicine). The experimental results demonstrate that, based on the two quantitative metrics, our framework is superior at explaining clinical time-series classifiers, while also reducing the complexity of computations. We show that for time-series data with 120 time steps (hours), merging 10 adjacent time points can reduce the CPU time of WindowSHAP by 80 % compared to KernelSHAP. We also show that our Dynamic WindowSHAP algorithm focuses more on the most important time steps and provides more understandable explanations. As a result, WindowSHAP not only accelerates the calculation of Shapley values for time-series data, but also delivers more understandable explanations with higher quality.",https://doi.org/10.1016/j.jbi.2023.104438,https://www.sciencedirect.com/science/article/pii/S1532046423001594,Journal of Biomedical Informatics,Amin Nayebi;Sindhu Tipirneni;Chandan K. Reddy;Brandon Foreman;Vignesh Subbian,2023,46,"@article{2-14874,
  title={Windowshap: an efficient framework for explaining time-series classifiers based on shapley values},
  author={Nayebi, Amin and Tipirneni, Sindhu and Reddy, Chandan K. and Foreman, Brandon and Subbian, Vignesh},
  year={2023},
  journal={Journal of Biomedical Informatics},
  doi={10.1016/j.jbi.2023.104438}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Analyzing","Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-14896,iclr,Bayesian neural controlled differential equations for treatment effect estimation,"Treatment effect estimation in continuous time is crucial for personalized medicine. However, existing methods for this task are limited to point estimates of the potential outcomes, whereas uncertainty estimates have been ignored. Needless to say, uncertainty quantification is crucial for reliable decision-making in medical applications. To fill this gap, we propose a novel Bayesian neural controlled differential equation (BNCDE) for treatment effect estimation in continuous time. In our BNCDE, the time dimension is modeled through a coupled system of neural controlled differential equations and neural stochastic differential equations, where the neural stochastic differential equations allow for tractable variational Bayesian inference. Thereby, for an assigned sequence of treatments, our BNCDE provides meaningful posterior predictive distributions of the potential outcomes. To the best of our knowledge, ours is the first tailored neural method to provide uncertainty estimates of treatment effects in continuous time. As such, our method is of direct practical value for promoting reliable decision-making in medicine.",NA,https://openreview.net/forum?id=uwO71a8wET,International Conference on Learning Representations,NA,2024,23,"@inproceedings{2-14896,
  title={Bayesian neural controlled differential equations for treatment effect estimation},
  author={NA},
  year={2024},
  booktitle={International Conference on Learning Representations}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-14897,iclr,Designing skill-compatible ai: methodologies and frameworks in chess,"Powerful artificial intelligence systems are often used in settings where they must interact with agents that are computationally much weaker, for example when they work alongside humans or operate in complex environments where some tasks are handled by algorithms, heuristics, or other entities of varying computational power. For AI agents to successfully interact in these settings, however, achieving superhuman performance alone is not sufficient; they also need to account for suboptimal actions or idiosyncratic style from their less-skilled counterparts. We propose a formal evaluation framework for assessing the compatibility of near-optimal AI with interaction partners who may have much lower levels of skill; we use popular collaborative chess variants as model systems to study and develop AI agents that can successfully interact with lower-skill entities. Traditional chess engines designed to output near-optimal moves prove to be inadequate partners when paired with engines of various lower skill levels in this domain, as they are not designed to consider the presence of other agents. We contribute three methodologies to explicitly create skill-compatible AI agents in complex decision-making settings, and two chess game frameworks designed to foster collaboration between powerful AI agents and less-skilled partners. On these frameworks, our agents outperform state-of-the-art chess AI (based on AlphaZero) despite being weaker in conventional chess, demonstrating that skill-compatibility is a tangible trait that is qualitatively and measurably distinct from raw performance. Our evaluations further explore and clarify the mechanisms by which our agents achieve skill-compatibility.",NA,https://openreview.net/forum?id=79rfgv3jw4,International Conference on Learning Representations,NA,2024,17,"@inproceedings{2-14897,
  title     = {Designing Skill-Compatible AI: Methodologies and Frameworks in Chess},
  author    = {{NA}},
  year      = {2024},
  booktitle = {International Conference on Learning Representations}
}",Methodological contributions,Media / Communication / Entertainment,Individual,"Executing, Collaborating","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-14901,iclr,Explaining by imitating: understanding decisions by interpretable policy learning,"Understanding human behavior from observed data is critical for transparency and accountability in decision-making. Consider real-world settings such as healthcare, in which modeling a decision-maker’s policy is challenging—with no access to underlying states, no knowledge of environment dynamics, and no allowance for live experimentation. We desire learning a data-driven representation of decision- making behavior that (1) inheres transparency by design, (2) accommodates partial observability, and (3) operates completely offline. To satisfy these key criteria, we propose a novel model-based Bayesian method for interpretable policy learning (“Interpole”) that jointly estimates an agent’s (possibly biased) belief-update process together with their (possibly suboptimal) belief-action mapping. Through experiments on both simulated and real-world data for the problem of Alzheimer’s disease diagnosis, we illustrate the potential of our approach as an investigative device for auditing, quantifying, and understanding human decision-making behavior.",NA,https://openreview.net/forum?id=unI5ucw_Jk,International Conference on Learning Representations,Alihan Hüyük;Daniel Jarrett;Cem Tekin;Mihaela van der Schaar,2021,37,"@inproceedings{2-14901,
  title = {Explaining by imitating: understanding decisions by interpretable policy learning},
  author = {H\""{u}y\""{u}k, Alihan and Jarrett, Daniel and Tekin, Cem and van der Schaar, Mihaela},
  year = {2021},
  booktitle = {International Conference on Learning Representations}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Individual,"Analyzing, Explaining, Auditing","Decision-maker, Knowledge provider, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-14904,iclr,Inverse online learning: understanding non-stationary and reactionary policies,"Human decision making is well known to be imperfect and the ability to analyse such processes individually is crucial when attempting to aid or improve a decision-maker's ability to perform a task, e.g. to alert them to potential biases or oversights on their part. To do so, it is necessary to develop interpretable representations of how agents make decisions and how this process changes over time as the agent learns online in reaction to the accrued experience. To then understand the decision-making processes underlying a set of observed trajectories, we cast the policy inference problem as the inverse to this online learning problem. By interpreting actions within a potential outcomes framework, we introduce a meaningful mapping based on agents choosing an action they believe to have the greatest treatment effect. We introduce a practical algorithm for retrospectively estimating such perceived effects, alongside the process through which agents update them, using a novel architecture built upon an expressive family of deep state-space models. Through application to the analysis of UNOS organ donation acceptance decisions, we demonstrate that our approach can bring valuable insights into the factors that govern decision processes and how they change over time.",NA,https://openreview.net/forum?id=DYypjaRdph2,International Conference on Learning Representations,Alex Chan;Alicia Curth;Mihaela van der Schaar,2022,4,"@inproceedings{2-14904,
  title={Inverse online learning: understanding non-stationary and reactionary policies},
  author={Chan, Alex and Curth, Alicia and van der Schaar, Mihaela},
  year={2022},
  booktitle={International Conference on Learning Representations}
}",Algorithmic contributions,"Education / Teaching / Research, Healthcare / Medicine / Surgery",Organizational,Advising,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-14906,iclr,Learning <q>what-if<q> explanations for sequential decision-making,"Building interpretable parameterizations of real-world decision-making on the basis of demonstrated behavior--i.e. trajectories of observations and actions made by an expert maximizing some unknown reward function--is essential for introspecting and auditing policies in different institutions. In this paper, we propose learning explanations of expert decisions by modeling their reward function in terms of preferences with respect to ``<q>what if'' outcomes: Given the current history of observations, what would happen if we took a particular action? To learn these cost-benefit tradeoffs associated with the expert's actions, we integrate counterfactual reasoning into batch inverse reinforcement learning. This offers a principled way of defining reward functions and explaining expert behavior, and also satisfies the constraints of real-world decision-making---where active experimentation is often impossible (e.g. in healthcare). Additionally, by estimating the effects of different actions, counterfactuals readily tackle the off-policy nature of policy evaluation in the batch setting, and can naturally accommodate settings where the expert policies depend on histories of observations rather than just current states. Through illustrative experiments in both real and simulated medical environments, we highlight the effectiveness of our batch, counterfactual inverse reinforcement learning approach in recovering accurate and interpretable descriptions of behavior.",NA,https://openreview.net/forum?id=h0de3QWtGG,International Conference on Learning Representations,Ioana Bica;Daniel Jarrett;Alihan Hüyük;Mihaela van der Schaar,2021,47,"@inproceedings{2-14906,
  title={Learning ""what-if"" explanations for sequential decision-making},
  author={Bica, Ioana and Jarrett, Daniel and Hüyük, Alihan and van der Schaar, Mihaela},
  year={2021},
  booktitle={International Conference on Learning Representations}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,"Explaining, Auditing","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-14907,iclr,Learning human-compatible representations for case-based decision support,"Algorithmic case-based decision support provides examples to help human make sense of predicted labels and aid human in decision-making tasks. Despite the promising performance of supervised learning, representations learned by supervised models may not align well with human intuitions: what models consider as similar examples can be perceived as distinct by humans. As a result, they have limited effectiveness in case-based decision support. In this work, we incorporate ideas from metric learning with supervised learning to examine the importance of alignment for effective decision support. In addition to instance-level labels, we use human-provided triplet judgments to learn human-compatible decision-focused representations. Using both synthetic data and human subject experiments in multiple classification tasks, we demonstrate that such representation is better aligned with human perception than representation solely optimized for classification. Human-compatible representations identify nearest neighbors that are perceived as more similar by humans and allow humans to make more accurate predictions, leading to substantial improvements in human decision accuracies (17.8% in butterfly vs. moth classification and 13.2% in pneumonia classification).",NA,https://openreview.net/forum?id=r0xte-t40I,International Conference on Learning Representations,Han Liu;Yizhou Tian;Chacha Chen;Shi Feng;Yuxin Chen;Chenhao Tan,2023,0,"@inproceedings{2-14907,
  title={Learning human-compatible representations for case-based decision support},
  author={Liu, Han and Tian, Yizhou and Chen, Chacha and Feng, Shi and Chen, Yuxin and Tan, Chenhao},
  year={2023},
  booktitle={International Conference on Learning Representations}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,"Operational, Individual","Explaining, Analyzing, Advising","Decision-maker, Knowledge provider","Alter decision outcomes, Change cognitive demands","Update AI competence, Change AI responses",human-compatible representations,NA,"Visual, Textual, Conversational/Natural Language",Yes,Yes
2-14908,iclr,Modeling boundedly rational agents with latent inference budgets,"We study the problem of modeling a population of agents pursuing unknown goals subject to unknown computational constraints. In standard models of bounded rationality, sub-optimal decision-making is simulated by adding homoscedastic noise to optimal decisions rather than actually simulating constrained inference. In this work, we introduce a latent inference budget model (L-IBM) that models these constraints explicitly, via a latent variable (inferred jointly with a model of agents’ goals) that controls the runtime of an iterative inference algorithm. L-IBMs make it possible to learn agent models using data from diverse populations of suboptimal actors. In three modeling tasks—inferring navigation goals from routes, inferring communicative intents from human utterances, and predicting next moves in human chess games—we show that L-IBMs match or outperforms Boltzmann models of decision-making under uncertainty. Moreover, the inferred inference budgets are themselves meaningful, efficient to compute, and correlated with measures of player skill, partner skill and task difficulty.",NA,https://openreview.net/forum?id=W3VsHuga3j,International Conference on Learning Representations,NA,2024,0,"@inproceedings{2-14908,
  title     = {Modeling boundedly rational agents with latent inference budgets},
  author    = {NA},
  year      = {2024},
  booktitle = {International Conference on Learning Representations}
}",Algorithmic contributions,"Media / Communication / Entertainment, Generic / Abstract / Domain-agnostic",Individual,"Explaining, Forecasting, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-14909,iclr,Reward design for justifiable sequential decision-making,"Equipping agents with the capacity to justify made decisions using supporting evidence represents a cornerstone of accountable decision-making. Furthermore, ensuring that justifications are in line with human expectations and societal norms is vital, especially in high-stakes situations such as healthcare. In this work, we propose the use of a debate-based reward model for reinforcement learning agents, where the outcome of a zero-sum debate game quantifies the justifiability of a decision in a particular state. This reward model is then used to train a justifiable policy, whose decisions can be more easily corroborated with supporting evidence. In the debate game, two argumentative agents take turns providing supporting evidence for two competing decisions. Given the proposed evidence, a proxy of a human judge evaluates which decision is better justified. We demonstrate the potential of our approach in learning policies for prescribing and justifying treatment decisions of septic patients. We show that augmenting the reward with the feedback signal generated by the debate-based reward model yields policies highly favored by the judge when compared to the policy obtained solely from the environment rewards, while hardly sacrificing any performance. Moreover, in terms of the overall performance and justifiability of trained policies, the debate-based feedback is comparable to the feedback obtained from an ideal judge proxy that evaluates decisions using the full information encoded in the state. This suggests that the debate game outputs key information contained in states that is most relevant for evaluating decisions, which in turn substantiates the practicality of combining our approach with human-in-the-loop evaluations. Lastly, we showcase that agents trained via multi-agent debate learn to propose evidence that is resilient to refutations and closely aligns with human preferences.",NA,https://openreview.net/forum?id=OUkZXbbwQr,International Conference on Learning Representations,NA,2024,0,"@inproceedings{2-14909,
  title={Reward design for justifiable sequential decision-making},
  author={NA},
  year={2024},
  booktitle={International Conference on Learning Representations}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Analyzing, Advising","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-14910,iclr,Sociodojo: building lifelong analytical agents with real-world text and time series,"We introduce SocioDojo, an open-ended lifelong learning environment for developing ready-to-deploy autonomous agents capable of performing human-like analysis and decision-making on societal topics such as economics, finance, politics, and culture. It consists of (1) information sources from news, social media, reports, etc., (2) a knowledge base built from books, journals, and encyclopedias, plus a toolbox of Internet and knowledge graph search interfaces, (3) 30K high-quality time series in finance, economy, society, and polls, which support a novel task called <q>hyperportfolio<q>, that can reliably and scalably evaluate societal analysis and decision-making power of agents, inspired by portfolio optimization with time series as assets to <q>invest<q>. We also propose a novel Analyst-Assistant-Actuator architecture for the hyperportfolio task, and a Hypothesis & Proof prompting for producing in-depth analyses on input news, articles, etc. to assist decision-making. We perform experiments and ablation studies to explore the factors that impact performance. The results show that our proposed method achieves improvements of 32.4% and 30.4% compared to the state-of-the-art method in the two experimental settings.",NA,https://openreview.net/forum?id=s9z0HzWJJp,International Conference on Learning Representations,NA,2024,26,"@inproceedings{2-14910,
  title={Sociodojo: building lifelong analytical agents with real-world text and time series},
  author={{Anonymous}},
  year={2024},
  booktitle={International Conference on Learning Representations}
}","Algorithmic contributions, System/Artifact contributions","Generic / Abstract / Domain-agnostic, Finance / Business / Economy, Law / Policy / Governance, Everyday / Employment / Public Service",Operational,"Analyzing, Advising","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-14911,iclr,Stochastic differentially private and fair learning,"Machine learning models are increasingly used in high-stakes decision-making systems. In such applications, a major concern is that these models sometimes discriminate against certain demographic groups such as individuals with certain race, gender, or age. Another major concern in these applications is the violation of the privacy of users. While fair learning algorithms have been developed to mitigate discrimination issues, these algorithms can still leak sensitive information, such as individuals’ health or financial records. Utilizing the notion of differential privacy (DP), prior works aimed at developing learning algorithms that are both private and fair. However, existing algorithms for DP fair learning are either not guaranteed to converge or require full batch of data in each iteration of the algorithm to converge. In this paper, we provide the first stochastic differentially private algorithm for fair learning that is guaranteed to converge. Here, the term “stochastic<q> refers to the fact that our proposed algorithm converges even when minibatches of data are used at each iteration (i.e. stochastic optimization). Our framework is flexible enough to permit different fairness notions, including demographic parity and equalized odds. In addition, our algorithm can be applied to non-binary classification tasks with multiple (non-binary) sensitive attributes. As a byproduct of our convergence analysis, we provide the first utility guarantee for a DP algorithm for solving nonconvex-strongly concave min-max problems. Our numerical experiments show that the proposed algorithm consistently offers significant performance gains over the state-of-the-art baselines, and can be applied to larger scale problems with non-binary target/sensitive attributes.",NA,https://openreview.net/forum?id=3nM5uhPlfv6,International Conference on Learning Representations,Andrew Lowy;Devansh Gupta;Meisam Razaviyayn,2023,21,"@inproceedings{2-14911,
  title={Stochastic differentially private and fair learning},
  author={Lowy, Andrew and Gupta, Devansh and Razaviyayn, Meisam},
  year={2023},
  booktitle={International Conference on Learning Representations}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Institutional,"Advising, Analyzing","Decision-subject, Developer",NA,NA,NA,NA,NA,Yes,No
2-14912,iclr,Towards interpretable deep reinforcement learning with human-friendly prototypes,"Despite recent success of deep learning models in research settings, their application in sensitive domains remains limited because of their opaque decision-making processes. Taking to this challenge, people have proposed various eXplainable AI (XAI) techniques designed to calibrate trust and understandability of black-box models, with the vast majority of work focused on supervised learning. Here, we focus on making an <q>interpretable-by-design<q> deep reinforcement learning agent which is forced to use human-friendly prototypes in its decisions, thus making its reasoning process clear. Our proposed method, dubbed Prototype-Wrapper Network (PW-Net), wraps around any neural agent backbone, and results indicate that it does not worsen performance relative to black-box models. Most importantly, we found in a user study that PW-Nets supported better trust calibration and task performance relative to standard interpretability approaches and black-boxes.",NA,https://openreview.net/forum?id=hWwY_Jq0xsN,International Conference on Learning Representations,Eoin M. Kenny;Mycal Tucker;Julie Shah,2023,79,"@inproceedings{2-14912,
  title={Towards interpretable deep reinforcement learning with human-friendly prototypes},
  author={Kenny, Eoin M. and Tucker, Mycal and Shah, Julie},
  year={2023},
  booktitle={International Conference on Learning Representations}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Individual,"Executing, Explaining, Advising",Decision-maker,"Alter decision outcomes, Change trust, Change cognitive demands",Update AI competence,case-based reasoning explanations,NA,"Textual, Visual, Autonomous System",Yes,Yes
2-14993,iclr,Domain constraints improve risk prediction when outcome data is missing,"Machine learning models are often trained to predict the outcome resulting from a human decision. For example, if a doctor decides to test a patient for disease, will the patient test positive? A challenge is that historical decision-making determines whether the outcome is observed: we only observe test outcomes for patients doctors historically tested. Untested patients, for whom outcomes are unobserved, may differ from tested patients along observed and unobserved dimensions. We propose a Bayesian model class which captures this setting. The purpose of the model is to accurately estimate risk for both tested and untested patients. Estimating this model is challenging due to the wide range of possibilities for untested patients. To address this, we propose two domain constraints which are plausible in health settings: a prevalence constraint, where the overall disease prevalence is known, and an expertise constraint, where the human decision-maker deviates from purely risk-based decision-making only along a constrained feature set. We show theoretically and on synthetic data that domain constraints improve parameter inference. We apply our model to a case study of cancer risk prediction, showing that the model's inferred risk predicts cancer diagnoses, its inferred testing policy captures known public health policies, and it can identify suboptimalities in test allocation. Though our case study is in healthcare, our analysis reveals a general class of domain constraints which can improve model estimation in many settings.",NA,https://openreview.net/forum?id=1mNFsbvo2P,International Conference on Learning Representations,NA,2024,10,"@inproceedings{2-14993,
  title={Domain constraints improve risk prediction when outcome data is missing},
  author={{NA}},
  year={2024},
  booktitle={International Conference on Learning Representations}
}","Methodological contributions, Algorithmic contributions",Healthcare / Medicine / Surgery,Operational,"Forecasting, Monitoring","Knowledge provider, Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-14996,iclr,Empirical likelihood for fair classification,"Machine learning algorithms are commonly being deployed in decision-making systems that have a direct impact on human lives. However, if these algorithms are trained solely to minimize training/test errors, they may inadvertently discriminate against individuals based on their sensitive attributes, such as gender, race or age. Recently, algorithms that ensure the fairness are developed in the machine learning community. Fairness criteria are applied by these algorithms to measure the fairness, but they often use the point estimate to assess the fairness and fail to consider the uncertainty of the sample fairness criterion once the algorithms are deployed. We suggest that assessing the fairness should take the uncertainty into account. In this paper, we use the covariance as a proxy for the fairness and develop the confidence region of the covariance vector using empirical likelihood \\citep{Owen1988}. Our confidence region based fairness constraints for classification take uncertainty into consideration during fairness assessment. The proposed confidence region can be used to test the fairness and impose fairness constraint using the significant level as a tool to balance the accuracy and fairness. Simulation studies show that our method exactly covers the target Type I error rate and effectively balances the trade-off between accuracy and fairness. Finally, we conduct data analysis to demonstrate the effectiveness of our method.",NA,https://openreview.net/forum?id=GACjMj1MS1,International Conference on Learning Representations,NA,2024,640,"@inproceedings{2-14996,
  title={Empirical likelihood for fair classification},
  author={NA},
  year={2024},
  booktitle={International Conference on Learning Representations}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Institutional,"Forecasting, Executing",Decision-subject,NA,NA,NA,NA,NA,Yes,No
2-1505,acl,CookingSense: A Culinary Knowledgebase with Multidisciplinary Assertions,"This paper introduces CookingSense, a descriptive collection of knowledge assertions in the culinary domain extracted from various sources, including web data, scientific papers, and recipes, from which knowledge covering a broad range of aspects is acquired. CookingSense is constructed through a series of dictionary-based filtering and language model-based semantic filtering techniques, which results in a rich knowledgebase of multidisciplinary food-related assertions. Additionally, we present FoodBench, a novel benchmark to evaluate culinary decision support systems. From evaluations with FoodBench, we empirically prove that CookingSense improves the performance of retrieval augmented language models. We also validate the quality and variety of assertions in CookingSense through qualitative analysis.",NA,https://aclanthology.org/2024.lrec-main.354,"Joint Conference on Computational Linguistics, Language Resources and Evaluation","Choi, Donghee; Gim, Mogan; Park, Donghyeon; Sung, Mujeen; Kim, Hyunjae; Kang, Jaewoo; Choi, Jihun",2024,1,"@inproceedings{2-1505,
  title={CookingSense: A Culinary Knowledgebase with Multidisciplinary Assertions},
  author={Choi, Donghee and Gim, Mogan and Park, Donghyeon and Sung, Mujeen and Kim, Hyunjae and Kang, Jaewoo and Choi, Jihun},
  year={2024},
  booktitle={Joint Conference on Computational Linguistics, Language Resources and Evaluation}
}",Dataset/Benchmark contributions,Everyday / Employment / Public Service,Individual,"Advising, Analyzing, Explaining",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15081,iclr,Learning to make adherence-aware advice,"As artificial intelligence (AI) systems play an increasingly prominent role in human decision-making, challenges surface in the realm of human-AI interactions. One challenge arises from the suboptimal AI policies due to the inadequate consideration of humans disregarding AI recommendations, as well as the need for AI to provide advice selectively when it is most pertinent. This paper presents a sequential decision-making model that (i) takes into account the human's adherence level (the probability that the human follows/rejects machine advice) and (ii) incorporates a defer option so that the machine can temporarily refrain from making advice. We provide learning algorithms that learn the optimal advice policy and make advice only at critical time stamps. Compared to problem-agnostic reinforcement learning algorithms, our specialized learning algorithms not only enjoy better theoretical convergence properties but also show strong empirical performance.",NA,https://openreview.net/forum?id=RgELE1dQXx,International Conference on Learning Representations,NA,2024,21,"@inproceedings{2-15081,
  title={Learning to make adherence-aware advice},
  author={Unknown},
  year={2024},
  booktitle={International Conference on Learning Representations}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Operational,"Advising, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-151,aaai,Does Explainable Artificial Intelligence Improve Human Decision-Making?,"Explainable AI provides insights to users into the why for model predictions, offering potential for users to better understand and trust a model, and to recognize and correct AI predictions that are incorrect. Prior research on human and explainable AI interactions has focused on measures such as interpretability, trust, and usability of the explanation. There are mixed findings whether explainable AI can improve actual human decision-making and the ability to identify the problems with the underlying model. Using real datasets, we compare objective human decision accuracy without AI( control) , with an AI prediction( no explanation) , and AI prediction with explanation. We find providing any kind of AI prediction tends to improve user decision accuracy, but no conclusive evidence that explainable AI has a meaningful impact. Moreover, we observed the strongest predictor for human decision accuracy was AI accuracy and that users were somewhat able to detect when the AI was correct vs. incorrect, but this was not significantly affected by including an explanation. Our results indicate that, at least in some situations, the why information provided in explainable AI may not enhance user decision-making, and further research may be needed to understand how to integrate explainable AI into real systems.",10.1609/aaai.v35i8.16819,https://ojs.aaai.org/index.php/AAAI/article/view/16819,AAAI Conference on Artificial Intelligence,Yasmeen Alufaisan;Laura R. Marusich;Jonathan Z. Bakdash;Yan Zhou;Murat Kantarcioglu,2021,0,"@inproceedings{2-151,
  title     = {Does Explainable Artificial Intelligence Improve Human Decision-Making?},
  author    = {Yasmeen Alufaisan and Laura R. Marusich and Jonathan Z. Bakdash and Yan Zhou and Murat Kantarcioglu},
  year      = {2021},
  doi       = {10.1609/aaai.v35i8.16819},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence}
}",Empirical contributions,"Law / Policy / Governance, Finance / Business / Economy, Generic / Abstract / Domain-agnostic",Individual,"Forecasting, Advising, Explaining",Decision-maker,"Alter decision outcomes, Change affective-perceptual",no such info,"textual explanations, prediction of alternative",NA,"Textual, Conversational/Natural Language",Yes,Yes
2-15104,iclr,Rényi fair inference,"Machine learning algorithms have been increasingly deployed in critical automated decision-making systems that directly affect human lives. When these algorithms are solely trained to minimize the training/test error, they could suffer from systematic discrimination against individuals based on their sensitive attributes, such as gender or race. Recently, there has been a surge in machine learning society to develop algorithms for fair machine learning. In particular, several adversarial learning procedures have been proposed to impose fairness. Unfortunately, these algorithms either can only impose fairness up to linear dependence between the variables, or they lack computational convergence guarantees. In this paper, we use Rényi correlation as a measure of fairness of machine learning models and develop a general training framework to impose fairness. In particular, we propose a min-max formulation which balances the accuracy and fairness when solved to optimality. For the case of discrete sensitive attributes, we suggest an iterative algorithm with theoretical convergence guarantee for solving the proposed min-max problem. Our algorithm and analysis are then specialized to fair classification and fair clustering problems. To demonstrate the performance of the proposed Rényi fair inference framework in practice, we compare it with well-known existing methods on several benchmark datasets. Experiments indicate that the proposed method has favorable empirical performance against state-of-the-art approaches.",NA,https://openreview.net/forum?id=HkgsUJrtDB,International Conference on Learning Representations,Sina Baharlouei;Maher Nouiehed;Ahmad Beirami;Meisam Razaviyayn,2020,0,"@inproceedings{2-15104,
  title={R{\'e}nyi fair inference},
  author={Baharlouei, Sina and Nouiehed, Maher and Beirami, Ahmad and Razaviyayn, Meisam},
  year={2020},
  booktitle={International Conference on Learning Representations}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Institutional,"Executing, Analyzing","Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-1514,acl,CryptoTrade: A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading,"The utilization of Large Language Models (LLMs) in financial trading has primarily been concentrated within the stock market, aiding in economic and financial decisions. Yet, the unique opportunities presented by the cryptocurrency market, noted for its on-chain data's transparency and the critical influence of off-chain signals like news, remain largely untapped by LLMs. This work aims to bridge the gap by developing an LLM-based trading agent, CryptoTrade, which uniquely combines the analysis of on-chain and off-chain data. This approach leverages the transparency and immutability of on-chain data, as well as the timeliness and influence of off-chain signals, providing a comprehensive overview of the cryptocurrency market. CryptoTrade incorporates a reflective mechanism specifically engineered to refine its daily trading decisions by analyzing the outcomes of prior trading decisions. This research makes two significant contributions. Firstly, it broadens the applicability of LLMs to the domain of cryptocurrency trading. Secondly, it establishes a benchmark for cryptocurrency trading strategies. Through extensive experiments, CryptoTrade has demonstrated superior performance in maximizing returns compared to time-series baselines, but not compared to traditional trading signals, across various cryptocurrencies and market conditions. Our code and data are available at https://github.com/Xtra-Computing/CryptoTrade",10.18653/v1/2024.emnlp-main.63,https://aclanthology.org/2024.emnlp-main.63,Empirical Methods in Natural Language Processing,"Li, Yuan; Luo, Bingqiao; Wang, Qian; Chen, Nuo; Liu, Xu; He, Bingsheng",2024,5,"@inproceedings{2-1514,
  title = {CryptoTrade: A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading},
  author = {Li, Yuan and Luo, Bingqiao and Wang, Qian and Chen, Nuo and Liu, Xu and He, Bingsheng},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-main.63},
  booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
}",System/Artifact contributions,Finance / Business / Economy,Individual,"Analyzing, Executing","Guardian, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-15203,iclr,Poetree: interpretable policy learning with adaptive decision trees,"Building models of human decision-making from observed behaviour is critical to better understand, diagnose and support real-world policies such as clinical care. As established policy learning approaches remain focused on imitation performance, they fall short of explaining the demonstrated decision-making process. Policy Extraction through decision Trees (POETREE) is a novel framework for interpretable policy learning, compatible with fully-offline and partially-observable clinical decision environments -- and builds probabilistic tree policies determining physician actions based on patients' observations and medical history. Fully-differentiable tree architectures are grown incrementally during optimization to adapt their complexity to the modelling task, and learn a representation of patient history through recurrence, resulting in decision tree policies that adapt over time with patient information. This policy learning method outperforms the state-of-the-art on real and synthetic medical datasets, both in terms of understanding, quantifying and evaluating observed behaviour as well as in accurately replicating it -- with potential to improve future decision support systems.",NA,https://openreview.net/forum?id=AJsI-ymaKn_,International Conference on Learning Representations,Alizée Pace;Alex Chan;Mihaela van der Schaar,2022,33,"@inproceedings{2-15203,
  title={Poetree: interpretable policy learning with adaptive decision trees},
  author={Pace, Alizée and Chan, Alex and van der Schaar, Mihaela},
  year={2022},
  booktitle={International Conference on Learning Representations}
}","Algorithmic contributions, Empirical contributions",Healthcare / Medicine / Surgery,Operational,"Explaining, Analyzing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-15358,iclr,Clairvoyance: a pipeline toolkit for medical time series,"Time-series learning is the bread and butter of data-driven *clinical decision support*, and the recent explosion in ML research has demonstrated great potential in various healthcare settings. At the same time, medical time-series problems in the wild are challenging due to their highly *composite* nature: They entail design choices and interactions among components that preprocess data, impute missing values, select features, issue predictions, estimate uncertainty, and interpret models. Despite exponential growth in electronic patient data, there is a remarkable gap between the potential and realized utilization of ML for clinical research and decision support. In particular, orchestrating a real-world project lifecycle poses challenges in engineering (i.e. hard to build), evaluation (i.e. hard to assess), and efficiency (i.e. hard to optimize). Designed to address these issues simultaneously, Clairvoyance proposes a unified, end-to-end, autoML-friendly pipeline that serves as a (i) software toolkit, (ii) empirical standard, and (iii) interface for optimization. Our ultimate goal lies in facilitating transparent and reproducible experimentation with complex inference workflows, providing integrated pathways for (1) personalized prediction, (2) treatment-effect estimation, and (3) information acquisition. Through illustrative examples on real-world data in outpatient, general wards, and intensive-care settings, we illustrate the applicability of the pipeline paradigm on core tasks in the healthcare journey. To the best of our knowledge, Clairvoyance is the first to demonstrate viability of a comprehensive and automatable pipeline for clinical time-series ML.",NA,https://openreview.net/forum?id=xnC8YwKUE3k,International Conference on Learning Representations,Daniel Jarrett;Jinsung Yoon;Ioana Bica;Zhaozhi Qian;Ari Ercole;Mihaela van der Schaar,2021,0,"@inproceedings{2-15358,
  title={Clairvoyance: a pipeline toolkit for medical time series},
  author={Jarrett, Daniel and Yoon, Jinsung and Bica, Ioana and Qian, Zhaozhi and Ercole, Ari and van der Schaar, Mihaela},
  year={2021},
  booktitle={International Conference on Learning Representations}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Developer","Change trust, Change cognitive demands, Change affective-perceptual",no such info,"personalized prediction, treatment-effect estimation, information acquisition",NA,"Textual, Conversational/Natural Language, Autonomous System",Yes,Yes
2-15359,iclr,Dilu: a knowledge-driven approach to autonomous driving with large language models,"Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain. Project page: https://pjlab-adg.github.io/DiLu/",NA,https://openreview.net/forum?id=OqTMUPuLuC,International Conference on Learning Representations,NA,2024,282,"@inproceedings{2-15359,
  title={Dilu: a knowledge-driven approach to autonomous driving with large language models},
  author={{N/A}},
  year={2024},
  booktitle={International Conference on Learning Representations}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Explaining, Executing, Analyzing",Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-15360,iclr,Hazard challenge: embodied decision making in dynamically changing environments,"Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world. Typically, these environments remain unchanged unless agents interact with them. However, in real-world scenarios, agents might also face dynamically changing environments characterized by unexpected events and need to rapidly take action accordingly. To remedy this gap, we propose a new simulated embodied benchmark, called HAZARD, specifically designed to assess the decision-making abilities of embodied agents in dynamic situations. HAZARD consists of three unexpected disaster scenarios, including fire, flood, and wind, and specifically supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making. This benchmark enables us to evaluate autonomous agents' decision-making capabilities across various pipelines, including reinforcement learning (RL), rule-based, and search-based methods in dynamically changing environments. As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.",NA,https://openreview.net/forum?id=n6mLhaBahJ,International Conference on Learning Representations,NA,2024,378,"@inproceedings{2-15360,
  title = {Hazard Challenge: Embodied Decision Making in Dynamically Changing Environments},
  author = {{NA}},
  year = {2024},
  booktitle = {International Conference on Learning Representations}
}",Dataset/Benchmark contributions,"Generic / Abstract / Domain-agnostic, Defense / Military / Emergency",Operational,"Explaining, Executing",Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-15365,ieee,A Data-Driven Optimal Control Decision-Making System for Multiple Autonomous Vehicles,"With the fast development and rising popularity of autonomous vehicle (AV) technology, multiple AVs may soon be driving on the same road simultaneously. Such multi-AV coexistence driving situations will lead to new and persistent challenges. Therefore, improvements on making control decisions for multiple AVs becomes necessary for continued driving safety. In this paper, we propose a multi-AV decision making system (MADM), which considers multi-AV coexistence driving situations during the decision-making process. In MADM, we first build a policy formation method to generate policies that learn the driving behaviors of an expert based on the expert's driving trajectory data. We then develop a multi-AV decision-making method, which adjusts the formed policies through multi-agent reinforcement learning. The adjusted policies make control decisions for multiple AVs with safety guarantee. We used a real-world traffic dataset to evaluate the decision making performance of MADM in comparison with several state-of-the-art methods. Experimental results show that MADM reduces emergency rate by as high as 51% when compared with existing methods.",10.1145/3453142.3493686,https://ieeexplore.ieee.org/document/9708991,IEEE/ACM Symposium on Edge Computing,Liuwang Kang;Haiying Shen,2021,1,"@inproceedings{2-15365,
  title = {A Data-Driven Optimal Control Decision-Making System for Multiple Autonomous Vehicles},
  author = {Liuwang Kang and Haiying Shen},
  year = {2021},
  doi = {10.1145/3453142.3493686},
  booktitle = {IEEE/ACM Symposium on Edge Computing}
}",System/Artifact contributions,Transportation / Mobility / Planning,Individual,"Executing, Advising","Knowledge provider, Developer",NA,NA,NA,NA,NA,Yes,No
2-15366,ieee,A Decision-Making Strategy for Vehicle Autonomous Braking in Emergency via Deep Reinforcement Learning,"Autonomous braking through vehicle precise decision-making and control to reduce accidents is a key issue, especially in the early diffusion phase of autonomous vehicle development. This paper proposes a deep reinforcement learning (DRL)-based autonomous braking decision-making strategy in an emergency situation. Three key influencing factors, including efficiency, accuracy and passengers' comfort, are fully considered and satisfied by the proposed strategy. First, the vehicle lane-changing process and the braking process are analyzed in detail, which include the critical factors in the design of the autonomous braking strategy. Second, we propose a DRL process that determines the optimal strategy for autonomous braking. Particularly, a multi-objective reward function is designed, which can compromise the rewards achieved of different brake moments, the degree of the accident, and the comfort of the passenger. Third, a typical actor-critic (AC) algorithm named deep deterministic policy gradient (DDPG) is adopted for solving the autonomous braking problem, which can improve the efficiency of the optimal strategy and be stable in continuous control tasks. Once the strategy is well trained, the vehicle can automatically take optimal braking behavior in an emergency to improve driving safety. Extensive simulations validate the effectiveness and efficiency of our proposal in terms of learning effectiveness, decision-making accuracy and driving safety.",10.1109/TVT.2020.2986005,https://ieeexplore.ieee.org/document/9067008,IEEE Transactions on Vehicular Technology,Yuchuan Fu;Changle Li;Fei Richard Yu;Tom H. Luan;Yao Zhang,2020,194,"@article{2-15366,
  title = {A Decision-Making Strategy for Vehicle Autonomous Braking in Emergency via Deep Reinforcement Learning},
  author = {Yuchuan Fu and Changle Li and Fei Richard Yu and Tom H. Luan and Yao Zhang},
  year = {2020},
  doi = {10.1109/TVT.2020.2986005},
  journal = {IEEE Transactions on Vehicular Technology}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,"Developer, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-15369,ieee,A Generative Adversarial Network Based Learning Approach to the Autonomous Decision Making of High-Speed Trains,"Nowadays, the autonomous driving transportation systems are at the heart of both academic and industry research for the distinguished advantages including increased network capacity, enhanced punctuality, greater flexibility and improved overall safety level. With the responsibility of transporting passengers in a safe, comfortable and efficient way, the decision making method plays a critical position in the autonomous driving of high-speed trains. Focusing on solving the autonomous decision making problem, this paper proposes a novel learning based framework by combining the deep learning technology with the distributed tracking control approach. To cope with the data insufficiency problem in training the deep learning network, a generative adversarial network (GAN) based data argumentation scheme is proposed to generate data samples that have the same distribution with actual data samples, and a hybrid learning network is constructed to predict the speed trajectory from the multi-attribute data with both temporal sequences and static features. Then, based on the model predictive control (MPC) scheme, a distributed tracking control model is formulated to minimize the tracking deviations and balance the performance of punctuality, energy-efficiency and riding comfort. Further, the dual decomposition technique is adopted to deal with the coupling constraints for the safe distance headway such that the separation for the autonomous driving of high-speed trains is achieved. Finally, simulation experiments based on actual scenarios of the Beijing-Shanghai high-speed railway are conducted to illustrate the effectiveness of our methods.",10.1109/TVT.2022.3141880,https://ieeexplore.ieee.org/document/9678037,IEEE Transactions on Vehicular Technology,Xi Wang;Tianpeng Xin;Hongwei Wang;Li Zhu;Dongliang Cui,2022,24,"@article{2-15369,
  title={A Generative Adversarial Network Based Learning Approach to the Autonomous Decision Making of High-Speed Trains},
  author={Wang, Xi and Xin, Tianpeng and Wang, Hongwei and Zhu, Li and Cui, Dongliang},
  year={2022},
  doi={10.1109/TVT.2022.3141880},
  journal={IEEE Transactions on Vehicular Technology}
}",Methodological contributions,Transportation / Mobility / Planning,Individual,"Executing, Forecasting",Developer,NA,NA,NA,NA,NA,Yes,No
2-15370,ieee,A Health Decision Support System for Disease Diagnosis Based on Wearable Medical Sensors and Machine Learning Ensembles,"Even with an annual expenditure of more than $3 trillion, the U.S. healthcare system is far from optimal. For example, the third leading cause of death in the U.S. is preventable medical error, immediately after heart disease and cancer. Computer-based clinical decision support systems (CDSSs) have been proposed to address such deficiencies and have significantly improved clinical practice over the past decade. However, they remain limited to clinics and hospitals, and do not take advantage of patient data that are obtained on a daily basis using wearable medical sensors (WMSs) that have the ability to bridge this information gap. WMSs can collect physiological signals from anyone anywhere anytime. Thus, they have the potential to usher in an era of pervasive healthcare. However, most prior work on WMSs only focuses on hardware and protocol design, and not on an information system that can fully utilize the collected signals for efficient disease diagnosis. In this paper, for the first time, we introduce a hierarchical health decision support system for disease diagnosis that integrates health data from WMSs into CDSSs. The proposed system has a multi-tier structure, starting with a WMS tier, backed by robust machine learning, that enables diseases to be tracked individually by a disease diagnosis module. We demonstrate the feasibility of such a system through six disease diagnosis modules aimed at four ICD-10-CM disease categories. We show that the system is scalable using five more disease categories. Just the WMS tier offers impressive diagnostic accuracies for various diseases: arrhythmia (86 percent), type-2 diabetes (78 percent), urinary bladder disorder (99 percent), renal pelvis nephritis (94 percent), and hypothyroid (95 percent). We estimate that the disease diagnosis modules of all known 69,000 human diseases would require just 62 GB of storage space in the WMS tier. This is practical even in today's cloud or base station oriented WMS systems.",10.1109/TMSCS.2017.2710194,https://ieeexplore.ieee.org/document/7936635,IEEE Transactions on Multi-Scale Computing Systems,Hongxu Yin;Niraj K. Jha,2017,172,"@article{2-15370,
  title={A Health Decision Support System for Disease Diagnosis Based on Wearable Medical Sensors and Machine Learning Ensembles},
  author={Yin, Hongxu and Jha, Niraj K.},
  year={2017},
  doi={10.1109/TMSCS.2017.2710194},
  journal={IEEE Transactions on Multi-Scale Computing Systems}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Individual,"Advising, Explaining","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-15372,ieee,A Human Feedback-Driven Decision-Making Method Based on Multi-Modal Deep Reinforcement Learning in Ethical Dilemma Traffic Scenarios,"Ethical decision-making in autonomous vehicles has been a significant area of research since the emergence of the Trolley Problem. However, current studies fail to effectively incorporate the operative state of the vehicle and instead rely exclusively on sociological attributes for decision-making. This paper establishes three ethical traffic scenarios that reflect the most typical ethical dilemmas. Based on this, we examine the ethical decision-making of autonomous vehicles in each scenario. Firstly, to enable the decision-making system of autonomous vehicles to solve ethical dilemmas, a coupled ethical reward function model is innovatively proposed based on human feedback that integrates knowledge from sociology, economics, and vehicle dynamics. Furthermore, an ethics-driven multi-modal network model is proposed to extract morphological features and dynamic features from perceptual information and road test data, respectively. Finally, an ethical simulation experiment is conducted, which demonstrates that the decision-making strategies generated by the proposed model in the ethical traffic scenario are more aligned with human intentions compared to those of the control group.",10.1109/ITSC57777.2023.10422393,https://ieeexplore.ieee.org/document/10422393,IEEE International Conference on Intelligent Transportation Systems,Xin Gao;Tian Luan;Xueyuan Li;Qi Liu;Xiaoqiang Meng;Zirui Li,2023,0,"@inproceedings{2-15372,
  title = {A Human Feedback-Driven Decision-Making Method Based on Multi-Modal Deep Reinforcement Learning in Ethical Dilemma Traffic Scenarios},
  author = {Gao, Xin and Luan, Tian and Li, Xueyuan and Liu, Qi and Meng, Xiaoqiang and Li, Zirui},
  year = {2023},
  doi = {10.1109/ITSC57777.2023.10422393},
  booktitle = {IEEE International Conference on Intelligent Transportation Systems}
}",Methodological contributions,Transportation / Mobility / Planning,Individual,Executing,Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-15373,ieee,A Hybrid Tactical Decision-Making Approach in Automated Driving Combining Knowledge-Based Systems and Reinforcement Learning,"Decision-making in automated driving is influenced both by objective traffic rules and subjective perceptions and goals of the driver. Thus, a suitable representation of the environment of the autonomous vehicle is required to model complex traffic situations and extract key features. To achieve this objective, this work uses an ontology-based situation interpretation (OBSI) to model traffic situations. The resulting semantic state representation is used to train models of vehicle-controlling agents using reinforcement learning. Based on our simulations, it can be shown that the semantic preprocessing of traffic situations significantly improves the agent's performance regarding safety and driving style.",10.1109/ITSC55140.2022.9922505,https://ieeexplore.ieee.org/document/9922505,IEEE International Conference on Intelligent Transportation Systems,Julius Fiedler;Maximilian Gerwien;Carsten Knoll,2022,3,"@inproceedings{2-15373,
  title={A Hybrid Tactical Decision-Making Approach in Automated Driving Combining Knowledge-Based Systems and Reinforcement Learning},
  author={Fiedler, Julius and Gerwien, Maximilian and Knoll, Carsten},
  year={2022},
  doi={10.1109/ITSC55140.2022.9922505},
  booktitle={IEEE International Conference on Intelligent Transportation Systems}
}",Methodological contributions,Transportation / Mobility / Planning,Individual,"Executing, Analyzing","Developer, Decision-subject, Knowledge provider, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-15374,ieee,A Learning Classifier System Approach to Time-Critical Decision-Making in Dynamic Alternate Airport Selection,"The goal of the paper is to address the need for methods to handle time-sensitive, human-centered, multi-criteria decision-making problems. In the current literature, prevalent methods rely on expressing decision-maker/stakeholder preferences through weights, ideal points, and trade-off matrices. However, these conventional approaches prove unsuitable for time-constrained, atypical, and stressful situations, such as emergencies. In such scenarios, where both time and additional factors significantly affect decision-making abilities, the effective utilization of advanced decision-making techniques becomes chal-lenging. Therefore, this paper explores the possibility of how an intelligent agent might be used to provide possible courses of action to human decision-makers/stakeholders. The agent will be put to the test to tackle the dynamic alternate airport selection problem. In emergency and time-critical situations, like an engine fire or a medical emergency, there is often a need to select an alternate airport destination dynamically midflight. During such emergencies, a lot of information must be collected and evaluated by the pilots as a basis for the decision-making process. The pilots need to compare multiple characteristics of the available airports and weigh the pros and cons of each. Given the need for clear and interpretable retroactive analysis in decision-making in general and in the aviation field in particular, the focus was placed on more interpretable and explainable models from the field of AI. Due to this, the Learning Classifier System (LCS) is to be the primary model explored. The LCS is trained on a custom dataset composed of various decision-making scenarios. The approach shows promising results and appears to merit further investigation.",10.1109/CEC60901.2024.10612016,https://ieeexplore.ieee.org/document/10612016,IEEE Congress on Evolutionary Computation,Boris Djartov;Sanaz Mostaghim;Anne Papenfuß;Matthias Wies,2024,0,"@inproceedings{2-15374,
  title     = {A Learning Classifier System Approach to Time-Critical Decision-Making in Dynamic Alternate Airport Selection},
  author    = {Boris Djartov and Sanaz Mostaghim and Anne Papenfuß and Matthias Wies},
  year      = {2024},
  doi       = {10.1109/CEC60901.2024.10612016},
  booktitle = {IEEE Congress on Evolutionary Computation}
}",Methodological contributions,Transportation / Mobility / Planning,Organizational,"Advising, Explaining","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-15380,ieee,A Real-World Reinforcement Learning Framework for Safe and Human-Like Tactical Decision-Making,"Lane-change decision-making for vehicles is a challenging task for many reasons, including traffic rules, safety, and the stochastic nature of driving. Because of its success in solving complex problems, deep reinforcement learning (DRL) has been suggested for addressing these issues. However, the studies on DRL to date have gone no further than validation in simulation and failed to address what are arguably the most critical issues, namely, the mismatch between simulation and reality, human-likeness, and safety. This paper introduces a real-world DRL framework for decision-making to design safe and human-like agents that can operate in the real world without extra tuning. We propose a new learning paradigm for DRL integrated with Real2Sim transfer, which comprises training, validation, and testing phases. The approach involves two simulator environments with different levels of fidelity, which are parameterized via real-world data. Within the framework, a large amount of randomized experience is generated with a low-fidelity simulator, whereupon the learned skills are validated regularly in a high-fidelity simulator to avoid overfitting. Finally, in the testing phase, the agent is examined concerning safety and human-like decision-making. Extensive simulation and real-world evaluations show the superiority of the proposed approach. To the best of the authors’ knowledge, this is the first application of DRL lane-changing policy in the real world.",10.1109/TITS.2023.3292981,https://ieeexplore.ieee.org/document/10194473,IEEE Transactions on Intelligent Transportation Systems,Muharrem Ugur Yavas;Tufan Kumbasar;Nazim Kemal Ure,2023,16,"@article{2-15380,
  title={A Real-World Reinforcement Learning Framework for Safe and Human-Like Tactical Decision-Making},
  author={Muharrem Ugur Yavas and Tufan Kumbasar and Nazim Kemal Ure},
  year={2023},
  doi={10.1109/TITS.2023.3292981},
  journal={IEEE Transactions on Intelligent Transportation Systems}
}",Methodological contributions,Transportation / Mobility / Planning,Individual,Executing,Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-15381,ieee,A Reinforcement Learning Approach to Autonomous Decision Making of Intelligent Vehicles on Highways,"Autonomous decision making is a critical and difficult task for intelligent vehicles in dynamic transportation environments. In this paper, a reinforcement learning approach with value function approximation and feature learning is proposed for autonomous decision making of intelligent vehicles on highways. In the proposed approach, the sequential decision making problem for lane changing and overtaking is modeled as a Markov decision process with multiple goals, including safety, speediness, smoothness, etc. In order to learn optimized policies for autonomous decision-making, a multiobjective approximate policy iteration (MO-API) algorithm is presented. The features for value function approximation are learned in a data-driven way, where sparse kernel-based features or manifold-based features can be constructed based on data samples. Compared with previous RL algorithms such as multiobjective Q-learning, the MO-API approach uses data-driven feature representation for value and policy approximation so that better learning efficiency can be achieved. A highway simulation environment using a 14 degree-of-freedom vehicle dynamics model was established to generate training data and test the performance of different decision-making methods for intelligent vehicles on highways. The results illustrate the advantages of the proposed MO-API method under different traffic conditions. Furthermore, we also tested the learned decision policy on a real autonomous vehicle to implement overtaking decision and control under normal traffic on highways. The experimental results also demonstrate the effectiveness of the proposed method.",10.1109/TSMC.2018.2870983,https://ieeexplore.ieee.org/document/8571191,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",Xin Xu;Lei Zuo;Xin Li;Lilin Qian;Junkai Ren;Zhenping Sun,2020,221,"@article{2-15381,
  title={A Reinforcement Learning Approach to Autonomous Decision Making of Intelligent Vehicles on Highways},
  author={Xu, Xin and Zuo, Lei and Li, Xin and Qian, Lilin and Ren, Junkai and Sun, Zhenping},
  year={2020},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume={50},
  number={8},
  pages={2920--2934},
  doi={10.1109/TSMC.2018.2870983}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,"Decision-subject, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-15382,ieee,A Resource-Constrained and Privacy-Preserving Edge-Computing-Enabled Clinical Decision System: A Federated Reinforcement Learning Approach,"Internet-of-Things-enabled E-health system, which could monitor and collect the personal health information (PHI), has gradually transformed the clinical treatment to a more personalized way with in-home monitoring smart devices. Then, with the collected PHI, clinical decision support systems (CDSSs), which are based on data mining techniques and historical electronic medical records (EMRs) to help clinicians make proper treatment decisions, have attracted considerable attention. To address issues, such as network congestion and low rate of responsiveness for traditional methods when implementing CDSSs, we integrate the technologies mobile-edge computing (MEC) and software-defined networking for exploiting the computation resources and storage capacities among edge nodes (ENs) (i.e., MEC servers) in our model. Based on this integrated system, each edge node will deploy a double deep Q-network (DDQN) to obtain a stable and sequential clinical treatment policy. It is enabled by a novel fully decentralized federated framework (FDFF) for aggregating models of DDQN and extracting the knowledge from EMRs across all ENs. Furthermore, we discuss the convergence of FDFF in resource-constrained environments. However, since most EMRs are faced with stringent privacy concerns, we adopt two additively homomorphic encryption schemes to prevent leakage of EMRs' privacy during the training process of FDFF. Finally, we measure the time cost of our additively homomorphic encryption schemes and validate DDQN with experiments on large data sets based on FDFF, which shows promising performance on clinician treatment.",10.1109/JIOT.2021.3057653,https://ieeexplore.ieee.org/document/9349772,IEEE Internet of Things Journal,Zeyue Xue;Pan Zhou;Zichuan Xu;Xiumin Wang;Yulai Xie;Xiaofeng Ding;Shiping Wen,2021,0,"@article{2-15382,
  title={A Resource-Constrained and Privacy-Preserving Edge-Computing-Enabled Clinical Decision System: A Federated Reinforcement Learning Approach},
  author={Xue, Zeyue and Zhou, Pan and Xu, Zichuan and Wang, Xiumin and Xie, Yulai and Ding, Xiaofeng and Wen, Shiping},
  year={2021},
  journal={IEEE Internet of Things Journal},
  doi={10.1109/JIOT.2021.3057653}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Executing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-15384,ieee,A Safe Pre-Decision Making Approach for Power System Voltage Stability Enhancement via Safe Deep Reinforcement Learning,"The high penetration of renewable energy and power electronic equipment brings difficulties to emergency control decision making against various presumed contingencies in power systems. Artificial intelligence provides some solutions to this problem, but the existing method lacks security guarantees due to the absence of hard constraints. In this paper, we propose a pre-decision making approach for power system short-term voltage stability based on safe reinforcement learning (SRL). In this work, both pre-decision formulation and correction are based on neural networks to reduce the reliance on specific system parameters. By exploiting the gradient of the evaluated risk probability, we propose a security projecting correction algorithm to correct risky actions. Test results based on the IEEE 39-bus system illustrate the effectiveness of the proposed approach.",10.1109/PESGM51994.2024.10688758,https://ieeexplore.ieee.org/document/10688758,IEEE Power & Energy Society General Meeting,Congbo Bi;Lipeng Zhu;Di Liu;Chao Lu,2024,0,"@inproceedings{2-15384,
  title     = {A Safe Pre-Decision Making Approach for Power System Voltage Stability Enhancement via Safe Deep Reinforcement Learning},
  author    = {Congbo Bi and Lipeng Zhu and Di Liu and Chao Lu},
  year      = {2024},
  doi       = {10.1109/PESGM51994.2024.10688758},
  booktitle = {IEEE Power \& Energy Society General Meeting}
}",Algorithmic contributions,Environment / Resources / Energy,Operational,Executing,Guardian,NA,NA,NA,NA,NA,Yes,No
2-15389,ieee,A Unified Framework for Differentiated Services in Intelligent Healthcare Systems,"The Coronavirus disease 2019 (COVID-19) outbreak continues to significantly expose the vulnerabilities of healthcare systems around the world. These unprecedented circumstances create an opportunity for improving healthcare services which is desperately needed. This paper proposes a novel framework that distributes the patients across heterogeneous medical facilities (MFs) so that a weighted sum of the expected service time (EST) and service time tail probability (STTP) for all patients is minimized. We propose a model-based and model-free algorithms to schedule patients requests across the MFs. Our algorithms prioritize the patients with severe/critical conditions over others who can tolerate more delay in service. Based on the model-based approach, we formulate an optimization problem as a convex combination of both EST and STTP metrics, and apply an efficient iterative algorithm to solve it. Then, a more practical model-free scheme is proposed by adopting a deep reinforcement learning approach. Our model-free approach does not rely on pre-defined models or assumptions about the environment. Rather, it learns to choose scheduling decisions solely through observations of the resulting performance of past decisions. Our extensive results demonstrate a significant performance improvement of our proposed scheduling schemes when compared with other algorithms and competitive baselines.",10.1109/TNSE.2021.3127942,https://ieeexplore.ieee.org/document/9614994,IEEE Transactions on Network Science and Engineering,A. O. Al-Abbasi;L. Samara;S. Salem;R. Hamila;N. Al-Dhahir,2022,4,"@article{2-15389,
  title = {A Unified Framework for Differentiated Services in Intelligent Healthcare Systems},
  author = {Al-Abbasi, A. O. and Samara, L. and Salem, S. and Hamila, R. and Al-Dhahir, N.},
  year = {2022},
  doi = {10.1109/TNSE.2021.3127942},
  journal = {IEEE Transactions on Network Science and Engineering}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,Executing,Decision-subject,NA,NA,NA,NA,NA,Yes,No
2-15393,ieee,Adaptive Decision-Making for Automated Vehicles Under Roundabout Scenarios Using Optimization Embedded Reinforcement Learning,"The roundabout is a typical changeable, interactive scenario in which automated vehicles should make adaptive and safe decisions. In this article, an optimization embedded reinforcement learning (OERL) is proposed to achieve adaptive decision-making under the roundabout. The promotion is the modified actor of the Actor–Critic framework, which embeds the model-based optimization method in reinforcement learning to explore continuous behaviors in action space directly. Therefore, the proposed method can determine the macroscale behavior (change lane or not) and medium-scale behaviors of desired acceleration and action time simultaneously with high sample efficiency. When scenarios change, medium-scale behaviors can be adjusted timely by the embedded direct search method, promoting the adaptability of decision-making. More notably, the modified actor matches human drivers’ behaviors, macroscale behavior captures the human mind’s jump, and medium-scale behaviors are preferentially adjusted through driving skills. To enable the agent adapts to different types of the roundabout, task representation is designed to restructure the policy network. In experiments, the algorithm efficiency and the learned driving strategy are compared with decision-making containing macroscale behavior and constant medium-scale behaviors of the desired acceleration and action time. To investigate the adaptability, the performance under an untrained type of roundabout and two more dangerous situations are simulated to verify that the proposed method changes the decisions with changeable scenarios accordingly. The results show that the proposed method has high algorithm efficiency and better system performance.",10.1109/TNNLS.2020.3042981,https://ieeexplore.ieee.org/document/9311168,IEEE Transactions on Neural Networks and Learning Systems,Yuxiang Zhang;Bingzhao Gao;Lulu Guo;Hongyan Guo;Hong Chen,2021,89,"@article{2-15393,
  title={Adaptive Decision-Making for Automated Vehicles Under Roundabout Scenarios Using Optimization Embedded Reinforcement Learning},
  author={Zhang, Yuxiang and Gao, Bingzhao and Guo, Lulu and Guo, Hongyan and Chen, Hong},
  year={2021},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  doi={10.1109/TNNLS.2020.3042981}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,"Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-15396,ieee,AdViCE: Aggregated Visual Counterfactual Explanations for Machine Learning Model Validation,"Rapid improvements in the performance of machine learning models have pushed them to the forefront of data-driven decision-making. Meanwhile, the increased integration of these models into various application domains has further highlighted the need for greater interpretability and transparency. To identify problems such as bias, overfitting, and incorrect correlations, data scientists require tools that explain the mechanisms with which these model decisions are made. In this paper we introduce AdViCE, a visual analytics tool that aims to guide users in black-box model debugging and validation. The solution rests on two main visual user interface innovations: (1) an interactive visualization design that enables the comparison of decisions on user-defined data subsets; (2) an algorithm and visual design to compute and visualize counterfactual explanations - explanations that depict model outcomes when data features are perturbed from their original values. We provide a demonstration of the tool through a use case that showcases the capabilities and potential limitations of the proposed approach.",10.1109/VIS49827.2021.9623271,https://ieeexplore.ieee.org/document/9623271,IEEE Visualization Conference,Oscar Gomez;Steffen Holter;Jun Yuan;Enrico Bertini,2021,0,"@inproceedings{2-15396,
  title={AdViCE: Aggregated Visual Counterfactual Explanations for Machine Learning Model Validation},
  author={Gomez, Oscar and Holter, Steffen and Yuan, Jun and Bertini, Enrico},
  booktitle={IEEE Visualization Conference},
  year={2021},
  doi={10.1109/VIS49827.2021.9623271}
}",System/Artifact contributions,Generic / Abstract / Domain-agnostic,no such info,"Advising, Explaining",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15397,ieee,AI-Driven Decision Making for Auxiliary Diagnosis of Epidemic Diseases,"Delay in diagnosis often leads to difficulties in the treatment of epidemics and disease spread. Therefore, early diagnosis plays an important role in the control of epidemic diseases. The rise of artificial intelligence(AI) technology provides more intelligent and effective methods for realizing auxiliary epidemic diagnosis. This paper first designs an auxiliary diagnosis architecture for epidemic, which supports data collection and processing for long-term monitoring of the target state. Then, using the iterative characteristics of time sequential decision, an auxiliary diagnosis decision-making model based on the partially observable Markov decision process is built to achieve early diagnosis of epidemics. Combined with state abstraction, a deep Q-learning auxiliary diagnosis(DQAD) algorithm is proposed to improve the timeliness and accuracy of epidemic diagnosis. Extensive simulations have been carried out to evaluate DQAD in terms of several performance criteria including average time per iteration and diagnosis accuracy. The result analysis verifies that the designed method is more accurate and reduces the diagnosis time than existing methods.",10.1109/TMBMC.2021.3120646,https://ieeexplore.ieee.org/document/9576516,"IEEE Transactions on Molecular, Biological, and Multi-Scale Communications",Kai Lin;Jiayi Liu;Jian Gao,2022,18,"@article{2-15397,
  title={AI-Driven Decision Making for Auxiliary Diagnosis of Epidemic Diseases},
  author={Lin, Kai and Liu, Jiayi and Gao, Jian},
  year={2022},
  doi={10.1109/TMBMC.2021.3120646},
  journal={IEEE Transactions on Molecular, Biological, and Multi-Scale Communications}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-15398,ieee,Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems,"Algorithmic systems that employ machine learning play an increasing role in making substantive decisions in modern society, ranging from online personalization to insurance and credit decisions to predictive policing. But their decision-making processes are often opaque-it is difficult to explain why a certain decision was made. We develop a formal foundation to improve the transparency of such decision-making systems. Specifically, we introduce a family of Quantitative Input Influence (QII) measures that capture the degree of influence of inputs on outputs of systems. These measures provide a foundation for the design of transparency reports that accompany system decisions (e.g., explaining a specific credit decision) and for testing tools useful for internal and external oversight (e.g., to detect algorithmic discrimination). Distinctively, our causal QII measures carefully account for correlated inputs while measuring influence. They support a general class of transparency queries and can, in particular, explain decisions about individuals (e.g., a loan decision) and groups (e.g., disparate impact based on gender). Finally, since single inputs may not always have high influence, the QII measures also quantify the joint influence of a set of inputs (e.g., age and income) on outcomes (e.g. loan decisions) and the marginal influence of individual inputs within such a set (e.g., income). Since a single input may be part of multiple influential sets, the average marginal influence of the input is computed using principled aggregation measures, such as the Shapley value, previously applied to measure influence in voting. Further, since transparency reports could compromise privacy, we explore the transparency-privacy tradeoff and prove that a number of useful transparency reports can be made differentially private with very little addition of noise.  Our empirical validation with standard machine learning algorithms demonstrates that QII measures are a useful transparency mechanism when black box access to the learning system is available. In particular, they provide better explanations than standard associative measures for a host of scenarios that we consider. Further, we show that in the situations we consider, QII is efficiently approximable and can be made differentially private while preserving accuracy.",10.1109/SP.2016.42,https://ieeexplore.ieee.org/document/7546525,IEEE Symposium on Security and Privacy,Anupam Datta;Shayak Sen;Yair Zick,2016,1149,"@inproceedings{2-15398,
  title={Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems},
  author={Datta, Anupam and Sen, Shayak and Zick, Yair},
  year={2016},
  booktitle={Proceedings of the IEEE Symposium on Security and Privacy},
  doi={10.1109/SP.2016.42}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Operational,"Explaining, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-15406,ieee,An Integrated Decision-Making Framework for Highway Autonomous Driving Using Combined Learning and Rule-Based Algorithm,"In order to solve the manual labelling, long-tail effect and driving conservatism of the existing decision-making algorithm. This paper proposed an integrated decision-making framework (IDF) for highway autonomous vehicles. Firstly, states of the highway traffic are extracted by the velocity, time headway (TH) and the probabilistic lane distribution of the surrounding vehicles. With the extracted traffic state, the reinforcement learning (RL) is adopted to learn the optimal state-action pair for specific scenario. Analogously, by mapping millions of traffic scenarios, huge amounts of state-action pairs can be stored in the experience pool. Then the imitation learning (IL) is further employed to memorize the experience pool by deep neural networks. The learning result shows that the accuracy of the decision network can reach 94.17%. Besides, for some imperfect decisions of the network, the rule-based method is taken to rectify by judging the long-term reward. Finally, the IDF is simulated in G25 highway and has promising results, which can always drive the vehicle to the state with high efficiency while ensuring safety.",10.1109/TVT.2022.3150343,https://ieeexplore.ieee.org/document/9712209,IEEE Transactions on Vehicular Technology,Can Xu;Wanzhong Zhao;Jinqiang Liu;Chunyan Wang;Chen Lv,2022,56,"@article{2-15406,
  title={An Integrated Decision-Making Framework for Highway Autonomous Driving Using Combined Learning and Rule-Based Algorithm},
  author={Xu, Can and Zhao, Wanzhong and Liu, Jinqiang and Wang, Chunyan and Lv, Chen},
  year={2022},
  journal={IEEE Transactions on Vehicular Technology},
  doi={10.1109/TVT.2022.3150343}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,Developer,NA,NA,NA,NA,NA,Yes,No
2-15407,ieee,An Integrated Framework of Lateral and Longitudinal Behavior Decision-Making for Autonomous Driving Using Reinforcement Learning,"Lateral lane-changing and longitudinal car-following behavior decision-making is crucial for the implementation of autonomous driving (AD) in complex and dynamic traffic environment. Reinforcement learning (RL) has been increasingly applied to these behaviors due to its powerful environmental adaptability. However, typical RL-based research focused solely on one directional decision-making or output discrete actions in both lateral and longitudinal directions, which is not sufficiently flexible for AD. In view of the close interaction between lane-changing and car-following tasks, this study proposed an integrated framework to realize coupled decision-making using RL. Specifically, double deep Q-network (DDQN) was used to make the lateral lane change command decision-making, and twin delay deep deterministic policy gradient (TD3) used to make longitudinal car-following acceleration decision-making. The integration of DDQN and TD3 was deployed to tackle the problem of discrete-continuous hybrid action space in RL. The lateral decision-making was integrated into the actor-critic network for longitudinal decision-making to generate accurate acceleration in lane-changing or lane-keeping conditions. A series of simulation experiments were conducted in typical two-lane straight driving scenarios, validating the feasibility and applicability of the proposed integrated framework. The results reveal that the RL-based coupled decision-making helps enhance the implementation of AD.",10.1109/TVT.2024.3377288,https://ieeexplore.ieee.org/document/10472707,IEEE Transactions on Vehicular Technology,Haoyuan Ni;Guizhen Yu;Peng Chen;Bin Zhou;Yaping Liao;Han Li,2024,0,"@article{2-15407,
  title={An Integrated Framework of Lateral and Longitudinal Behavior Decision-Making for Autonomous Driving Using Reinforcement Learning},
  author={Ni, Haoyuan and Yu, Guizhen and Chen, Peng and Zhou, Bin and Liao, Yaping and Li, Han},
  year={2024},
  doi={10.1109/TVT.2024.3377288},
  journal={IEEE Transactions on Vehicular Technology}
}","Algorithmic contributions, Methodological contributions",Transportation / Mobility / Planning,Individual,"Executing, Analyzing",Developer,NA,NA,NA,NA,NA,Yes,No
2-15408,ieee,An Integrated Model for Autonomous Speed and Lane Change Decision-Making Based on Deep Reinforcement Learning,"The implementation of autonomous driving is inseparable from developing intelligent driving decision-making models, which are facing high scene complexity, poor decision-making coupling, and the inability to guarantee decision-making safety. This paper starts with the priority and logic of lane change and car-following decision-making, considering driving efficiency, safety, and comfort, then constructs a double-layer decision-making model. This paper uses two deep reinforcement learning algorithms for the upper and lower layers to process large-scale mixed state space and ensure the composite action output of lane-changing decisions and car-following decisions. In the upper layer model, we use the D3QN algorithm to distinguish the potential value of the environment and the value of selecting lane-changing actions when making lane-changing decisions. Different from the traditional mechanisms that only use negative rewards, the lane changing benefit function and dangerous action shielding mechanism are used to eliminate collisions. DDPG algorithm is adopted in the lower layer model to process car-following decisions and output continuous vehicle speed control. Besides, coupled training is taken for the two algorithms to improve the coordination of the double-layer model. This paper selected mixed standard driving cycle conditions to build a highly complex training environment and used NGSIM data to reconstruct scenes to test our model. Simulations in SUMO are presented that the double-layer model can increase the driving speed of the original data by 23.99%, which has higher effectiveness than other models.",10.1109/TITS.2022.3185255,https://ieeexplore.ieee.org/document/9843863,IEEE Transactions on Intelligent Transportation Systems,Jiankun Peng;Siyu Zhang;Yang Zhou;Zhibin Li,2022,93,"@article{2-15408,
  title={An Integrated Model for Autonomous Speed and Lane Change Decision-Making Based on Deep Reinforcement Learning},
  author={Jiankun Peng and Siyu Zhang and Yang Zhou and Zhibin Li},
  year={2022},
  doi={10.1109/TITS.2022.3185255},
  journal={IEEE Transactions on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,Developer,NA,NA,NA,NA,NA,Yes,No
2-15412,ieee,Artificial Intelligence and Smart Cities: A DEMATEL Approach to Adaptation Challenges and Initiatives,"This article focused on two main topics currently on among many agendas: smart cities and Artificial Intelligence (AI). The growing interest in the former is due to these cities’ multidimensionality and adaptability in terms of residents’ needs and the requirements of each municipality's reality. AI, in turn, currently plays a transformative, disruptive role in various areas by performing “smart” tasks, thereby facilitating the automation of processes and differentiation initiatives. Smart city strategic planning, especially in the long term, will most likely have to deal with adaptations to AI. Thus, research is needed that can contribute to a more holistic view of these topics and support decision-making processes in these areas. Based on the epistemological principles of the multiple-criteria decision analysis approach (MCDA), this article developed and tested a dynamic analysis system that allows smart city initiatives to address the challenges of adapting to AI. The proposed system highlights the cause-and-effect relationships in this context. The article included an application of the decision-making trial and evaluation laboratory (DEMATEL) technique. The procedural steps followed to implement this methodology were enhanced by close collaboration with an experienced decision maker, who has coordinated various projects in this research context. The proposed system's contributions and limitations were also analyzed in this article.",10.1109/TEM.2021.3098665,https://ieeexplore.ieee.org/document/9530200,IEEE Transactions on Engineering Management,Cátia A. R. Freire;Fernando A. F. Ferreira;Elias G. Carayannis;João J. M. Ferreira,2023,40,"@article{2-15412,
  title={Artificial Intelligence and Smart Cities: A DEMATEL Approach to Adaptation Challenges and Initiatives},
  author={C{\'a}tia A. R. Freire and Fernando A. F. Ferreira and Elias G. Carayannis and Jo{\~a}o J. M. Ferreira},
  year={2023},
  doi={10.1109/TEM.2021.3098665},
  journal={IEEE Transactions on Engineering Management}
}",System/Artifact contributions,Transportation / Mobility / Planning,Organizational,"Analyzing, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15415,ieee,ATHENA: Machine Learning and Reasoning for Radio Resources Scheduling in vRAN Systems,"Next-generation mobile networks will rely on their autonomous operation. Virtual Network Functions empowered by Artificial Intelligence (AI) and Machine Learning (ML) can adapt to varying environments that encompass both network conditions and the cloud platform executing them. In this view, it becomes paramount to understand why AI/ML algorithms made a decision, to be able to reason upon those decisions and, eventually, take further decisions related to e.g., network orchestration. In this paper, we present \nATHENA\n, an ML-based radio resource scheduler for virtualized Radio Access Network (RAN) system. Our real-software implementation shows that the proposed ML-based approach can outperform the baseline solution. We discuss how additional re-orchestration actions can be taken by analyzing our scheduling decisions and learning from the past.",10.1109/JSAC.2023.3336155,https://ieeexplore.ieee.org/document/10329922,IEEE Journal on Selected Areas in Communications,Nikolaos Apostolakis;Marco Gramaglia;Livia Elena Chatzieleftheriou;Tejas Subramanya;Albert Banchs;Henning Sanneck,2024,0,"@article{2-15415,
  title = {ATHENA: Machine Learning and Reasoning for Radio Resources Scheduling in vRAN Systems},
  author = {Nikolaos Apostolakis and Marco Gramaglia and Livia Elena Chatzieleftheriou and Tejas Subramanya and Albert Banchs and Henning Sanneck},
  year = {2024},
  doi = {10.1109/JSAC.2023.3336155},
  journal = {IEEE Journal on Selected Areas in Communications}
}",System/Artifact contributions,"Software / Systems / Security, Generic / Abstract / Domain-agnostic",Operational,"Explaining, Executing","Developer, Guardian",NA,NA,NA,NA,NA,Yes,No
2-15416,ieee,Attention-Based Interrelation Modeling for Explainable Automated Driving,"Automated driving desires better performance on tasks like motion planning and interacting with pedestrians in mixed-traffic environments. Deep learning algorithms can achieve high performance in these tasks with remarkable visual scene understanding and generalization abilities. However, when common scene-parsing methods are used to train end-to-end models, limitations of explainability in such algorithms inhibit their implementations in fully automated driving. The main challenges include algorithm performance deficiencies and inconsistencies, insufficient AI transparency, degraded user trust, and undermining human-AI interactions. This research aids the decision-making performance and transparency of automated driving systems by providing multi-modal explanations, especially when interacting with pedestrians. The proposed algorithm combines global visual features and interrelation features by parsing scene images as self-constructed graphs and using an attention-based module to capture the interrelationship among the ego-vehicle and other traffic-related objects. The output modules make decisions while simultaneously generating semantic text explanations. The results show that the fusion of the features from global frames and interrelational graphs improves decision-making and explanation predictions compared to two state-of-the-art benchmark algorithms. The interrelation module also enhances algorithm transparency by disclosing the visual attention used for decision-making. The importance of interrelation features on the two prediction tasks is further revealed along with the underlying mechanism of multitask learning on the datasets with hierarchical labels. The proposed model improves driving decision-making during pedestrian interactions with intelligible reasoning cues for building an appropriate mental model of automated driving performance for human users.",10.1109/TIV.2022.3229682,https://ieeexplore.ieee.org/document/9991055,IEEE Transactions on Intelligent Vehicles,Zhengming Zhang;Renran Tian;Rini Sherony;Joshua Domeyer;Zhengming Ding,2023,90,"@article{2-15416,
  title={Attention-Based Interrelation Modeling for Explainable Automated Driving},
  author={Zhang, Zhengming and Tian, Renran and Sherony, Rini and Domeyer, Joshua and Ding, Zhengming},
  year={2023},
  doi={10.1109/TIV.2022.3229682},
  journal={IEEE Transactions on Intelligent Vehicles}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Explaining, Advising, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15418,ieee,Augmented Reinforcement Learning with Efficient Social-Based Motion Prediction for Autonomous Decision-Making,"This paper presents an approach that improves the efficiency and generalization capabilities of Reinforcement Learning-based autonomous vehicles operating in urban driving scenarios. The proposed method introduces an Efficient Social-based Motion Prediction module, which predicts the future positions of vehicles within the scenario. These predictions serve as input to a Reinforcement Learning-based Decision-Making module, responsible for executing high-level actions. The Proximal Policy Optimization algorithm is employed to develop our approach. We conduct experiments in an unsignalized T-intersection scenario using the SMARTS framework, comparing our approach with and without the proposed state representation, as well as against various baseline methods. Through this study, we demonstrate that our approach achieves performance improvements, particularly in scenarios involving high velocities. Our code and qualitative results are available at https://github.com/Cram3r95/argo2goalmp.",10.1109/ITSC57777.2023.10422277,https://ieeexplore.ieee.org/document/10422277,IEEE International Conference on Intelligent Transportation Systems,Rodrigo Gutiérrez-Moreno;Carlos Gómez-Huelamo;Rafael Barea;Elena López-Guillén;Felipe Arango;Luis M. Bergasa,2023,0,"@inproceedings{2-15418,
  title={Augmented Reinforcement Learning with Efficient Social-Based Motion Prediction for Autonomous Decision-Making},
  author={Guti{\'e}rrez-Moreno, Rodrigo and G{\'o}mez-Huelamo, Carlos and Barea, Rafael and L{\'o}pez-Guill{\'e}n, Elena and Arango, Felipe and Bergasa, Luis M.},
  year={2023},
  doi={10.1109/ITSC57777.2023.10422277},
  booktitle={IEEE International Conference on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Organizational,"Forecasting, Executing",Decision-subject,NA,NA,NA,NA,NA,Yes,No
2-15419,ieee,Augmenting Reinforcement Learning With Transformer-Based Scene Representation Learning for Decision-Making of Autonomous Driving,"Decision-making for urban autonomous driving is challenging due to the stochastic nature of interactive traffic participants and the complexity of road structures. Although reinforcement learning (RL)-based decision-making schemes are promising to handle urban driving scenarios, they suffer from low sample efficiency and poor adaptability. In this paper, we propose the Scene-Rep Transformer to enhance RL decision-making capabilities through improved scene representation encoding and sequential predictive latent distillation. Specifically, a multi-stage Transformer (MST) encoder is constructed to model not only the interaction awareness between the ego vehicle and its neighbors but also intention awareness between the agents and their candidate routes. A sequential latent Transformer (SLT) with self-supervised learning objectives is employed to distill future predictive information into the latent scene representation, in order to reduce the exploration space and speed up training. The final decision-making module based on soft actor-critic (SAC) takes as input the refined latent scene representation from the Scene-Rep Transformer and generates decisions. The framework is validated in five challenging simulated urban scenarios with dense traffic, and its performance is manifested quantitatively by substantial improvements in data efficiency and performance in terms of success rate, safety, and efficiency. Qualitative results reveal that our framework is able to extract the intentions of neighbor agents, enabling better decision-making and more diversified driving behaviors.",10.1109/TIV.2024.3372625,https://ieeexplore.ieee.org/document/10458341,IEEE Transactions on Intelligent Vehicles,Haochen Liu;Zhiyu Huang;Xiaoyu Mo;Chen Lv,2024,64,"@article{2-15419,
  title={{Augmenting Reinforcement Learning With Transformer-Based Scene Representation Learning for Decision-Making of Autonomous Driving}},
  author={Haochen Liu and Zhiyu Huang and Xiaoyu Mo and Chen Lv},
  year={2024},
  doi={10.1109/TIV.2024.3372625},
  journal={IEEE Transactions on Intelligent Vehicles}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Forecasting, Executing","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-15423,ieee,Autonomous Reinforcement Control of Visual Underwater Vehicles: Real-Time Experiments Using Computer Vision,"Swift decision-making based on visual environment perception is crucial for autonomous control of visual underwater vehicles (VUVs) during underwater missions. However, learning perception and decision models individually might result in weak robustness of overall control system as the mismatched state extraction and control decision making are asynchronous. As a remedy, we will introduce in this paper an end-to-end monocular autonomous reinforcement control (MARC) framework for autonomous control of VUVs, which is performed in two cascaded procedures, i.e., 1) perception, where a geometric network (GeoNet) is designed based on a convolutional encoder-decoder network to generate depth maps from input environmental videos; 2) decision, where with depth maps as input, a reinforcement control network (CtrlNet) integrates a convolutional neural network into a deep deterministic policy gradient network and outputs action decisions, which are refined by reinforcement learning algorithm for obstacle-avoiding based autonomous control. Numerical and experimental results demonstrate that the proposed MARC exhibits high-quality depth prediction and is capable of conducting obstacle-avoiding navigation and autonomous control of VUVs with high accuracy and strong robustness.",10.1109/TVT.2022.3177596,https://ieeexplore.ieee.org/document/9780564,IEEE Transactions on Vehicular Technology,Pengli Zhu;Siyuan Liu;Tao Jiang;Yancheng Liu;Xuzhou Zhuang;Zhenrui Zhang,2022,15,"@article{2-15423,
  title={Autonomous Reinforcement Control of Visual Underwater Vehicles: Real-Time Experiments Using Computer Vision},
  author={Zhu, Pengli and Liu, Siyuan and Jiang, Tao and Liu, Yancheng and Zhuang, Xuzhou and Zhang, Zhenrui},
  year={2022},
  journal={IEEE Transactions on Vehicular Technology},
  volume={},
  number={},
  pages={},
  doi={10.1109/TVT.2022.3177596}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,Developer,NA,NA,NA,NA,NA,Yes,No
2-15433,ieee,Combining Decision Making and Trajectory Planning for Lane Changing Using Deep Reinforcement Learning,"In the context of Automated Vehicles, the Automated Lane Change system, is fundamentally based upon the separate constructs of Perception, Decision making, Trajectory Planning, and Execution. However, in existing works there are many simplistic and unplausible assumptions in applying these constructs that severely restrict their operational effectiveness in realistic and complex driving scenarios. For instance, there are rigid assumptions about the disposition of vehicles and that lane-changing maneuvers can occur instantaneously, but that highly desirable features such as the ability for real-time trajectory re-planning are lacking. In this paper, we address these limitations through an integrated methodology for lane-change decision making and trajectory planning, in which a deep Reinforcement Learning algorithm with a safe action set technique is employed in decision making that is effectively coupled to a specially devised trajectory planning model. The proposed new methodology is computationally efficient, supporting real-time implementation, and provides for lane-changing maneuvers that can be made simultaneously with other vehicles and can be dynamically re-planned; thus, enabling flexible, robust, and safe lane-changing maneuvers under the guidance of a new decision-making module. Finally, the veracity of the proposed methodology in guiding a vehicle to improve travel times and accomplish high-level driving behaviors such as overtaking and desired-speed maintenance in a range of road traffic scenarios is demonstrated in a number of numerical experiments.",10.1109/TITS.2022.3148085,https://ieeexplore.ieee.org/document/9726894,IEEE Transactions on Intelligent Transportation Systems,Shurong Li;Chong Wei;Ying Wang,2022,16,"@article{2-15433,
  title={Combining Decision Making and Trajectory Planning for Lane Changing Using Deep Reinforcement Learning},
  author={Li, Shurong and Wei, Chong and Wang, Ying},
  year={2022},
  doi={10.1109/TITS.2022.3148085},
  journal={IEEE Transactions on Intelligent Transportation Systems}
}","Algorithmic contributions, Methodological contributions",Transportation / Mobility / Planning,Individual,"Executing, Advising",Decision-subject,NA,NA,NA,NA,NA,Yes,No
2-15439,ieee,Cooperative Decision-Making of Connected and Autonomous Vehicles in an Emergency,"Safety is one of the major concerns in autonomous driving tasks, and enhancing the collision avoidance ability of connected and autonomous vehicles (CAVs) is an effective way to improve road safety. Most current autonomous driving algorithms make braking and stopping decisions in a traffic emergency. However, such decisions may not be optimal. In this paper, we study the optimal collision avoidance decision-making method for emergencies. To address this challenge, we propose a cooperative decision-making scheme for CAVs in an emergency. Unlike previous decision-making methods, our scheme enables vehicles to avoid collisions by indicating the optimal emergency destinations, which leads to a new task: the prediction of the optimal collision avoidance destination. In the proposed scheme, the potential collision avoidance destinations are evaluated based on a deep reinforcement learning (DRL) model, and we propose a safety evaluation map (SEM) to describe the evaluation results. The cooperative ability of CAVs is considered in the proposed scheme, and a well-designed reward function is applied to train the DRL model. Extensive experiments demonstrate that the proposed model can accurately evaluate potential collision avoidance destinations and is effective in reducing traffic accident rates and accident damage in various traffic emergencies compared to state-of-the-art baseline methods.",10.1109/TVT.2022.3211884,https://ieeexplore.ieee.org/document/9910427,IEEE Transactions on Vehicular Technology,Pin Lv;Jinlei Han;Jiangtian Nie;Yang Zhang;Jia Xu;Chao Cai;Zhe Chen,2023,116,"@article{2-15439,
  title={Cooperative Decision-Making of Connected and Autonomous Vehicles in an Emergency},
  author={Lv, Pin and Han, Jinlei and Nie, Jiangtian and Zhang, Yang and Xu, Jia and Cai, Chao and Chen, Zhe},
  year={2023},
  doi={10.1109/TVT.2022.3211884},
  journal={IEEE Transactions on Vehicular Technology}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Forecasting, Advising","Decision-maker, Developer, Guardian",NA,NA,NA,NA,NA,Yes,No
2-15440,ieee,Cooperative Driving of Connected Autonomous Vehicles in Heterogeneous Mixed Traffic: A Game Theoretic Approach,"High-density, unsignalized intersections have always been a bottleneck of efficiency and safety. The emergence of Connected Autonomous Vehicles (CAVs) results in a mixed traffic condition, further increasing the complexity of the transportation system. Against this background, this paper aims to study the intricate and heterogeneous interaction of vehicles and conflict resolution at the high-density, mixed, unsignalized intersection. Theoretical insights about the interaction between CAVs and Human-driven Vehicles (HVs) and the cooperation of CAVs are synthesized, based on which a novel cooperative decision-making framework in heterogeneous mixed traffic is proposed. Normalized Cooperative game is concatenated with Level-k game (NCL game) to generate a system optimal solution. Then Lattice planner generates the optimal and collision-free trajectories for CAVs. To reproduce HVs in mixed traffic, interactions from naturalistic human driving data are extracted as prior knowledge. Non-cooperative game and Inverse Reinforcement Learning (IRL) are integrated to mimic the decision-making of heterogeneous HVs. Finally, three cases are conducted to verify the performance of the proposed algorithm, including the comparative analysis with different methods, the case study under different Rates of Penetration (ROP) and the interaction analysis with heterogeneous HVs. It is found that the proposed cooperative decision-making framework is beneficial to driving conflict resolution and the traffic efficiency improvement of the mixed unsignalized intersection. Besides, due to the consideration of driving heterogeneity, better human-machine interaction and cooperation can be realized in this paper.",10.1109/TIV.2024.3399694,https://ieeexplore.ieee.org/document/10529605,IEEE Transactions on Intelligent Vehicles,Shiyu Fang;Peng Hang;Chongfeng Wei;Yang Xing;Jian Sun,2024,21,"@article{2-15440,
  title={Cooperative Driving of Connected Autonomous Vehicles in Heterogeneous Mixed Traffic: A Game Theoretic Approach},
  author={Fang, Shiyu and Hang, Peng and Wei, Chongfeng and Xing, Yang and Sun, Jian},
  year={2024},
  doi={10.1109/TIV.2024.3399694},
  journal={IEEE Transactions on Intelligent Vehicles}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15446,ieee,DECE: Decision Explorer with Counterfactual Explanations for Machine Learning Models,"With machine learning models being increasingly applied to various decision-making scenarios, people have spent growing efforts to make machine learning models more transparent and explainable. Among various explanation techniques, counterfactual explanations have the advantages of being human-friendly and actionable-a counterfactual explanation tells the user how to gain the desired prediction with minimal changes to the input. Besides, counterfactual explanations can also serve as efficient probes to the models' decisions. In this work, we exploit the potential of counterfactual explanations to understand and explore the behavior of machine learning models. We design DECE, an interactive visualization system that helps understand and explore a model's decisions on individual instances and data subsets, supporting users ranging from decision-subjects to model developers. DECE supports exploratory analysis of model decisions by combining the strengths of counterfactual explanations at instance- and subgroup-levels. We also introduce a set of interactions that enable users to customize the generation of counterfactual explanations to find more actionable ones that can suit their needs. Through three use cases and an expert interview, we demonstrate the effectiveness of DECE in supporting decision exploration tasks and instance explanations.",10.1109/TVCG.2020.3030342,https://ieeexplore.ieee.org/document/9229232,IEEE Transactions on Visualization and Computer Graphics,Furui Cheng;Yao Ming;Huamin Qu,2021,159,"@article{2-15446,
  title={DECE: Decision Explorer with Counterfactual Explanations for Machine Learning Models},
  author={Cheng, Furui and Ming, Yao and Qu, Huamin},
  year={2021},
  doi={10.1109/TVCG.2020.3030342},
  journal={IEEE Transactions on Visualization and Computer Graphics}
}",System/Artifact contributions,Generic / Abstract / Domain-agnostic,no such info,"Advising, Explaining","Decision-maker, Developer",Alter decision outcomes,"Update AI competence, Change AI responses","counterfactual explanations, data storage, visual analysis",NA,"Textual, Visual, Conversational/Natural Language",Yes,Yes
2-15447,ieee,Decision Making for Autonomous Driving via Augmented Adversarial Inverse Reinforcement Learning,"Making decisions in complex driving environments is a challenging task for autonomous agents. Imitation learning methods have great potentials for achieving such a goal. Adversarial Inverse Reinforcement Learning (AIRL) is one of the state-of-art imitation learning methods that can learn both a behavioral policy and a reward function simultaneously, yet it is only demonstrated in simple and static environments where no interactions are introduced. In this paper, we improve and stabilize AIRL’s performance by augmenting it with semantic rewards in the learning framework. Additionally, we adapt the augmented AIRL to a more practical and challenging decision-making task in a highly interactive environment in autonomous driving. The proposed method is compared with four baselines and evaluated by four performance metrics. Simulation results show that the augmented AIRL outperforms all the baseline methods, and its performance is comparable with that of the experts on all of the four metrics.",10.1109/ICRA48506.2021.9560907,https://ieeexplore.ieee.org/document/9560907,IEEE International Conference on Robotics and Automation,Pin Wang;Dapeng Liu;Jiayu Chen;Hanhan Li;Ching-Yao Chan,2021,67,"@inproceedings{2-15447,
  title     = {Decision Making for Autonomous Driving via Augmented Adversarial Inverse Reinforcement Learning},
  author    = {Pin Wang and Dapeng Liu and Jiayu Chen and Hanhan Li and Ching-Yao Chan},
  year      = {2021},
  doi       = {10.1109/ICRA48506.2021.9560907},
  booktitle = {IEEE International Conference on Robotics and Automation}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Explaining, Executing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-15448,ieee,Decision Making for Driving Agent in Traffic Simulation via Adversarial Inverse Reinforcement Learning,"Traffic simulation has the potential to facilitate the development and testing of autonomous vehicles, as a supplement to road testing. Since autonomous vehicles will coexist with human drivers in the transportation system for a period of time, it is important to have intelligent driving agents in traffic simulation to interact with them just like human drivers. Directly learning from human drivers' driving behavior is an attractive solution with potential. In this study, Adversarial Inverse Reinforcement Learning (AIRL) is applied to learn decision-making policies in complex and interactive traffic simulation environments with high traffic density. Bird's Eye View (BEV) is proposed as an observation model for driving agents, providing effective information for the agents' decision-making. Results show that compared with Behavioral Cloning (BC) and Proximal Policy Optimization (PPO), the driving agents generated by AIRL demonstrate higher levels of safety and robustness and they are capable of imitating the car-following and lane-changing characteristics from expert demonstrations. The results further confirm that different driving characteristics can be learned based on AIRL method.",10.1109/ITSC57777.2023.10421936,https://ieeexplore.ieee.org/document/10421936,IEEE International Conference on Intelligent Transportation Systems,Naiting Zhong;Junyi Chen;Yining Ma;Wei Jiang,2023,1,"@inproceedings{2-15448,
  title={Decision Making for Driving Agent in Traffic Simulation via Adversarial Inverse Reinforcement Learning},
  author={Zhong, Naiting and Chen, Junyi and Ma, Yining and Jiang, Wei},
  year={2023},
  doi={10.1109/ITSC57777.2023.10421936},
  booktitle={IEEE International Conference on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Analyzing, Executing","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-15449,ieee,Decision Making for Human-in-the-loop Robotic Agents via Uncertainty-Aware Reinforcement Learning,"In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly autonomously in solving a task, but can request help from an external expert when needed. However, knowing when to request such assistance is critical: too few requests can lead to the robot making mistakes, but too many requests can overload the expert. In this paper, we present a Reinforcement Learning based approach to this problem, where a semi-autonomous agent asks for external assistance when it has low confidence in the eventual success of the task. The confidence level is computed by estimating the variance of the return from the current state. We show that this estimate can be iteratively improved during training using a Bellman-like recursion. On discrete navigation problems with both fully-and partially-observable state information, we show that our method makes effective use of a limited budget of expert calls at run-time, despite having no access to the expert at training time.",10.1109/ICRA57147.2024.10611425,https://ieeexplore.ieee.org/document/10611425,IEEE International Conference on Robotics and Automation,Siddharth Singi;Zhanpeng He;Alvin Pan;Sandip Patel;Gunnar A. Sigurdsson;Robinson Piramuthu;Shuran Song;Matei Ciocarlie,2024,0,"@inproceedings{2-15449,
  title = {Decision Making for Human-in-the-loop Robotic Agents via Uncertainty-Aware Reinforcement Learning},
  author = {Siddharth Singi and Zhanpeng He and Alvin Pan and Sandip Patel and Gunnar A. Sigurdsson and Robinson Piramuthu and Shuran Song and Matei Ciocarlie},
  year = {2024},
  doi = {10.1109/ICRA57147.2024.10611425},
  booktitle = {IEEE International Conference on Robotics and Automation}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Individual,"Executing, Collaborating","Knowledge provider, Developer, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-15450,ieee,Decision Making in Health Care Diagnosis: Evidence From Parkinson's Disease Via Hybrid Machine Learning,"Health care is a complex system that demands critical decision making, especially in the diagnosis of various conditions in patients. To minimize possible errors in diagnosis, an emerging technology, machine learning (ML), is being effectively used. ML classifiers can be used to proactively diagnose the medical conditions, which are identified based on the presence or absence of specific characteristics of the diseases. Therefore, in this article, we demonstrate how ML can be used to determine Parkinson's disease (PD) and thereby, provide early diagnosis using nonclinical data of the patients. Novel ensembles are developed in this article to improve the diagnostic capability and the experimental results show that the improved versions of artificial neural network (ANN) could yield 13.4% more accurate results compared with the traditional ANN classifier. PD is considered a challenging medical condition, owing to its global relevance and complexity in diagnosis. Moreover, the early detection of PD is instrumental for patient recovery, and any lapses in diagnosis can lead to an immeasurable loss to patients. Also, the study has developed an effective diagnostic tool for PD and detects the disease at an early stage using the voice data of individuals, and this will aid in making better clinical decisions related to PD, thus rendering better health services.",10.1109/TEM.2021.3096862,https://ieeexplore.ieee.org/document/9507055,IEEE Transactions on Engineering Management,Jinil Persis Devarajan;V. Raja Sreedharan;Gopalakrishnan Narayanamurthy,2023,26,"@article{2-15450,
  title={Decision Making in Health Care Diagnosis: Evidence From Parkinson's Disease Via Hybrid Machine Learning},
  author={Devarajan, Jinil Persis and Sreedharan, V. Raja and Narayanamurthy, Gopalakrishnan},
  year={2023},
  doi={10.1109/TEM.2021.3096862},
  journal={IEEE Transactions on Engineering Management}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Forecasting,"Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-15451,ieee,Decision Support for Aircraft Taxi Time based on Deep Metric Learning,"As an essential part of a flight life cycle, the surface taxiing process is closely related to the operational efficiency of the airport. Routing and scheduling can be optimized with an accurate prediction of aircraft taxi time in advance, thus improving the ability of refined management of airport surface. However, the past methods merely provide a taxi time predicted by their models, which are of limited help to airport controllers in decision-making. In order to alleviate this problem, this paper proposes to use a deep metric learning (DML) model to learn the similarity between historical operation scenarios based on basic flight properties, surface traffic situation, and airport weather information. For a given reference flight, the taxi time can be reasonably predicted by finding its similar historical scenarios. In this way, the relevant controllers can make flexible decisions at the tactical level. Experimental verification on the historical data of Shanghai Pudong International Airport shows that the deep model can effectively capture the similarity of taxi time between different scenarios. Besides, compared with the classical machine learning prediction models, the proposed model can predict the taxi time more accurately. With similar historical scenarios as the basis for decision support, the implementation and interpretability of Airport Collaborative Decision-Making (A-CDM) system will be enhanced.",10.1109/ITSC45102.2020.9294241,https://ieeexplore.ieee.org/document/9294241,IEEE International Conference on Intelligent Transportation Systems,Jinghan Du;Minghua Hu;Weining Zhang,2020,10,"@inproceedings{2-15451,
  title={Decision Support for Aircraft Taxi Time based on Deep Metric Learning},
  author={Du, Jinghan and Hu, Minghua and Zhang, Weining},
  year={2020},
  booktitle={IEEE International Conference on Intelligent Transportation Systems},
  doi={10.1109/ITSC45102.2020.9294241}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,"Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15461,ieee,DriveLLM: Charting the Path Toward Full Autonomous Driving With Large Language Models,"Human drivers instinctively reason with commonsense knowledge to predict hazards in unfamiliar scenarios and to understand the intentions of other road users. However, this essential capability is entirely missing from traditional decision-making systems in autonomous driving. In response, this paper presents DriveLLM, a decision-making framework that integrates large language models (LLMs) with existing autonomous driving stacks. This integration allows for commonsense reasoning in decision-making. DriveLLM also features a unique cyber-physical feedback system, allowing it to learn and improve from its mistakes. In real-world case studies, the proposed framework outperforms traditional decision-making methods in complex scenarios, including difficult edge cases. Furthermore, we propose a novel approach that allows the decision-making system to interact with human inputs while guarding against adversarial attacks. Empirical evaluations demonstrate that this framework responds correctly to complex human instructions.",10.1109/TIV.2023.3327715,https://ieeexplore.ieee.org/document/10297415,IEEE Transactions on Intelligent Vehicles,Yaodong Cui;Shucheng Huang;Jiaming Zhong;Zhenan Liu;Yutong Wang;Chen Sun;Bai Li;Xiao Wang;Amir Khajepour,2024,10,"@article{2-15461,
  title={DriveLLM: Charting the Path Toward Full Autonomous Driving With Large Language Models},
  author={Cui, Yaodong and Huang, Shucheng and Zhong, Jiaming and Liu, Zhenan and Wang, Yutong and Sun, Chen and Li, Bai and Wang, Xiao and Khajepour, Amir},
  year={2024},
  doi={10.1109/TIV.2023.3327715},
  journal={IEEE Transactions on Intelligent Vehicles}
}",System/Artifact contributions,Transportation / Mobility / Planning,Individual,"Executing, Forecasting, Explaining","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-15462,ieee,Driving Tasks Transfer Using Deep Reinforcement Learning for Decision-Making of Autonomous Vehicles in Unsignalized Intersection,"Knowledge transfer is a promising concept to achieve real-time decision-making for autonomous vehicles. This paper constructs a transfer deep reinforcement learning (RL) framework to transform the driving tasks in the intersection environments. The driving missions at the unsignalized intersection are cast into a left turn, right turn, and running straight for automated vehicles. The goal of the autonomous ego vehicle (AEV) is to drive through the intersection situation efficiently and safely. This objective promotes the studied vehicle to increase its speed and avoid crashing other vehicles. The decision-making policy learned from one driving task is transferred through three transfer rules in another driving mission and evaluated. Simulation results reveal that the decision-making strategies related to similar tasks are transferable and have a high success rate. It indicates that the presented control framework could reduce time consumption and realize online implementation. Therefore, the transfer RL concept is helpful for establishing the real-time decision-making policy for autonomous vehicles.",10.1109/TVT.2021.3121985,https://ieeexplore.ieee.org/document/9583858,IEEE Transactions on Vehicular Technology,Hong Shu;Teng Liu;Xingyu Mu;Dongpu Cao,2022,83,"@article{2-15462,
  title={Driving Tasks Transfer Using Deep Reinforcement Learning for Decision-Making of Autonomous Vehicles in Unsignalized Intersection},
  author={Shu, Hong and Liu, Teng and Mu, Xingyu and Cao, Dongpu},
  year={2022},
  doi={10.1109/TVT.2021.3121985},
  journal={IEEE Transactions on Vehicular Technology}
}",Methodological contributions,Transportation / Mobility / Planning,Individual,Executing,Developer,NA,NA,NA,NA,NA,Yes,No
2-15463,ieee,Dynamic Balancing-Charging Management for Shared Autonomous Electric Vehicle Systems: A Two-Stage Learning-Based Approach,"The proliferation of car-sharing systems, a key component of sustainable urban mobility, has significantly contributed to the shift towards green transportation modes. However, balancing supply and demand in large-scale Shared Autonomous Electric Vehicle Services (SAEVSs) remains a critical yet challenging task, particularly considering the stochastic spatio-temporal dynamics and limited charging resources. To jointly optimize fleet management and charging decisions, we propose a two-stage learning-based approach that integrates the foresight capabilities of Deep Reinforcement Learning (DRL) techniques with the precision of optimization methods. We develop a multi-agent DRL model, acting as a “Manager” that generates prescient grid-level Balancing-Charging (BC) strategies, guiding the relocation and charging decisions. Furthermore, we customize the space-time-battery network flow model to make optimal vehicle-level decisions as a “Worker”, which follows the far-sighted guidance from the Manager to make high-quality and real-time decisions. Through extensive experiments conducted on a city-scale SAEVS, we demonstrate our proposed approach significantly outperforms benchmark models in terms of both system-level profit and service quality. Our two-stage approach also lays a methodological basis for further exploring the integration of DRL and optimization techniques, with the aim of enhancing decision-making capabilities in urban mobility systems.",10.1109/ITSC57777.2023.10422187,https://ieeexplore.ieee.org/document/10422187,IEEE International Conference on Intelligent Transportation Systems,Xiaolei Zhu;Xindi Tang;Jiaohong Xie;Yang Liu,2023,5,"@inproceedings{2-15463,
  title={Dynamic Balancing-Charging Management for Shared Autonomous Electric Vehicle Systems: A Two-Stage Learning-Based Approach},
  author={Zhu, Xiaolei and Tang, Xindi and Xie, Jiaohong and Liu, Yang},
  year={2023},
  booktitle={IEEE International Conference on Intelligent Transportation Systems},
  doi={10.1109/ITSC57777.2023.10422187}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,Executing,Developer,NA,NA,NA,NA,NA,Yes,No
2-15467,ieee,Enhancing Decision-Making in Optimization through LLM-Assisted Inference: A Neural Networks Perspective,"This paper explores the seamless integration of Generative AI (GenAI) and Evolutionary Algorithms (EAs) within the domain of large-scale multi-objective optimization. Focusing on the transformative role of Large Language Models (LLMs), our study investigates the potential of LLM-Assisted Inference to automate and enhance decision-making processes. Specifically, we highlight its effectiveness in illuminating key decision variables in evolutionarily optimized solutions while articulating contextual trade-offs. Tailored to address the challenges inherent in inferring complex multi-objective optimization solutions at scale, our approach emphasizes the adaptive nature of LLMs, allowing them to provide nuanced explanations and align their language with diverse stakeholder expertise levels and domain preferences. Empirical studies underscore the practical applicability and impact of LLM-Assisted Inference in real-world decision-making scenarios.",10.1109/IJCNN60899.2024.10649965,https://ieeexplore.ieee.org/document/10649965,International Joint Conference on Neural Networks,Gaurav Singh;Kavitesh Kumar Bali,2024,0,"@inproceedings{2-15467,
  title={Enhancing Decision-Making in Optimization through LLM-Assisted Inference: A Neural Networks Perspective},
  author={Singh, Gaurav and Bali, Kavitesh Kumar},
  year={2024},
  doi={10.1109/IJCNN60899.2024.10649965},
  booktitle={International Joint Conference on Neural Networks}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Transportation / Mobility / Planning",no such info,"Explaining, Advising","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-15468,ieee,Ensemble Quantile Networks: Uncertainty-Aware Reinforcement Learning With Applications in Autonomous Driving,"Reinforcement learning (RL) can be used to create a decision-making agent for autonomous driving. However, previous approaches provide black-box solutions, which do not offer information on how confident the agent is about its decisions. An estimate of both the aleatoric and epistemic uncertainty of the agent’s decisions is fundamental for real-world applications of autonomous driving. Therefore, this paper introduces the Ensemble Quantile Networks (EQN) method, which combines distributional RL with an ensemble approach, to obtain a complete uncertainty estimate. The distribution over returns is estimated by learning its quantile function implicitly, which gives the aleatoric uncertainty, whereas an ensemble of agents is trained on bootstrapped data to provide a Bayesian estimation of the epistemic uncertainty. A criterion for classifying which decisions that have an unacceptable uncertainty is also introduced. The results show that the EQN method can balance risk and time efficiency in different occluded intersection scenarios, by considering the estimated aleatoric uncertainty. Furthermore, it is shown that the trained agent can use the epistemic uncertainty information to identify situations that the agent has not been trained for and thereby avoid making unfounded, potentially dangerous, decisions outside of the training distribution.",10.1109/TITS.2023.3251376,https://ieeexplore.ieee.org/document/10073955,IEEE Transactions on Intelligent Transportation Systems,Carl-Johan Hoel;Krister Wolff;Leo Laine,2023,2,"@article{2-15468,
  title={Ensemble Quantile Networks: Uncertainty-Aware Reinforcement Learning With Applications in Autonomous Driving},
  author={Hoel, Carl-Johan and Wolff, Krister and Laine, Leo},
  year={2023},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  doi={10.1109/TITS.2023.3251376}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Auditing",Developer,NA,NA,NA,NA,NA,Yes,No
2-15469,ieee,Ethical Alignment Decision-Making for Connected Autonomous Vehicle in Traffic Dilemmas via Reinforcement Learning From Human Feedback,"Since the introduction of the trolley problem, the ethical decision-making conundrum has evolved from autonomous vehicles (AVs) to connected autonomous vehicles (CAVs), continuing as a prominent challenge. When confronted with ethical dilemmas, CAVs must align their responses not merely with value-neutral human preferences, but also with broader moral and ethical frameworks. Consequently, to ensure that CAVs do not engage in actions that contravene established human moral principles, it is imperative that ethical considerations are meticulously integrated into their decision-making systems. In this paper, we introduce an innovative Multi-scale Multi-modal Ethical Network (M2ENet), which aims to align the autonomous vehicle decision-making system with human ethical feedback in ethical dilemma scenarios. Firstly, we extract morphological and dynamic features from sensory information and signal data, respectively, using Multi-scale Multi-modal Representation. Additionally, Ethical Policy-based Network is devised to enable autonomous vehicles to comprehend ethical information, which includes the introduction of an ethical alignment factor to ethically align the feature matrix from human feedback. Furthermore, the accuracy of ethical interaction information is improved through coupled ethical module informed by human feedback. Finally, the efficacy of the system is demonstrated through three representative ethical dilemmas in traffic scenarios, employing both simulation experiments and hardware-in-the-loop testing. The simulation experiments reveal that our proposed model can generate decision-making strategies more aligned with human preferences in ethical traffic scenarios. In addition, in our hardware-in-the-loop tests, it is observed that the average percentage of ethical bias weights decreases by 45.06% after 150 episodes of training.",10.1109/JIOT.2024.3447070,https://ieeexplore.ieee.org/document/10643139,IEEE Internet of Things Journal,Xin Gao;Tian Luan;Xueyuan Li;Qi Liu;Zhaoyang Ma;Xiaoqiang Meng;Zirui Li,2024,5,"@article{2-15469,
  title={Ethical Alignment Decision-Making for Connected Autonomous Vehicle in Traffic Dilemmas via Reinforcement Learning From Human Feedback},
  author={Gao, Xin and Luan, Tian and Li, Xueyuan and Liu, Qi and Ma, Zhaoyang and Meng, Xiaoqiang and Li, Zirui},
  year={2024},
  journal={IEEE Internet of Things Journal},
  doi={10.1109/JIOT.2024.3447070}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Advising, Analyzing","Knowledge provider, Guardian",NA,NA,NA,NA,NA,Yes,No
2-15470,ieee,Evaluating Explanations From AI Algorithms for Clinical Decision-Making: A Social Science-Based Approach,"Explainable Artificial Intelligence (XAI) techniques generate explanations for predictions from AI models. These explanations can be evaluated for (i) faithfulness to the prediction, i.e., its correctness about the reasons for prediction, and (ii) usefulness to the user. While there are metrics to evaluate faithfulness, to our knowledge, there are no automated metrics to evaluate the usefulness of explanations in the clinical context. Our objective is to develop a new metric to evaluate usefulness of AI explanations to clinicians. Usefulness evaluation needs to consider both (a) how humans generally process explanations and (b) clinicians' specific requirements from explanations presented by clinical decision support systems (CDSS). Our new scoring method can evaluate the usefulness of explanations generated by any XAI method that provides importance values for the input features of the prediction model. Our method draws on theories from social science to gauge usefulness, and uses literature-derived biomedical knowledge graphs to quantify support for the explanations from clinical literature. We evaluate our method in a case study on predicting onset of sepsis in intensive care units. Our analysis shows that the scores obtained using our method corroborate with independent evidence from clinical literature and have the required qualities expected from such a metric. Thus, our method can be used to evaluate and select useful explanations from a diverse set of XAI techniques in clinical contexts, making it a fundamental tool for future research in the design of AI-driven CDSS.",10.1109/JBHI.2024.3393719,https://ieeexplore.ieee.org/document/10508367,IEEE Journal of Biomedical and Health Informatics,Suparna Ghanvatkar;Vaibhav Rajan,2024,11,"@article{2-15470,
  title={Evaluating Explanations From AI Algorithms for Clinical Decision-Making: A Social Science-Based Approach},
  author={Ghanvatkar, Suparna and Rajan, Vaibhav},
  year={2024},
  doi={10.1109/JBHI.2024.3393719},
  journal={IEEE Journal of Biomedical and Health Informatics}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting, Explaining, Monitoring","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-15472,ieee,Evaluation of Strategic Decision taken by Autonomous Agent using Explainable AI,Autonomous intrusion detection systems assess the data intelligently and take strategic decision to detect and mitigate cyber-attacks. These decisions have to be explained and evaluated for the transparency and correctness. Explainable Artificial Intelligent (XAI) methods that explore how features contribute or influence a decision taken using an algorithm can be useful for the purpose. XAI method of Testing with Concept Activation Vectors (TCAV) has been used recently to show the importance of high level concepts for a prediction class in order to deliver explanations in the way humans communicate with each other. This work explores the possibility of using TCAV to evaluate the strategic decision made by autonomous agents. A case study in the context of DoS attack is analysed to show that TCAV scores for various DoS attack classes and normal class of KDD99 data set can be used to evaluate the strategic decisions. The proposed method of analysis provides a quantifiable method to justify the current strategy or change in the strategy if required.,10.1109/ISEA-ISAP54304.2021.9689715,https://ieeexplore.ieee.org/document/9689715,IEEE Symposium on Security and Privacy,Rendhir R. Prasad;R. R. Rejimol Robinson;Ciza Thomas;N. Balakrishnan,2021,6,"@inproceedings{2-15472,
  title={Evaluation of Strategic Decision taken by Autonomous Agent using Explainable AI},
  author={Prasad, Rendhir R. and Robinson, R. R. Rejimol and Thomas, Ciza and Balakrishnan, N.},
  year={2021},
  doi={10.1109/ISEA-ISAP54304.2021.9689715},
  booktitle={IEEE Symposium on Security and Privacy}
}",Methodological contributions,Software / Systems / Security,Operational,"Explaining, Executing",Guardian,NA,NA,NA,NA,NA,Yes,No
2-15473,ieee,Evolutionary Decision-Making and Planning for Autonomous Driving: A Hybrid Augmented Intelligence Framework,"Recently, thanks to the introduction of human feedback, Chat Generative Pre-trained Transformer (ChatGPT) has achieved remarkable success in the language processing field. Analogically, human drivers are expected to have great potential in improving the performance of autonomous driving under real-world traffic. Therefore, this study proposes a novel framework for evolutionary decision-making and planning by developing a hybrid augmented intelligence (HAI) method to introduce human feedback into the learning process. In the framework, a decision-making scheme based on interactive reinforcement learning (Int-RL) is first developed. Specifically, a human driver evaluates the learning level of the ego vehicle in real-time and intervenes to assist the learning of the vehicle with a conditional sampling mechanism, which encourages the vehicle to pursue human preferences and punishes the bad experience of conflicts with the human. Then, the longitudinal and lateral motion planning tasks are performed utilizing model predictive control (MPC), respectively. The multiple constraints from the vehicle’s physical limitation and driving task requirements are elaborated. Finally, a safety guarantee mechanism is proposed to ensure the safety of the HAI system. Specifically, a safe driving envelope is established, and a safe exploration/exploitation logic based on the trial-and-error on the desired decision is designed. Simulation with a high-fidelity vehicle model is conducted, and results show the proposed framework can realize an efficient, reliable, and safe evolution to pursue higher traffic efficiency of the ego vehicle in both multi-lane and congested ramp scenarios.",10.1109/TITS.2023.3349198,https://ieeexplore.ieee.org/document/10400976,IEEE Transactions on Intelligent Transportation Systems,Kang Yuan;Yanjun Huang;Shuo Yang;Mingzhi Wu;Dongpu Cao;Qijun Chen;Hong Chen,2024,28,"@article{2-15473,
  title={Evolutionary Decision-Making and Planning for Autonomous Driving: A Hybrid Augmented Intelligence Framework},
  author={Yuan, Kang and Huang, Yanjun and Yang, Shuo and Wu, Mingzhi and Cao, Dongpu and Chen, Qijun and Chen, Hong},
  year={2024},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  doi={10.1109/TITS.2023.3349198}
}",Methodological contributions,Transportation / Mobility / Planning,Individual,"Executing, Collaborating","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-15477,ieee,Explainable AI for Glaucoma Prediction Analysis to Understand Risk Factors in Treatment Planning,"Glaucoma causes irreversible blindness. In 2020, about 80 million people worldwide had glaucoma. Existing machine learning (ML) models are limited to glaucoma prediction, where clinicians, patients, and medical experts are unaware of how data analysis and decision-making are handled. Explainable artificial intelligence (XAI) and interpretable ML (IML) create opportunities to increase user confidence in the decision-making process. This article proposes XAI and IML models for analyzing glaucoma predictions/results. XAI primarily uses adaptive neuro-fuzzy inference system (ANFIS) and pixel density analysis (PDA) to provide trustworthy explanations for glaucoma predictions from infected and healthy images. IML uses sub-modular pick local interpretable model-agonistic explanation (SP-LIME) to explain results coherently. SP-LIME interprets spike neural network (SNN) results. Using two different publicly available datasets, namely fundus images, i.e., coherence tomography images of the eyes and clinical medical records of glaucoma patients, our experimental results show that XAI and IML models provide convincing and coherent decisions for clinicians/medical experts and patients.",10.1109/TIM.2022.3171613,https://ieeexplore.ieee.org/document/9766027,IEEE Transactions on Instrumentation and Measurement,Md. Sarwar Kamal;Nilanjan Dey;Linkon Chowdhury;Syed Irtija Hasan;KC Santosh,2022,92,"@article{2-15477,
  title={Explainable AI for Glaucoma Prediction Analysis to Understand Risk Factors in Treatment Planning},
  author={Md. Sarwar Kamal and Nilanjan Dey and Linkon Chowdhury and Syed Irtija Hasan and KC Santosh},
  year={2022},
  doi={10.1109/TIM.2022.3171613},
  journal={IEEE Transactions on Instrumentation and Measurement}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Forecasting, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-15480,ieee,Explainable Artificial Intelligence: Counterfactual Explanations for Risk-Based Decision-Making in Construction,"Artificial intelligence (AI) approaches, such as deep learning models, are increasingly used to determine risks in construction. However, the black-box nature of AI models makes their inner workings difficult to understand and interpret. Deploying explainable artificial intelligence (XAI) can help explain why and how the output of AI models is generated. This article addresses the following research question: \nHow can we accurately identify the critical factors influencing tunnel-induced ground settlement and provide counterfactual explanations to support risk-based decision-making?\n We apply an XAI approach using counterfactual explanations to help understand decision-making surrounding risks when considering control ground settlement. Our approach consists of a: 1) construction of Kernel principal components analysis-based deep neural network (DNN) model; 2) generation of counterfactual explanations; 3) analysis of risk prediction and assessment factors' importance, necessity, and sufficiency. We apply our approach to the San-yang road tunnel project in Wuhan, China. The results demonstrate that the KPCA-DNN model better predicted ground settlement based on high-dimensional input features than the baseline model (i.e., AdaBoost and RandomForest). The bubble chamber pressure→ cutter-head speed→ equipment inclination is also identified as the primary risk path. Our findings indicate that using counterfactual explanations enables transparency and trust in AI-based risk models to be acquired. Moreover, our approach can help site managers, engineers, and tunnel-boring machine operators understand how to manage better and mitigate the risk of ground settlement.",10.1109/TEM.2023.3325951,https://ieeexplore.ieee.org/document/10413227,IEEE Transactions on Engineering Management,Jianjiang Zhan;Weili Fang;Peter E. D. Love;Hanbin Luo,2024,20,"@article{2-15480,
  title={Explainable Artificial Intelligence: Counterfactual Explanations for Risk-Based Decision-Making in Construction},
  author={Zhan, Jianjiang and Fang, Weili and Love, Peter E. D. and Luo, Hanbin},
  year={2024},
  doi={10.1109/TEM.2023.3325951},
  journal={IEEE Transactions on Engineering Management}
}","Algorithmic contributions, Empirical contributions",Manufacturing / Industry / Automation,Operational,"Forecasting, Explaining, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15481,ieee,Exploring Visualization for Fairness in AI Education,"AI systems are becoming omnipresent in our daily lives, but they can sometimes be a source of bias for disadvantaged groups. Lack of fairness in AI systems is not just an engineering issue that influences public policy, it also has important implications for business ethics and corporate social responsibility. To educate nontechnical students at the business school, we have developed educational modules on fairness in AI that convey the importance of making not just accurate but also equitable business decisions. We introduce an educational module with six interactive components that illustrate how to detect, quantify, and mitigate biases in a logistic regression model. When such a module was deployed in a Fair Algorithms for Business course, it was shown to increase students’ engagement and understanding. We further conducted a user study with 413 participants to examine whether adding visualizations and interactions (or not) could lead to an increased understanding of fairness concepts.",10.1109/PacificVis60374.2024.00010,https://ieeexplore.ieee.org/document/10541664,IEEE Pacific Visualization Conference,Xinyuan Yan;Youjia Zhou;Arul Mishra;Himanshu Mishra;Bei Wang,2024,3,"@inproceedings{2-15481,
  title = {Exploring Visualization for Fairness in AI Education},
  author = {Xinyuan Yan and Youjia Zhou and Arul Mishra and Himanshu Mishra and Bei Wang},
  year = {2024},
  doi = {10.1109/PacificVis60374.2024.00010},
  booktitle = {IEEE Pacific Visualization Conference}
}",System/Artifact contributions,Education / Teaching / Research,Operational,"Advising, Auditing, Explaining","Decision-maker, Knowledge provider","Change cognitive demands, Alter decision outcomes",no such info,"visual explanations, prediction of alternative, highlighting and extraction, bias disclosure",NA,"Visual, Interactive interface, Textual",Yes,Yes
2-15482,ieee,Fair Decision-making Under Uncertainty,"There has been concern within the artificial intelligence (AI) community and the broader society regarding the potential lack of fairness of AI-based decision-making systems. Surprisingly, there is little work quantifying and guaranteeing fairness in the presence of uncertainty which is prevalent in many socially sensitive applications, ranging from marketing analytics to actuarial analysis and recidivism prediction instruments. To this end, we study a longitudinal censored learning problem subject to fairness constraints, where we require that algorithmic decisions made do not affect certain individuals or social groups negatively in the presence of uncertainty on class label due to censorship. We argue that this formulation has a broader applicability to practical scenarios concerning fairness. We show how the newly devised fairness notions involving censored information and the general framework for fair predictions in the presence of censorship allow us to measure and mitigate discrimination under uncertainty that bridges the gap with real-world applications. Empirical evaluations on real-world discriminated datasets with censorship demonstrate the practicality of our approach.",10.1109/ICDM51629.2021.00100,https://ieeexplore.ieee.org/document/9679035,IEEE International Conference on Data Mining,Wenbin Zhang;Jeremy C. Weiss,2021,7,"@inproceedings{2-15482,
  title={Fair Decision-making Under Uncertainty},
  author={Zhang, Wenbin and Weiss, Jeremy C.},
  year={2021},
  doi={10.1109/ICDM51629.2021.00100},
  booktitle={Proceedings of the IEEE International Conference on Data Mining}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Operational,Forecasting,"Decision-subject, Guardian, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-15483,ieee,FAIRO: Fairness-aware Sequential Decision Making for Human-in-the-Loop CPS,"Achieving fairness in sequential decision making systems within Human-in-the-Loop (HITL) environments is a critical concern, especially when multiple humans with different behavior and expectations are affected by the same adaptation decisions in the system. This human variability factor adds more complexity since policies deemed fair at one point in time may become discriminatory over time due to variations in human preferences resulting from inter- and intra-human variability. This paper addresses the fairness problem from an equity lens, considering human behavior variability, and the changes in human preferences over time. We propose FAIRO, a novel algorithm for fairnessaware sequential decision making in HITL adaptation, which incorporates these notions into the decision-making process. In particular, FAIRO decomposes this complex fairness task into adaptive sub-tasks based on individual human preferences through leveraging the Options reinforcement learning framework. We design FAIRO to generalize to three types of HITL application setups that have the shared adaptation decision problemFurthermore, we recognize that fairness-aware policies can sometimes conflict with the application’s utility. To address this challenge, we provide a fairness-utility tradeoff in FAIRO, allowing system designers to balance the objectives of fairness and utility based on specific application requirements. Extensive evaluations of FAIRO on the three HITL applications demonstrate its generalizability and effectiveness in promoting fairness while accounting for human variability. On average, FAIRO can improve fairness compared with other methods across all three applications by 35.36%.",10.1109/ICCPS61052.2024.00015,https://ieeexplore.ieee.org/document/10571637,ACM/IEEE International Conference on Cyber-Physical Systems,Tianyu Zhao;Mojtaba Taherisadr;Salma Elmalaki,2024,8,"@inproceedings{2-15483,
  title={FAIRO: Fairness-aware Sequential Decision Making for Human-in-the-Loop CPS},
  author={Zhao, Tianyu and Taherisadr, Mojtaba and Elmalaki, Salma},
  year={2024},
  doi={10.1109/ICCPS61052.2024.00015},
  booktitle={ACM/IEEE International Conference on Cyber-Physical Systems}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Institutional,"Executing, Advising","Developer, Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-15484,ieee,Feasibility Study on Using AI and VR for Decision-Making Training of Basketball Players,"Decision-making plays an essential role in basketball offense. Offensive players must make effective decisions to score a basket in various defensive situations. Virtual reality (VR) has been widely used in the training of athletes to strengthen their ability to make optimal decisions by creating controllable repeatable training scenarios. In this article, an action-aware offensive decision-making training system for basketball using VR and artificial intelligence is proposed. The proposed system is composed of different virtual defensive scenarios and an offensive action recognition framework. Trainees wearing head-mounted display and a motion capture suit are trained by intuitively interacting with the VR system and receive decision suggestions when a bad one is made. This study changes the training media and methods to create an immersive training environment during the training phase and evaluates the training effectiveness. These training scenarios are a conventional tactics board, the proposed VR system with a prerecorded \n360∘360^\\circ\n panorama video, and the VR system with computer-simulated virtual content. Results indicate that the training scenario affects the training in terms of decision time.",10.1109/TLT.2022.3145093,https://ieeexplore.ieee.org/document/9693281,IEEE Transactions on Learning Technologies,Wan-Lun Tsai;Li-Wen Su;Tsai-Yen Ko;Tse-Yu Pan;Min-Chun Hu,2021,0,"@article{2-15484,
  title={Feasibility Study on Using AI and VR for Decision-Making Training of Basketball Players},
  author={Tsai, Wan-Lun and Su, Li-Wen and Ko, Tsai-Yen and Pan, Tse-Yu and Hu, Min-Chun},
  year={2021},
  doi={10.1109/TLT.2022.3145093},
  journal={IEEE Transactions on Learning Technologies}
}",System/Artifact contributions,"Education / Teaching / Research, Media / Communication / Entertainment",Individual,"Analyzing, Advising","Knowledge provider, Decision-subject, Decision-maker","Alter decision outcomes, Change cognitive demands",no such info,"decision suggestions, (continuous) support",NA,3D,Yes,Yes
2-15486,ieee,Federated Inverse Reinforcement Learning for Smart ICUs With Differential Privacy,"Clinical decision-making models have been developed to support therapeutic interventions based on medical data from either a single hospital or multiple hospitals. However, models based on multihospital data require collaboration among hospitals to integrate local data, which can result in information leakage and violate patient privacy. To address this challenge, we propose a novel approach that combines federated learning (FL) with inverse reinforcement learning (IRL) to create an efficient medical decision-making support tool while preserving patient privacy. Our approach uses an IRL algorithm with differential privacy to train a neural network-based agent on local data containing clinician trajectories, which learns a private treatment policy by observing patients’ conditions. Additionally, we integrate FL into the proposed algorithm to learn a global optimal action policy collaboratively among various smart intensive care units, overcoming data limitations at each hospital. We evaluate our approach using real-world medical data and demonstrate that it achieves superior performance in a distributed manner.",10.1109/JIOT.2023.3281347,https://ieeexplore.ieee.org/document/10138664,IEEE Internet of Things Journal,Wei Gong;Linxiao Cao;Yifei Zhu;Fang Zuo;Xin He;Haoquan Zhou,2023,1,"@article{2-15486,
  title={Federated Inverse Reinforcement Learning for Smart ICUs With Differential Privacy},
  author={Gong, Wei and Cao, Linxiao and Zhu, Yifei and Zuo, Fang and He, Xin and Zhou, Haoquan},
  year={2023},
  journal={IEEE Internet of Things Journal},
  doi={10.1109/JIOT.2023.3281347}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Executing, Advising","Guardian, Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-15490,ieee,From Naturalistic Traffic Data to Learning-Based Driving Policy: A Sim-to-Real Study,"Reinforcement learning (RL) is a promising way to achieve human-like autonomous driving (HAD) in complex and dynamic traffic, but faces challenges such as low sample efficiency, partial observability, and sim2real transfer. In light of this, a comprehensive solution for RL-driven HAD is established. First, an efficient training scheme called Deep Recurrent Q-learning from demonstration algorithm (DRQfD) is proposed for lane-changing decision-making to address the low sample efficiency in RL and the poor generalization capability in Imitation Learning (IL). The inherent LSTM structure potentially learns to predict future states of surrounding vehicles, helping to address the partially observable problem in autonomous driving (AD). Second, to reduce the sim2real gap, a twin high-fidelity simulator is built based on ROS-Gazebo for simulating LiDAR sensing, model training, and evaluations. Domain randomization is used to improve the robustness and generalization ability, making it easier for the model to be transferred to real-world scenarios. In addition, for the multi-objective optimization and imbalanced data issues in this scenario, a hierarchical decision-making framework is proposed to decompose the complex decision-making problem into several subtasks, making the driving policies easier to converge. To avoid the excessive dependence of the decision-making module on the output of perception module in modular systems, we train each modularized skill in an end-to-end manner. Moreover, we compare our method with a vanilla RL method to show improvement in learning efficiency. Comparisons between RL-based model and IL baseline in terms of safety, travel efficiency, and human-likeness are also given. To further validate the generalization ability of our model, we test the model on real traffic dataset. Finally, we implement the RL model on physical cars to demonstrate the performance of sim2real transfer.",10.1109/TVT.2023.3307409,https://ieeexplore.ieee.org/document/10226252,IEEE Transactions on Vehicular Technology,Mingfeng Yuan;Jinjun Shan;Kevin Mi,2024,7,"@article{2-15490,
  title={From Naturalistic Traffic Data to Learning-Based Driving Policy: A Sim-to-Real Study},
  author={Mingfeng Yuan and Jinjun Shan and Kevin Mi},
  year={2024},
  doi={10.1109/TVT.2023.3307409},
  journal={IEEE Transactions on Vehicular Technology}
}","Algorithmic contributions, Methodological contributions",Transportation / Mobility / Planning,Individual,"Executing, Analyzing","Knowledge provider, Developer",NA,NA,NA,NA,NA,Yes,No
2-155,aaai,Fairness in Decision-Making — The Causal Explanation Formula,"AI plays an increasingly prominent role in society since decisions that were once made by humans are now delegated to automated systems. These systems are currently in charge of deciding bank loans, criminals incarceration, and the hiring of new employees, and its not difficult to envision that they will in the future underpin most of the decisions in society. Despite the high complexity entailed by this task, there is still not much understanding of basic properties of such systems. For instance, we currently cannot detect( neither explain nor correct) whether an AI system can be deemed fair( i. e. , is abiding by the decision-constraints agreed by society) or it is reinforcing biases and perpetuating a preceding prejudicial practice. Issues of discrimination have been discussed extensively in political and legal circles, but there exists still not much understanding of the formal conditions that a system must meet to be deemed fair. In this paper, we use the language of structural causality( Pearl, 2000) to fill in this gap. We start by introducing three new fine-grained measures of transmission of change from stimulus to effect, which we called counterfactual direct( Ctf-DE) , indirect( Ctf-IE) , and spurious( Ctf-SE) effects. We then derive what we call the causal explanation formula, which allows the AI designer to quantitatively evaluate fairness and explain the total observed disparity of decisions through different discriminatory mechanisms. We apply these measures to various discrimination analysis tasks and run extensive simulations, including detection, evaluation, and optimization of decision-making under fairness constraints. We conclude studying the trade-off between different types of fairness criteria( outcome and procedural) , and provide a quantitative approach to policy implementation and the design of fair AI systems.",10.1609/aaai.v32i1.11564,https://ojs.aaai.org/index.php/AAAI/article/view/11564,AAAI Conference on Artificial Intelligence,Junzhe Zhang;Elias Bareinboim,2024,0,"@inproceedings{2-155,
  title = {Fairness in Decision-Making — The Causal Explanation Formula},
  author = {Junzhe Zhang and Elias Bareinboim},
  year = {2024},
  doi = {10.1609/aaai.v32i1.11564},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Operational,"Explaining, Advising","Knowledge provider, Guardian, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-15500,ieee,HDMTK: Full Integration of Hierarchical Decision-Making and Tactical Knowledge in Multi-Agent Adversarial Games,"In the field of adversarial games, existing decision-making algorithms primarily rely on reinforcement learning, which can theoretically adapt to diverse scenarios through trial and error. However, these algorithms often face the challenges of low effectiveness and slow convergence in complex wargame environments. Inspired by how human commanders make decisions, this paper proposes a novel method named Fully Integrating Hierarchical Decision-Making and Tactical Knowledge (HDMTK). This method comprises an upper reinforcement learning module and a lower multi-agent reinforcement learning module. To enable agents to efficiently learn the cooperative strategy, in HDMTK, we separate the whole task into explainable subtasks, and design their corresponding subgoals for shaping the online rewards upon tactical knowledge. Experimental results on the wargame simulation platform “MiaoSuan” show that, compared to the advanced multi-agent reinforcement learning methods, HDMTK exhibits superior performance and faster convergence in the complex scenarios.",10.1109/TCDS.2024.3470068,https://ieeexplore.ieee.org/document/10697976,IEEE Transactions on Cognitive and Developmental Systems,Wei Li;Boling Hu;Aiguo Song;Kaizhu Huang,2024,0,"@article{2-15500,
  title={HDMTK: Full Integration of Hierarchical Decision-Making and Tactical Knowledge in Multi-Agent Adversarial Games},
  author={Li, Wei and Hu, Boling and Song, Aiguo and Huang, Kaizhu},
  year={2024},
  doi={10.1109/TCDS.2024.3470068},
  journal={IEEE Transactions on Cognitive and Developmental Systems}
}",Algorithmic contributions,Media / Communication / Entertainment,Organizational,Executing,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15503,ieee,HGRL: Human-Driving-Data Guided Reinforcement Learning for Autonomous Driving,"Reinforcement learning (RL) shows promise for autonomous driving decision-making. However, designing appropriate reward functions to guide RL agents towards complex optimization objectives is challenging. This article proposes a framework that learns the reward function from human driving data to guide RL agent's learning. The proposed framework consists of three components: trajectory sample, offline preference learning, and RL. Firstly, feasible trajectories are generated by sampling end targets from a reachable state space. Subsequently, a novel offline preference learning framework is utilized to train a transformer network by comparing generated feasible trajectories with human driving trajectories. The transformer network is used to model the human driving decision-making process, thereby obtaining a reward function. Finally, to obtain the final driving decision network, the derived reward function is incorporated into a RL framework. To validate the proposed method, a highway simulator is established where the surrounding vehicle trajectories are derived from real-world driving scenarios. Compared with baseline algorithms, the proposed method achieves the best performances in terms of decision safety and human-likeness. Additionally, the learned policy network performs well in driving decision-making tasks with longer total decision steps. Experimental results demonstrate that the proposed method can obviate the requirement for manual design of sophisticated reward functions in RL-based autonomous driving decision-making systems.",10.1109/TIV.2024.3406679,https://ieeexplore.ieee.org/document/10541090,IEEE Transactions on Intelligent Vehicles,Hejian Zhuang;Hongqing Chu;Yafei Wang;Bingzhao Gao;Hong Chen,2024,11,"@article{2-15503,
  title={HGRL: Human-Driving-Data Guided Reinforcement Learning for Autonomous Driving},
  author={Zhuang, Hejian and Chu, Hongqing and Wang, Yafei and Gao, Bingzhao and Chen, Hong},
  year={2024},
  doi={10.1109/TIV.2024.3406679},
  journal={IEEE Transactions on Intelligent Vehicles}
}",Methodological contributions,Transportation / Mobility / Planning,Individual,"Executing, Analyzing",Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-15505,ieee,High Speed Serial Links Risk Assessment in Industrial Post-Silicon Validation Exploiting Machine Learning Techniques,"Post-Silicon system margin validation consumes a significant amount of time and resources. To overcome this, a reduced validation plan for derivative products has previously been used. However, a certain amount of validation is still needed to avoid escapes, which is prone to subjective bias by the validation engineer comparing a reduced set of derivative validation data against the base product data. Machine Learning techniques allow to perform automatic decisions based on already available historical data. In this work, we present an efficient methodology implemented with Machine Learning to make an automatic risk assessment decision for derivative products, considering a large set of parameters obtained from the base product. The proposed methodology yields a high performance on the risk assessment decision, which translates into a significant reduction in time, effort, and resources.",10.1109/ITC44778.2020.9325238,https://ieeexplore.ieee.org/document/9325238,IEEE International Test Conference,Cesar A. Sánchez-Martínez;Paulo López-Meyer;Esdras Juarez-Hernandez;Aaron Desiga-Orenday;Andrés Viveros-Wacher,2020,0,"@inproceedings{2-15505,
  title = {High Speed Serial Links Risk Assessment in Industrial Post-Silicon Validation Exploiting Machine Learning Techniques},
  author = {Cesar A. S{\'a}nchez-Mart{\'i}nez and Paulo L{\'o}pez-Meyer and Esdras Juarez-Hernandez and Aaron Desiga-Orenday and Andr{\'e}s Viveros-Wacher},
  year = {2020},
  doi = {10.1109/ITC44778.2020.9325238},
  booktitle = {IEEE International Test Conference}
}",Methodological contributions,Manufacturing / Industry / Automation,Organizational,"Executing, Forecasting, Analyzing",Developer,NA,NA,NA,NA,NA,Yes,No
2-15506,ieee,High-Speed Ramp Merging Behavior Decision for Autonomous Vehicles Based on Multiagent Reinforcement Learning,"To improve the decision success rate of a multiagent reinforcement learning algorithm in merging high-speed ramps of autonomous vehicles, the independent proximal policy optimization (IPPO) method is presented. The Markov decision process (MDP) model for autonomous vehicle behavioral decision making is developed. Moreover, the state space, reward function, and action space are all designed. An IPPO method is proposed using independent learning and parameter-sharing strategies based on the proximal policy optimization algorithm. And further, a decision-making model for autonomous driving behavior is built. For simulation experiments, a highway ramp scenario is set. The experiment findings indicate that the IPPO algorithm can significantly increase the decision success rate of autonomous vehicles in the ramp merging assignment. Also, as compared to the MAACKTR and GPPO algorithms, the IPPO algorithm can achieve a better average reward and finish the ramp merging more rapidly.",10.1109/JIOT.2023.3304890,https://ieeexplore.ieee.org/document/10215357,IEEE Internet of Things Journal,Xinfeng Zhang;Lin Wu;Huan Liu;Yajun Wang;Hao Li;Bin Xu,2023,0,"@article{2-15506,
  title={High-Speed Ramp Merging Behavior Decision for Autonomous Vehicles Based on Multiagent Reinforcement Learning},
  author={Zhang, Xinfeng and Wu, Lin and Liu, Huan and Wang, Yajun and Li, Hao and Xu, Bin},
  year={2023},
  doi={10.1109/JIOT.2023.3304890},
  journal={IEEE Internet of Things Journal}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,Developer,NA,NA,NA,NA,NA,Yes,No
2-15509,ieee,Human-Guided Deep Reinforcement Learning for Optimal Decision Making of Autonomous Vehicles,"Although deep reinforcement learning (DRL) methods are promising for making behavioral decisions in autonomous vehicles (AVs), their low training efficiency and difficulty to adapt to untrained cases hinder their applications. Introducing a human role in the DRL paradigm could improve training efficiency by using human prior knowledge and overcome untrained cases in deployment by online human takeover. In this study, a novel value-based DRL algorithm that leverages human guidance to improve its performance is proposed for addressing high-level decision-making problems in autonomous driving. We develop a new learning objective for DRL to increase the value of the human policy over the undertrained DRL policy so that the DRL agent can be encouraged to mimic human behaviors and thereby utilizing human guidance more efficiently. Our method can autonomously evaluate the importance of different human guidance, which makes it more robust for variation of human performance. The proposed DRL algorithm was used to address a challenging multiobjective lane-change decision-making problem. We collected human guidance from a human-in-the-loop driving experiment and evaluated our method in a high-fidelity simulator. Results validated the advantages of the proposed algorithm in terms of training efficiency and optimality in the decision-making problem compared to the baselines of state-of-the-art existing methods. Results also revealed the favorable fine-tuning ability of the proposed algorithm, which is promising for addressing the long-tail issue in DRL-based autonomous driving. Our methodology does not introduce additional domain knowledge so that it can be seamlessly applied to other similar issues. The supplementary video is available at \nhttps://youtu.be/Ec7WkqeLsB8\n.",10.1109/TSMC.2024.3384992,https://ieeexplore.ieee.org/document/10507015,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",Jingda Wu;Haohan Yang;Lie Yang;Yi Huang;Xiangkun He;Chen Lv,2024,44,"@article{2-15509,
  title={Human-Guided Deep Reinforcement Learning for Optimal Decision Making of Autonomous Vehicles},
  author={Wu, Jingda and Yang, Haohan and Yang, Lie and Huang, Yi and He, Xiangkun and Lv, Chen},
  year={2024},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  doi={10.1109/TSMC.2024.3384992}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-15510,ieee,Human-Like Decision Making for Autonomous Driving With Social Skills,"There may exist long-term mixed traffic that consists of human-driven vehicles (HDV) and autonomous driving vehicles (ADV). Hence, a formidable challenge arises: the effective decision-making process among these heterogeneous vehicle types. The disparity in the level of decision-making among heterogeneous vehicles is significant. Human driving behaviors and volition, performed in HDV, are speculative and uncertain, while ADV’s behavior is unitary and conservative. To address this issue, a human-like decision-making framework for ADV considering social skills is designed, by introducing social value orientation (SVO) which is used to measure the degree of altruism of human drivers, and a sociality-aware Stackelberg game model and a social potential field model are proposed. Firstly, an inverse reinforcement learning (IRL) algorithm is applied to construct a structural cost function about human-driven interactive trajectories in order to estimate the SVO of HDV and endow ADV with the ability to respond to SVO. Secondly, the sociality-aware Stackelberg game approach is designed to capture the social interaction between heterogeneous vehicles, considering personal and public interests. Furthermore, a social potential field model is proposed, and then combined with receding horizon optimization (RHO) to plan socially-skilled trajectories. Finally, three traffic scenarios are used to verify that the developed decision-making algorithm can make safe and socially-skilled decisions in mixed traffic scenarios, in which several cases in terms of HDV with various SVO values are tested to prove the validity of human-like decision making process of an ADV.",10.1109/TITS.2024.3366699,https://ieeexplore.ieee.org/document/10457970,IEEE Transactions on Intelligent Transportation Systems,Chenyang Zhao;Duanfeng Chu;Zejian Deng;Liping Lu,2024,21,"@article{2-15510,
  title={Human-Like Decision Making for Autonomous Driving With Social Skills},
  author={Zhao, Chenyang and Chu, Duanfeng and Deng, Zejian and Lu, Liping},
  year={2024},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  doi={10.1109/TITS.2024.3366699}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Analyzing, Collaborating","Decision-subject, Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-15512,ieee,Humanlike Driving: Empirical Decision-Making System for Autonomous Vehicles,"The autonomous vehicle, as an emerging and rapidly growing field, has received extensive attention for its futuristic driving experiences. Although the fast developing depth sensors and machine learning methods have given a huge boost to self-driving research, existing autonomous driving vehicles do meet with several avoidable accidents during their road testings. The major cause is the misunderstanding between self-driving systems and human drivers. To solve this problem, we propose a humanlike driving system in this paper to give autonomous vehicles the ability to make decisions like a human. In our method, a convolutional neural network model is used to detect, recognize, and abstract the information in the input road scene, which is captured by the on-board sensors. And then a decision-making system calculates the specific commands to control the vehicles based on the abstractions. The biggest advantage of our work is that we implement a decision-making system which can well adapt to real-life road conditions, in which a massive number of human drivers exist. In addition, we build our perception system with only the depth information, rather than the unstable RGB data. The experimental results give a good demonstration of the efficiency and robustness of the proposed method.",10.1109/TVT.2018.2822762,https://ieeexplore.ieee.org/document/8330044,IEEE Transactions on Vehicular Technology,Liangzhi Li;Kaoru Ota;Mianxiong Dong,2018,327,"@article{2-15512,
  title={Humanlike Driving: Empirical Decision-Making System for Autonomous Vehicles},
  author={Li, Liangzhi and Ota, Kaoru and Dong, Mianxiong},
  year={2018},
  doi={10.1109/TVT.2018.2822762},
  journal={IEEE Transactions on Vehicular Technology}
}",System/Artifact contributions,Transportation / Mobility / Planning,Individual,"Analyzing, Executing","Decision-subject, Knowledge provider, Developer",NA,NA,NA,NA,NA,Yes,No
2-15513,ieee,iFair: Learning Individually Fair Data Representations for Algorithmic Decision Making,"People are rated and ranked, towards algorithmic decision making in an increasing number of applications, typically based on machine learning. Research on how to incorporate fairness into such tasks has prevalently pursued the paradigm of group fairness: giving adequate success rates to specifically protected groups. In contrast, the alternative paradigm of individual fairness has received relatively little attention, and this paper advances this less explored direction. The paper introduces a method for probabilistically mapping user records into a low-rank representation that reconciles individual fairness and the utility of classifiers and rankings in downstream applications. Our notion of individual fairness requires that users who are similar in all task-relevant attributes such as job qualification, and disregarding all potentially discriminating attributes such as gender, should have similar outcomes. We demonstrate the versatility of our method by applying it to classification and learning-to-rank tasks on a variety of real-world datasets. Our experiments show substantial improvements over the best prior work for this setting.",10.1109/ICDE.2019.00121,https://ieeexplore.ieee.org/document/8731591,IEEE International Conference on Data Engineering,Preethi Lahoti;Krishna P. Gummadi;Gerhard Weikum,2019,244,"@inproceedings{2-15513,
  title={iFair: Learning Individually Fair Data Representations for Algorithmic Decision Making},
  author={Lahoti, Preethi and Gummadi, Krishna P. and Weikum, Gerhard},
  year={2019},
  booktitle={IEEE International Conference on Data Engineering},
  doi={10.1109/ICDE.2019.00121}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Institutional,"Advising, Explaining, Auditing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-15515,ieee,Improving Clinical Decision Making With a Two-Stage Recommender System,"Clinical decision-making is complex and time-intensive. To help in this effort, clinical recommender systems (RS) have been designed to facilitate healthcare practitioners with personalized advice. However, designing an effective clinical RS poses challenges due to the multifaceted nature of clinical data and the demand for tailored recommendations. In this article, we introduce a 2-Stage Recommendation framework for clinical decision-making, which leverages a publicly accessible dataset of electronic health records. In the first stage, a deep neural network-based model is employed to extract a set of candidate items, such as diagnoses, medications, and prescriptions, from a patient's electronic health records. Subsequently, the second stage utilizes a deep learning model to rank and pinpoint the most relevant items for healthcare providers. Both retriever and ranker are based on pre-trained transformer models that are stacked together as a pipeline. To validate our model, we compared its performance against several baseline models using different evaluation metrics. The results reveal that our proposed model attains a performance gain of approximately 12.3% macro-average F1 compared to the second best performing baseline. Qualitative analysis across various dimensions also confirms the model's high performance. Furthermore, we discuss challenges like data availability, privacy concerns, and shed light on future exploration in this domain.",10.1109/TCBB.2023.3318209,https://ieeexplore.ieee.org/document/10261229,IEEE/ACM Transactions on Computational Biology and Bioinformatics,Shaina Raza;Chen Ding,2024,30,"@article{2-15515,
  title={Improving Clinical Decision Making With a Two-Stage Recommender System},
  author={Raza, Shaina and Ding, Chen},
  year={2024},
  doi={10.1109/TCBB.2023.3318209},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-15523,ieee,Interaction-aware Decision Making with Adaptive Strategies under Merging Scenarios,"In order to drive safely and efficiently under merging scenarios, autonomous vehicles should be aware of their surroundings and make decisions by interacting with other road participants. Moreover, different strategies should be made when the autonomous vehicle is interacting with drivers having different level of cooperativeness. Whether the vehicle is on the merge-lane or main-lane will also influence the driving maneuvers since drivers will behave differently when they have the right-of-way than otherwise. Many traditional methods have been proposed to solve decision making problems under merging scenarios. However, these works either are incapable of modeling complicated interactions or require implementing hand-designed rules which cannot properly handle the uncertainties in real-world scenarios. In this paper, we proposed an interaction-aware decision making with adaptive strategies (IDAS) approach that can let the autonomous vehicle negotiate the road with other drivers by leveraging their cooperativeness under merging scenarios. A single policy is learned under the multi-agent reinforcement learning (MARL) setting via the curriculum learning strategy, which enables the agent to automatically infer other drivers' various behaviors and make decisions strategically. A masking mechanism is also proposed to prevent the agent from exploring states that violate common sense of human judgment and increase the learning efficiency. An exemplar merging scenario was used to implement and examine the proposed method.",10.1109/IROS40897.2019.8968478,https://ieeexplore.ieee.org/document/8968478,IEEE/RSJ International Conference on Intelligent Robots and Systems,Yeping Hu;Alireza Nakhaei;Masayoshi Tomizuka;Kikuo Fujimura,2019,95,"@inproceedings{2-15523,
  title     = {Interaction-aware Decision Making with Adaptive Strategies under Merging Scenarios},
  author    = {Yeping Hu and Alireza Nakhaei and Masayoshi Tomizuka and Kikuo Fujimura},
  year      = {2019},
  doi       = {10.1109/IROS40897.2019.8968478},
  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15524,ieee,Interaction-Aware Planning With Deep Inverse Reinforcement Learning for Human-Like Autonomous Driving in Merge Scenarios,"Merge scenarios on highway are often challenging for autonomous driving, due to its lack of sufficient tacit understanding on and subtle interaction with human drivers in the traffic flow. This, as a result, may impose serious safety risks, and often cause traffic jam with autonomous driving. Therefore, human-like autonomous driving becomes important, yet imperative. This article presents an interaction-aware decision-making and planning method for human-like autonomous driving in merge scenarios. Rather than directly mimicking human behavior, deep inverse reinforcement learning is employed to learn the human-used reward function for decision-making and planning from naturalistic driving data to enhance interpretability and generalizability. To consider the interaction factor, the reward function for planning is utilized to evaluate the joint trajectories of the autonomous driving vehicle (ADV) and traffic vehicles. In contrast to predicting trajectories of traffic vehicles with the fixed trajectory of ADV given by the upstream prediction model, the trajectories of traffic vehicles are predicted by responding to the ADV's behavior in this article. Additionally, the decision-making module is employed to reduce the solution space of planning via the selection of a proper gap for merging. Both the decision-making and planning algorithms follow a “sampling, evaluation, and selection” framework. After being verified through experiments, the results indicate that the planned trajectories with the presented method are highly similar to those of human drivers. Moreover, compared to the interaction-unaware planning method, the interaction-aware planning method behaves closer to human drivers.",10.1109/TIV.2023.3298912,https://ieeexplore.ieee.org/document/10195221,IEEE Transactions on Intelligent Vehicles,Jiangfeng Nan;Weiwen Deng;Ruzheng Zhang;Ying Wang;Rui Zhao;Juan Ding,2024,31,"@article{2-15524,
  title={Interaction-Aware Planning With Deep Inverse Reinforcement Learning for Human-Like Autonomous Driving in Merge Scenarios},
  author={Nan, Jiangfeng and Deng, Weiwen and Zhang, Ruzheng and Wang, Ying and Zhao, Rui and Ding, Juan},
  year={2024},
  doi={10.1109/TIV.2023.3298912},
  journal={IEEE Transactions on Intelligent Vehicles}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Forecasting, Monitoring, Analyzing",Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-15529,ieee,Joint-Channel-Connectivity-Based Feature Selection and Classification on fNIRS for Stress Detection in Decision-Making,"Stress is one of the contributing factors affecting decision-making. Therefore, early stress recognition is essential to improve clinicians’ decision-making performance. Functional near-infrared spectroscopy (fNIRS) has shown great potential in detecting stress. However, the majority of previous studies only used fNIRS features at the individual level for classification without considering the correlations among channels corresponding to the brain, which may provide distinguishing features. Hence, this study proposes a novel joint-channel-connectivity-based feature selection and classification algorithm for fNIRS to detect stress in decision-making. Specifically, this approach integrates feature selection and classifier modeling into a sparse model, where intra- and inter-channel regularizers are designed to explore potential correlations among channels to obtain discriminating features. In this paper, we simulated the decision-making of medical students under stress through the Trier Social Stress Test and the Balloon Analog Risk Task and recorded their cerebral hemodynamic alterations by fNIRS device. Experimental results illustrated that our method with the accuracy of 0.961 is superior to other machine learning methods. Additionally, the stress correlation and connectivity of brain regions calculated by feature selection have been confirmed in previous studies, which validates the effectiveness of our method and helps optimize the channel settings of fNIRS. This work was the first attempt to utilize a sparse model that simultaneously considers the sparsity of features and the correlation of brain regions for stress detection and obtained an admirable classification performance. Thus, the proposed model might be a useful tool for medical personnel to automatically detect stress in clinical decision-making situations.",10.1109/TNSRE.2022.3188560,https://ieeexplore.ieee.org/document/9815279,IEEE Transactions on Neural Systems and Rehabilitation Engineering,Meiyan Huang;Xiaoling Zhang;Xiumei Chen;Yiling Mai;Xiaohua Wu;Jiubo Zhao;Qianjin Feng,2022,24,"@article{2-15529,
  title={Joint-Channel-Connectivity-Based Feature Selection and Classification on fNIRS for Stress Detection in Decision-Making},
  author={Huang, Meiyan and Zhang, Xiaoling and Chen, Xiumei and Mai, Yiling and Wu, Xiaohua and Zhao, Jiubo and Feng, Qianjin},
  year={2022},
  journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  volume={},
  number={},
  pages={},
  doi={10.1109/TNSRE.2022.3188560}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Forecasting","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-15532,ieee,Lane Change Decision-making through Deep Reinforcement Learning with Rule-based Constraints,"Autonomous driving decision-making is a great challenge due to the complexity and uncertainty of the traffic environment. Combined with the rule-based constraints, a Deep Q-Network (DQN) based method is applied for autonomous driving lane change decision-making task in this study. Through the combination of high-level lateral decision-making and low-level rule-based trajectory modification, a safe and efficient lane change behavior can be achieved. With the setting of our state representation and reward function, the trained agent is able to take appropriate actions in a real-world-like simulator. The generated policy is evaluated on the simulator for 10 times, and the results demonstrate that the proposed rule-based DQN method outperforms the rule-based approach and the DQN method.",10.1109/IJCNN.2019.8852110,https://ieeexplore.ieee.org/document/8852110,International Joint Conference on Neural Networks,Junjie Wang;Qichao Zhang;Dongbin Zhao;Yaran Chen,2019,222,"@inproceedings{2-15532,
  title={Lane Change Decision-making through Deep Reinforcement Learning with Rule-based Constraints},
  author={Wang, Junjie and Zhang, Qichao and Zhao, Dongbin and Chen, Yaran},
  year={2019},
  doi={10.1109/IJCNN.2019.8852110},
  booktitle={International Joint Conference on Neural Networks}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,Developer,NA,NA,NA,NA,NA,Yes,No
2-15534,ieee,Learning Automated Driving in Complex Intersection Scenarios Based on Camera Sensors: A Deep Reinforcement Learning Approach,"Making proper decisions at intersections that are one of the most dangerous and sophisticated driving scenarios is full of challenges, especially for autonomous vehicles (AVs). The existing decision-making approaches for AVs at intersections are limited as they only consider driving safety in simple intersection scenarios while sacrificing travel efficiency and driving comfort. To solve this issue, a decision-making structure motivated by deep reinforcement learning was proposed for autonomous driving at complex intersection scenarios based on long short-term memory (LSTM). The mapping relationship between traffic images collected from camera sensors and AVs’ actions was established by constructing convolutional-recurrent neural networks in a decision-making framework. Traffic images collected from camera sensors at two different timesteps were used to understand the relative motion information between AVs and other vehicles. To model the interaction between the AV and other vehicles, Markov decision process was used. The deep Q-network (DQN) algorithm was applied to generate the optimal driving policy that could comprehensively consider driving safety, travel efficiency and driving comfort. Three crash-prone complex intersection scenarios were reconstructed in CARLA (car learning to act) to evaluate the performance of our proposed method. The results indicate that our method can make AV drive through intersections safely and efficiently with desirable driving comfort in all the examined scenarios.",10.1109/JSEN.2022.3146307,https://ieeexplore.ieee.org/document/9694607,IEEE Sensors Journal,Guofa Li;Siyan Lin;Shen Li;Xingda Qu,2022,31,"@article{2-15534,
  title={Learning Automated Driving in Complex Intersection Scenarios Based on Camera Sensors: A Deep Reinforcement Learning Approach},
  author={Li, Guofa and Lin, Siyan and Li, Shen and Qu, Xingda},
  year={2022},
  doi={10.1109/JSEN.2022.3146307},
  journal={IEEE Sensors Journal}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,"Developer, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-15536,ieee,Learning Interaction-Aware Motion Prediction Model for Decision-Making in Autonomous Driving,"Predicting the behaviors of other road users is crucial to safe and intelligent decision-making for autonomous vehicles (AVs). However, most motion prediction models ignore the influence of the AV's actions and the planning module has to treat other agents as unalterable moving obstacles. To address this problem, this paper proposes an interaction-aware motion prediction model that is able to predict other agents' future trajectories according to the ego agent's future plan, i.e., their reactions to the ego's actions. Specifically, we employ Transformers to effectively encode the driving scene and incorporate the AV's future plan in decoding the predicted trajectories. To train the model to accurately predict the reactions of other agents, we develop an online learning framework, where the ego agent explores the environment and collects other agents' reactions to itself. We validate the decision-making and learning framework in three highly interactive simulated driving scenarios. The results reveal that our decision-making method significantly outperforms the reinforcement learning methods in terms of data efficiency and performance. We also find that using the interaction-aware model can bring better performance than the non-interaction-aware model and the exploration process helps improve the success rate in testing.",10.1109/ITSC57777.2023.10422695,https://ieeexplore.ieee.org/document/10422695,IEEE International Conference on Intelligent Transportation Systems,Zhiyu Huang;Haochen Liu;Jingda Wu;Wenhui Huang;Chen Lv,2023,28,"@inproceedings{2-15536,
  title={Learning Interaction-Aware Motion Prediction Model for Decision-Making in Autonomous Driving},
  author={Huang, Zhiyu and Liu, Haochen and Wu, Jingda and Huang, Wenhui and Lv, Chen},
  year={2023},
  doi={10.1109/ITSC57777.2023.10422695},
  booktitle={IEEE International Conference on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Forecasting, Executing","Decision-subject, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-15537,ieee,Learning Safe and Human-Like High-Level Decisions for Unsignalized Intersections From Naturalistic Human Driving Trajectories,"Automated driving systems need to behave as human-like as possible, especially in highly interactive scenarios. In this way, the behavior can be better interpreted and predicted by other traffic participants, in order to prevent misunderstanding, and in the worst case, accidents. With this purpose, more and more human-driven trajectories in real traffic are recorded, making it possible to learn human-like driving styles. In this paper, we extend our previous behavior cloning approach, which has been successfully applied to highway driving, to generate high-level decisions for unsignalized intersections that are challenging during urban driving. Unlike many other approaches that utilize neural networks, either for end-to-end behavior cloning or for approximating Q-functions in reinforcement learning, where their decisions are intractable to understand, the output decisions of our approach are interpretable and easy to track. Meanwhile, the driving decisions are provably safe under reasonable assumptions by generalizing the Responsibility-Sensitive Safety (RSS) concept to complex intersections. Simulation evaluations show that our learned policy produces a more human-like behavior, and meanwhile, balances driving efficiency, comfort, perceived safety, and politeness better.",10.1109/TITS.2023.3286454,https://ieeexplore.ieee.org/document/10164673,IEEE Transactions on Intelligent Transportation Systems,Lingguang Wang;Carlos Fernandez;Christoph Stiller,2023,3,"@article{2-15537,
  title={Learning Safe and Human-Like High-Level Decisions for Unsignalized Intersections From Naturalistic Human Driving Trajectories},
  author={Wang, Lingguang and Fernandez, Carlos and Stiller, Christoph},
  year={2023},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  doi={10.1109/TITS.2023.3286454}
}",Methodological contributions,Transportation / Mobility / Planning,Individual,"Executing, Advising","Knowledge provider, Guardian",NA,NA,NA,NA,NA,Yes,No
2-15540,ieee,Long-Term Fairness for Real-Time Decision Making: A Constrained Online Optimization Approach,"As machine learning (ML)-driven decisions proliferate, particularly in cases involving sensitive attributes, such as gender, race, and age, to name a few, the need for equity and impartiality has emerged as a fundamental concern. In situations demanding real-time decision-making, fairness objectives become more nuanced and complex: instantaneous fairness to ensure equity in every time slot and long-term fairness to ensure fairness over a period of time. There is a growing awareness that real-world systems operating over long periods require fairness over different timelines. Most existing approaches mainly address dynamic costs with time-invariant fairness constraints, often disregarding the challenges posed by time-varying fairness constraints. Time-varying fairness constraints require the learners to adapt their decisions to meet the changing constraints. However, long-term dynamics are hard to assess and accurately predicting the changes in constraints can be difficult. To bridge this gap, this work introduces a framework for ensuring long-term fairness within dynamic decision-making systems characterized by time-varying fairness constraints. We formulate the decision problem with fairness constraints over a period as a constrained online optimization problem. A novel online algorithm, named long-term fairness-aware online learning algorithm (LoTFair), is presented that solves the problem “on the fly.” We demonstrate that long-term fairness for real-time decision making can be addressed flexibly and efficiently by LoTFair: it achieves overall fairness while maintaining performance over the long run.",10.1109/TNNLS.2024.3476038,https://ieeexplore.ieee.org/document/10752738,IEEE Transactions on Neural Networks and Learning Systems,Ruijie Du;Deepan Muthirayan;Pramod P. Khargonekar;Yanning Shen,2024,3,"@article{2-15540,
  title={Long-Term Fairness for Real-Time Decision Making: A Constrained Online Optimization Approach},
  author={Du, Ruijie and Muthirayan, Deepan and Khargonekar, Pramod P. and Shen, Yanning},
  year={2024},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  doi={10.1109/TNNLS.2024.3476038}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Institutional,"Executing, Auditing","Guardian, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-15541,ieee,Machine Learning for Surgical Risk Assessment Decision Systems,"Surgical risk assessments are central to the decision-making process utilized by medical professionals in the planning and execution of surgical procedures. In this regard, several tools are currently relied upon by surgeons to evaluate potential outcomes and associated risk. A popular and widely used assessment tool is the American College of Surgeons (ACS) National Surgical Quality Improvement Program (NSQIP) Universal Surgical Risk Calculator (ACS-SRC). One important limitation of this approach stands in its inability to exploit complex relationships among data variables, which may limit its classification accuracy. This paper tries to fill this gap by proposing a viable, low-resourced, and generalized decision support model using machine learning techniques that emulates the same general risk assessment functionality as the ACS-SRC, but utilizes more complex models such as gradient-boosted trees and deep neural networks. The experimental results show that our approach performs competitively with respect to state-of-the-art tools.",10.1109/IJCNN55064.2022.9892752,https://ieeexplore.ieee.org/document/9892752,International Joint Conference on Neural Networks,Myles Russell;Dylan Russell;Roberto Corizzo;Nathalie Japkowicz,2022,2,"@inproceedings{2-15541,
  title={Machine Learning for Surgical Risk Assessment Decision Systems},
  author={Russell, Myles and Russell, Dylan and Corizzo, Roberto and Japkowicz, Nathalie},
  year={2022},
  booktitle={International Joint Conference on Neural Networks},
  doi={10.1109/IJCNN55064.2022.9892752}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-1555,acl,"Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender?","We examine whether large language models (LLMs) exhibit race- and gender-based name discrimination in hiring decisions, similar to classic findings in the social sciences (Bertrand and Mullainathan, 2004). We design a series of templatic prompts to LLMs to write an email to a named job applicant informing them of a hiring decision. By manipulating the applicant's first name, we measure the effect of perceived race, ethnicity, and gender on the probability that the LLM generates an acceptance or rejection email. We find that the hiring decisions of LLMs in many settings are more likely to favor White applicants over Hispanic applicants. In aggregate, the groups with the highest and lowest acceptance rates respectively are masculine White names and masculine Hispanic names. However, the comparative acceptance rates by group vary under different templatic settings, suggesting that LLMs' race- and gender-sensitivity may be idiosyncratic and prompt-sensitive.",10.18653/v1/2024.acl-short.37,https://aclanthology.org/2024.acl-short.37,Annual Meeting of the Association for Computational Linguistics (Short Papers),"An, Haozhe; Acquaye, Christabel; Wang, Colin; Li, Zongxia; Rudinger, Rachel",2024,61,"@inproceedings{2-1555,
  title = {Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender?},
  author = {An, Haozhe and Acquaye, Christabel and Wang, Colin and Li, Zongxia and Rudinger, Rachel},
  year = {2024},
  booktitle = {Annual Meeting of the Association for Computational Linguistics (Short Papers)},
  doi = {10.18653/v1/2024.acl-short.37}
}",Empirical contributions,Everyday / Employment / Public Service,Operational,"Executing, Analyzing","Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-15555,ieee,Modeling of decision-making behavior for discretionary lane-changing execution,"Modeling of decision-making behavior for discretionary lane-changing execution (DLCE) is fundamental to both movement simulation and controlling design of automatic vehicles. The existing gap acceptance models ingored the nonlinearity of drivers' DLCE decision-making behavior. Therefore, this study tries to analyze and simulate the DLCE decision-making behavior using the real trajectory data. First, a threshold of the lane-changer's lateral velocity is introduced to identify the starting point of DLCE process based on vehicle trajectories from the NGSIM data set. In the following, the empirical analysis based on traffic state variables at the instant of accepting DLCE event are presented, which prove the necessity of modeling DLCE decision-making behavior with machine learning method. Then, we propose a DLCE decision-making model using the Support Vector Machine (SVM). For verifying the prediction performance, the proposed model is compared with the Nagel's model based on the NGSIM data set. The comparison results indicate that the proposed model using SVM outperforms the Nagel's model in predicting the DLCE decision.",10.1109/ITSC.2016.7795631,https://ieeexplore.ieee.org/document/7795631,IEEE International Conference on Intelligent Transportation Systems,Jianqiang Nie;Jian Zhang;Xia Wan;Wanting Ding;Bin Ran,2016,39,"@inproceedings{2-15555,
  title={Modeling of decision-making behavior for discretionary lane-changing execution},
  author={Nie, Jianqiang and Zhang, Jian and Wan, Xia and Ding, Wanting and Ran, Bin},
  year={2016},
  doi={10.1109/ITSC.2016.7795631},
  booktitle={IEEE International Conference on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Analyzing, Forecasting","Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-15557,ieee,MTD-GPT: A Multi-Task Decision-Making GPT Model for Autonomous Driving at Unsignalized Intersections,"Autonomous driving technology is poised to transform transportation systems. However, achieving safe and accurate multi-task decision-making in complex scenarios, such as unsignalized intersections, remains a challenge for autonomous vehicles. This paper presents a novel approach to this issue with the development of a Multi-Task Decision-Making Generative Pre-trained Transformer (MTD-GPT) model. Leveraging the inherent strengths of reinforcement learning (RL) and the sophisticated sequence modeling capabilities of the Generative Pre-trained Transformer (GPT), the MTD-GPT model is designed to simultaneously manage multiple driving tasks, such as left turns, straight-ahead driving, and right turns at unsignalized intersections. We initially train a single-task RL expert model, sample expert data in the environment, and subsequently utilize a mixed multi-task dataset for offline GPT training. This approach abstracts the multi-task decision-making problem in autonomous driving as a sequence modeling task. The MTD-GPT model is trained and evaluated across several decision-making tasks, demonstrating performance that is either superior or comparable to that of state-of-the-art single-task decision-making models.",10.1109/ITSC57777.2023.10421993,https://ieeexplore.ieee.org/document/10421993,IEEE International Conference on Intelligent Transportation Systems,Jiaqi Liu;Peng Hang;Xiao Qi;Jianqiang Wang;Jian Sun,2023,1,"@inproceedings{2-15557,
  title={MTD-GPT: A Multi-Task Decision-Making GPT Model for Autonomous Driving at Unsignalized Intersections},
  author={Liu, Jiaqi and Hang, Peng and Qi, Xiao and Wang, Jianqiang and Sun, Jian},
  year={2023},
  doi={10.1109/ITSC57777.2023.10421993},
  booktitle={IEEE International Conference on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,"Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-15558,ieee,Multi-Agent Q-Net Enhanced Coevolutionary Algorithm for Resource Allocation in Emergency Human-Machine Fusion UAV-MEC System,"Unmanned aerial vehicle (UAV) assisted communication has emerged as a powerful technology for reliable and flexible emergency communications (e.g., earthquakes, hurricanes and floods), especially when the mobile infrastructure is seriously damaged. UAV assisted mobile edge computing (UAV-MEC) system can be deployed in the natural disaster area as communication relay or air mobile base stations to resume communication and provide computing resources for the users in disaster areas. However, the optimized resource allocation performance of UAV-MEC system can be further guaranteed with the human-fusion decision making. In this paper, we construct an emergency human-machine fusion UAV-MEC system consisting of multiple UAVs equipped with computing resources, and the human-machine decision makings are fused for UAV deployment. In order to solve the resource allocation problem of human-machine fusion UAV-MEC system, we establish an human-machine deep integration model for UAV-MEC system, and the UAVs are dispatched reasonably through human-machine fusion decision makings to maintain efficient communication in emergency communication areas. To minimize task latency and improve the computation efficiency in emergency human-machine fusion UAV-MEC system, we consider the number of dispatched UAVs, deployment plans, flight plans, and simultaneously optimize the task allocation scheme, priority order, and task offloading ratio. We propose a reinforcement learning framework combined with evolutionary algorithms, which is named as multi-agent Q-net enhanced cooperative genetic algorithm (MQCGA), for resource allocation of UAV. Based on neural network forecasts, the greedy rate during training processing can be dynamically controlled, and the learning ability of different agents can be strengthened. Simulation experiments are conducted to evaluate the proposed framework, and the results show that our proposed MQCGA algorithm is significantly superior to other algorithms in terms of...",10.1109/TASE.2024.3409551,https://ieeexplore.ieee.org/document/10570258,IEEE Transactions on Automation Science and Engineering,Lu Sun;Ziqian Liu;Zhaolong Ning;Jie Wang;Xianping Fu,2024,1,"@article{2-15558,
  title={Multi-Agent Q-Net Enhanced Coevolutionary Algorithm for Resource Allocation in Emergency Human-Machine Fusion UAV-MEC System},
  author={Sun, Lu and Liu, Ziqian and Ning, Zhaolong and Wang, Jie and Fu, Xianping},
  year={2024},
  doi={10.1109/TASE.2024.3409551},
  journal={IEEE Transactions on Automation Science and Engineering}
}",Algorithmic contributions,Defense / Military / Emergency,Organizational,"Advising, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15559,ieee,Multi-Agent Reinforcement Learning for Mobile Energy Resources Scheduling Amidst Typhoons,"Substantial threats are posed by typhoon events to critical electrical power infrastructure that can result in human casualties and significant economic losses. Both traditional and renewable power generation systems can be negatively impacted, leading to widespread power outages that compromise public safety. In response, we introduce a novel spatial-topological multi-agent reinforcement learning (ST-MARL) method to optimize post-typhoon power system recovery by leveraging mobile energy resources (MERs). Compared to existing MARL methods, the proposed ST-MARL method utilizes convolutional neural network (CNN) and graph convolutional network (GCN) models to extract spatial-topological information from environmental data like typhoon meteorological conditions and power system states, which facilitates real-time decision-making for the routing and scheduling of MERs. Furthermore, this ST-MARL method achieves a two-stage decision-making process by allocating MERs' rewards with an Alternating Current Optimal Power Flow(AC-OPF) model. This ensures safety and feasibility of the decisions. Additionally, we employ the centralized training and distributed execution (CTDE) paradigm to address coordination challenges among MERs. These approaches collectively aim to enhance coordination among MERs, improve economic efficiency, and ensure the power supply of critical loads during post-typhoon power system recovery. Finally, in our case study of the Hong Kong power network, the results indicate that our ST-MARL method outperforms two main existing MARL methods, achieving an improvement of 5.13% and 6.77% on the reward, respectively.",10.1109/TIA.2024.3463608,https://ieeexplore.ieee.org/document/10684059,IEEE Transactions on Industry Applications,Yang Zou;Ziwei Wang;Jingsi Huang;Jie Song;Luo Xu,2024,2,"@article{2-15559,
  title={Multi-Agent Reinforcement Learning for Mobile Energy Resources Scheduling Amidst Typhoons},
  author={Yang Zou and Ziwei Wang and Jingsi Huang and Jie Song and Luo Xu},
  year={2024},
  journal={IEEE Transactions on Industry Applications},
  doi={10.1109/TIA.2024.3463608}
}",Algorithmic contributions,Environment / Resources / Energy,Operational,"Executing, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-1556,acl,Do LLMs Plan Like Human Writers? Comparing Journalist Coverage of Press Releases with LLMs,"Journalists engage in multiple steps in the news writing process that depend on human creativity, like exploring different “angles” (i.e. the specific perspectives a reporter takes). These can potentially be aided by large language models (LLMs). By affecting planning decisions, such interventions can have an outsize impact on creative output. We advocate a careful approach to evaluating these interventions to ensure alignment with human values.In a case study of journalistic coverage of press releases, we assemble a large dataset of 250k press releases and 650k articles covering them. We develop methods to identify news articles that _challenge and contextualize_ press releases. Finally, we evaluate suggestions made by LLMs for these articles and compare these with decisions made by human journalists. Our findings are three-fold: (1) Human-written news articles that challenge and contextualize press releases more take more creative angles and use more informational sources. (2) LLMs align better with humans when recommending angles, compared with informational sources. (3) Both the angles and sources LLMs suggest are significantly less creative than humans.",10.18653/v1/2024.emnlp-main.1216,https://aclanthology.org/2024.emnlp-main.1216,Empirical Methods in Natural Language Processing,"Spangher, Alexander; Peng, Nanyun; Gehrmann, Sebastian; Dredze, Mark",2024,23,"@inproceedings{2-1556,
  title = {Do {LLMs} Plan Like Human Writers? Comparing Journalist Coverage of Press Releases with {LLMs}},
  author = {Spangher, Alexander and Peng, Nanyun and Gehrmann, Sebastian and Dredze, Mark},
  year = {2024},
  booktitle = {Empirical Methods in Natural Language Processing},
  doi = {10.18653/v1/2024.emnlp-main.1216}
}",Empirical contributions,Media / Communication / Entertainment,Operational,"Collaborating, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-15562,ieee,Multi-objective Longitudinal Decision-making for Autonomous Electric Vehicle: A Entropy-constrained Reinforcement Learning Approach,"The challenging task of “autonomous electric vehicle” opens up a new frontier to improving traffic, saving energy and reducing emission. However, many driving decision-making problems are characterized by multiple competing objectives whose relative importance is dynamic, and that makes developing high-performance decision-making system difficult. Therefore, this paper proposes a novel entropy-constrained reinforcement learning (RL) scheme for multi-objective longitudinal decision-making of autonomous electric vehicle. Firstly, in order to prevent the policy from prematurely converging to a local optimum, the policy's entropy is embedded in proximal policy optimization (PPO) algorithm based on actor-critic architecture. Secondly, a self-adjusting mechanism to the weight of entropy is developed to accelerate model training and improve algorithm stability through entropy constraint. Thirdly, multimodal reward signals are designed to guide the RL agent learning complex multi-modal driving policies by considering safety, comfort, economy and transport efficiency. Finally, simulation results show that, the proposed longitudinal decision-making approach for autonomous electric vehicle is feasible and effective.",10.1109/ITSC45102.2020.9294736,https://ieeexplore.ieee.org/document/9294736,IEEE International Conference on Intelligent Transportation Systems,Xiangkun He;Cong Fei;Yulong Liu;Kaiming Yang;Xuewu Ji,2020,17,"@inproceedings{2-15562,
  title={Multi-objective Longitudinal Decision-making for Autonomous Electric Vehicle: A Entropy-constrained Reinforcement Learning Approach},
  author={He, Xiangkun and Fei, Cong and Liu, Yulong and Yang, Kaiming and Ji, Xuewu},
  year={2020},
  doi={10.1109/ITSC45102.2020.9294736},
  booktitle={Proceedings of the IEEE International Conference on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Analyzing",Developer,NA,NA,NA,NA,NA,Yes,No
2-15564,ieee,Multi-Scale Hybrid Vision Transformer for Learning Gastric Histology: AI-Based Decision Support System for Gastric Cancer Treatment,"Gastric endoscopic screening is an effective way to decide appropriate gastric cancer treatment at an early stage, reducing gastric cancer-associated mortality rate. Although artificial intelligence has brought a great promise to assist pathologist to screen digitalized endoscopic biopsies, existing artificial intelligence systems are limited to be utilized in planning gastric cancer treatment. We propose a practical artificial intelligence-based decision support system that enables five subclassifications of gastric cancer pathology, which can be directly matched to general gastric cancer treatment guidance. The proposed framework is designed to efficiently differentiate multi-classes of gastric cancer through multiscale self-attention mechanism using 2-stage hybrid vision transformer networks, by mimicking the way how human pathologists understand histology. The proposed system demonstrates its reliable diagnostic performance by achieving class-average sensitivity of above 0.85 for multicentric cohort tests. Moreover, the proposed system demonstrates its great generalization capability on gastrointestinal track organ cancer by achieving the best class-average sensitivity among contemporary networks. Furthermore, in the observational study, artificial intelligence-assisted pathologists show significantly improved diagnostic sensitivity within saved screening time compared to human pathologists. Our results demonstrate that the proposed artificial intelligence system has a great potential for providing presumptive pathologic opinion and supporting decision of appropriate gastric cancer treatment in practical clinical settings.",10.1109/JBHI.2023.3276778,https://ieeexplore.ieee.org/document/10124975,IEEE Journal of Biomedical and Health Informatics,Yujin Oh;Go Eun Bae;Kyung-Hee Kim;Min-Kyung Yeo;Jong Chul Ye,2023,1,"@article{2-15564,
  title={{Multi-Scale Hybrid Vision Transformer for Learning Gastric Histology: AI-Based Decision Support System for Gastric Cancer Treatment}},
  author={Oh, Yujin and Bae, Go Eun and Kim, Kyung-Hee and Yeo, Min-Kyung and Ye, Jong Chul},
  year={2023},
  journal={IEEE Journal of Biomedical and Health Informatics},
  doi={10.1109/JBHI.2023.3276778}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Forecasting, Advising","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-15572,ieee,On Trustworthy Decision-Making Process of Human Drivers From the View of Perceptual Uncertainty Reduction,"Humans are experts at making decisions for challenging driving tasks with uncertainties. Many efforts have been made to model the decision-making process of human drivers at the behavior level. However, limited studies explain how human drivers actively make trustworthy sequential decisions to complete interactive driving tasks in an uncertain environment. This paper argues that human drivers intently search for actions to reduce the uncertainty of their perception of the environment, i.e., perceptual uncertainty, to a low level that allows them to make a trustworthy decision easily. This paper provides a proof-of-concept framework to empirically reveal that human drivers’ perceptual uncertainty decreases when executing interactive tasks with uncertainties. We first introduce an explainable-artificial intelligence approach (i.e., SHapley Additive exPlanation, SHAP) to determine the salient features on which human drivers base decisions. Then, we use entropy-based measures to quantify the drivers’ perceptual changes in these ranked salient features across the decision-making process, reflecting the changes in uncertainties. The validation and verification of our proposed method are conducted in the highway on-ramp merging scenario with congested traffic using the INTERACTION dataset. Experimental results support that human drivers intentionally seek information to reduce their perceptual uncertainties in the number and rank of salient features of their perception of environments to make a trustworthy decision.",10.1109/TITS.2023.3316455,https://ieeexplore.ieee.org/document/10265753,IEEE Transactions on Intelligent Transportation Systems,Huanjie Wang;Haibin Liu;Wenshuo Wang;Lijun Sun,2024,7,"@article{2-15572,
  title={On Trustworthy Decision-Making Process of Human Drivers From the View of Perceptual Uncertainty Reduction},
  author={Wang, Huanjie and Liu, Haibin and Wang, Wenshuo and Sun, Lijun},
  year={2024},
  doi={10.1109/TITS.2023.3316455},
  journal={IEEE Transactions on Intelligent Transportation Systems}
}",Empirical contributions,Transportation / Mobility / Planning,Individual,"Analyzing, Explaining, Advising","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-15574,ieee,Outcome-Explorer: A Causality Guided Interactive Visual Interface for Interpretable Algorithmic Decision Making,"The widespread adoption of algorithmic decision-making systems has brought about the necessity to interpret the reasoning behind these decisions. The majority of these systems are complex black box models, and auxiliary models are often used to approximate and then explain their behavior. However, recent research suggests that such explanations are not overly accessible to lay users with no specific expertise in machine learning and this can lead to an incorrect interpretation of the underlying model. In this article, we show that a predictive and interactive model based on causality is inherently interpretable, does not require any auxiliary model, and allows both expert and non-expert users to understand the model comprehensively. To demonstrate our method we developed Outcome Explorer, a causality guided interactive interface, and evaluated it by conducting think-aloud sessions with three expert users and a user study with 18 non-expert users. All three expert users found our tool to be comprehensive in supporting their explanation needs while the non-expert users were able to understand the inner workings of a model easily.",10.1109/TVCG.2021.3102051,https://ieeexplore.ieee.org/document/9507307,IEEE Transactions on Visualization and Computer Graphics,Md Naimul Hoque;Klaus Mueller,2022,53,"@article{2-15574,
  title={Outcome-Explorer: A Causality Guided Interactive Visual Interface for Interpretable Algorithmic Decision Making},
  author={Hoque, Md Naimul and Mueller, Klaus},
  year={2022},
  doi={10.1109/TVCG.2021.3102051},
  journal={IEEE Transactions on Visualization and Computer Graphics}
}","Empirical contributions, System/Artifact contributions",Generic / Abstract / Domain-agnostic,no such info,"Explaining, Forecasting","Knowledge provider, Guardian, Decision-maker","Change cognitive demands, Alter decision outcomes, Change affective-perceptual",no such info,"visual explanations, textual explanations, visual analysis",domain knowledge,"Interactive interface, Autonomous System, Conversational/Natural Language",Yes,Yes
2-15576,ieee,PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets,"Acute stroke demands prompt diagnosis and treatment to achieve optimal patient outcomes. However, the intricate and irregular nature of clinical data associated with acute stroke, particularly blood pressure (BP) measurements, presents substantial obstacles to effective visual analytics and decision-making. Through a year-long collaboration with experienced neurologists, we developed PhenoFlow, a visual analytics system that leverages the collaboration between human and Large Language Models (LLMs) to analyze the extensive and complex data of acute ischemic stroke patients. PhenoFlow pioneers an innovative workflow, where the LLM serves as a data wrangler while neurologists explore and supervise the output using visualizations and natural language interactions. This approach enables neurologists to focus more on decision-making with reduced cognitive load. To protect sensitive patient information, PhenoFlow only utilizes metadata to make inferences and synthesize executable codes, without accessing raw patient data. This ensures that the results are both reproducible and interpretable while maintaining patient privacy. The system incorporates a slice-and-wrap design that employs temporal folding to create an overlaid circular visualization. Combined with a linear bar graph, this design aids in exploring meaningful patterns within irregularly measured BP data. Through case studies, PhenoFlow has demonstrated its capability to support iterative analysis of extensive clinical datasets, reducing cognitive load and enabling neurologists to make well-informed decisions. Grounded in long-term collaboration with domain experts, our research demonstrates the potential of utilizing LLMs to tackle current challenges in data-driven clinical decision-making for acute ischemic stroke patients",10.1109/TVCG.2024.3456215,https://ieeexplore.ieee.org/document/10689638,IEEE Transactions on Visualization and Computer Graphics,Jaeyoung Kim;Sihyeon Lee;Hyeon Jeon;Keon-Joo Lee;Hee-Joon Bae;Bohyoung Kim;Jinwook Seo,2024,0,"@article{2-15576,
  title={PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets},
  author={Kim, Jaeyoung and Lee, Sihyeon and Jeon, Hyeon and Lee, Keon-Joo and Bae, Hee-Joon and Kim, Bohyoung and Seo, Jinwook},
  year={2024},
  doi={10.1109/TVCG.2024.3456215},
  journal={IEEE Transactions on Visualization and Computer Graphics}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Knowledge provider, Decision-maker, Decision-subject","Alter decision outcomes, Change cognitive demands",no such info,visual analysis,NA,"Visual, Interactive interface",Yes,Yes
2-15578,ieee,Predicting ICU Interventions: A Transparent Decision Support Model Based on Multivariate Time Series Graph Convolutional Neural Network,"In this study, we present a novel approach for predicting interventions for patients in the intensive care unit using a multivariate time series graph convolutional neural network. Our method addresses two critical challenges: the need for timely and accurate decisions based on changing physiological signals, drug administration information, and static characteristics; and the need for interpretability in the decision-making process. Drawing on real-world ICU records from the MIMIC-III dataset, we demonstrate that our approach significantly improves upon existing machine learning and deep learning methods for predicting two targeted interventions, mechanical ventilation and vasopressors. Our model achieved an accuracy improvement from 81.6% to 91.9% and a F1 score improvement from 0.524 to 0.606 for predicting mechanical ventilation interventions. For predicting vasopressor interventions, our model achieved an accuracy improvement from 76.3% to 82.7% and a F1 score improvement from 0.509 to 0.619. We also assessed the interpretability by performing an adjacency matrix importance analysis, which revealed that our model uses clinically meaningful and appropriate features for prediction. This critical aspect can help clinicians gain insights into the underlying mechanisms of interventions, allowing them to make more informed and precise clinical decisions. Overall, our study represents a significant step forward in the development of decision support systems for ICU patient care, providing a powerful tool for improving clinical outcomes and enhancing patient safety.",10.1109/JBHI.2024.3379998,https://ieeexplore.ieee.org/document/10477486,IEEE Journal of Biomedical and Health Informatics,Zhen Xu;Jinjin Guo;Lang Qin;Yuntao Xie;Yao Xiao;Xinran Lin;Qiming Li;Xinyang Li,2024,11,"@article{2-15578,
  title={Predicting ICU Interventions: A Transparent Decision Support Model Based on Multivariate Time Series Graph Convolutional Neural Network},
  author={Xu, Zhen and Guo, Jinjin and Qin, Lang and Xie, Yuntao and Xiao, Yao and Lin, Xinran and Li, Qiming and Li, Xinyang},
  year={2024},
  journal={IEEE Journal of Biomedical and Health Informatics},
  doi={10.1109/JBHI.2024.3379998}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-15579,ieee,Prediction Based Decision Making for Autonomous Highway Driving,"Autonomous driving decision-making is a challenging task due to the inherent complexity and uncertainty in traffic. For example, adjacent vehicles may change their lane or overtake at any time to pass a slow vehicle or to help traffic flow. Anticipating the intention of surrounding vehicles, estimating their future states and integrating them into the decision-making process of an automated vehicle can enhance the reliability of autonomous driving in complex driving scenarios. This paper proposes a Prediction-based Deep Reinforcement Learning (PDRL) decision-making model that considers the manoeuvre intentions of surrounding vehicles in the decision-making process for highway driving. The model is trained using real traffic data and tested in various traffic conditions through a simulation platform. The results show that the proposed PDRL model improves the decision-making performance compared to a Deep Reinforcement Learning (DRL) model by decreasing collision numbers, resulting in safer driving.",10.1109/ITSC55140.2022.9922398,https://ieeexplore.ieee.org/document/9922398,IEEE International Conference on Intelligent Transportation Systems,Mustafa Yildirim;Sajjad Mozaffari;Luc McCutcheon;Mehrdad Dianati;Alireza Tamaddoni-Nezhad;Saber Fallah,2022,18,"@inproceedings{2-15579,
  title = {Prediction Based Decision Making for Autonomous Highway Driving},
  author = {Yildirim, Mustafa and Mozaffari, Sajjad and McCutcheon, Luc and Dianati, Mehrdad and Tamaddoni-Nezhad, Alireza and Fallah, Saber},
  year = {2022},
  doi = {10.1109/ITSC55140.2022.9922398},
  booktitle = {IEEE International Conference on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15581,ieee,Privacy-Preserving Optimal Insulin Dosing Decision,"Precision diagnosis and treatment are blending outcomes of machine learning and the Internet of Medical Things (IoMT). In the diabetes treatment, a medical center acts as a medical service provider (MSP) with patients data from IoMT devices. The MSP calculates the accurate dosage by importing the health index data into a corresponding decision-making model. However, the outsourcing unprotected patient data directly to the MSP suffers privacy leakage. In this paper, we propose a privacy-preserving optimal insulin dosing decision in the IoMT system (PIDM) to assist doctors in their decision-making with the patients privacy. To achieve practicality and confidentiality simultaneously, we design a series of secure and efficient interactive protocols depending on additive secret sharing to perform in one stage of DQN, namely, optimal decision making. Contrasted to the most relevant schemes, no additional trusted party is needed in our PIDM, which makes our system more practical and efficient. The security of PIDM is testified, meanwhile, the system effectiveness, and the overall efficiency of PIDM is demonstrated through theoretical analysis and simulation experiments.",10.1109/ICASSP39728.2021.9414807,https://ieeexplore.ieee.org/document/9414807,"IEEE International Conference on Acoustics, Speech and Signal Processing",Zuobin Ying;Shuanglong Cao;Shengmin Xu;Ximeng Liu;Lingjuan Lyu;Cen Chen;Li Wang,2021,5,"@inproceedings{2-15581,
  title={Privacy-Preserving Optimal Insulin Dosing Decision},
  author={Ying, Zuobin and Cao, Shuanglong and Xu, Shengmin and Liu, Ximeng and Lyu, Lingjuan and Chen, Cen and Wang, Li},
  year={2021},
  doi={10.1109/ICASSP39728.2021.9414807},
  booktitle={Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Individual,"Executing, Advising, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-15584,ieee,Probabilistic Prediction of Interactive Driving Behavior via Hierarchical Inverse Reinforcement Learning,"Autonomous vehicles (AVs) are on the road. To safely and efficiently interact with other road participants, AVs have to accurately predict the behavior of surrounding vehicles and plan accordingly. Such prediction should be probabilistic, to address the uncertainties in human behavior. Such prediction should also be interactive, since the distribution over all possible trajectories of the predicted vehicle depends not only on historical information, but also on future plans of other vehicles that interact with it. To achieve such interaction-aware predictions, we propose a probabilistic prediction approach based on hierarchical inverse reinforcement learning (IRL). First, we explicitly consider the hierarchical trajectory-generation process of human drivers involving both discrete and continuous driving decisions. Based on this, the distribution over all future trajectories of the predicted vehicle is formulated as a mixture of distributions partitioned by the discrete decisions. Then we apply IRL hierarchically to learn the distributions from real human demonstrations. A case study for the ramp-merging driving scenario is provided. The quantitative results show that the proposed approach can accurately predict both the discrete driving decisions such as yield or pass as well as the continuous trajectories.",10.1109/ITSC.2018.8569453,https://ieeexplore.ieee.org/document/8569453,IEEE International Conference on Intelligent Transportation Systems,Liting Sun;Wei Zhan;Masayoshi Tomizuka,2018,34,"@inproceedings{2-15584,
  title={Probabilistic Prediction of Interactive Driving Behavior via Hierarchical Inverse Reinforcement Learning},
  author={Sun, Liting and Zhan, Wei and Tomizuka, Masayoshi},
  year={2018},
  doi={10.1109/ITSC.2018.8569453},
  booktitle={IEEE International Conference on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Analyzing, Forecasting","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-15586,ieee,QoE-based Situational Awareness-Centric Decision Support for Network Video Surveillance,"Control room video surveillance is an important source of information for ensuring public safety. To facilitate the process, a Decision-Support System (DSS) designed for the security task force is vital and necessary to take decisions rapidly using a sea of information. In case of mission critical operation, Situational Awareness (SA) which consists of knowing what is going on around you at any given time plays a crucial role across a variety of industries and should be placed at the center of our DSS. In our approach, SA system will take advantage of the human factor thanks to the reinforcement signal whereas previous work on this field focus on improving knowledge level of DSS at first and then, uses the human factor only for decision-making. In this paper, we propose a situational awareness-centric decision-support system framework for mission-critical operations driven by Quality of Experience (QoE). Our idea is inspired by the reinforcement learning feedback process which updates the environment understanding of our DSS. The feedback is injected by a QoE built on user perception. Our approach will allow our DSS to evolve according to the context with an up-to-date SA.",10.1109/ICC45855.2022.9838601,https://ieeexplore.ieee.org/document/9838601,IEEE International Conference on Communications,Abhishek Djeachandrane;Said Hoceini;Serge Delmas;Jean-Michel Duquerrois;Abdelhamid Mellouk,2022,3,"@inproceedings{2-15586,
  title={QoE-based Situational Awareness-Centric Decision Support for Network Video Surveillance},
  author={Djeachandrane, Abhishek and Hoceini, Said and Delmas, Serge and Duquerrois, Jean-Michel and Mellouk, Abdelhamid},
  booktitle={IEEE International Conference on Communications},
  year={2022},
  doi={10.1109/ICC45855.2022.9838601}
}",System/Artifact contributions,Defense / Military / Emergency,Operational,"Advising, Executing, Monitoring","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-15587,ieee,Randomized Explainable Machine Learning Models for Efficient Medical Diagnosis,"Deep learning-based models have revolutionized medical diagnostics by using Big Data to enhance disease diagnosis and clinical decision-making. However, their significant computational demands and opaque decision making processes, often characterized as “black-box” systems, pose major challenges in time-critical and resource constrained healthcare settings. To address these issues, this study explores the application of randomized machine learning models, specifically Extreme Learning Machines (ELMs) and Random Vector Functional Link (RVFL) networks, in medical diagnostics. These models introduce stochasticity into their training processes, reducing computational complexity and training times while maintaining accuracy. Furthermore, we integrate Explainable AI techniques namely Local Interpretable Model-agnostic Explanations (LIME) and Shapley Additive Explanations (SHAP) to explain the decision-making rationale of ELMs and RVFL. Performance evaluations on genitourinary cancers and coronary artery disease datasets demonstrate that RVFL outperforms traditional deep learning models, achieving superioraccuracyof88.29%withacomputationaloverhead of 6.22 seconds for genitourinary cancers, and an accuracy of 81.64% with a computational time of 0.0308 seconds for coronary artery disease. This research highlights the potential of randomized models in enhancing efficiency and transparency in medical diagnosis, thereby accelerating better treatment outcomes and advocating for more accessible and interpretable AI solutions in healthcare.",10.1109/JBHI.2024.3491593,https://ieeexplore.ieee.org/document/10752346,IEEE Journal of Biomedical and Health Informatics,Dost Muhammad;Iftikhar Ahmed;Muhammad Ovais Ahmad;Malika Bendechache,2024,9,"@article{2-15587,
  title={Randomized Explainable Machine Learning Models for Efficient Medical Diagnosis},
  author={Muhammad, Dost and Ahmed, Iftikhar and Ahmad, Muhammad Ovais and Bendechache, Malika},
  year={2024},
  journal={IEEE Journal of Biomedical and Health Informatics},
  doi={10.1109/JBHI.2024.3491593}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Explaining","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-15590,ieee,Reinforced Sequential Decision-Making for Sepsis Treatment: The PosNegDM Framework With Mortality Classifier and Transformer,"Sepsis, a life-threatening condition triggered by the body's exaggerated response to infection, demands urgent intervention to prevent severe complications. Existing machine learning methods for managing sepsis struggle in offline scenarios, exhibiting suboptimal performance with survival rates below 50%. This paper introduces the \nPosNegDM\n — “Reinforcement Learning with Positive and Negative Demonstrations for Sequential Decision-Making” framework utilizing an innovative transformer-based model and a feedback reinforcer to replicate expert actions while considering individual patient characteristics. A mortality classifier with 96.7% accuracy guides treatment decisions towards positive outcomes. The \nPosNegDM\n framework significantly improves patient survival, saving 97.39% of patients, outperforming established machine learning algorithms (Decision Transformer and Behavioral Cloning) with survival rates of 33.4% and 43.5%, respectively. Additionally, ablation studies underscore the critical role of the transformer-based decision maker and the integration of a mortality classifier in enhancing overall survival rates. In summary, our proposed approach presents a promising avenue for enhancing sepsis treatment outcomes, contributing to improved patient care and reduced healthcare costs.",10.1109/JBHI.2024.3377214,https://ieeexplore.ieee.org/document/10472521,IEEE Journal of Biomedical and Health Informatics,Dipesh Tamboli;Jiayu Chen;Kiran Pranesh Jotheeswaran;Denny Yu;Vaneet Aggarwal,2024,0,"@article{2-15590,
  title={Reinforced Sequential Decision-Making for Sepsis Treatment: The PosNegDM Framework With Mortality Classifier and Transformer},
  author={Tamboli, Dipesh and Chen, Jiayu and Jotheeswaran, Kiran Pranesh and Yu, Denny and Aggarwal, Vaneet},
  year={2024},
  doi={10.1109/JBHI.2024.3377214},
  journal={IEEE Journal of Biomedical and Health Informatics}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Executing, Forecasting","Knowledge provider, Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-15591,ieee,Reinforcement Learning for Blast Furnace Ironmaking Operation With Safety and Partial Observation Considerations,"Making proper decision online in complex environment during the blast furnace (BF) operation is a key factor in achieving long-term success and profitability in the steel manufacturing industry. Regulatory lags, ore source uncertainty, and continuous decision requirement make it a challenging task. Recently, reinforcement learning (RL) has demonstrated state-of-the-art performance in various sequential decision-making problems. However, the strict safety requirements make it impossible to explore optimal decisions through online trial and error. Therefore, this article proposes a novel offline RL approach designed to ensure safety, maximize return, and address issues of partially observed states. Specifically, it utilizes an off-policy actor-critic framework to infer the optimal decision from expert operation trajectories. The “actor” in this framework is jointly trained by the supervision and evaluation signals to make decision with low risk and high return. Furthermore, we investigate a recurrent version of the actor and critic networks to better capture the complete observations, which solves the partially observed Markov decision process (POMDP) arising from sensor limitations. Verification within the BF smelting process demonstrates the improvements of the proposed algorithm in performance, i.e., safety and return.",10.1109/TNNLS.2023.3340741,https://ieeexplore.ieee.org/document/10402559,IEEE Transactions on Neural Networks and Learning Systems,Ke Jiang;Zhaohui Jiang;Xudong Jiang;Yongfang Xie;Weihua Gui,2024,7,"@article{2-15591,
  title={Reinforcement Learning for Blast Furnace Ironmaking Operation With Safety and Partial Observation Considerations},
  author={Jiang, Ke and Jiang, Zhaohui and Jiang, Xudong and Xie, Yongfang and Gui, Weihua},
  year={2024},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={},
  number={},
  pages={},
  doi={10.1109/TNNLS.2023.3340741}
}",Algorithmic contributions,Manufacturing / Industry / Automation,Operational,"Executing, Advising",Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-15593,ieee,Reinforcement Learning with Uncertainty Estimation for Tactical Decision-Making in Intersections,"This paper investigates how a Bayesian reinforcement learning method can be used to create a tactical decision-making agent for autonomous driving in an intersection scenario, where the agent can estimate the confidence of its decisions. An ensemble of neural networks, with additional randomized prior functions (RPF), are trained by using a bootstrapped experience replay memory. The coefficient of variation in the estimated Q-values of the ensemble members is used to approximate the uncertainty, and a criterion that determines if the agent is sufficiently confident to make a particular decision is introduced. The performance of the ensemble RPF method is evaluated in an intersection scenario and compared to a standard Deep Q-Network method, which does not estimate the uncertainty. It is shown that the trained ensemble RPF agent can detect cases with high uncertainty, both in situations that are far from the training distribution, and in situations that seldom occur within the training distribution. This work demonstrates one possible application of such a confidence estimate, by using this information to choose safe actions in unknown situations, which removes all collisions from within the training distribution, and most collisions outside of the distribution.",10.1109/ITSC45102.2020.9294407,https://ieeexplore.ieee.org/document/9294407,IEEE International Conference on Intelligent Transportation Systems,Carl-Johan Hoel;Tommy Tram;Jonas Sjöberg,2020,363,"@inproceedings{2-15593,
  title={Reinforcement Learning with Uncertainty Estimation for Tactical Decision-Making in Intersections},
  author={Hoel, Carl-Johan and Tram, Tommy and Sj{\""o}berg, Jonas},
  year={2020},
  doi={10.1109/ITSC45102.2020.9294407},
  booktitle={IEEE International Conference on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Analyzing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15605,ieee,Robust Lane Change Decision Making for Autonomous Vehicles: An Observation Adversarial Reinforcement Learning Approach,"Reinforcementlearning holds the promise of allowing autonomous vehicles to learn complex decision making behaviors through interacting with other traffic participants. However, many real-world driving tasks involve unpredictable perception errors or measurement noises which may mislead an autonomous vehicle into making unsafe decisions, even cause catastrophic failures. In light of these risks, to ensure safety under perception uncertainty, autonomous vehicles are required to be able to cope with the worst case observation perturbations. Therefore, this paper proposes a novel observation adversarial reinforcement learning approach for robust lane change decision making of autonomous vehicles. A constrained observation-robust Markov decision process is presented to model lane change decision making behaviors of autonomous vehicles under policy constraints and observation uncertainties. Meanwhile, a black-box attack technique based on Bayesian optimization is implemented to approximate the optimal adversarial observation perturbations efficiently. Furthermore, a constrained observation-robust actor-critic algorithm is advanced to optimize autonomous driving lane change policies while keeping the variations of the policies attacked by the optimal adversarial observation perturbations within bounds. Finally, the robust lane change decision making approach is evaluated in three stochastic mixed traffic flows based on different densities. The results demonstrate that the proposed method can not only enhance the performance of an autonomous vehicle but also improve the robustness of lane change policies against adversarial observation perturbations.",10.1109/TIV.2022.3165178,https://ieeexplore.ieee.org/document/9750867,IEEE Transactions on Intelligent Vehicles,Xiangkun He;Haohan Yang;Zhongxu Hu;Chen Lv,2023,210,"@article{2-15605,
  title={Robust Lane Change Decision Making for Autonomous Vehicles: An Observation Adversarial Reinforcement Learning Approach},
  author={He, Xiangkun and Yang, Haohan and Hu, Zhongxu and Lv, Chen},
  year={2023},
  journal={IEEE Transactions on Intelligent Vehicles},
  doi={10.1109/TIV.2022.3165178}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,Developer,NA,NA,NA,NA,NA,Yes,No
2-15606,ieee,Safe Decision-making for Lane-change of Autonomous Vehicles via Human Demonstration-aided Reinforcement Learning,"Decision-making is critical for lane change in autonomous driving. Reinforcement learning (RL) algorithms aim to identify the values of behaviors in various situations and thus they become a promising pathway to address the decision- making problem. However, poor runtime safety hinders RL- based decision-making strategies from complex driving tasks in practice. To address this problem, human demonstrations are incorporated into the RL-based decision-making strategy in this paper. Decisions made by human subjects in a driving simulator are treated as safe demonstrations, which are stored into the replay buffer and then utilized to enhance the training process of RL. A complex lane change task in an off-ramp scenario is established to examine the performance of the developed strategy. Simulation results suggest that human demonstrations can effectively improve the safety of decisions of RL. And the proposed strategy surpasses other existing learning-based decision-making strategies with respect to multiple driving performances.",10.1109/ITSC55140.2022.9921872,https://ieeexplore.ieee.org/document/9921872,IEEE International Conference on Intelligent Transportation Systems,Jingda Wu;Wenhui Huang;Niels de Boer;Yanghui Mo;Xiangkun He;Chen Lv,2022,32,"@inproceedings{2-15606,
  title={Safe Decision-making for Lane-change of Autonomous Vehicles via Human Demonstration-aided Reinforcement Learning},
  author={Wu, Jingda and Huang, Wenhui and de Boer, Niels and Mo, Yanghui and He, Xiangkun and Lv, Chen},
  year={2022},
  doi={10.1109/ITSC55140.2022.9921872},
  booktitle={IEEE International Conference on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15608,ieee,Safe Reinforcement Learning of Lane Change Decision Making with Risk-Fused Constraint,"Deep reinforcement learning (DRL) has become a powerful method for autonomous driving while often lacking safety guarantees. In this paper, we propose a Risk-fused Constraint Deep Reinforcement Learning (RCDRL) with D3QN network for safe decision making in lane change maneuver. The problem is formulated as a state-wise MDP (SCMDP), which embeds a rule-based risk-fused Constraint module. We map the decision action to the trajectory layer via a polynomial curve-based trajectory planner, which is combined with the predicted trajectories of surrounding vehicles to assess future risk and correct the unsafe action. Therefore, the proposed method can deal with unsafe decision actions when training the policy network. To investigate the decision performance, the trained RCDRL policy is tested and validated under different traffic densities. In particular, we implement real vehicle tests to validate the effectiveness of the proposed method. Simulation and real vehicle tests demonstrated that the proposed RCDRL method achieves better performance, especially in safe decision. In addition, the framework can be extended with other advanced DRL networks.",10.1109/ITSC57777.2023.10422331,https://ieeexplore.ieee.org/document/10422331,IEEE International Conference on Intelligent Transportation Systems,Zhuoren Li;Lu Xiong;Bo Leng;Puhang Xu;Zhiqing Fu,2023,1,"@inproceedings{2-15608,
  title={Safe Reinforcement Learning of Lane Change Decision Making with Risk-Fused Constraint},
  author={Li, Zhuoren and Xiong, Lu and Leng, Bo and Xu, Puhang and Fu, Zhiqing},
  year={2023},
  doi={10.1109/ITSC57777.2023.10422331},
  booktitle={IEEE International Conference on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,Developer,NA,NA,NA,NA,NA,Yes,No
2-15610,ieee,Safety-based Reinforcement Learning Longitudinal Decision for Autonomous Driving in Crosswalk Scenarios,"Autonomous vehicles (AVs) need to make driving decisions to interact with other traffic participants. By adapting to different scenarios with specific parameters, traditional strategies attempt to leverage rule-based methods to solve the decision problems. In this paper, we present a novel reinforcement learning method for resolving interaction uncertainty in the decision-making problem. We construct prior knowledge by introducing traffic regulations and constraints and then converting them into rules that govern the learning of driving policies. To promote safe driving, a safety-aware module equipped with a mathematical collision correlation analysis is developed to anticipate and handle dangerous traffic scenarios. A realistic scenario involving an AV approaching a crosswalk is used to validate the proposed method. The experimental results indicate that the proposed method improves driving safety and efficiency significantly when compared to alternative approaches and can be generalized to more difficult scenarios.",10.1109/IJCNN55064.2022.9892236,https://ieeexplore.ieee.org/document/9892236,International Joint Conference on Neural Networks,Fangzhou Xiong;Dongchun Ren;Mingyu Fan;Shuguang Ding;Zhiyong Liu,2022,0,"@inproceedings{2-15610,
  title={Safety-based Reinforcement Learning Longitudinal Decision for Autonomous Driving in Crosswalk Scenarios},
  author={Xiong, Fangzhou and Ren, Dongchun and Fan, Mingyu and Ding, Shuguang and Liu, Zhiyong},
  year={2022},
  doi={10.1109/IJCNN55064.2022.9892236},
  booktitle={International Joint Conference on Neural Networks}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Forecasting","Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-15614,ieee,Selective Presentation of AI Object Detection Results While Maintaining Human Reliance,"Transparency in decision-making is an important factor for AI-driven autonomous systems to be trusted and relied on by users. Studies in the field of visual information processing typically attempt to make an AI system's behavior transparent by showing bounding boxes or heatmaps as explanations. However, it has also been found that an excessive amount of explanations sometimes causes information overload and brings negative results. This paper proposes SmartBBox, a method for reducing the number of bounding boxes to show while maintaining human reliance on an AI. It infers if each bounding box is worth showing by predicting its effect on human reliance. SmartBBox can autonomously learn to decide whether to show bounding boxes from humans' usage data. We implemented and tested SmartBBox in an autonomous driving scenario in which a human continuously decides whether to rely on an autonomous driving system while observing the dynamic results of object detection by the system. The results suggest that SmartBBox can reduce bounding boxes 64.8% on average from object recognition results while keeping human reliance at the same level as in the case where all the bounding boxes are presented.",10.1109/IROS55552.2023.10341684,https://ieeexplore.ieee.org/document/10341684,IEEE/RSJ International Conference on Intelligent Robots and Systems,Yosuke Fukuchi;Seiji Yamada,2023,2,"@inproceedings{2-15614,
  title={Selective Presentation of AI Object Detection Results While Maintaining Human Reliance},
  author={Fukuchi, Yosuke and Yamada, Seiji},
  year={2023},
  doi={10.1109/IROS55552.2023.10341684},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Forecasting, Explaining",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15615,ieee,Sensor based Prediction of Human Driving Decisions using Feed forward Neural Networks for Intelligent Vehicles,Prediction of human driving decisions is an important aspect of modeling human behavior for the application to Advanced Driver Assistance Systems (ADAS) in the intelligent vehicles. This paper presents a sensor based receding horizon model for the prediction of human driving commands. Human driving decisions are expressed in terms of the vehicle speed and steering wheel angle profiles. Environmental state and human intention are the two major factors influencing the human driving decisions. The environment around the vehicle is perceived using LIDAR sensor. Feature extractor computes the occupancy grid map from the sensor data which is filtered and processed to provide precise and relevant information to the feed-forward neural network. Human intentions can be identified from the past driving decisions and represented in the form of time series data for the neural network. Supervised machine learning is used to train the neural network. Data collection and model validation is performed in the driving simulator using the SCANeR studio software. Simulation results are presented alone with the analysis.,10.1109/ITSC.2018.8569441,https://ieeexplore.ieee.org/document/8569441,IEEE International Conference on Intelligent Transportation Systems,Shriram C Jugade;Alessandro C Victorino;Véronique B Cherfaoui;Stratis Kanarachos,2018,3,"@inproceedings{2-15615,
  title={Sensor based Prediction of Human Driving Decisions using Feed forward Neural Networks for Intelligent Vehicles},
  author={Jugade, Shriram C and Victorino, Alessandro C and Cherfaoui, Véronique B and Kanarachos, Stratis},
  year={2018},
  doi={10.1109/ITSC.2018.8569441},
  booktitle={IEEE International Conference on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Forecasting, Advising","Guardian, Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-15619,ieee,Social Cascade FNN: An Interpretable Learning-Based Decision-Making Framework for Autonomous Driving in Lane Changing Scenarios,"Lane changing behavior causes a considerable proportion of traffic accidents. Effective decision-making strategies for autonomous vehicles are promising to enhance traffic safety in lane changing scenarios. Naturalistic driving datasets driven deep learning has emerged as a competitive approach to making lane changing decisions, which is capable to consider social interactions, however, the lack of interpretability hinders its application in safety-critical autonomous driving. To address this issue, we proposed a learning-based lane changing decision-making framework that extracts rules from naturalistic driving datasets. The proposed method employed a cascade Fuzzy Neural Network (FNN) to learn from sequential data, coupled with a social pooling layer that extracts interactions among vehicles. By integrating both temporal and spatial information, this framework enhances the learning ability of the system while preserving the interpretability of FNN. Our method out-performs state-of-the-art approaches on two publicly available datasets, demonstrating its effectiveness in lane changes. The method can also accurately make decisions in diverse driving scenarios and provide decision rules.",10.1109/ITSC57777.2023.10422302,https://ieeexplore.ieee.org/document/10422302,IEEE International Conference on Intelligent Transportation Systems,Hairui Wang;Yanbo Chen;Huilong Yu;Junqiang Xi,2023,5,"@inproceedings{2-15619,
  title={Social Cascade FNN: An Interpretable Learning-Based Decision-Making Framework for Autonomous Driving in Lane Changing Scenarios},
  author={Wang, Hairui and Chen, Yanbo and Yu, Huilong and Xi, Junqiang},
  year={2023},
  doi={10.1109/ITSC57777.2023.10422302},
  booktitle={IEEE International Conference on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Explaining","Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-15624,ieee,SVCE: Shapley Value Guided Counterfactual Explanation for Machine Learning-Based Autonomous Driving,"The explainability of complex machine-learning models is becoming increasingly significant in safety-critical domains such as autonomous driving. In this context, counterfactual explanation (CE), as an effective explainability method in explainable artificial intelligence, plays an important role. It aims to identify minimal alterations to input that can change the model’s output, thereby revealing key factors influencing model decisions. However, generating counterfactual samples might involve manually selecting input features, potentially leading to suboptimal and biased explanations. This study introduces a feature contribution guided CE generation framework to address this issue. Our method utilizes feature contributions based on Shapley values to guide the model’s focus on the most influential features. This enables end-users to quickly pinpoint the search direction in generating CEs (e.g., prioritizing the most critical features) and producing representative CEs. To comprehensively evaluate our method, we conducted experimental validation on two representative machine learning models: autonomous driving decision-making using Deep Q-Network and lane-changing prediction using deep learning. In addition, we conducted a user-centered study to evaluate the practical applicability of the SVCE in autonomous driving scenarios, which serves as a crucial validation of the presented SVCE. The results show that SVCE can help users understand and diagnose the model.",10.1109/TITS.2024.3393634,https://ieeexplore.ieee.org/document/10521560,IEEE Transactions on Intelligent Transportation Systems,Meng Li;Hengyang Sun;Yanjun Huang;Hong Chen,2024,9,"@article{2-15624,
  title={SVCE: Shapley Value Guided Counterfactual Explanation for Machine Learning-Based Autonomous Driving},
  author={Li, Meng and Sun, Hengyang and Huang, Yanjun and Chen, Hong},
  year={2024},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  doi={10.1109/TITS.2024.3393634}
}",Methodological contributions,Transportation / Mobility / Planning,Individual,"Explaining, Executing","Developer, Guardian",NA,NA,NA,NA,NA,Yes,No
2-15627,ieee,Synthesis of Provably Correct Autonomy Protocols for Shared Control,"We synthesize shared control protocols subject to probabilistic temporal logic specifications. Specifically, we develop a framework in which a human and an autonomy protocol can issue commands to carry out a certain task. We blend these commands into a joint input to a robot. We model the interaction between the human and the robot as a Markov decision process representing the shared control scenario. Using inverse reinforcement learning, we obtain an abstraction of the human's behavior. We use randomized strategies to account for randomness in human's decisions, caused by factors such as the complexity of the task specifications or imperfect interfaces. We design the autonomy protocol to ensure that the resulting robot behavior satisfies given safety and performance specifications in probabilistic temporal logic. Additionally, the resulting strategies generate behavior as similar to the behavior induced by the human's commands as possible. We solve the underlying problem efficiently using quasiconvex programming. Case studies involving autonomous wheelchair navigation and unmanned aerial vehicle mission planning showcase the applicability of our approach.",10.1109/TAC.2020.3018029,https://ieeexplore.ieee.org/document/9171419,IEEE Transactions on Automatic Control,Murat Cubuktepe;Nils Jansen;Mohammed Alshiekh;Ufuk Topcu,2021,7,"@article{2-15627,
  title     = {Synthesis of Provably Correct Autonomy Protocols for Shared Control},
  author    = {Murat Cubuktepe and Nils Jansen and Mohammed Alshiekh and Ufuk Topcu},
  year      = {2021},
  doi       = {10.1109/TAC.2020.3018029},
  journal   = {IEEE Transactions on Automatic Control}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Transportation / Mobility / Planning",Operational,"Executing, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15629,ieee,Task-agnostic Decision Transformer for Multi-type Agent Control with Federated Split Training,"With the rapid advancements in artificial intelligence, the development of knowledgeable and personalized agents has become increasingly prevalent. However, the inherent variability in state variables and action spaces among personalized agents poses significant aggregation challenges for traditional federated learning algorithms. To tackle these challenges, we introduce the Federated Split Decision Transformer (FSDT), an innovative framework designed explicitly for AI agent decision tasks. The FSDT framework excels at navigating the intricacies of personalized agents by harnessing distributed data for training while preserving data privacy. It employs a two-stage training process, with local embedding and prediction models on client agents and a global transformer decoder model on the server. Our comprehensive evaluation using the benchmark D4RL dataset highlights the superior performance of our algorithm in federated split learning for personalized agents, coupled with significant reductions in communication and computational overhead compared to traditional centralized training approaches. The FSDT framework demonstrates strong potential for enabling efficient and privacy-preserving collaborative learning in applications such as autonomous driving decision systems. Our findings underscore the efficacy of the FSDT framework in effectively leveraging distributed offline reinforcement learning data to enable powerful multi-type agent decision systems.",10.1109/IJCNN60899.2024.10651270,https://ieeexplore.ieee.org/document/10651270,International Joint Conference on Neural Networks,Zhiyuan Wang;Bokui Chen;Xiaoyang Qu;Zhenhou Hong;Jing Xiao;Jianzong Wang,2024,0,"@inproceedings{2-15629,
  title={Task-agnostic Decision Transformer for Multi-type Agent Control with Federated Split Training},
  author={Wang, Zhiyuan and Chen, Bokui and Qu, Xiaoyang and Hong, Zhenhou and Xiao, Jing and Wang, Jianzong},
  year={2024},
  doi={10.1109/IJCNN60899.2024.10651270},
  booktitle={International Joint Conference on Neural Networks}
}",Algorithmic contributions,"Transportation / Mobility / Planning, Generic / Abstract / Domain-agnostic",no such info,"Executing, Forecasting",Developer,NA,NA,NA,NA,NA,Yes,No
2-15632,ieee,TDM: Trustworthy Decision-Making Via Interpretability Enhancement,"Human-robot interactive decision-making is increasingly becoming ubiquitous, and trust is an influential factor in determining the reliance on autonomy. However, it is not reasonable to trust systems that are beyond our comprehension, and typical machine learning and data-driven decision-making are black-box paradigms that impede interpretability. Therefore, it is critical to establish computational trustworthy decision-making mechanisms enhanced by interpretability-aware strategies. To this end, we propose a Trustworthy Decision-Making (TDM) framework, which integrates symbolic planning into sequential decision-making. The framework learns interpretable subtasks that result in a complex, higher-level composite task that can be formally evaluated using the proposed trust metric. TDM enables the subtask-level interpretability by design and converges to an optimal symbolic plan from the learned subtasks. Moreover, a TDM-based algorithm is introduced to demonstrate the unification of symbolic planning with other sequential-decision making algorithms, reaping the benefits of both. Experimental results validate the effectiveness of trust-score-based planning while improving the interpretability of subtasks.",10.1109/TETCI.2021.3084290,https://ieeexplore.ieee.org/document/9455355,IEEE Transactions on Emerging Topics in Computational Intelligence,Daoming Lyu;Fangkai Yang;Hugh Kwon;Wen Dong;Levent Yilmaz;Bo Liu,2022,27,"@article{2-15632,
  title={TDM: Trustworthy Decision-Making Via Interpretability Enhancement},
  author={Lyu, Daoming and Yang, Fangkai and Kwon, Hugh and Dong, Wen and Yilmaz, Levent and Liu, Bo},
  year={2022},
  doi={10.1109/TETCI.2021.3084290},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}
}","Algorithmic contributions, Methodological contributions",Manufacturing / Industry / Automation,no such info,"Explaining, Advising, Collaborating","Developer, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-15640,ieee,Towards Robust Decision-Making for Autonomous Driving on Highway,"Reinforcement learning (RL) methods are commonly regarded as effective solutions for designing intelligent driving policies. Nonetheless, even if the RL policy is converged after training, it is notoriously difficult to ensure safety. In particular, RL policy is susceptible to insecurity in the presence of long-tail or unseen traffic scenarios, \ni.e.\n, out-of-distribution test data. Therefore, the design of the RL-based decision-making method must account for this shift in distribution. This paper proposes a robust decision-making framework for autonomous driving on the highway to improve driving safety. First, a Deep Deterministic Policy Gradient (DDPG)-based RL policy that directly maps observations to actions is constructed. Subsequently, the model uncertainty of the DDPG policy is evaluated at runtime to quantify the policy's reliability and identify unseen scenarios. In addition, a complementary principle-based policy is developed using the intelligent driver model (IDM) and the model for minimizing overall braking induced by lane changes (MOBIL). It will take over the DDPG policy when encountering unseen scenarios to guarantee a lower-bound performance of the decision-making system. Finally, the proposed method is implemented on an embedded system, \ni.e.\n, NVIDIA Jetson AGX Xavier, and out-of-training distribution challenging cases are considered in the experiment, \ni.e.\n, observation with sensor noise, traffic density increasing significantly, objects falling from the front vehicle, and road construction causing temporal changes in road structure. Results indicate that the proposed framework outperforms state-of-the-art benchmarks. Additionally, the code is provided.",10.1109/TVT.2023.3268500,https://ieeexplore.ieee.org/document/10107652,IEEE Transactions on Vehicular Technology,Kai Yang;Xiaolin Tang;Sen Qiu;Shufeng Jin;Zichun Wei;Hong Wang,2023,126,"@article{2-15640,
  title={Towards Robust Decision-Making for Autonomous Driving on Highway},
  author={Yang, Kai and Tang, Xiaolin and Qiu, Sen and Jin, Shufeng and Wei, Zichun and Wang, Hong},
  year={2023},
  journal={IEEE Transactions on Vehicular Technology},
  volume={},
  number={},
  pages={},
  doi={10.1109/TVT.2023.3268500}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Monitoring","Guardian, Developer",NA,NA,NA,NA,NA,Yes,No
2-15642,ieee,Traded Control of Human–Machine Systems for Sequential Decision-Making Based on Reinforcement Learning,"The human–machine SDM problem refers to the SDM problem in which humans and machines participate together. They alleviate the burden on human decision-makers and also allow humans to have more final decision-making authority than fully autonomous machine control. At present, there are very few methods to study the problem of human–machine SDM, and the human–machine collaboration involved in this field lacks powerful exploration. The traded control method of the human–machine systems proposed by us provides a solution to the human–machine SDM. It can provide help for the field of human–machine co-driving, human–machine minimally invasive surgery, and other more decision-making problems involving humans and machines.",10.1109/TAI.2021.3127857,https://ieeexplore.ieee.org/document/9613742,IEEE Transactions on Artificial Intelligence,Qianqian Zhang;Yu Kang;Yun-Bo Zhao;Pengfei Li;Shiyi You,2022,0,"@article{2-15642,
  title={Traded Control of Human--Machine Systems for Sequential Decision-Making Based on Reinforcement Learning},
  author={Zhang, Qianqian and Kang, Yu and Zhao, Yun-Bo and Li, Pengfei and You, Shiyi},
  year={2022},
  doi={10.1109/TAI.2021.3127857},
  journal={IEEE Transactions on Artificial Intelligence}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,"Operational, Individual","Advising, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15643,ieee,TrajGAIL: Trajectory Generative Adversarial Imitation Learning for Long-Term Decision Analysis,"Mobile sensing and information technology have enabled us to collect a large amount of mobility data from human decision-makers, for example, GPS trajectories from taxis, Uber cars, and passenger trip data of taking buses and trains. Understanding and learning human decision-making strategies from such data can potentially promote individual's well-being and improve the transportation service quality. Existing works on human strategy learning, such as inverse reinforcement learning, all model the decision-making process as a Markov decision process, thus assuming the Markov property. In this work, we show that such Markov property does not hold in real-world human decision-making processes. To tackle this challenge, we develop a Trajectory Generative Adversarial Imitation Learning (TrajGAIL) framework. It captures the long-term decision dependency by modeling the human decision processes as variable length Markov decision processes (VLMDPs), and designs a deep-neural-network-based framework to inversely learn the decision-making strategy from the human agent's historical dataset. We validate our framework using two real world human-generated spatial-temporal datasets including taxi driver passenger-seeking decision data and public transit trip data. Results demonstrate significant accuracy improvement in learning human decision-making strategies, when comparing to baselines with Markov property assumptions.",10.1109/ICDM50108.2020.00089,https://ieeexplore.ieee.org/document/9338370,IEEE International Conference on Data Mining,Xin Zhang;Yanhua Li;Xun Zhou;Ziming Zhang;Jun Luo,2020,34,"@inproceedings{2-15643,
  title={TrajGAIL: Trajectory Generative Adversarial Imitation Learning for Long-Term Decision Analysis},
  author={Zhang, Xin and Li, Yanhua and Zhou, Xun and Zhang, Ziming and Luo, Jun},
  booktitle={IEEE International Conference on Data Mining},
  year={2020},
  doi={10.1109/ICDM50108.2020.00089}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,"Analyzing, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15644,ieee,Travel Demand Forecasting: A Fair AI Approach,"Artificial Intelligence (AI) and machine learning have been increasingly adopted for travel demand forecasting. The AI-based travel demand forecasting models, though generate accurate predictions, may produce prediction biases and raise fairness issues. Using such biased models for decision-making may lead to transportation policies that exacerbate social inequalities. However, limited studies have been focused on addressing the fairness issues of these models. Therefore, in this study, we propose a novel methodology to develop fairness-aware, highly-accurate travel demand forecasting models. Particularly, the proposed methodology can enhance the fairness of AI models for multiple protected attributes (such as race and income) simultaneously. Specifically, we introduce a new fairness regularization term, which is explicitly designed to measure the correlation between prediction accuracy and multiple protected attributes, into the loss function of the travel demand forecasting model. We conduct two case studies to evaluate the performance of the proposed methodology using real-world ridesourcing-trip data in Chicago, IL and Austin, TX, respectively. Results highlight that our proposed methodology can effectively enhance fairness for multiple protected attributes while preserving prediction accuracy. Additionally, we have compared our methodology with three state-of-the-art methods that adopt the regularization term approach, and the results demonstrate that our approach significantly outperforms them in both preserving prediction accuracy and enhancing fairness. This study can provide transportation professionals with a new tool to achieve fair and accurate travel demand forecasting.",10.1109/TITS.2024.3395061,https://ieeexplore.ieee.org/document/10529943,IEEE Transactions on Intelligent Transportation Systems,Xiaojian Zhang;Qian Ke;Xilei Zhao,2024,0,"@article{2-15644,
  title={Travel Demand Forecasting: A Fair AI Approach},
  author={Zhang, Xiaojian and Ke, Qian and Zhao, Xilei},
  year={2024},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  doi={10.1109/TITS.2024.3395061}
}",Methodological contributions,Transportation / Mobility / Planning,Institutional,"Advising, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15646,ieee,Uncertainty-Aware Decision-Making for Autonomous Driving at Uncontrolled Intersections,"Reinforcement learning (RL) has been widely used in the decision-making of autonomous vehicles (AVs) in recent studies. However, existing RL methods generally find the optimal policy by maximizing the expectation of future returns, which lacks distributional treatments of risky situations. Additionally, various uncertainties arising from the environment could also cause unreliable decisions, particularly in some complex urban environments. In this paper, the fully parameterized quantile network (FPQN) is utilized to estimate the full return distribution. Then, the conditional value-at-risk (CVaR) is utilized with the return distribution information to generate uncertainty-aware driving behavior. Additionally, an uncontrolled four-way intersection is developed by the Simulation of Urban Mobility (SUMO) simulation platform, which considers both the surrounding vehicles (SVs) and pedestrians. More specifically, to simulate the real-world traffic environment, the uncertainty arising from the occlusion, and the behavior uncertainty of surrounding traffic participants are also considered. The experiment results suggest that the proposed method outperforms the baseline methods in terms of safety. Furthermore, the results also indicate that the proposed method can make reasonable decisions in some challenging driving cases in the presence of uncertainty.",10.1109/TITS.2023.3283019,https://ieeexplore.ieee.org/document/10155311,IEEE Transactions on Intelligent Transportation Systems,Xiaolin Tang;Guichuan Zhong;Shen Li;Kai Yang;Keqi Shu;Dongpu Cao;Xianke Lin,2023,43,"@article{2-15646,
  title={Uncertainty-Aware Decision-Making for Autonomous Driving at Uncontrolled Intersections},
  author={Tang, Xiaolin and Zhong, Guichuan and Li, Shen and Yang, Kai and Shu, Keqi and Cao, Dongpu and Lin, Xianke},
  year={2023},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  doi={10.1109/TITS.2023.3283019}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,Developer,NA,NA,NA,NA,NA,Yes,No
2-15650,ieee,VBridge: Connecting the Dots Between Features and Data to Explain Healthcare Models,"Machine learning (ML) is increasingly applied to Electronic Health Records (EHRs) to solve clinical prediction tasks. Although many ML models perform promisingly, issues with model transparency and interpretability limit their adoption in clinical practice. Directly using existing explainable ML techniques in clinical settings can be challenging. Through literature surveys and collaborations with six clinicians with an average of 17 years of clinical experience, we identified three key challenges, including clinicians' unfamiliarity with ML features, lack of contextual information, and the need for cohort-level evidence. Following an iterative design process, we further designed and developed VBridge, a visual analytics tool that seamlessly incorporates ML explanations into clinicians' decision-making workflow. The system includes a novel hierarchical display of contribution-based feature explanations and enriched interactions that \nconnect the dots\n between ML features, explanations, and data. We demonstrated the effectiveness of VBridge through two case studies and expert interviews with four clinicians, showing that visually associating model explanations with patients' situational records can help clinicians better interpret and use model predictions when making clinician decisions. We further derived a list of design implications for developing future explainable ML tools to support clinical decision-making.",10.1109/TVCG.2021.3114836,https://ieeexplore.ieee.org/document/9555810,IEEE Transactions on Visualization and Computer Graphics,Furui Cheng;Dongyu Liu;Fan Du;Yanna Lin;Alexandra Zytek;Haomin Li;Huamin Qu;Kalyan Veeramachaneni,2022,289,"@article{2-15650,
  title={VBridge: Connecting the Dots Between Features and Data to Explain Healthcare Models},
  author={Cheng, Furui and Liu, Dongyu and Du, Fan and Lin, Yanna and Zytek, Alexandra and Li, Haomin and Qu, Huamin and Veeramachaneni, Kalyan},
  year={2022},
  doi={10.1109/TVCG.2021.3114836},
  journal={IEEE Transactions on Visualization and Computer Graphics}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Explaining, Forecasting","Decision-maker, Decision-subject","Alter decision outcomes, Change trust",Change AI responses,"contribution-based feature explanations, risk level",NA,"Textual, Visual",Yes,Yes
2-15651,ieee,Vision-Based Autonomous Driving: A Hierarchical Reinforcement Learning Approach,"Human drivers have excellent perception and reaction abilities in complex environments such as dangerous highways, busy intersections, and harsh weather conditions. To achieve human-level driving performance, autonomous driving systems require powerful environmental perception systems and the ability to make accurate decisions in difficult situations, enabling vehicles to maintain smooth driving. However, making decisions based on visual perception is still a daunting challenge for autonomous vehicles, also current end-to-end networks and modular frameworks have limitations in effectively addressing perception, decision-making, and control, such as a lack of interpretability and weak generalization ability in complex environments. This paper proposes an elaborate modular pipeline for autonomous driving that effectively integrates semantic perception information, multi-level decision tasks, and control modules. The decision-making module comprehensively considers high-level maneuver selection and low-level motion control in both horizontal and vertical directions. The proposed MP framework is trained end-to-end by a novel hierarchical reinforcement learning method with a new action sampling mechanism. To adapt to the experimental scenarios, we use the CARLA simulation platform to collect data to evaluate the proposed autonomous driving framework and its new training method, considering various environmental factors such as sunny, dusk, and rainy weather. The results show that the framework exhibits smooth and effective driving strategies in different environments and can converge quickly and stably. The method improves learning efficiency and reduces unnecessary coupling and error propagation. Overall, the proposed framework and training method provide new ideas for improving existing autonomous driving systems.",10.1109/TVT.2023.3266940,https://ieeexplore.ieee.org/document/10114943,IEEE Transactions on Vehicular Technology,Jiao Wang;Haoyi Sun;Can Zhu,2023,34,"@article{2-15651,
  title={Vision-Based Autonomous Driving: A Hierarchical Reinforcement Learning Approach},
  author={Wang, Jiao and Sun, Haoyi and Zhu, Can},
  year={2023},
  journal={IEEE Transactions on Vehicular Technology},
  doi={10.1109/TVT.2023.3266940}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-15655,ieee,Why Do Retail Customers Adopt Artificial Intelligence (AI) Based Autonomous Decision-Making Systems?,"Advancements in Artificial Intelligence (AI) have led to the development of autonomous decision-making processes, allowing customers to delegate decisions and tasks. Such technologies have the potential to alter the retailing landscape. Grounded in the unified theory of acceptance and use of technology and Hofstede's cultural theory, this article investigates customers’ adoption of AI-based autonomous decision-making processes by analyzing 454 customer responses using covariance-based structural equation modeling. The results reveal that effort expectancy, performance expectancy, facilitating conditions, and social influence are positively associated with customers’ adoption of autonomous decision-making processes. Collectivism strengthened the positive association of social influence with customer attitude, whereas uncertainty avoidance dampened the associations of performance expectancy, effort expectancy, and social influence with attitude. The findings provide useful implications for system developers and managers while providing future researchers with directions to further explore autonomous decision-making processes.",10.1109/TEM.2022.3157976,https://ieeexplore.ieee.org/document/9759231,IEEE Transactions on Engineering Management,Shavneet Sharma;Nazrul Islam;Gurmeet Singh;Amandeep Dhir,2024,0,"@article{2-15655,
  title={Why Do Retail Customers Adopt Artificial Intelligence (AI) Based Autonomous Decision-Making Systems?},
  author={Sharma, Shavneet and Islam, Nazrul and Singh, Gurmeet and Dhir, Amandeep},
  year={2024},
  journal={IEEE Transactions on Engineering Management},
  doi={10.1109/TEM.2022.3157976}
}",Empirical contributions,Finance / Business / Economy,Individual,Executing,Developer,"Change cognitive demands, Restrict human agency, Shift responsibility, Change affective-perceptual",no such info,"delegation, recommendations",NA,"Textual, Autonomous System",Yes,Yes
2-15657,ieee,X-CaD: Explainable AI for Skin Cancer Diagnosis in Healthcare 4.0 Telesurgery,"The advent of healthcare 4.0 has catalyzed a paradigm shift in medical practices, ushering in innovative approaches such as telesurgery, a groundbreaking method for remote patient surgery and monitoring. This transformative technique extends beyond traditional surgeries, finding application in dermatological procedures. The success of telesurgery in skin-related surgeries hinges on accurate and efficient skin cancer detection using dermoscopic images. Recognizing the inherent complexities in interpreting deep learning models, especially in the context of healthcare, Explainable Artificial Intelligence (X-AI) becomes imperative. In this context, we propose a novel CNN-powered X-AI mechanism i.e., X-CaD, tailored for skin cancer detection in telesurgery environments, leveraging ResNet and MobileNet for feature extraction. To enhance interpretability and bridge the gap between model predictions and clinical decision-making, we employ X-AI techniques such as Local Interpretable Model-agnostic Explanations (LIME) and Integrated Gradient (IG). LIME provides granular insights into model predictions, elucidating decision-making processes, while IG offers a comprehensive view of feature attributions. X-CaD relies on the synergistic integration of advanced CNN architectures, i.e. ResNet and MobileNet along with X-AI techniques to identify skin cancer accurately and provide clinicians with clear insights. The effective impact of X-CaD is evaluated through the observed loss and accuracy values for the DL models, and heat map outputs for X-AI. This represents a significant advancement in the integration of state-of-the-art technology and healthcare, offering a dependable telesurgery solution for the diagnosis of skin cancer in surgical procedures.",10.1109/ICC51166.2024.10622832,https://ieeexplore.ieee.org/document/10622832,IEEE International Conference on Communications,Fenil Ramoliya;Keyaba Gohil;Aditya Gohil;Rajesh Gupta;Riya Kakkar;Sudeep Tanwar;Joel J. P. C. Rodrigues,2024,4,"@inproceedings{2-15657,
  title={X-CaD: Explainable AI for Skin Cancer Diagnosis in Healthcare 4.0 Telesurgery},
  author={Ramoliya, Fenil and Gohil, Keyaba and Gohil, Aditya and Gupta, Rajesh and Kakkar, Riya and Tanwar, Sudeep and Rodrigues, Joel J. P. C.},
  year={2024},
  doi={10.1109/ICC51166.2024.10622832},
  booktitle={IEEE International Conference on Communications}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Forecasting","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-15658,ieee,“Why did my AI agent lose?”: Visual Analytics for Scaling Up After-Action Review,"How can we help domain-knowledgeable users who do not have expertise in AI analyze why an AI agent failed? Our research team previously developed a new structured process for such users to assess AI, called After-Action Review for AI (AAR/AI), consisting of a series of steps a human takes to assess an AI agent and formalize their understanding. In this paper, we investigate how the AAR/AI process can scale up to support reinforcement learning (RL) agents that operate in complex environments. We augment the AAR/AI process to be performed at three levels—episode-level, decision-level, and explanation-level—and integrate it into our redesigned visual analytics interface. We illustrate our approach through a usage scenario of analyzing why a RL agent lost in a complex real-time strategy game built with the StarCraft 2 engine. We believe integrating structured processes like AAR/AI into visualization tools can help visualization play a more critical role in AI interpretability.",10.1109/VIS49827.2021.9623268,https://ieeexplore.ieee.org/document/9623268,IEEE Visualization Conference,Delyar Tabatabai;Anita Ruangrotsakun;Jed Irvine;Jonathan Dodge;Zeyad Shureih;Kin-Ho Lam;Margaret Burnett;Alan Fern;Minsuk Kahng,2021,0,"@inproceedings{2-15658,
  title = {“Why did my AI agent lose?”: Visual Analytics for Scaling Up After-Action Review},
  author = {Delyar Tabatabai and Anita Ruangrotsakun and Jed Irvine and Jonathan Dodge and Zeyad Shureih and Kin-Ho Lam and Margaret Burnett and Alan Fern and Minsuk Kahng},
  year = {2021},
  doi = {10.1109/VIS49827.2021.9623268},
  booktitle = {IEEE Visualization Conference}
}",System/Artifact contributions,Generic / Abstract / Domain-agnostic,no such info,"Executing, Explaining, Analyzing",Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-15711,ieee,A Decision-Making Approach for Complex Unsignalized Intersection by Deep Reinforcement Learning,"Decision-making for automatic vehicles at unsignalized intersections with dense traffic is one of the most challenging tasks. Due to the complex structure and frequent traffic accidents, traditional rule-based methods struggle to address this issue flexibly and often produce suboptimal strategies. Recently, deep reinforcement learning (DRL) has garnered significant attention for its exceptional performance in decision-making problems. We propose a local attention safety deep reinforcement learning (LA-SRL) decision-making method for ego vehicle right-turns at unsignalized intersections. LA-SRL enables paying attention to different states of social vehicles within complex traffic environments and effectively deals with the impact of surrounding vehicles on ego vehicle. This contributes to enhancement of safe driving efficiency. To further balance the safety and efficiency of decision-making for ego vehicle at unsignalized intersections with dense traffic flow, we design a safety-reward function composed of risk reward and avail reward. The safety-reward function enables ego vehicle to promptly navigate out of high-risk areas, meanwhile avoiding collisions and reducing waiting periods. Finally, we evaluate our method in the CARLA simulator. The results demonstrate that LA-SRL outperforms state-of-the-art methods, achieving a remarkable success rate of 98.25% and reducing the average time to 6.6 seconds.",10.1109/TVT.2024.3408917,https://ieeexplore.ieee.org/document/10559897,IEEE Transactions on Vehicular Technology,Shanke Li;Kun Peng;Fei Hui;Ziqi Li;Cheng Wei;Wenbo Wang,2024,16,"@article{2-15711,
  title={A Decision-Making Approach for Complex Unsignalized Intersection by Deep Reinforcement Learning},
  author={Li, Shanke and Peng, Kun and Hui, Fei and Li, Ziqi and Wei, Cheng and Wang, Wenbo},
  year={2024},
  doi={10.1109/TVT.2024.3408917},
  journal={IEEE Transactions on Vehicular Technology}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-15748,ieee,A Deep-Reinforcement-Learning-Based Recommender System for Occupant-Driven Energy Optimization in Commercial Buildings,"In this article, we present recEnergy, a recommender system for reducing energy consumption in commercial buildings with human-in-the-loop. We formulate the building energy optimization problem as a Markov decision process, show how deep reinforcement learning can be used to learn energy-saving recommendations, and effectively engage occupants in energy-saving actions. recEnergy is a recommender system that learns actions with high-energy-saving potential, actively distributes recommendations to occupants in a commercial building, and utilizes feedback from the occupants to learn better energy-saving recommendations. Over a four-week user study, four different types of energy-saving recommendations were trained and learned. recEnergy improves building energy reduction from a baseline saving (passive-only strategy) of 19%-26%.",10.1109/JIOT.2020.2974848,https://ieeexplore.ieee.org/document/9001078,IEEE Internet of Things Journal,Peter Wei;Stephen Xia;Runfeng Chen;Jingyi Qian;Chong Li;Xiaofan Jiang,2020,3,"@article{2-15748,
  title        = {A Deep-Reinforcement-Learning-Based Recommender System for Occupant-Driven Energy Optimization in Commercial Buildings},
  author       = {Peter Wei and Stephen Xia and Runfeng Chen and Jingyi Qian and Chong Li and Xiaofan Jiang},
  year         = {2020},
  doi          = {10.1109/JIOT.2020.2974848},
  journal      = {IEEE Internet of Things Journal}
}",System/Artifact contributions,Environment / Resources / Energy,Individual,"Advising, Collaborating","Decision-maker, Decision-subject",Alter decision outcomes,"Change AI responses, Update AI competence",NA,NA,NA,Yes,Yes
2-15758,ieee,"A Distributed Model-Free Ride-Sharing Approach for Joint Matching, Pricing, and Dispatching Using Deep Reinforcement Learning","Significant development of ride-sharing services presents a plethora of opportunities to transform urban mobility by providing personalized and convenient transportation while ensuring the efficiency of large-scale ride pooling. However, a core problem for such services is route planning for each driver to fulfill the dynamically arriving requests while satisfying given constraints. Current models are mostly limited to static routes with only two rides per vehicle (optimally) or three (with heuristics) (Alonso-Mora \net al.\n, 2017), at least in the initial allocation while not ascertaining that opposite-direction rides are not grouped together. In this paper, we present a dynamic, demand aware, and pricing-based vehicle-passenger matching and route planning framework that (1) dynamically generates optimal routes for each vehicle based on online demand, pricing associated with each ride, vehicle capacities and locations. This matching algorithm starts greedily and optimizes over time using an insertion operation, (2) involves drivers in the decision-making process by allowing them to propose a different price based on the expected reward for a particular ride as well as the destination locations for future rides, which is influenced by supply-and-demand computed by the Deep Q-network. (3) allows customers to accept or reject rides based on their set of preferences with respect to pricing and delay windows, vehicle type and carpooling preferences. These (1-3) in tandem with each other enforce grouping rides with the most route-intersections together. (4) Based on demand prediction, our approach re-balances idle vehicles by dispatching them to the areas of anticipated high demand using deep Reinforcement Learning (RL). Our framework is validated using millions of trips extracted from the New York City Taxi public dataset; however, we consider different vehicle types and designed customer utility functions to validate the setup and study different settings. Experimental ...",10.1109/TITS.2021.3096537,https://ieeexplore.ieee.org/document/9507388,IEEE Transactions on Intelligent Transportation Systems,Marina Haliem;Ganapathy Mani;Vaneet Aggarwal;Bharat Bhargava,2021,113,"@article{2-15758,
  title={A Distributed Model-Free Ride-Sharing Approach for Joint Matching, Pricing, and Dispatching Using Deep Reinforcement Learning},
  author={Haliem, Marina and Mani, Ganapathy and Aggarwal, Vaneet and Bhargava, Bharat},
  year={2021},
  doi={10.1109/TITS.2021.3096537},
  journal={IEEE Transactions on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,"Advising, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-160,aaai,Improvement-Focused Causal Recourse( ICR),"Algorithmic recourse recommendations inform stakeholders of how to act to revert unfavorable decisions. However, existing methods may recommend actions that lead to acceptance( i. e. , revert the models decision) but do not lead to improvement( i. e. , may not revert the underlying real-world state). To recommend such actions is to recommend fooling the predictor. We introduce a novel method, Improvement-Focused Causal Recourse( ICR) , which involves a conceptual shift: Firstly, we require ICR recommendations to guide toward improvement. Secondly, we do not tailor the recommendations to be accepted by a specific predictor. Instead, we leverage causal knowledge to design decision systems that predict accurately preand post-recourse, such that improvement guarantees translate into acceptance guarantees. Curiously, optimal pre-recourse classifiers are robust to ICR actions and thus suitable post-recourse. In semi-synthetic experiments, we demonstrate that given correct causal knowledge ICR, in contrast to existing approaches, guides toward both acceptance and improvement.",10.1609/aaai.v37i10.26398,https://ojs.aaai.org/index.php/AAAI/article/view/26398,AAAI Conference on Artificial Intelligence,Gunnar König;Timo Freiesleben;Moritz Grosse-Wentrup,2023,0,"@inproceedings{2-160,
  title={Improvement-Focused Causal Recourse (ICR)},
  author={K{\""o}nig, Gunnar and Freiesleben, Timo and Grosse-Wentrup, Moritz},
  year={2023},
  doi={10.1609/aaai.v37i10.26398},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}
}","Methodological contributions, Algorithmic contributions","Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,"Advising, Analyzing","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-16029,ieee,Adaptive Alert Management for Balancing Optimal Performance among Distributed CSOCs using Reinforcement Learning,"Large organizations typically have Cybersecurity Operations Centers (CSOCs) distributed at multiple locations that are independently managed, and they have their own cybersecurity analyst workforce. Under normal operating conditions, the CSOC locations are ideally staffed such that the alerts generated from the sensors in a work-shift are thoroughly investigated by the scheduled analysts in a timely manner. Unfortunately, when adverse events such as increase in alert arrival rates or alert investigation rates occur, alerts have to wait for a longer duration for analyst investigation, which poses a direct risk to organizations. Hence, our research objective is to mitigate the impact of the adverse events by dynamically and autonomously re-allocating alerts to other location(s) such that the performances of all the CSOC locations remain balanced. This is achieved through the development of a novel centralized adaptive decision support system whose task is to re-allocate alerts from the affected locations to other locations. This re-allocation decision is non-trivial because the following must be determined: (1) timing of a re-allocation decision, (2) number of alerts to be reallocated, and (3) selection of the locations to which the alerts must be distributed. The centralized decision-maker (henceforth referred to as agent) continuously monitors and controls the level of operational effectiveness-LOE (a quantified performance metric) of all the locations. The agent's decision-making framework is based on the principles of stochastic dynamic programming and is solved using reinforcement learning (RL). In the experiments, the RL approach is compared with both rule-based and load balancing strategies. By simulating real-world scenarios, learning the best decisions for the agent, and applying the decisions on sample realizations of the CSOC's daily operation, the results show that the RL agent outperforms both approaches by generating (near-) optimal decisions that maintain a balanced LOE among the CSOC locations. Furthermore, the scalability experiments highlight the practicality of adapting the method to a large number of CSOC locations.",10.1109/TPDS.2019.2927977,https://ieeexplore.ieee.org/document/8762232,IEEE Transactions on Parallel and Distributed Systems,Ankit Shah;Rajesh Ganesan;Sushil Jajodia;Pierangela Samarati;Hasan Cam,2020,0,"@article{2-16029,
  title = {Adaptive Alert Management for Balancing Optimal Performance among Distributed CSOCs using Reinforcement Learning},
  author = {Ankit Shah and Rajesh Ganesan and Sushil Jajodia and Pierangela Samarati and Hasan Cam},
  year = {2020},
  doi = {10.1109/TPDS.2019.2927977},
  journal = {IEEE Transactions on Parallel and Distributed Systems}
}","System/Artifact contributions, Algorithmic contributions",Software / Systems / Security,Institutional,"Executing, Monitoring, Analyzing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-1608,acl,Explainability and Hate Speech: Structured Explanations Make Social Media Moderators Faster,"Content moderators play a key role in keeping the conversation on social media healthy. While the high volume of content they need to judge represents a bottleneck to the moderation pipeline, no studies have explored how models could support them to make faster decisions. There is, by now, a vast body of research into detecting hate speech, sometimes explicitly motivated by a desire to help improve content moderation, but published research using real content moderators is scarce. In this work we investigate the effect of explanations on the speed of real-world moderators. Our experiments show that while generic explanations do not affect their speed and are often ignored, structured explanations lower moderators' decision making time by 7.4%.",10.18653/v1/2024.acl-short.38,https://aclanthology.org/2024.acl-short.38,Annual Meeting of the Association for Computational Linguistics (Short Papers),"Calabrese, Agostina; Neves, Leonardo; Shah, Neil; Bos, Maarten; Ross, Björn; Lapata, Mirella; Barbieri, Francesco",2024,14,"@inproceedings{2-1608,
  title = {Explainability and Hate Speech: Structured Explanations Make Social Media Moderators Faster},
  author = {Calabrese, Agostina and Neves, Leonardo and Shah, Neil and Bos, Maarten and Ross, Björn and Lapata, Mirella and Barbieri, Francesco},
  year = {2024},
  doi = {10.18653/v1/2024.acl-short.38},
  booktitle = {Annual Meeting of the Association for Computational Linguistics (Short Papers)}
}",Empirical contributions,Media / Communication / Entertainment,Operational,"Advising, Explaining",Decision-maker,"Alter decision outcomes, Change affective-perceptual, Change trust",no such info,structured explanations,NA,Textual,Yes,Yes
2-1612,acl,Explaining Interactions Between Text Spans,"Reasoning over spans of tokens from different parts of the input is essential for natural language understanding (NLU) tasks such as fact-checking (FC), machine reading comprehension (MRC) or natural language inference (NLI). However, existing highlight-based explanations primarily focus on identifying individual important features or interactions only between adjacent tokens or tuples of tokens. Most notably, there is a lack of annotations capturing the human decision-making process with respect to the necessary interactions for informed decision-making in such tasks. To bridge this gap, we introduce SpanEx, a multi-annotator dataset of human span interaction explanations for two NLU tasks: NLI and FC. We then investigate the decision-making processes of multiple fine-tuned large language models in terms of the employed connections between spans in separate parts of the input and compare them to the human reasoning processes. Finally, we present a novel community detection based unsupervised method to extract such interaction explanations. We make the code and the dataset available on [Github](https://github.com/copenlu/spanex). The dataset is also available on [Huggingface datasets](https://huggingface.co/datasets/copenlu/spanex).",10.18653/v1/2023.emnlp-main.783,https://aclanthology.org/2023.emnlp-main.783,Conference on Empirical Methods in Natural Language Processing,"Ray Choudhury, Sagnik; Atanasova, Pepa; Augenstein, Isabelle",2023,13,"@inproceedings{2-1612,
  title={{Explaining Interactions Between Text Spans}},
  author={Ray Choudhury, Sagnik and Atanasova, Pepa and Augenstein, Isabelle},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  doi={10.18653/v1/2023.emnlp-main.783}
}",Dataset/Benchmark contributions,Generic / Abstract / Domain-agnostic,no such info,"Explaining, Analyzing","Developer, Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-16185,ieee,An Extensive-Form Game Paradigm for Visual Field Testing via Deep Reinforcement Learning,"Glaucoma is the leading cause of irreversible but preventable blindness worldwide, and visual field testing is an important tool for its diagnosis and monitoring. Testing using standard visual field thresholding procedures is time-consuming, and prolonged test duration leads to patient fatigue and decreased test reliability. Different visual field testing algorithms have been developed to shorten testing time while maintaining accuracy. However, the performance of these algorithms depends heavily on prior knowledge and manually crafted rules that determine the intensity of each light stimulus as well as the termination criteria, which is suboptimal. We leverage deep reinforcement learning to find improved decision strategies for visual field testing. In our proposed algorithms, multiple intelligent agents are employed to interact with the patient in an extensive-form game fashion, with each agent controlling the test on one of the testing locations in the patient's visual field. Through training, each agent learns an optimized policy that determines the intensities of light stimuli and the termination criteria, which minimizes the error in sensitivity estimation and test duration at the same time. In simulation experiments, we compare the performance of our algorithms against baseline visual field testing algorithms and show that our algorithms achieve a better trade-off between estimation accuracy and test duration. By retaining testing accuracy with reduced test duration, our algorithms improve test reliability, clinic efficiency, and patient satisfaction, and translationally affect clinical outcomes.",10.1109/TBME.2023.3308475,https://ieeexplore.ieee.org/document/10229188,IEEE Transactions on Biomedical Engineering,Rui Ma;Yudong Tao;Mohamed M. Khodeiry;Xiangxiang Liu;Ximena Mendoza;Yuan Liu;Mei-Ling Shyu;Richard K. Lee,2024,2,"@article{2-16185,
  title={An Extensive-Form Game Paradigm for Visual Field Testing via Deep Reinforcement Learning},
  author={Ma, Rui and Tao, Yudong and Khodeiry, Mohamed M. and Liu, Xiangxiang and Mendoza, Ximena and Liu, Yuan and Shyu, Mei-Ling and Lee, Richard K.},
  year={2024},
  journal={IEEE Transactions on Biomedical Engineering},
  doi={10.1109/TBME.2023.3308475}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,Executing,"Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-162,aaai,Improving Fairness and Privacy in Selection Problems,"Supervised learning models have been increasingly used for making decisions about individuals in applications such as hiring, lending, and college admission. These models may inherit pre-existing biases from training datasets and discriminate against protected attributes( e. g. , race or gender). In addition to unfairness, privacy concerns also arise when the use of models reveals sensitive personal information. Among various privacy notions, differential privacy has become popular in recent years. In this work, we study the possibility of using a differentially private exponential mechanism as a post-processing step to improve both fairness and privacy of supervised learning models. Unlike many existing works, we consider a scenario where a supervised model is used to select a limited number of applicants as the number of available positions is limited. This assumption is well-suited for various scenarios, such as job application and college admission. We use ``equal opportunity as the fairness notion and show that the exponential mechanisms can make the decision-making process perfectly fair. Moreover, the experiments on real-world datasets show that the exponential mechanism can improve both privacy and fairness, with a slight decrease in accuracy compared to the model without post-processing.",10.1609/aaai.v35i9.16986,https://ojs.aaai.org/index.php/AAAI/article/view/16986,AAAI Conference on Artificial Intelligence,Mohammad Mahdi Khalili;Xueru Zhang;Mahed Abroshan;Somayeh Sojoudi,2021,38,"@inproceedings{2-162,
  title = {Improving Fairness and Privacy in Selection Problems},
  author = {Khalili, Mohammad Mahdi and Zhang, Xueru and Abroshan, Mahed and Sojoudi, Somayeh},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year = {2021},
  doi = {10.1609/aaai.v35i9.16986}
}",Methodological contributions,"Finance / Business / Economy, Everyday / Employment / Public Service, Generic / Abstract / Domain-agnostic",Operational,"Advising, Executing","Decision-subject, Guardian, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-16346,ieee,Behavior Planning at Urban Intersections through Hierarchical Reinforcement Learning,"For autonomous vehicles, effective behavior planning is crucial to ensure safety of the ego car. In many urban scenarios, it is hard to create sufficiently general heuristic rules, especially for challenging scenarios that some new human drivers find difficult. In this work, we propose a behavior planning structure based on reinforcement learning (RL) which is capable of performing autonomous vehicle behavior planning with a hierarchical structure in simulated urban environments. Application of the hierarchical structure [1] allows the various layers of the behavior planning system to be satisfied. Our algorithms can perform better than heuristic-rule-based methods for elective decisions such as when to turn left between vehicles approaching from the opposite direction or possible lane-change when approaching an intersection due to lane blockage or delay in front of the ego car. Such behavior is hard to evaluate as correct or incorrect, but some aggressive expert human drivers handle such scenarios effectively and quickly. On the other hand, compared to traditional RL methods, our algorithm is more sample-efficient, due to the use of a hybrid reward mechanism and heuristic exploration during the training process. The results also show that the proposed method converges to an optimal policy faster than traditional RL methods.",10.1109/ICRA48506.2021.9561095,https://ieeexplore.ieee.org/document/9561095,IEEE International Conference on Robotics and Automation,Zhiqian Qiao;Jeff Schneider;John M. Dolan,2021,39,"@inproceedings{2-16346,
  title={Behavior Planning at Urban Intersections through Hierarchical Reinforcement Learning},
  author={Qiao, Zhiqian and Schneider, Jeff and Dolan, John M.},
  year={2021},
  doi={10.1109/ICRA48506.2021.9561095},
  booktitle={IEEE International Conference on Robotics and Automation}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Analyzing",Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-16397,ieee,CAC: Enabling Customer-Centered Passenger-Seeking for Self-Driving Ride Service with Conservative Actor-Critic,"Rapid advances in perception, planning, and decision-making areas for self-driving vehicles have led to great improvements in their function and capabilities and enabled several prototypes to be driving on the roads and streets, such as Waymo Driver, TuSimple, Nuro, etc. Among various applications of self-driving vehicles, a promising one is the ride service as it has the potential to improve service quality and productivity and to provide service to anyone at any time. Extensive studies have been conducted on self-driving planning and safety, but few works focus on self-driving ride service decision-making and routing. In this work, we take the lead to study self-driving ride service planning and decision-making problem leveraging human-generated spatial-temporal data, and propose the data-driven Conservative Actor-Critic approach – CAC – based on offline reinforcement learning. Our CAC is able to make conservative decisions in a complicated environment with multiple goal states, and avoid dangerous and overly optimistic behaviors by exploiting human decisions. Extensive experiments with real-world data demonstrate that our CAC-learned policies are able to improve taxi service operation efficiency and quality drastically in terms of shortening passenger waiting time and improving service revenue.",10.1109/ICDM58522.2023.00011,https://ieeexplore.ieee.org/document/10415849,IEEE International Conference on Data Mining,Palawat Busaranuvong;Xin Zhang;Yanhua Li;Xun Zhou;Jun Luo,2023,0,"@inproceedings{2-16397,
  title     = {CAC: Enabling Customer-Centered Passenger-Seeking for Self-Driving Ride Service with Conservative Actor-Critic},
  author    = {Palawat Busaranuvong and Xin Zhang and Yanhua Li and Xun Zhou and Jun Luo},
  year      = {2023},
  doi       = {10.1109/ICDM58522.2023.00011},
  booktitle = {IEEE International Conference on Data Mining}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,"Executing, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-16414,ieee,Can LLMs Generate Architectural Design Decisions? - An Exploratory Empirical Study,"Architectural Knowledge Management (AKM) involves the organized handling of information related to architectural decisions and design within a project or organization. An essential artefact of AKM is the Architecture Decision Records (ADR), which documents key design decisions. ADRs are documents that capture decision context, decision made and various aspects related to a design decision, thereby promoting transparency, collaboration, and understanding. Despite their benefits, ADR adoption in software development has been slow due to challenges like time constraints and inconsistent uptake. Recent advancements in Large Language Models (LLMs) may help bridge this adoption gap by facilitating ADR generation. However, the effectiveness of LLM for ADR generation or understanding is something that has not been explored. To this end, in this work, we perform an exploratory study which aims to investigate the feasibility of using LLM for the generation of ADRs given the decision context. In our exploratory study, we utilize GPT and T5-based models with 0-shot, few-shot, and fine-tuning approaches to generate the Decision of an ADR given its Context. Our results indicate that in a 0-shot setting, state-of-the-art models such as GPT-4 generate relevant and accurate Design Decisions, although they fall short of human-level performance. Additionally, we observe that more cost-effective models like GPT-3.5 can achieve similar outcomes in a few-shot setting, and smaller models such as Flan-T5 can yield comparable results after fine-tuning. To conclude, this exploratory study suggests that LLM can generate Design Decisions, but further research is required to attain human-level generation and establish standardized widespread adoption.",10.1109/ICSA59870.2024.00016,https://ieeexplore.ieee.org/document/10592785,IEEE International Conference on Software Architecture,Rudra Dhar;Karthik Vaidhyanathan;Vasudeva Varma,2024,50,"@inproceedings{2-16414,
  title={Can LLMs Generate Architectural Design Decisions? - An Exploratory Empirical Study},
  author={Dhar, Rudra and Vaidhyanathan, Karthik and Varma, Vasudeva},
  year={2024},
  doi={10.1109/ICSA59870.2024.00016},
  booktitle={IEEE International Conference on Software Architecture}
}",Empirical contributions,Design / Creativity / Architecture,Operational,"Explaining, Advising, Forecasting","Developer, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-1650,acl,Generative Models for Automatic Medical Decision Rule Extraction from Text,"Medical decision rules play a key role in many clinical decision support systems (CDSS). However, these rules are conventionally constructed by medical experts, which is expensive and hard to scale up. In this study, we explore the automatic extraction of medical decision rules from text, leading to a solution to construct large-scale medical decision rules. We adopt a formulation of medical decision rules as binary trees consisting of condition/decision nodes. Such trees are referred to as medical decision trees and we introduce several generative models to extract them from text. The proposed models inherit the merit of two categories of successful natural language generation frameworks, i.e., sequence-to-sequence generation and autoregressive generation. To unleash the potential of pretrained language models, we design three styles of linearization (natural language, augmented natural language and JSON code), acting as the target sequence for our models. Our final system achieves 67% tree accuracy on a comprehensive Chinese benchmark, outperforming state-of-the-art baseline by 12%. The result demonstrates the effectiveness of generative models on explicitly modeling structural decision-making roadmaps, and shows great potential to boost the development of CDSS and explainable AI. Our code will be open-source upon acceptance.",10.18653/v1/2024.emnlp-main.399,https://aclanthology.org/2024.emnlp-main.399,Empirical Methods in Natural Language Processing,"He, Yuxin; Tang, Buzhou; Wang, Xiaoling",2024,86,"@inproceedings{2-1650,
  title={Generative Models for Automatic Medical Decision Rule Extraction from Text},
  author={He, Yuxin and Tang, Buzhou and Wang, Xiaoling},
  year={2024},
  doi={10.18653/v1/2024.emnlp-main.399},
  booktitle={Empirical Methods in Natural Language Processing}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Explaining","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-16633,ieee,Crowd Decision Making: Sparse Representation Guided by Sentiment Analysis for Leveraging the Wisdom of the Crowd,"The “wisdom of the crowd” theory states that a nonexpert crowd makes smarter decisions than a reduced set of experts. Social network platforms are a source of evaluations in the natural language of any topic, which may be considered as the evaluations of a nonexpert crowd. Decision-making (DM) models are constrained by their inability of processing large amounts of evaluations in natural language, as those ones from social networks. We claim that evaluations from social networks can enhance the quality of multiperson multicriteria DM models. Accordingly, we propose a crowd DM model guided by sentiment analysis (SA), which solves decision situations leveraging the wisdom of the crowd available in social networks. The model uses several deep-learning SA classification models through opinion triplets to incorporate all the evaluation shades in the DM model. Likewise, the likely lack of information stemmed from the consideration of a large set of users is tackled with a sparse representation of the evaluations. We annotate and release the TripR-2020Large dataset, and we use it to evaluate the model in the use case of restaurant choice. The results show that the integration of the wisdom of the crowd and the different shades of the evaluations enhances the quality of the decision.",10.1109/TSMC.2022.3180938,https://ieeexplore.ieee.org/document/9800192,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",Cristina Zuheros;Eugenio Martínez-Cámara;Enrique Herrera-Viedma;Francisco Herrera,2023,0,"@article{2-16633,
  title={Crowd Decision Making: Sparse Representation Guided by Sentiment Analysis for Leveraging the Wisdom of the Crowd},
  author={Zuheros, Cristina and Mart{\'\i}nez-C{\'a}mara, Eugenio and Herrera-Viedma, Enrique and Herrera, Francisco},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  year={2023},
  doi={10.1109/TSMC.2022.3180938}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Operational,"Executing, Analyzing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-1668,acl,Hierarchical Attention Network for Explainable Depression Detection on Twitter Aided by Metaphor Concept Mappings,"Automatic depression detection on Twitter can help individuals privately and conveniently understand their mental health status in the early stages before seeing mental health professionals. Most existing black-box-like deep learning methods for depression detection largely focused on improving classification performance. However, explaining model decisions is imperative in health research because decision-making can often be high-stakes and life-and-death. Reliable automatic diagnosis of mental health problems including depression should be supported by credible explanations justifying models' predictions. In this work, we propose a novel explainable model for depression detection on Twitter. It comprises a novel encoder combining hierarchical attention mechanisms and feed-forward neural networks. To support psycholinguistic studies, our model leverages metaphorical concept mappings as input. Thus, it not only detects depressed individuals, but also identifies features of such users' tweets and associated metaphor concept mappings.",NA,https://aclanthology.org/2022.coling-1.9,International Conference on Computational Linguistics,"Han, Sooji; Mao, Rui; Cambria, Erik",2022,23,"@inproceedings{2-1668,
  title={Hierarchical Attention Network for Explainable Depression Detection on Twitter Aided by Metaphor Concept Mappings},
  author={Han, Sooji and Mao, Rui and Cambria, Erik},
  year={2022},
  booktitle={International Conference on Computational Linguistics}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-16714,ieee,Decoding Decision Reasoning: A Counterfactual-Powered Model for Knowledge Discovery,"In medical imaging, particularly in early disease detection and prognosis tasks, discerning the rationale behind an AI model’s predictions is crucial for evaluating the reliability of its decisions. Conventional explanation methods face challenges in identifying discernible decisive features in medical image classifications, where discriminative features are subtle or not immediately apparent. To bridge this gap, we propose an explainable model that is equipped with both decision reasoning and feature identification capabilities. Our approach not only detects influential image patterns but also uncovers the decisive features that drive the model’s final predictions. By implementing our method, we can efficiently identify and visualise class-specific features leveraged by the data-driven model, providing insights into the decision-making processes of deep learning models. We validated our model in the demanding realm of medical prognosis task, demonstrating its efficacy and potential in enhancing the reliability of AI in healthcare and in discovering new knowledge in diseases where prognostic understanding is limited.",10.1109/ISBI56570.2024.10635275,https://ieeexplore.ieee.org/document/10635275,IEEE International Symposium on Biomedical Imaging,Yingying Fang;Zihao Jin;Xiaodan Xing;Simon Walsh;Guang Yang,2024,1,"@inproceedings{2-16714,
  title={Decoding Decision Reasoning: A Counterfactual-Powered Model for Knowledge Discovery},
  author={Fang, Yingying and Jin, Zihao and Xing, Xiaodan and Walsh, Simon and Yang, Guang},
  year={2024},
  doi={10.1109/ISBI56570.2024.10635275},
  booktitle={IEEE International Symposium on Biomedical Imaging}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Auditing, Explaining, Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-168,aaai,Indecision Modeling,"AI systems are often used to make or contribute to important decisions in a growing range of applications, including criminal justice, hiring, and medicine. Since these decisions impact human lives, it is important that the AI systems act in ways which align with human values. Techniques for preference modeling and social choice help researchers learn and aggregate peoples preferences, which are used to guide AI behavior; thus, it is imperative that these learned preferences are accurate. These techniques often assume that people are willing to express strict preferences over alternatives; which is not true in practice. People are often indecisive, and especially so when their decision has moral implications. The philosophy and psychology literature shows that indecision is a measurable and nuanced behavior---and that there are several different reasons people are indecisive. This complicates the task of both learning and aggregating preferences, since most of the relevant literature makes restrictive assumptions on the meaning of indecision. We begin to close this gap by formalizing several mathematical indecision models based on theories from philosophy, psychology, and economics; these models can be used to describe( indecisive) agent decisions, both when they are allowed to express indecision and when they are not. We test these models using data collected from an online survey where participants choose how to( hypothetically) allocate organs to patients waiting for a transplant.",10.1609/aaai.v35i7.16746,https://ojs.aaai.org/index.php/AAAI/article/view/16746,AAAI Conference on Artificial Intelligence,Duncan C. McElfresh;Lok Chan;Kenzie Doyle;Walter Sinnott-Armstrong;Vincent Conitzer;Jana Schaich Borg;John P. Dickerson,2021,14,"@inproceedings{2-168,
  title = {Indecision Modeling},
  author = {McElfresh, Duncan C. and Chan, Lok and Doyle, Kenzie and Sinnott-Armstrong, Walter and Conitzer, Vincent and Schaich Borg, Jana and Dickerson, John P.},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year = {2021},
  doi = {10.1609/aaai.v35i7.16746}
}",Theoretical contributions,"Law / Policy / Governance, Healthcare / Medicine / Surgery",Operational,"Analyzing, Advising","Guardian, Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-1685,acl,Identifying Helpful Sentences in Product Reviews,"In recent years online shopping has gained momentum and became an important venue for customers wishing to save time and simplify their shopping process. A key advantage of shopping online is the ability to read what other customers are saying about products of interest. In this work, we aim to maintain this advantage in situations where extreme brevity is needed, for example, when shopping by voice. We suggest a novel task of extracting a single representative helpful sentence from a set of reviews for a given product. The selected sentence should meet two conditions: first, it should be helpful for a purchase decision and second, the opinion it expresses should be supported by multiple reviewers. This task is closely related to the task of Multi Document Summarization in the product reviews domain but differs in its objective and its level of conciseness. We collect a dataset in English of sentence helpfulness scores via crowd-sourcing and demonstrate its reliability despite the inherent subjectivity involved. Next, we describe a complete model that extracts representative helpful sentences with positive and negative sentiment towards the product and demonstrate that it outperforms several baselines.",10.18653/v1/2021.naacl-main.55,https://aclanthology.org/2021.naacl-main.55,NAACL-HLT,"Gamzu, Iftah; Gonen, Hila; Kutiel, Gilad; Levy, Ran; Agichtein, Eugene",2021,30,"@inproceedings{2-1685,
  title     = {Identifying Helpful Sentences in Product Reviews},
  author    = {Gamzu, Iftah and Gonen, Hila and Kutiel, Gilad and Levy, Ran and Agichtein, Eugene},
  year      = {2021},
  doi       = {10.18653/v1/2021.naacl-main.55},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  venue     = {NAACL-HLT}
}",Algorithmic contributions,Finance / Business / Economy,Individual,Analyzing,"Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-1686,acl,Identifying High Consideration E-Commerce Search Queries,"In e-commerce, high consideration search missions typically require careful and elaborate decision making, and involve a substantial research investment from customers. We consider the task of identifying High Consideration (HC) queries. Identifying such queries enables e-commerce sites to better serve user needs using targeted experiences such as curated QA widgets that help users reach purchase decisions. We explore the task by proposing an Engagement-based Query Ranking (EQR) approach, focusing on query ranking to indicate potential engagement levels with query-related shopping knowledge content during product search. Unlike previous studies on predicting trends, EQR prioritizes query-level features related to customer behavior, finance, and catalog information rather than popularity signals. We introduce an accurate and scalable method for EQR and present experimental results demonstrating its effectiveness. Offline experiments show strong ranking performance. Human evaluation shows a precision of 96% for HC queries identified by our model. The model was commercially deployed, and shown to outperform human-selected queries in terms of downstream customer impact, as measured through engagement.",10.18653/v1/2024.emnlp-industry.42,https://aclanthology.org/2024.emnlp-industry.42,Empirical Methods in Natural Language Processing (EMNLP),"Chen, Zhiyu; Choi, Jason Ingyu; Fetahu, Besnik; Malmasi, Shervin",2024,5,"@inproceedings{2-1686,
  title = {Identifying High Consideration E-Commerce Search Queries},
  author = {Chen, Zhiyu and Choi, Jason Ingyu and Fetahu, Besnik and Malmasi, Shervin},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-industry.42},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)}
}",Algorithmic contributions,Finance / Business / Economy,Individual,"Analyzing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-16953,ieee,Deep RL-based Abnormal Behavior Detection and Prevention in Network Video Surveillance,"An important source of information for ensuring public safety is control room video surveillance. A Decision-Support System (DSS) designed for the security task force is vital and necessary to take decisions rapidly using a sea of information. In case of mission-critical operation, Situational Awareness (SA) which consists of being aware of what is going on around you in real time plays a crucial role across a variety of industries and should be placed at the center of our system. In this paper, in order to satisfy the understanding and projection levels of SA in real time, we propose a method based on Reinforcement Learning (RL) using an Actor-Critic algorithm. This algorithm permits our DSS to first, keep SA knowledge up to date by performing online adaptive learning and second, develop a long-term vision and strategy by exploiting a non-myopic agent for real-time sequential decision-making. In contrast to other approaches, our results permit demonstrating the practical potential of our method in real-world scenarios by preventing abnormal situations in an ever-changing environment.",10.1109/GLOBECOM48099.2022.10001471,https://ieeexplore.ieee.org/document/10001471,IEEE Global Communications Conference,Abhishek Djeachandrane;Said Hoceini;Serge Delmas;Jean-Michel Duquerrois;Alain Dubois;Abdelhamid Mellouk,2022,2,"@inproceedings{2-16953,
  title={Deep RL-based Abnormal Behavior Detection and Prevention in Network Video Surveillance},
  author={Djeachandrane, Abhishek and Hoceini, Said and Delmas, Serge and Duquerrois, Jean-Michel and Dubois, Alain and Mellouk, Abdelhamid},
  booktitle={IEEE Global Communications Conference},
  year={2022},
  doi={10.1109/GLOBECOM48099.2022.10001471}
}",Algorithmic contributions,Defense / Military / Emergency,Operational,"Auditing, Executing, Advising, Monitoring","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-16979,ieee,DeepASPeer: Towards an Aspect-level Sentiment Controllable Framework for Decision Prediction from Academic Peer Reviews,"Peer review is the widely accepted mechanism to determine the quality of scientific work. Even though peer-reviewing has been an integral part of academia since the 1600s, it frequently receives criticism for the lack of transparency and consistency. Even for humans, predicting the peer review outcome is a challenging task as there are many dimensions and human factors involved. However, Artificial Intelligence (AI) techniques can assist the editor/chair anticipate the final decision based on the reviews from the human reviewers. Peer review texts reflect the reviewers’ opinions/ sentiments on various aspects (e.g., novelty, substance, soundness, etc.) of the paper concerning the research in the paper, which may be valuable to predict a manuscript’s acceptance or rejection. The exact types and number of aspects could vary from one to the other venue (i.e., the conferences or journals). Peer review texts, however, which often contain rich sentiment information about the reviewers and, therefore, their overall opinion of the paper’s research, can be useful in predicting a manuscript’s acceptance or rejection. Here in this work, we study how we could take advantage of aspects and their corresponding sentiment to build a generic controllable system to assist the editor/chair in determining the outcome based on the reviews of a paper to make better editorial decisions. Our proposed deep neural architecture considers three information channels, including reviews, review aspect category, and its sentiment, to predict the final decision. Experimental results show that our model can achieve up to 76.67% accuracy on the ASAP-Review dataset (Aspect-enhanced Peer Review) consisting of ICLR and NIPS reviews considering the sentiment of the reviews. Empirical results also show an improvement of around 3.3 points while aspect information is added to the sentiment information \n1\n.\n1\nWe make our code publicly available athttps://github.com/sandeep82945/-PEERREVIEW-DECISION-Public.gi...",NA,https://ieeexplore.ieee.org/document/9852959,ACM/IEEE Joint Conference on Digital Libraries,Sandeep Kumar;Hardik Arora;Tirthankar Ghosal;Asif Ekbal,2022,2,"@inproceedings{2-16979,
  title = {DeepASPeer: Towards an Aspect-level Sentiment Controllable Framework for Decision Prediction from Academic Peer Reviews},
  author = {Sandeep Kumar and Hardik Arora and Tirthankar Ghosal and Asif Ekbal},
  year = {2022},
  booktitle = {ACM/IEEE Joint Conference on Digital Libraries}
}","Algorithmic contributions, System/Artifact contributions",Education / Teaching / Research,Operational,"Advising, Forecasting","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-17012,ieee,Demand-Side Management Using Deep Learning for Smart Charging of Electric Vehicles,"The use of electric vehicles (EVs) load management is relevant to support electricity demand softening, making the grid more economic, efficient, and reliable. However, the absence of flexible strategies reflecting the self-interests of EV users may reduce their participation in this kind of initiative. In this paper, we are proposing an intelligent charging strategy using machine learning (ML) tools, to determine when to charge the EV during connection sessions. This is achieved by making real-time charging decisions based on various auxiliary data, including driving, environment, pricing, and demand time series, in order to minimize the overall vehicle energy cost. The first step of the approach is to calculate the optimal solution of historical connection sessions using dynamic programming. Then, from these optimal decisions and other historical data, we train ML models to learn how to make the right decisions in real time, without knowledge of future energy prices and car usage. We demonstrated that a properly trained deep neural network is able to reduce charging costs significantly, often close to the optimal charging costs computed in a retrospective fashion.",10.1109/TSG.2018.2808247,https://ieeexplore.ieee.org/document/8299470,IEEE Transactions on Smart Grid,Karol Lina López;Christian Gagné;Marc-André Gardner,2019,264,"@article{2-17012,
  title={Demand-Side Management Using Deep Learning for Smart Charging of Electric Vehicles},
  author={López, Karol Lina and Gagné, Christian and Gardner, Marc-André},
  year={2019},
  journal={IEEE Transactions on Smart Grid},
  doi={10.1109/TSG.2018.2808247}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,"Executing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-1706,acl,Interpretability for Language Learners Using Example-Based Grammatical Error Correction,"Grammatical Error Correction (GEC) should not focus only on high accuracy of corrections but also on interpretability for language learning. However, existing neural-based GEC models mainly aim at improving accuracy, and their interpretability has not been explored.A promising approach for improving interpretability is an example-based method, which uses similar retrieved examples to generate corrections. In addition, examples are beneficial in language learning, helping learners understand the basis of grammatically incorrect/correct texts and improve their confidence in writing. Therefore, we hypothesize that incorporating an example-based method into GEC can improve interpretability as well as support language learners. In this study, we introduce an Example-Based GEC (EB-GEC) that presents examples to language learners as a basis for a correction result. The examples consist of pairs of correct and incorrect sentences similar to a given input and its predicted correction. Experiments demonstrate that the examples presented by EB-GEC help language learners decide to accept or refuse suggestions from the GEC output. Furthermore, the experiments also show that retrieved examples improve the accuracy of corrections.",10.18653/v1/2022.acl-long.496,https://aclanthology.org/2022.acl-long.496,Annual Meeting of the Association for Computational Linguistics,"Kaneko, Masahiro; Takase, Sho; Niwa, Ayana; Okazaki, Naoaki",2022,41,"@inproceedings{2-1706,
  title={Interpretability for Language Learners Using Example-Based Grammatical Error Correction},
  author={Kaneko, Masahiro and Takase, Sho and Niwa, Ayana and Okazaki, Naoaki},
  year={2022},
  doi={10.18653/v1/2022.acl-long.496},
  booktitle={Annual Meeting of the Association for Computational Linguistics}
}",Empirical contributions,Education / Teaching / Research,Individual,"Advising, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-17087,ieee,Distributed Learning for Vehicle Routing Decision in Software Defined Internet of Vehicles,"With the increasing number of vehicles, the traffic congestion is becoming more and more serious. In order to alleviate such a problem, this article considers transmission and inference delay of cloud centralized computing in the software defined Internet of Vehicles (SDIoV), and builds a new SDIoV architecture based on edge intelligence, for supporting real-time vehicle routing decision through distributed multi-agent reinforcement learning model. Then, a software defined device collaboration optimization method is designed to improve the efficiency of distributed training. Combined with multi-agent reinforcement learning, a distributed-learning-based vehicle routing decision algorithm (DLRD) is proposed to adaptively adjust vehicle routing online. The performed simulations show that the DLRD can successfully realize real-time routing decision for vehicles and alleviate traffic congestion with the dynamic changes of the road environment.",10.1109/TITS.2020.3023958,https://ieeexplore.ieee.org/document/9207840,IEEE Transactions on Intelligent Transportation Systems,Kai Lin;Chensi Li;Yihui Li;Claudio Savaglio;Giancarlo Fortino,2021,1,"@article{2-17087,
  title={Distributed Learning for Vehicle Routing Decision in Software Defined Internet of Vehicles},
  author={Lin, Kai and Li, Chensi and Li, Yihui and Savaglio, Claudio and Fortino, Giancarlo},
  year={2021},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  doi={10.1109/TITS.2020.3023958}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,Executing,"Stakeholder, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-17146,ieee,DQN Dynamic Pricing and Revenue Driven Service Federation Strategy,"This paper proposes a dynamic pricing and revenue-driven service federation strategy based on a Deep Q-Network (DQN) to instantly and automatically decide federation across different service provider domains, each introduces dynamic service prices offering to its customers and towards other domains. A dynamic pricing model is considered in this work based on the analysis of real pricing data collected from public cloud provider, and upon this a dynamic arrival process as a result of the price changes is proposed for formulating the service federation problem as a Markov Decision Problem (MDP). In this work, several reinforcement learning algorithms are developed to solve the problem, and the presented results show that the DQN method reached 90% of the optimal revenue and outperformed existing state-of-the-art strategies, and it can learn the federation pricing dynamics to make optimum federation decisions according to price changes.",10.1109/TNSM.2021.3117589,https://ieeexplore.ieee.org/document/9559399,IEEE Transactions on Network and Service Management,Jorge Martín-Pérez;Kiril Antevski;Andres Garcia-Saavedra;Xi Li;Carlos J. Bernardos,2021,8,"@article{2-17146,
  title={DQN Dynamic Pricing and Revenue Driven Service Federation Strategy},
  author={Martín-Pérez, Jorge and Antevski, Kiril and Garcia-Saavedra, Andres and Li, Xi and Bernardos, Carlos J.},
  year={2021},
  journal={IEEE Transactions on Network and Service Management},
  doi={10.1109/TNSM.2021.3117589}
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Executing, Analyzing",Developer,NA,NA,NA,NA,NA,Yes,No
2-17151,ieee,Driving Behavior Modeling and Characteristic Learning for Human-like Decision-Making in Highway,"To make autonomous vehicles consider driver's personalized characteristics, this paper proposes an integrated model and learning combined (IMLC) algorithm to realize human-like driving. It includes the integrated driving behavior modeling to ensure basic safety and the characteristic learning to further imitate human driver's style. Firstly, an integrated behavior model is built according to driver's operation logics, including lane advantage assessment, target lane selection and acceleration determination. The lane advantage is assessed by five lane features, like safety, efficiency, cooperativity, etc. Then, parameters of the built model are learned from human's demonstrations. For the lane selection parameter, a novel lane feature extraction method is presented and the maximum entropy inverse reinforcement learning (IRL) is adopted to solve. For the acceleration parameter, since it's hard to extract human's acceleration features accurately, the particle filtering is used to estimate. Finally, the IMLC algorithm is validated in highD dataset compared to existing algorithms. The results show that the RMSE of position and velocity in 9s are within 4.2 m and 0.9 m/s, which has great advantage. Moreover, we test the human-like performance in driver simulator. The safety and efficiency in this process are fairly approximate.",10.1109/TIV.2022.3224912,https://ieeexplore.ieee.org/document/9964318,IEEE Transactions on Intelligent Vehicles,Can Xu;Wanzhong Zhao;Chunyan Wang;Taowen Cui;Chen Lv,2023,0,"@article{2-17151,
  title={Driving Behavior Modeling and Characteristic Learning for Human-like Decision-Making in Highway},
  author={Xu, Can and Zhao, Wanzhong and Wang, Chunyan and Cui, Taowen and Lv, Chen},
  year={2023},
  journal={IEEE Transactions on Intelligent Vehicles},
  doi={10.1109/TIV.2022.3224912}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-172,aaai,Individual Fairness in Kidney Exchange Programs,"Kidney transplant is the preferred method of treatment for patients suffering from kidney failure. However, not all patients can find a donor which matches their physiological characteristics. Kidney exchange programs( KEPs) seek to match such incompatible patient-donor pairs together, usually with the main objective of maximizing the total number of transplants. Since selecting one optimal solution translates to a decision on who receives a transplant, it has a major effect on the lives of patients. The current practice in selecting an optimal solution does not necessarily ensure fairness in the selection process. In this paper, the existence of multiple optimal plans for a KEP is explored as a mean to achieve individual fairness. We propose the use of randomized policies for selecting an optimal solution in which patients equal opportunity to receive a transplant is promoted. Our approach gives rise to the problem of enumerating all optimal solutions, which we tackle using a hybrid of constraint programming and linear programming. The advantages of our proposed method over the common practice of using the optimal solution obtained by a solver are stressed through computational experiments. Our methodology enables decision makers to fully control KEP outcomes, overcoming any potential bias or vulnerability intrinsic to a deterministic solver.",10.1609/aaai.v35i13.17369,https://ojs.aaai.org/index.php/AAAI/article/view/17369,AAAI Conference on Artificial Intelligence,Golnoosh Farnadi;William St-Arnaud;Behrouz Babaki;Margarida Carvalho,2021,0,"@inproceedings{2-172,
  title = {Individual Fairness in Kidney Exchange Programs},
  author = {Farnadi, Golnoosh and St-Arnaud, William and Babaki, Behrouz and Carvalho, Margarida},
  year = {2021},
  doi = {10.1609/aaai.v35i13.17369},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Executing, Advising","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-17215,ieee,Dynamic Input for Deep Reinforcement Learning in Autonomous Driving,"In many real-world decision making problems, reaching an optimal decision requires taking into account a variable number of objects around the agent. Autonomous driving is a domain in which this is especially relevant, since the number of cars surrounding the agent varies considerably over time and affects the optimal action to be taken. Classical methods that process object lists can deal with this requirement. However, to take advantage of recent high-performing methods based on deep reinforcement learning in modular pipelines, special architectures are necessary. For these, a number of options exist, but a thorough comparison of the different possibilities is missing. In this paper, we elaborate limitations of fully-connected neural networks and other established approaches like convolutional and recurrent neural networks in the context of reinforcement learning problems that have to deal with variable sized inputs. We employ the structure of Deep Sets in off-policy reinforcement learning for high-level decision making, highlight their capabilities to alleviate these limitations, and show that Deep Sets not only yield the best overall performance but also offer better generalization to unseen situations than the other approaches.",10.1109/IROS40897.2019.8968560,https://ieeexplore.ieee.org/document/8968560,IEEE/RSJ International Conference on Intelligent Robots and Systems,Maria Huegle;Gabriel Kalweit;Branka Mirchevska;Moritz Werling;Joschka Boedecker,2019,0,"@inproceedings{2-17215,
  title={Dynamic Input for Deep Reinforcement Learning in Autonomous Driving},
  author={Huegle, Maria and Kalweit, Gabriel and Mirchevska, Branka and Werling, Moritz and Boedecker, Joschka},
  year={2019},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi={10.1109/IROS40897.2019.8968560}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,"Executing, Analyzing",Developer,NA,NA,NA,NA,NA,Yes,No
2-17282,ieee,Effective Charging Planning Based on Deep Reinforcement Learning for Electric Vehicles,"Electric vehicles (EVs) are viewed as an attractive option to reduce carbon emission and fuel consumption, but the popularization of EVs has been hindered by the cruising range limitation and the inconvenient charging process. In public charging stations, EVs usually spend a lot of time on queuing especially during peak hours of charging. Therefore, building an effective charging planning system has become a crucial task to reduce the total charging time for EVs. In this paper, we first introduce EVs charging scheduling problem and prove the NP-hardness of the problem. Then, we formalize the scheduling problem of EV charging as a Markov Decision Process and propose deep reinforcement learning algorithms to address it. The objective of the proposed algorithms is to minimize the total charging time of EVs and maximal reduction in the origin-destination distance. Finally, we experiment on real-world data and compare with two baseline algorithms to demonstrate the effectiveness of our approach. It shows that the proposed algorithms can significantly reduce the charging time of EVs compared to EST and NNCR algorithms.",10.1109/TITS.2020.3002271,https://ieeexplore.ieee.org/document/9124679,IEEE Transactions on Intelligent Transportation Systems,Cong Zhang;Yuanan Liu;Fan Wu;Bihua Tang;Wenhao Fan,2021,0,"@article{2-17282,
  title={Effective Charging Planning Based on Deep Reinforcement Learning for Electric Vehicles},
  author={Zhang, Cong and Liu, Yuanan and Wu, Fan and Tang, Bihua and Fan, Wenhao},
  year={2021},
  doi={10.1109/TITS.2020.3002271},
  journal={IEEE Transactions on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,"Executing, Advising","Decision-subject, Developer, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-17323,ieee,Emotional Contagion-Aware Deep Reinforcement Learning for Antagonistic Crowd Simulation,"The antagonistic behavior in the crowd usually exacerbates the seriousness of the situation in sudden riots, where the antagonistic emotional contagion and behavioral decision making play very important roles. However, the complex mechanism of antagonistic emotion influencing decision making, especially in the environment of sudden confrontation, has not yet been explored very clearly. In this paper, we propose an Emotional contagion-aware Deep reinforcement learning model for Antagonistic Crowd Simulation (ACSED). First, we build a group emotional contagion module based on the improved Susceptible Infected Susceptible (SIS) infection disease model, and estimate the emotional state of the group at each time step during the simulation. Then, the tendency of crowd antagonistic action is estimated based on Deep Q Network (DQN), where the agent learns the action autonomously, and leverages the mean field theory to quickly calculate the influence of other surrounding individuals on the central one. Finally, the rationality of the predicted actions by DQN is further analyzed in combination with group emotion, and the final action of the agent is determined. The proposed method in this paper is verified through several experiments with different settings. We can conclude antagonistic emotions play a critical role in the decision making of the crowd through influencing the individual behavior in the riot scenario, where individual behaviors are primarily driven by emotions and goals, rather than common rules. The experiment results also prove that the antagonistic emotion has a vital impact on the group combat, and positive emotional states are more conducive to combat. Moreover, by comparing the simulation results with real scenes, the feasibility of our method is further confirmed, which can provide good reference to formulate battle plans and improve the win rate of righteous groups in a variety of situations.",10.1109/TAFFC.2022.3225037,https://ieeexplore.ieee.org/document/9964278,IEEE Transactions on Affective Computing,Pei Lv;Qingqing Yu;Boya Xu;Chaochao Li;Bing Zhou;Mingliang Xu,2023,9,"@article{2-17323,
  title={Emotional Contagion-Aware Deep Reinforcement Learning for Antagonistic Crowd Simulation},
  author={Lv, Pei and Yu, Qingqing and Xu, Boya and Li, Chaochao and Zhou, Bing and Xu, Mingliang},
  year={2023},
  doi={10.1109/TAFFC.2022.3225037},
  journal={IEEE Transactions on Affective Computing}
}",Algorithmic contributions,Defense / Military / Emergency,Operational,"Executing, Forecasting, Analyzing","Stakeholder, Developer",NA,NA,NA,NA,NA,Yes,No
2-1745,acl,Large Language Models Help Humans Verify Truthfulness – Except When They Are Convincingly Wrong,"Large Language Models (LLMs) are increasingly used for accessing information on the web. Their truthfulness and factuality are thus of great interest. To help users make the right decisions about the information they get, LLMs should not only provide information but also help users fact-check it. We conduct human experiments with 80 crowdworkers to compare language models with search engines (information retrieval systems) at facilitating fact-checking. We prompt LLMs to validate a given claim and provide corresponding explanations. Users reading LLM explanations are significantly more efficient than those using search engines while achieving similar accuracy. However, they over-rely on the LLMs when the explanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide contrastive information—explain both why the claim is true and false, and then we present both sides of the explanation to users. This contrastive explanation mitigates users' over-reliance on LLMs, but cannot significantly outperform search engines. Further, showing both search engine results and LLM explanations offers no complementary benefits compared to search engines alone. Taken together, our study highlights that natural language explanations by LLMs may not be a reliable replacement for reading the retrieved passages, especially in high-stakes settings where over-relying on wrong AI explanations could lead to critical consequences.",10.18653/v1/2024.naacl-long.81,https://aclanthology.org/2024.naacl-long.81,NAACL HLT,"Si, Chenglei; Goyal, Navita; Wu, Tongshuang; Zhao, Chen; Feng, Shi; Daumé Iii, Hal; Boyd-Graber, Jordan",2024,71,"@inproceedings{2-1745,
  title = {Large Language Models Help Humans Verify Truthfulness – Except When They Are Convincingly Wrong},
  author = {Si, Chenglei and Goyal, Navita and Wu, Tongshuang and Zhao, Chen and Feng, Shi and Daumé Iii, Hal and Boyd-Graber, Jordan},
  year = {2024},
  booktitle = {Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT)},
  doi = {10.18653/v1/2024.naacl-long.81}
}",Empirical contributions,"Media / Communication / Entertainment, Generic / Abstract / Domain-agnostic","Individual, Institutional","Analyzing, Explaining, Advising","Decision-maker, Decision-subject","Alter decision outcomes, Change trust, Change cognitive demands",no such info,"retrieval, contrastive explanations, non-contrastive explanations",NA,Textual,Yes,Yes
2-17494,ieee,Fast Decision Support for Air Traffic Management at Urban Air Mobility Vertiports Using Graph Learning,"Urban Air Mobility (UAM) promises a new dimension to decongested, safe, and fast travel in urban and suburban hubs. These UAM aircraft are conceived to operate from small airports called vertiports each comprising multiple take-offllanding and battery-recharging spots. Since they might be situated in dense urban areas and need to handle many aircraft landings and take-offs each hour, managing this schedule in real-time becomes challenging for a traditional air-traffic controller but instead calls for an automated solution. This paper provides a novel approach to this problem of Urban Air Mobility - Vertiport Schedule Management (UAM-VSM), which leverages graph reinforcement learning to generate decision-support policies. Here the designated physical spots within the vertiport's airspace and the vehicles being managed are represented as two separate graphs, with feature extraction performed through a graph convolutional network (GCN). Extracted features are passed onto perceptron layers to decide actions such as continue to hover or cruise, continue idling or take-off, or land on an allocated vertiport spot. Performance is measured based on delays, safety (no. of collisions) and battery consumption. Through realistic simulations in AirSim applied to scaled down multi-rotor vehicles, our results demonstrate the suitability of using graph reinforcement learning to solve the UAM-VSM problem and its superiority to basic reinforcement learning (with graph embed dings) or random choice baselines.",10.1109/IROS55552.2023.10341398,https://ieeexplore.ieee.org/document/10341398,IEEE/RSJ International Conference on Intelligent Robots and Systems,Prajit KrisshnaKumar;Jhoel Witter;Steve Paul;Hanvit Cho;Karthik Dantu;Souma Chowdhury,2023,1,"@inproceedings{2-17494,
  title={Fast Decision Support for Air Traffic Management at Urban Air Mobility Vertiports Using Graph Learning},
  author={KrisshnaKumar, Prajit and Witter, Jhoel and Paul, Steve and Cho, Hanvit and Dantu, Karthik and Chowdhury, Souma},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year={2023},
  doi={10.1109/IROS55552.2023.10341398}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,"Analyzing, Executing","Stakeholder, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-176,aaai,Learning Optimal and Fair Decision Trees for Non-Discriminative Decision-Making,"In recent years, automated data-driven decision-making systems have enjoyed a tremendous success in a variety of fields( e. g. , to make product recommendations, or to guide the production of entertainment). More recently, these algorithms are increasingly being used to assist socially sensitive decisionmaking( e. g. , to decide who to admit into a degree program or to prioritize individuals for public housing). Yet, these automated tools may result in discriminative decision-making in the sense that they may treat individuals unfairly or unequally based on membership to a category or a minority, resulting in disparate treatment or disparate impact and violating both moral and ethical standards. This may happen when the training dataset is itself biased( e. g. , if individuals belonging to a particular group have historically been discriminated upon). However, it may also happen when the training dataset is unbiased, if the errors made by the system affect individuals belonging to a category or minority differently( e. g. , if misclassification rates for Blacks are higher than for Whites). In this paper, we unify the definitions of unfairness across classification and regression. We propose a versatile mixed-integer optimization framework for learning optimal and fair decision trees and variants thereof to prevent disparate treatment and/or disparate impact as appropriate. This translates to a flexible schema for designing fair and interpretable policies suitable for socially sensitive decision-making. We conduct extensive computational studies that show that our framework improves the state-of-the-art in the field( which typically relies on heuristics) to yield non-discriminative decisions at lower cost to overall accuracy.",10.1609/aaai.v33i01.33011418,https://ojs.aaai.org/index.php/AAAI/article/view/3943,AAAI Conference on Artificial Intelligence,Sina Aghaei;Mohammad Javad Azizi;Phebe Vayanos,2019,23,"@inproceedings{2-176,
  title={Learning Optimal and Fair Decision Trees for Non-Discriminative Decision-Making},
  author={Aghaei, Sina and Azizi, Mohammad Javad and Vayanos, Phebe},
  year={2019},
  doi={10.1609/aaai.v33i01.33011418},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance",Institutional,"Executing, Explaining, Advising","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-17868,ieee,Interpretable Decision-Making for Autonomous Vehicles at Highway On-Ramps With Latent Space Reinforcement Learning,"This paper presents a latent space reinforcement learning method for interpretable decision-making of autonomous vehicles at highway on-ramps. This method is based on the latent model and the combination model of the hidden Markov model and Gaussian mixture regression (HMM-GMR). It is difficult for the traditional decision-making method to understand the environment because its input is high-dimensional and lacks an understanding of the task. By utilizing the HMM-GMR model, we can obtain the interpretable state providing semantic information and environment understanding. A framework is proposed to unify representation learning with the deep reinforcement learning (DRL) approach, in which the latent model is used to reduce the dimension of interpretable state by extracting underlying task-relevant information. Experimental results are presented and the results show the right balance between driving safety and efficiency in the challenging scenarios of highway on-ramps merging.",10.1109/TVT.2021.3098321,https://ieeexplore.ieee.org/document/9492817,IEEE Transactions on Vehicular Technology,Huanjie Wang;Hongbo Gao;Shihua Yuan;Hongfei Zhao;Kelong Wang;Xiulai Wang;Keqiang Li;Deyi Li,2021,0,"@article{2-17868,
  title={Interpretable Decision-Making for Autonomous Vehicles at Highway On-Ramps With Latent Space Reinforcement Learning},
  author={Wang, Huanjie and Gao, Hongbo and Yuan, Shihua and Zhao, Hongfei and Wang, Kelong and Wang, Xiulai and Li, Keqiang and Li, Deyi},
  year={2021},
  journal={IEEE Transactions on Vehicular Technology},
  doi={10.1109/TVT.2021.3098321}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Explaining",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-17947,ieee,"Joint Optimization of Pricing, Dispatching and Repositioning in Ride-Hailing With Multiple Models Interplayed Reinforcement Learning","Popular ride-hailing products, such as DiDi, Uber and Lyft, provide people with transportation convenience. Pricing, order dispatching and vehicle repositioning are three tasks with tight correlation and complex interactions in ride-hailing platforms, significantly impacting each other’s decisions and demand distribution or supply distribution. However, no past work considered combining the three tasks to improve platform efficiency. In this paper, we exploit to optimize pricing, dispatching and repositioning strategies simultaneously. Such a new multi-stage decision-making problem is quite challenging because it involves complex coordination and lacks a unified problem model. To address this problem, we propose a novel \nJ\noint optimization framework of \nP\nricing, \nD\nispatching and \nR\nepositioning (JPDR) integrating contextual bandit and multi-agent deep reinforcement learning. JPDR consists of two components, including a Soft Actor-Critic (SAC)-based centralized policy for dispatching and repositioning and a pricing strategy learned by a multi-armed contextual bandit algorithm based on the feedback from the former. The two components learn in a mutually guided way to achieve joint optimization because their updates are highly interdependent. Based on real-world data, we implement a realistic environment simulator. Extensive experiments conducted on it show our method outperforms state-of-the-art baselines in terms of both gross merchandise volume and success rate.",10.1109/TKDE.2024.3464563,https://ieeexplore.ieee.org/document/10684492,IEEE Transactions on Knowledge and Data Engineering,Zhongyun Zhang;Lei Yang;Jiajun Yao;Chao Ma;Jianguo Wang,2024,6,"@article{2-17947,
  title={Joint Optimization of Pricing, Dispatching and Repositioning in Ride-Hailing With Multiple Models Interplayed Reinforcement Learning},
  author={Zhang, Zhongyun and Yang, Lei and Yao, Jiajun and Ma, Chao and Wang, Jianguo},
  year={2024},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  doi={10.1109/TKDE.2024.3464563}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,Executing,"Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-17999,ieee,Lane Change Intention Awareness for Assisted and Automated Driving on Highways,"Today the automotive industry faces a robust trend toward assisted and automated driving. The technology to accomplish this ambition has evolved rapidly over the last few years, and yet there are still alot of algorithmical challenges left to make an automation of the driving task a safe and comfortable experience. One of the main remaining challenges is the comprehension of the current traffic situation and the anticipation of all traffic participants' future driving behavior, which is needed for the technical system to obtain situation awareness: an indispensable foundation for successful decision-making. In this paper, a prediction framework is presented that is able to infer a driver's maneuver intention. This is achieved via a hybrid Bayesian network whose hidden layers represent a driver's lane contentedness. A pre-training of the network's parameters with simulated data provides for human interpretable parameters even after running the expectation maximization algorithm based on data gathered on German highways. Moreover, the future driving path of any traffic participant is predicted by solving an optimal control problem, whereby the parameters of the optimal control formulation are found via inverse reinforcement learning.",10.1109/TIV.2019.2904386,https://ieeexplore.ieee.org/document/8671760,IEEE Transactions on Intelligent Vehicles,Tobias Rehder;Alexander Koenig;Michel Goehl;Lawrence Louis;Dieter Schramm,2019,0,"@article{2-17999,
  title={Lane Change Intention Awareness for Assisted and Automated Driving on Highways},
  author={Rehder, Tobias and Koenig, Alexander and Goehl, Michel and Louis, Lawrence and Schramm, Dieter},
  year={2019},
  doi={10.1109/TIV.2019.2904386},
  journal={IEEE Transactions on Intelligent Vehicles}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Analyzing, Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-1809,acl,MisinfoEval: Generative AI in the Era of “Alternative Facts”,"The spread of misinformation on social media platforms threatens democratic processes, contributes to massive economic losses, and endangers public health. Many efforts to address misinformation focus on a knowledge deficit model and propose interventions for improving users' critical thinking through access to facts. Such efforts are often hampered by challenges with scalability, and by platform users' personal biases. The emergence of generative AI presents promising opportunities for countering misinformation at scale across ideological barriers. In this paper, we introduce a framework (MisinfoEval) for generating and comprehensively evaluating large language model (LLM) based misinformation interventions. We present (1) an experiment with a simulated social media environment to measure effectiveness of misinformation interventions, and (2) a second experiment with personalized explanations tailored to the demographics and beliefs of users with the goal of countering misinformation by appealing to their pre-existing values. Our findings confirm that LLM-based interventions are highly effective at correcting user behavior (improving overall user accuracy at reliability labeling by up to 41.72%). Furthermore, we find that users favor more personalized interventions when making decisions about news reliability and users shown personalized interventions have significantly higher accuracy at identifying misinformation.",10.18653/v1/2024.emnlp-main.487,https://aclanthology.org/2024.emnlp-main.487,Empirical Methods in Natural Language Processing,"Gabriel, Saadia; Lyu, Liang; Siderius, James; Ghassemi, Marzyeh; Andreas, Jacob; Ozdaglar, Asuman E.",2024,10,"@inproceedings{2-1809,
  title = {MisinfoEval: Generative AI in the Era of {``Alternative Facts''}},
  author = {Gabriel, Saadia and Lyu, Liang and Siderius, James and Ghassemi, Marzyeh and Andreas, Jacob and Ozdaglar, Asuman E.},
  year = {2024},
  booktitle = {Empirical Methods in Natural Language Processing},
  doi = {10.18653/v1/2024.emnlp-main.487}
}","Methodological contributions, System/Artifact contributions",Media / Communication / Entertainment,Institutional,"Analyzing, Advising","Decision-maker, Decision-subject, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-181,aaai,LUCID: Exposing Algorithmic Bias through Inverse Design,"AI systems can create, propagate, support, and automate bias in decision-making processes. To mitigate biased decisions, we both need to understand the origin of the bias and define what it means for an algorithm to make fair decisions. Most group fairness notions assess a models equality of outcome by computing statistical metrics on the outputs. We argue that these output metrics encounter intrinsic obstacles and present a complementary approach that aligns with the increasing focus on equality of treatment. By Locating Unfairness through Canonical Inverse Design( LUCID) , we generate a canonical set that shows the desired inputs for a model given a preferred output. The canonical set reveals the models internal logic and exposes potential unethical biases by repeatedly interrogating the decision-making process. We evaluate LUCID on the UCI Adult and COMPAS data sets and find that some biases detected by a canonical set differ from those of output metrics. The results show that by shifting the focus towards equality of treatment and looking into the algorithms internal workings, the canonical sets are a valuable addition to the toolbox of algorithmic fairness evaluation.",10.1609/aaai.v37i12.26683,https://ojs.aaai.org/index.php/AAAI/article/view/26683,AAAI Conference on Artificial Intelligence,Carmen Mazijn;Carina Prunkl;Andres Algaba;Jan Danckaert;Vincent Ginis,2023,0,"@inproceedings{2-181,
  title={LUCID: Exposing Algorithmic Bias through Inverse Design},
  author={Mazijn, Carmen and Prunkl, Carina and Algaba, Andres and Danckaert, Jan and Ginis, Vincent},
  year={2023},
  doi={10.1609/aaai.v37i12.26683},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service, Law / Policy / Governance",Institutional,"Explaining, Executing, Monitoring, Auditing","Decision-maker, Stakeholder, Guardian",NA,NA,NA,NA,NA,Yes,No
2-18363,ieee,Multi-Agent Deep Reinforcement Learning for Traffic optimization through Multiple Road Intersections using Live Camera Feed,"Traffic signals provide one of the primary means to administer conflicting traffic flows. Existing signal control strategies, operating on hand-crafted rules, fail to efficiently, autonomously adapt to the changing traffic patterns. Each signal control system independently manages one intersection at a time and regulates navigation of vehicles through that intersection. Current systems cannot co-operate to optimize aggregate traffic flows through multiple road intersections. Consequently, they are susceptible to making myopic signal control decisions that might be effective locally, but not globally. Instead, we propose a system of multiple, coordinating traffic signal control systems. This paper presents the first application of multi-agent deep reinforcement learning (DRL) to achieve traffic optimization through multiple road intersections solely based on raw pixel input from CCTV cameras in real time. This set of traffic control agents is shown to significantly outperform independently operating (both DRL-trained and loop-induced) adaptive signal control systems, by increasing traffic throughput and reducing the average time a vehicle spends in an intersection. Additionally, this paper, introduces attention-based visualization to interpret and validate the proposed multi-agent signal control methodology.",10.1109/ITSC45102.2020.9294375,https://ieeexplore.ieee.org/document/9294375,IEEE International Conference on Intelligent Transportation Systems,Deepeka Garg;Maria Chli;George Vogiatzis,2020,0,"@inproceedings{2-18363,
  title={Multi-Agent Deep Reinforcement Learning for Traffic Optimization through Multiple Road Intersections using Live Camera Feed},
  author={Garg, Deepeka and Chli, Maria and Vogiatzis, George},
  year={2020},
  doi={10.1109/ITSC45102.2020.9294375},
  booktitle={IEEE International Conference on Intelligent Transportation Systems}
}",System/Artifact contributions,Transportation / Mobility / Planning,Operational,"Executing, Analyzing",Developer,NA,NA,NA,NA,NA,Yes,No
2-18535,ieee,Novel Decision-Making Strategy for Connected and Autonomous Vehicles in Highway On-Ramp Merging,"High-speed highway on-ramp merging is a significant challenge toward realizing fully automated driving (\nlevel 4\n). Connected Autonomous Vehicles (\nCAVs\n), that combine communication and autonomous driving technologies, may improve greatly the safety performances when performing highway on-ramp merging. However, even with the emergence of \nCAVs\n, some keys constraints should be considered to achieve a safe on-ramp merging. First, human-driven vehicles will still be present on the road, and it may take decades before all the commercialized vehicles will be fully autonomous and connected. Also, onboard vehicle sensors may provide inaccurate or incomplete data due to sensors limitations and blind spots, especially in such critical situations. To resolve these issues, the present work introduces a novel solution that uses an off-board Road-Side Unit (\nRSU\n) to realize fully automated highway on-ramp merging for connected and automated vehicles. Our proposed approach is based on an Artificial Neural Network (ANN) to predict drivers’ intentions. This prediction is used as an input state to a Deep Reinforcement Learning (\nDRL\n) agent that outputs the longitudinal acceleration for the merging vehicle. To achieve this, we first propose a data-driven model that can predict the behavior of the human-driven vehicles in the main highway lane, with \n99\n% accuracy. We use the output of this model as input state to train a Twin Delayed Deep Deterministic Policy Gradients (\nTD3\n) agent that learns “\nsafe\n” and “\ncooperative\n” driving policy to perform highway on-ramp merging. We show that our proposed decision-making strategy improves performance compared to the solutions proposed previously.",10.1109/TITS.2021.3114983,https://ieeexplore.ieee.org/document/9557770,IEEE Transactions on Intelligent Transportation Systems,Zine el abidine Kherroubi;Samir Aknine;Rebiha Bacha,2022,1,"@article{2-18535,
  title={Novel Decision-Making Strategy for Connected and Autonomous Vehicles in Highway On-Ramp Merging},
  author={Kherroubi, Zine el Abidine and Aknine, Samir and Bacha, Rebiha},
  year={2022},
  doi={10.1109/TITS.2021.3114983},
  journal={IEEE Transactions on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,"Forecasting, Executing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-186,aaai,OPRA: An Open-Source Online Preference Reporting and Aggregation System,"We introduce the Online Preference Reporting and Aggregation( OPRA) system, an open-source online system that aims at providing support for group decision-making. We illustrate OPRAs distinctive features: UI for reporting rankings with ties, comprehensive analytics of preferences, and group decision-making in combinatorial domains. We also discuss our work in an automatic mentor matching system. We hope that the open-source nature of OPRA will foster development of computerized group decision support systems.",10.1609/aaai.v35i18.17996,https://ojs.aaai.org/index.php/AAAI/article/view/17996,AAAI Conference on Artificial Intelligence,Yiwei Chen;Jingwen Qian;Junming Wang;Lirong Xia;Gavriel Zahavi,2021,4,"@inproceedings{2-186,
  title     = {OPRA: An Open-Source Online Preference Reporting and Aggregation System},
  author    = {Yiwei Chen and Jingwen Qian and Junming Wang and Lirong Xia and Gavriel Zahavi},
  year      = {2021},
  doi       = {10.1609/aaai.v35i18.17996},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence}
}",System/Artifact contributions,Generic / Abstract / Domain-agnostic,Operational,"Analyzing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-187,aaai,PIDS: An Intelligent Electric Power Management Platform,"Electricity information tracking systems are increasingly being adopted across China. Such systems can collect real-time power consumption data from users, and provide opportunities for artificial intelligence( AI) to help power companies and authorities make optimal demand-side management decisions. In this paper, we discuss power utilization improvement in Shandong Province, China with a deployed AI application - the Power Intelligent Decision Support( PIDS) platform. Based on improved short-term power consumption gap prediction, PIDS uses an optimal power adjustment plan which enables fine-grained Demand Response( DR) and Orderly Power Utilization( OPU) recommendations to ensure stable operation while minimizing power disruptions and improving fair treatment of participating companies. Deployed in August 2018, the platform is helping over 400 companies optimize their power consumption through DR while dynamically managing the OPU process for around 10, 000 companies. Compared to the previous system, power outage under PIDS through planned shutdown has been reduced from 16% to 0. 56%, resulting in significant gains in economic activities.",10.1609/aaai.v34i08.7027,https://ojs.aaai.org/index.php/AAAI/article/view/7027,AAAI Conference on Artificial Intelligence,Yongqing Zheng;Han Yu;Yuliang Shi;Kun Zhang;Shuai Zhen;Lizhen Cui;Cyril Leung;Chunyan Miao,2020,0,"@article{2-187,
        author = {Zheng, Yongqing and Yu, Han and Shi, Yuliang and Zhang, Kun and Zhen, Shuai and Cui, Lizhen and Leung, Cyril and Miao, Chunyan},
        journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
        month = {Apr.},
        number = {08},
        pages = {13220-13227},
        title = {PIDS: An Intelligent Electric Power Management Platform},
        volume = {34},
        doi = {10.1609/aaai.v34i08.7027},
        year = {2020}
}",System/Artifact contributions,Environment / Resources / Energy,Organizational,"Forecasting, Advising, Executing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-18731,ieee,Personality-Guided Cloud Pricing via Reinforcement Learning,"As an efficient commercial computing paradigm, cloud computing provides various computing and storage resources to users in a pay-as-you-go manner. However, existing cloud pricing models and mechanisms are deterministic to some degree, thus, may not work well in a real-world environment where user perceived values with respect to cloud services are dynamically changing and highly personalized. In this article, we develop a reinforcement learning (RL)-based dynamic cloud pricing scheme to optimize both cloud provider’s profit and costs of heterogeneous users with distinct personalities. Specifically, we first propose a novel personality-guided user perceived value prediction scheme to proactively capture the dynamics of the users’ perceived values with respect to cloud services. The prediction scheme models the relationship among user personality, service price, quality of service (QoS), user satisfaction and perceived value in the cloud service market. Second, on the basis of the prediction model, a RL-based cloud pricing mechanism is developed to learn sequential service pricing decision-making for profit and costs optimization. Particularly, the profit and costs optimization problem is modeled as a discrete-time Markov decision process (MDP) that is solved by using Q-learning. Finally, extensive simulation experiments have been conducted to verify our user perceived value prediction scheme and RL-based cloud service pricing mechanism. Simulation results show that our perceived value prediction scheme can achieve up to 87.50 percent prediction accuracy, and our RL-based pricing mechanism can obtain up to 19.39 percent more profit than the state-of-the-art scheme.",10.1109/TCC.2020.2992461,https://ieeexplore.ieee.org/document/9086147,IEEE Transactions on Cloud Computing,Peijin Cong;Junlong Zhou;Mingsong Chen;Tongquan Wei,2022,29,"@article{2-18731,
  title={Personality-Guided Cloud Pricing via Reinforcement Learning},
  author={Cong, Peijin and Zhou, Junlong and Chen, Mingsong and Wei, Tongquan},
  year={2022},
  doi={10.1109/TCC.2020.2992461},
  journal={IEEE Transactions on Cloud Computing}
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Executing, Forecasting, Analyzing","Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-188,aaai,Preventing Eviction-Caused Homelessness through ML-Informed Distribution of Rental Assistance,"Rental assistance programs provide individuals with financial assistance to prevent housing instabilities caused by evictions and avert homelessness. Since these programs operate under resource constraints, they must decide who to prioritize. Typically, funding is distributed by a reactive allocation process that does not systematically consider risk of future homelessness. We partnered with Anonymous County( PA) to explore a proactive and preventative allocation approach that prioritizes individuals facing eviction based on their risk of future homelessness. Our ML models, trained on state and county administrative data accurately identify at-risk individuals, outperforming simpler prioritization approaches by at least 20% while meeting our equity and fairness goals across race and gender. Furthermore, our approach would reach 28% of individuals who are overlooked by the current process and end up homeless. Beyond improvements to the rental assistance program in Anonymous County, this study can inform the development of evidence-based decision support tools in similar contexts, including lessons about data needs, model design, evaluation, and field validation.",10.1609/aaai.v38i20.30246,https://ojs.aaai.org/index.php/AAAI/article/view/30246,AAAI Conference on Artificial Intelligence,Catalina Vajiac;Arun Frey;Joachim Baumann;Abigail Smith;Kasun Amarasinghe;Alice Lai;Kit T. Rodolfa;Rayid Ghani,2024,8,"@inproceedings{2-188,
  title={Preventing Eviction-Caused Homelessness through ML-Informed Distribution of Rental Assistance},
  author={Catalina Vajiac and Arun Frey and Joachim Baumann and Abigail Smith and Kasun Amarasinghe and Alice Lai and Kit T. Rodolfa and Rayid Ghani},
  year={2024},
  doi={10.1609/aaai.v38i20.30246},
  booktitle={AAAI Conference on Artificial Intelligence}
}",Empirical contributions,Everyday / Employment / Public Service,Institutional,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-1882,acl,PoliToHFI at SemEval-2023 Task 6: Leveraging Entity-Aware and Hierarchical Transformers For Legal Entity Recognition and Court Judgment Prediction,"The use of Natural Language Processing techniques in the legal domain has become established for supporting attorneys and domain experts in content retrieval and decision-making. However, understanding the legal text poses relevant challenges in the recognition of domain-specific entities and the adaptation and explanation of predictive models. This paper addresses the Legal Entity Name Recognition (L-NER) and Court judgment Prediction (CPJ) and Explanation (CJPE) tasks. The L-NER solution explores the use of various transformer-based models, including an entity-aware method attending domain-specific entities. The CJPE proposed method relies on hierarchical BERT-based classifiers combined with local input attribution explainers. We propose a broad comparison of eXplainable AI methodologies along with a novel approach based on NER. For the L-NER task, the experimental results remark on the importance of domain-specific pre-training. For CJP our lightweight solution shows performance in line with existing approaches, and our NER-boosted explanations show promising CJPE results in terms of the conciseness of the prediction explanations.",10.18653/v1/2023.semeval-1.194,https://aclanthology.org/2023.semeval-1.194,International Workshop on Semantic Evaluation,"Benedetto, Irene; Koudounas, Alkis; Vaiani, Lorenzo; Pastor, Eliana; Baralis, Elena; Cagliero, Luca; Tarasconi, Francesco",2023,5,"@inproceedings{2-1882,
  title = {PoliToHFI at SemEval-2023 Task 6: Leveraging Entity-Aware and Hierarchical Transformers For Legal Entity Recognition and Court Judgment Prediction},
  author = {Benedetto, Irene and Koudounas, Alkis and Vaiani, Lorenzo and Pastor, Eliana and Baralis, Elena and Cagliero, Luca and Tarasconi, Francesco},
  year = {2023},
  booktitle = {International Workshop on Semantic Evaluation},
  doi = {10.18653/v1/2023.semeval-1.194}
}",Algorithmic contributions,Law / Policy / Governance,Operational,"Forecasting, Explaining","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-189,aaai,Principled Data-Driven Decision Support for Cyber-Forensic Investigations,"In the wake of a cybersecurity incident, it is crucial to promptly discover how the threat actors breached security in order to assess the impact of the incident and to develop and deploy countermeasures that can protect against further attacks. To this end, defenders can launch a cyber-forensic investigation, which discovers the techniques that the threat actors used in the incident. A fundamental challenge in such an investigation is prioritizing the investigation of particular techniques since the investigation of each technique requires time and effort, but forensic analysts cannot know which ones were actually used before investigating them. To ensure prompt discovery, it is imperative to provide decision support that can help forensic analysts with this prioritization. A recent study demonstrated that data-driven decision support, based on a dataset of prior incidents, can provide state-of-the-art prioritization. However, this data-driven approach, called DISCLOSE, is based on a heuristic that utilizes only a subset of the available information and does not approximate optimal decisions. To improve upon this heuristic, we introduce a principled approach for data-driven decision support for cyber-forensic investigations. We formulate the decision-support problem using a Markov decision process, whose states represent the states of a forensic investigation. To solve the decision problem, we propose a Monte Carlo tree search based method, which relies on a k-NN regression over prior incidents to estimate state-transition probabilities. We evaluate our proposed approach on multiple versions of the MITRE ATT&CK dataset, which is a knowledge base of adversarial techniques and tactics based on real-world cyber incidents, and demonstrate that our approach outperforms DISCLOSE in terms of techniques discovered per effort spent.",10.1609/aaai.v37i4.25628,https://ojs.aaai.org/index.php/AAAI/article/view/25628,AAAI Conference on Artificial Intelligence,Soodeh Atefi;Sakshyam Panda;Emmanouil Panaousis;Aron Laszka,2023,8,"@inproceedings{2-189,
  title={Principled Data-Driven Decision Support for Cyber-Forensic Investigations},
  author={Atefi, Soodeh and Panda, Sakshyam and Panaousis, Emmanouil and Laszka, Aron},
  year={2023},
  booktitle={AAAI Conference on Artificial Intelligence},
  doi={10.1609/aaai.v37i4.25628}
}",Algorithmic contributions,Software / Systems / Security,Institutional,"Analyzing, Advising, Monitoring",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-18911,ieee,Reasoning about Counterfactuals to Improve Human Inverse Reinforcement Learning,"To collaborate well with robots, we must be able to understand their decision making. Humans naturally infer other agents' beliefs and desires by reasoning about their observable behavior in a way that resembles inverse reinforcement learning (IRL). Thus, robots can convey their beliefs and desires by providing demonstrations that are informative for a human learner's IRL. An informative demonstration is one that differs strongly from the learner's expectations of what the robot will do given their current understanding of the robot's decision making. However, standard IRL does not model the learner's existing expectations, and thus cannot do this counterfactual reasoning. We propose to incorporate the learner's current understanding of the robot's decision making into our model of human IRL, so that a robot can select demonstrations that maximize the human's understanding. We also propose a novel measure for estimating the difficulty for a human to predict instances of a robot's behavior in unseen environments. A user study finds that our test difficulty measure correlates well with human performance and confidence. Interestingly, considering human beliefs and counterfactuals when selecting demonstrations decreases human performance on easy tests, but increases performance on difficult tests, providing insight on how to best utilize such models.",10.1109/IROS47612.2022.9982062,https://ieeexplore.ieee.org/document/9982062,IEEE/RSJ International Conference on Intelligent Robots and Systems,Michael S. Lee;Henny Admoni;Reid Simmons,2022,3,"@inproceedings{2-18911,
  title = {Reasoning about Counterfactuals to Improve Human Inverse Reinforcement Learning},
  author = {Michael S. Lee and Henny Admoni and Reid Simmons},
  year = {2022},
  doi = {10.1109/IROS47612.2022.9982062},
  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems}
}",Methodological contributions,Manufacturing / Industry / Automation,no such info,"Advising, Explaining, Analyzing, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-190,aaai,RADAR-X: An Interactive Interface Pairing Contrastive Explanations with Revised Plan Suggestions,"Automated Planning techniques can be leveraged to build effective decision support systems that assist the human-in-the-loop. Such systems must provide intuitive explanations when the suggestions made by these systems seem inexplicable to the human. In this regard, we consider scenarios where the user questions the systems suggestion by providing alternatives( referred to as foils). In response, we empower existing decision support technologies to engage in an interactive explanatory dialogue with the user and provide contrastive explanations based on user-specified foils to reach a consensus on proposed decisions. To provide contrastive explanations, we adapt existing techniques in Explainable AI Planning( XAIP). Furthermore, we use this dialog to elicit the users latent preferences and propose three modes of interaction that use these preferences to provide revised plan suggestions. Finally, we showcase a decision support system that provides all these capabilities.",10.1609/aaai.v35i18.18009,https://ojs.aaai.org/index.php/AAAI/article/view/18009,AAAI Conference on Artificial Intelligence,Valmeekam Karthik;Sarath Sreedharan;Sailik Sengupta;Subbarao Kambhampati,2021,12,"@inproceedings{2-190,
  title = {RADAR-X: An Interactive Interface Pairing Contrastive Explanations with Revised Plan Suggestions},
  author = {Valmeekam, Karthik and Sreedharan, Sarath and Sengupta, Sailik and Kambhampati, Subbarao},
  year = {2021},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v35i18.18009}
}",System/Artifact contributions,Generic / Abstract / Domain-agnostic,Individual,"Explaining, Advising",Decision-maker,Shape ethical norms,no such info,contrastive explanations,NA,Textual,Yes,Yes
2-1901,acl,Quantitative Day Trading from Natural Language using Reinforcement Learning,"It is challenging to design profitable and practical trading strategies, as stock price movements are highly stochastic, and the market is heavily influenced by chaotic data across sources like news and social media. Existing NLP approaches largely treat stock prediction as a classification or regression problem and are not optimized to make profitable investment decisions. Further, they do not model the temporal dynamics of large volumes of diversely influential text to which the market responds quickly. Building on these shortcomings, we propose a deep reinforcement learning approach that makes time-aware decisions to trade stocks while optimizing profit using textual data. Our method outperforms state-of-the-art in terms of risk-adjusted returns in trading simulations on two benchmarks: Tweets (English) and financial news (Chinese) pertaining to two major indexes and four global stock markets. Through extensive experiments and studies, we build the case for our method as a tool for quantitative trading.",10.18653/v1/2021.naacl-main.316,https://aclanthology.org/2021.naacl-main.316,NAACL-HLT,"Sawhney, Ramit; Wadhwa, Arnav; Agarwal, Shivam; Shah, Rajiv Ratn",2021,27,"@inproceedings{2-1901,
  title     = {Quantitative Day Trading from Natural Language using Reinforcement Learning},
  author    = {Sawhney, Ramit and Wadhwa, Arnav and Agarwal, Shivam and Shah, Rajiv Ratn},
  year      = {2021},
  doi       = {10.18653/v1/2021.naacl-main.316},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Executing, Forecasting","Guardian, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-19019,ieee,Reinforcement Learning-Based Energy Trading and Management of Regional Interconnected Microgrids,"In this paper, we present a Value-Decomposition Deep Deterministic Policy Gradients (V3DPG) based Reinforcement Learning (RL) method for energy trading and management of regional interconnected microgrids (MGs). In practice, the state of an MG is time-varying and the traded energy flows continuously, which is generally neglected in researches. To address this problem, an Actor-Critic framework is adopted. Each MG has to make energy trading decision based on local observation and has no access to any knowledge of other MGs. We bring in the idea of value-decomposition in the training process to ensure the generation of feasible cooperative policies while maintaining MGs’ privacy and autonomous decision-making ability. Furthermore, in light of the uncertainty and fluctuation of renewable energy generation and users’ demand, a recurrent neural network (RNN) with Burn-In initialization is combined with critic network to achieve implicit predictions. Meanwhile, we also take Energy Storage System (ESS) with operational constraints into consideration and deem it as a virtual market innovatively. Experiments have been carried out under real-world data to verify the merit of the proposed method, compared to existing RL-based works.",10.1109/TSG.2022.3214202,https://ieeexplore.ieee.org/document/9917497,IEEE Transactions on Smart Grid,Shuai Liu;Siyuan Han;Shanying Zhu,2023,22,"@article{2-19019,
  title={Reinforcement Learning-Based Energy Trading and Management of Regional Interconnected Microgrids},
  author={Liu, Shuai and Han, Siyuan and Zhu, Shanying},
  year={2023},
  journal={IEEE Transactions on Smart Grid},
  doi={10.1109/TSG.2022.3214202}
}",Algorithmic contributions,Environment / Resources / Energy,Operational,"Executing, Forecasting","Developer, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-19089,ieee,Resource Allocation for Dynamic Platoon Digital Twin Networks: A Multi-Agent Deep Reinforcement Learning Method,"Vehicle driving in a platoon is an efficient and ecological driving solution. Introducing the concept of digital twin (DT) into the platoon to establish platoon digital twin (PDT) can improve the management efficiency and driving safety of the platoon. However, the joint allocation of multiple types of resources in a platoon digital twin network (PDTN) is an important issue for the successful implementation and maintenance of the PDT. In this paper, we investigate the resource allocation problem in a PDTN. By comprehensively considering the effects of high mobility of platooning vehicles, real-time nature of the DTs, and multi-vehicle cooperation, we propose a PDT utility optimization model for bandwidth and computation resource allocation. We formulate the dynamic resource allocation problem as an \nMM\n-th order Markov decision process (MDP) and design a deep reinforcement learning (DRL)-based dynamic resource allocation (DRLDRA) method to solve it. To optimize the actions of the agent, we reshape the state in a smaller time granularity to better reflect the temporal variations of the state. Correspondingly, we design temporal feature extraction neural networks (TFENNs) based on multi-head self-attention (MHSA) mechanism and long short-term memory (LSTM) to extract the temporal features of the state. To improve the learning efficiency, a decentralized multi-agent deep deterministic policy gradient (DDPG)-based learning framework is proposed. Numerical results show that the DRLDRA method performs excellently and outperforms other benchmark methods.",10.1109/TVT.2024.3414447,https://ieeexplore.ieee.org/document/10557635,IEEE Transactions on Vehicular Technology,Lei Wang;Hongbin Liang;Guotao Mao;Dongmei Zhao;Qian Liu;Yiting Yao;Han Zhang,2024,9,"@article{2-19089,
  title={Resource Allocation for Dynamic Platoon Digital Twin Networks: A Multi-Agent Deep Reinforcement Learning Method},
  author={Lei Wang and Hongbin Liang and Guotao Mao and Dongmei Zhao and Qian Liu and Yiting Yao and Han Zhang},
  year={2024},
  journal={IEEE Transactions on Vehicular Technology},
  doi={10.1109/TVT.2024.3414447}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,Executing,Developer,NA,NA,NA,NA,NA,Yes,No
2-19103,ieee,Resource Reservation in Sliced Networks: An Explainable Artificial Intelligence (XAI) Approach,"The growing complexity of wireless networks has sparked an upsurge in the use of artificial intelligence (AI) within the telecommunication industry in recent years. In network slicing, a key component of 5G that enables network operators to lease their resources to third-party tenants, AI models may be employed in complex tasks, such as short-term resource reservation (STRR). When AI is used to make complex resource management decisions with financial and service quality implications, it is important that these decisions be understood by a human-in-the-loop. In this paper, we apply state-of-the-art techniques from the field of Explainable AI (XAI) to the problem of STRR. Using real-world data to develop an AI model for STRR, we demonstrate how our XAI methodology can be used to explain the real-time decisions of the model, to reveal trends about the model’s general behaviour, as well as aid in the diagnosis of potential faults during the model’s development. In addition, we quantitatively validate the faithfulness of the explanations across an extensive range of XAI metrics to ensure they remain trustworthy and actionable.",10.1109/ICC45855.2022.9838766,https://ieeexplore.ieee.org/document/9838766,IEEE International Conference on Communications,Pieter Barnard;Irene Macaluso;Nicola Marchetti;Luiz A. DaSilva,2022,34,"@inproceedings{2-19103,
  title = {Resource Reservation in Sliced Networks: An Explainable Artificial Intelligence (XAI) Approach},
  author = {Pieter Barnard and Irene Macaluso and Nicola Marchetti and Luiz A. DaSilva},
  year = {2022},
  doi = {10.1109/ICC45855.2022.9838766},
  booktitle = {IEEE International Conference on Communications}
}",Methodological contributions,Software / Systems / Security,Operational,"Explaining, Analyzing, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19144,ieee,RLPG: Reinforcement Learning Approach for Dynamic Intra-Platoon Gap Adaptation for Highway On-Ramp Merging,"A platoon refers to a group of vehicles traveling together in very close proximity using automated driving technology. Owing to its immense capacity to improve fuel efficiency, driving safety, and driver comfort, platooning technology has garnered substantial attention from the autonomous vehicle research community. Although highly advantageous, recent research has uncovered that an excessively small intra-platoon gap can impede traffic flow during highway on-ramp merging. While existing control-based methods allow for adaptation of the intra-platoon gap to improve traffic flow, making an optimal control decision under the complex dynamics of traffic conditions remains a challenge due to the massive computational complexity. In this paper, we present the design, implementation, and evaluation of a novel reinforcement learning framework that adaptively adjusts the intra-platoon gap of an individual platoon member to maximize traffic flow in response to dynamically changing, complex traffic conditions for highway on-ramp merging. The framework's state space has been meticulously designed in consultation with the transportation literature to take into account critical traffic parameters that bear direct relevance to merging efficiency. An intra-platoon gap decision making method based on the deep deterministic policy gradient algorithm is created to incorporate the continuous action space to ensure precise and continuous adaptation of the intra-platoon gap. An extensive simulation study demonstrates the effectiveness of the reinforcement learning-based approach for significantly improving traffic flow in various highway on-ramp merging scenarios.",10.1109/IROS55552.2023.10341918,https://ieeexplore.ieee.org/document/10341918,IEEE/RSJ International Conference on Intelligent Robots and Systems,Sushma Reddy Yadavalli;Lokesh Chandra Das;Myounggyu Won,2023,0,"@inproceedings{2-19144,
  title={RLPG: Reinforcement Learning Approach for Dynamic Intra-Platoon Gap Adaptation for Highway On-Ramp Merging},
  author={Sushma Reddy Yadavalli and Lokesh Chandra Das and Myounggyu Won},
  year={2023},
  doi={10.1109/IROS55552.2023.10341918},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,"Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-19281,ieee,Self-Organized Routing for Autonomous Vehicles via Deep Reinforcement Learning,"Routing for autonomous vehicles with global traffic information and sufficient direct cooperation among vehicles has been widely studied to relieve traffic congestion in recent years. However, the assembly rate of Vehicle-to-Everything (V2X) equipment in practical traffic systems is currently and could be at a low level in near future. Accordingly, autonomous vehicles can only access localized traffic information, and direct cooperation among them cannot always be guaranteed. Thus, how to optimize the routing choices in such scenarios is worthy of particular attention. In this article, we propose a self-organized routing strategy based on deep reinforcement learning (DRL). Under the condition of limited traffic information, the proposed self-organized mechanism well organizes localized traffic conditions through vehicle-level routing decisions, which are able to achieve network-wide benefits gains. In the specified DRL, we propose a novel reward mechanism to harmonize indirect interactions among vehicles by jointly learning individual and overall efficiency, even if each vehicle is modified to make individual decisions independently, rather than only focusing on individual interests as in the greedy strategy. Numerical experiments demonstrate that the proposed self-organized strategy is promising to resolve the routing problem from the perspective of individual decision-making with limited traffic information.",10.1109/TVT.2023.3311198,https://ieeexplore.ieee.org/document/10244078,IEEE Transactions on Vehicular Technology,Huaxin Pei;Jiawei Zhang;Yi Zhang;Huile Xu;Li Li,2024,1,"@article{2-19281,
  title={Self-Organized Routing for Autonomous Vehicles via Deep Reinforcement Learning},
  author={Pei, Huaxin and Zhang, Jiawei and Zhang, Yi and Xu, Huile and Li, Li},
  year={2024},
  doi={10.1109/TVT.2023.3311198},
  journal={IEEE Transactions on Vehicular Technology}
}","Methodological contributions, Algorithmic contributions",Transportation / Mobility / Planning,Individual,Executing,Developer,NA,NA,NA,NA,NA,Yes,No
2-19284,ieee,Self-Supervised Discovering of Interpretable Features for Reinforcement Learning,"Deep reinforcement learning (RL) has recently led to many breakthroughs on a range of complex control tasks. However, the agent’s decision-making process is generally not transparent. The lack of interpretability hinders the applicability of RL in safety-critical scenarios. While several methods have attempted to interpret vision-based RL, most come without detailed explanation for the agent’s behavior. In this paper, we propose a self-supervised interpretable framework, which can discover interpretable features to enable easy understanding of RL agents even for non-experts. Specifically, a self-supervised interpretable network (SSINet) is employed to produce fine-grained attention masks for highlighting task-relevant information, which constitutes most evidence for the agent’s decisions. We verify and evaluate our method on several Atari 2600 games as well as Duckietown, which is a challenging self-driving car simulator environment. The results show that our method renders empirical evidences about how the agent makes decisions and why the agent performs well or badly, especially when transferred to novel scenes. Overall, our method provides valuable insight into the internal decision-making process of vision-based RL. In addition, our method does not use any external labelled data, and thus demonstrates the possibility to learn high-quality mask through a self-supervised manner, which may shed light on new paradigms for label-free vision learning such as self-supervised segmentation and detection.",10.1109/TPAMI.2020.3037898,https://ieeexplore.ieee.org/document/9259236,IEEE Transactions on Pattern Analysis and Machine Intelligence,Wenjie Shi;Gao Huang;Shiji Song;Zhuoyuan Wang;Tingyu Lin;Cheng Wu,2022,94,"@article{2-19284,
  title = {Self-Supervised Discovering of Interpretable Features for Reinforcement Learning},
  author = {Wenjie Shi and Gao Huang and Shiji Song and Zhuoyuan Wang and Tingyu Lin and Cheng Wu},
  year = {2022},
  doi = {10.1109/TPAMI.2020.3037898},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Media / Communication / Entertainment",Individual,"Executing, Explaining","Guardian, Developer",NA,NA,NA,NA,NA,Yes,No
2-19326,ieee,Sharing is Caring! Joint Multitask Learning Helps Aspect-Category Extraction and Sentiment Detection in Scientific Peer Reviews,"The peer-review process is the benchmark of research validation. Peer-reviewed texts are the artifacts via which the editors/chairs decide the inclusion/exclusion of a paper in a journal or conference proceedings. Hence it is important for the editors/chairs to carefully analyze the peer-review text from various aspects of the paper (e.g., novelty, substance, soundness, etc.), identify the underlying sentiment of the reviewers, and thereby validate the informativeness of the reviews before making a decision. With the rise in research paper submissions, the current peer-review system is experiencing an unprecedented information overload. Sometimes it becomes stressful for the chairs/editors to make a reasonable decision within the stringent timelines. Here in this work, we attempt an interesting problem to automatically extract the aspect and sentiment from the peer-review texts. We design an end-to-end deep multitask learning model to perform aspect extraction and sentiment classification simultaneously. We show that both these tasks help each other in the predictions. We achieve encouraging performance on a recently released dataset of peer-review texts. We make our codes available for further research\n1\n1\nhttps://www.iitp.ac.in/~ai-nlp-ml/resources.html#aspect-category-sentiment.",10.1109/JCDL52503.2021.00081,https://ieeexplore.ieee.org/document/9651781,ACM/IEEE Joint Conference on Digital Libraries,Sandeep Kumar;Tirthankar Ghosal;Prabhat Kumar Bharti;Asif Ekbal,2021,14,"@inproceedings{2-19326,
  title = {Sharing is Caring! Joint Multitask Learning Helps Aspect-Category Extraction and Sentiment Detection in Scientific Peer Reviews},
  author = {Sandeep Kumar and Tirthankar Ghosal and Prabhat Kumar Bharti and Asif Ekbal},
  year = {2021},
  doi = {10.1109/JCDL52503.2021.00081},
  booktitle = {ACM/IEEE Joint Conference on Digital Libraries}
}",Algorithmic contributions,Education / Teaching / Research,Operational,"Forecasting, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-19328,ieee,Sibyl: Understanding and Addressing the Usability Challenges of Machine Learning In High-Stakes Decision Making,"Machine learning (ML) is being applied to a diverse and ever-growing set of domains. In many cases, domain experts - who often have no expertise in ML or data science - are asked to use ML predictions to make high-stakes decisions. Multiple ML usability challenges can appear as result, such as lack of user trust in the model, inability to reconcile human-ML disagreement, and ethical concerns about oversimplification of complex problems to a single algorithm output. In this paper, we investigate the ML usability challenges that present in the domain of child welfare screening through a series of collaborations with child welfare screeners. Following the iterative design process between the ML scientists, visualization researchers, and domain experts (child screeners), we first identified four key ML challenges and honed in on one promising explainable ML technique to address them (local factor contributions). Then we implemented and evaluated our visual analytics tool, Sibyl, to increase the interpretability and interactivity of local factor contributions. The effectiveness of our tool is demonstrated by two formal user studies with 12 non-expert participants and 13 expert participants respectively. Valuable feedback was collected, from which we composed a list of design implications as a useful guideline for researchers who aim to develop an interpretable and interactive visualization tool for ML prediction models deployed for child welfare screeners and other similar domain experts.",10.1109/TVCG.2021.3114864,https://ieeexplore.ieee.org/document/9552849,IEEE Transactions on Visualization and Computer Graphics,Alexandra Zytek;Dongyu Liu;Rhema Vaithianathan;Kalyan Veeramachaneni,2022,75,"@article{2-19328,
  title={Sibyl: Understanding and Addressing the Usability Challenges of Machine Learning In High-Stakes Decision Making},
  author={Zytek, Alexandra and Liu, Dongyu and Vaithianathan, Rhema and Veeramachaneni, Kalyan},
  year={2022},
  doi={10.1109/TVCG.2021.3114864},
  journal={IEEE Transactions on Visualization and Computer Graphics}
}",System/Artifact contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,"Advising, Forecasting, Explaining","Knowledge provider, Developer, Decision-maker",Change trust,no such info,"local explanations, prediction of alternative, risk score, explanations, what-if",NA,"Visual, Textual, Interactive interface",Yes,Yes
2-19394,ieee,Spatio-weighted information fusion and DRL-based control for connected autonomous vehicles,"While on-board sensing equipment of CAVs can reasonably characterize the surrounding traffic environment, their performance is limited by the range of the sensors. By integrating short- and long-range information, a CAV can comprehensively construct its surrounding environment, thereby allowing it to plan both short and long-term maneuvers. Coalescing local information and downstream information is critical for the CAV to make safe and effective driving decisions. While literature is replete with CAV control approaches that use information sensed from the local traffic environment, studies that fuse information from various temporal-spatial instances to facilitate CAV movements is limited. In this paper, we propose a Deep Reinforcement Learning (DRL) based approach that fuses information obtained (via sensing and connectivity) on the local downstream environment for CAV lane changing decisions. We adopt learning-based techniques to provide an integrated solution that incorporates the information fusion and movement-decision processor. We also determine the optimal connectivity range for each operating traffic density. We anticipate that deployment of the proposed algorithm in a CAV will facilitate reliable proactive driving decisions and ultimately enhance the overall operational efficiency of CAVs in terms of safety and mobility.",10.1109/ITSC45102.2020.9294550,https://ieeexplore.ieee.org/document/9294550,IEEE International Conference on Intelligent Transportation Systems,Jiqian Dong;Sikai Chen;Yujie Li;Paul Young Joun Ha;Runjia Du;Aaron Steinfeld;Samuel Labi,2020,35,"@inproceedings{2-19394,
  title={Spatio-weighted information fusion and DRL-based control for connected autonomous vehicles},
  author={Dong, Jiqian and Chen, Sikai and Li, Yujie and Ha, Paul Young Joun and Du, Runjia and Steinfeld, Aaron and Labi, Samuel},
  year={2020},
  doi={10.1109/ITSC45102.2020.9294550},
  booktitle={IEEE International Conference on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,Developer,NA,NA,NA,NA,NA,Yes,No
2-19435,ieee,Supervised Optimal Chemotherapy Regimen Based on Offline Reinforcement Learning,"In recent years, reinforcement learning (RL) has achieved a remarkable achievement and it has attracted researchers’ attention in modeling real-life scenarios by expanding its research beyond conventional complex games. Prediction of optimal treatment regimens from observational real clinical data is being popularized, and more advanced versions of RL algorithms are being implemented in the literature. However, RL-generated medications still need careful supervision of expertise parties or doctors in healthcare. Hence, in this paper, a Supervised Optimal Chemotherapy Regimen (SOCR) approach to investigate optimal chemotherapy-dosing schedule for cancer patients was presented by using Offline Reinforcement Learning. The optimal policy suggested by the RL approach was supervised by incorporating previous treatment decisions of oncologists, which could add clinical expertise knowledge on algorithmic results. Presented SOCR approach followed a model-based architecture using conservative Q-Learning (CQL) algorithm. The developed model was tested using a manually constructed database of forty Stage-IV colon cancer patients, receiving line-1 chemotherapy treatments, who were clinically classified as ‘Bevacizumab based patient’ and ‘Cetuximab based patient’. Experimental results revealed that the supervision from the oncologists has considered the effect to stabilize chemotherapy regimen and it was suggested that the proposed framework could be successfully used as a supportive model for oncologists in deciding their treatment decisions.",10.1109/JBHI.2022.3183854,https://ieeexplore.ieee.org/document/9798842,IEEE Journal of Biomedical and Health Informatics,Chamani Shiranthika;Kuo-Wei Chen;Chung-Yih Wang;Chan-Yun Yang;B. H. Sudantha;Wei-Fu Li,2022,41,"@article{2-19435,
  title={Supervised Optimal Chemotherapy Regimen Based on Offline Reinforcement Learning},
  author={Shiranthika, Chamani and Chen, Kuo-Wei and Wang, Chung-Yih and Yang, Chan-Yun and Sudantha, B. H. and Li, Wei-Fu},
  year={2022},
  journal={IEEE Journal of Biomedical and Health Informatics},
  doi={10.1109/JBHI.2022.3183854}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Executing","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-19490,ieee,The Role of Explainability in Assuring Safety of Machine Learning in Healthcare,"Established approaches to assuring safety-critical systems and software are difficult to apply to systems employing ML where there is no clear, pre-defined specification against which to assess validity. This problem is exacerbated by the “opaque” nature of ML where the learnt model is not amenable to human scrutiny. Explainable AI (XAI) methods have been proposed to tackle this issue by producing human-interpretable representations of ML models which can help users to gain confidence and build trust in the ML system. However, little work explicitly investigates the role of explainability for safety assurance in the context of ML development. This paper identifies ways in which XAI methods can contribute to safety assurance of ML-based systems. It then uses a concrete ML-based clinical decision support system, concerning weaning of patients from mechanical ventilation, to demonstrate how XAI methods can be employed to produce evidence to support safety assurance. The results are also represented in a safety argument to show where, and in what way, XAI methods can contribute to a safety case. Overall, we conclude that XAI methods have a valuable role in safety assurance of ML-based systems in healthcare but that they are not sufficient in themselves to assure safety.",10.1109/TETC.2022.3171314,https://ieeexplore.ieee.org/document/9769937,IEEE Transactions on Emerging Topics in Computing,Yan Jia;John McDermid;Tom Lawton;Ibrahim Habli,2022,367,"@article{2-19490,
  title={The Role of Explainability in Assuring Safety of Machine Learning in Healthcare},
  author={Jia, Yan and McDermid, John and Lawton, Tom and Habli, Ibrahim},
  year={2022},
  doi={10.1109/TETC.2022.3171314},
  journal={IEEE Transactions on Emerging Topics in Computing}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Explaining","Decision-subject, Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-19491,ieee,The SPATIAL Architecture: Design and Development Experiences from Gauging and Monitoring the AI Inference Capabilities of Modern Applications,"Despite its enormous economical and societal impact, lack of human-perceived control and safety is re-defining the design and development of emerging AI-based technologies. New regulatory requirements mandate increased human control and oversight of AI, transforming the development practices and responsibilities of individuals interacting with AI. In this paper, we present the SPATIAL architecture, a system that augments modern applications with capabilities to gauge and monitor trustworthy properties of AI inference capabilities. To design SPATIAL, we first explore the evolution of modern system architectures and how AI components and pipelines are integrated. With this information, we then develop a proof-of- concept architecture that analyzes AI models in a human-in-the- loop manner. SPATIAL provides an AI dashboard for allowing individuals interacting with applications to obtain quantifiable insights about the AI decision process. This information is then used by human operators to comprehend possible issues that influence the performance of AI models and adjust or counter them. Through rigorous benchmarks and experiments in real- world industrial applications, we demonstrate that SPATIAL can easily augment modern applications with metrics to gauge and monitor trustworthiness, however, this in turn increases the complexity of developing and maintaining systems implementing AI. Our work highlights lessons learned and experiences from augmenting modern applications with mechanisms that support regulatory compliance of AI. In addition, we also present a road map of on-going challenges that require attention to achieve robust trustworthy analysis of AI and greater engagement of human oversight.",10.1109/ICDCS60910.2024.00092,https://ieeexplore.ieee.org/document/10630929,IEEE International Conference on Distributed Computing Systems,Abdul-Rasheed Ottun;Rasinthe Marasinghe;Toluwani Elemosho;Mohan Liyanage;Mohamad Ragab;Prachi Bagave;Marcus Westberg;Mehrdad Asadi;Michell Boerger;Chamara Sandeepa;Thulitha Senevirathna;Bartlomiej Siniarski;Madhusanka Liyanage;Vinh Hoa La;Manh-Dung Nguyen;Edgardo Montes De Oca;Tessa Oomen;João Fernando Ferreira Gonçalves;Illija Tanasković;Sasa Klopanovic;Nicolas Kourtellis;Claudio Soriente;Jason Pridmore;Ana Rosa Cavalli;Drasko Draskovic;Samuel Marchal;Shen Wang;David Solans Noguero;Nikolay Tcholtchev;Aaron Yi Ding;Huber Flores,2024,9,"@inproceedings{2-19491,
  title = {The SPATIAL Architecture: Design and Development Experiences from Gauging and Monitoring the AI Inference Capabilities of Modern Applications},
  author = {Abdul-Rasheed Ottun and Rasinthe Marasinghe and Toluwani Elemosho and Mohan Liyanage and Mohamad Ragab and Prachi Bagave and Marcus Westberg and Mehrdad Asadi and Michell Boerger and Chamara Sandeepa and Thulitha Senevirathna and Bartlomiej Siniarski and Madhusanka Liyanage and Vinh Hoa La and Manh-Dung Nguyen and Edgardo Montes De Oca and Tessa Oomen and João Fernando Ferreira Gonçalves and Illija Tanasković and Sasa Klopanovic and Nicolas Kourtellis and Claudio Soriente and Jason Pridmore and Ana Rosa Cavalli and Drasko Draskovic and Samuel Marchal and Shen Wang and David Solans Noguero and Nikolay Tcholtchev and Aaron Yi Ding and Huber Flores},
  year = {2024},
  doi = {10.1109/ICDCS60910.2024.00092},
  booktitle = {IEEE International Conference on Distributed Computing Systems}
}",System/Artifact contributions,"Generic / Abstract / Domain-agnostic, Software / Systems / Security",Operational,"Analyzing, Explaining",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-195,aaai,Role of Human-AI Interaction in Selective Prediction,"Recent work has shown the potential benefit of selective prediction systems that can learn to defer to a human when the predictions of the AI are unreliable, particularly to improve the reliability of AI systems in high-stakes applications like healthcare or conservation. However, most prior work assumes that human behavior remains unchanged when they solve a prediction task as part of a human-AI team as opposed to by themselves. We show that this is not the case by performing experiments to quantify human-AI interaction in the context of selective prediction. In particular, we study the impact of communicating different types of information to humans about the AI systems decision to defer. Using real-world conservation data and a selective prediction system that improves expected accuracy over that of the human or AI system working individually, we show that this messaging has a significant impact on the accuracy of human judgements. Our results study two components of the messaging strategy: 1) Whether humans are informed about the prediction of the AI system and 2) Whether they are informed about the decision of the selective prediction system to defer. By manipulating these messaging components, we show that it is possible to significantly boost human performance by informing the human of the decision to defer, but not revealing the prediction of the AI. We therefore show that it is vital to consider how the decision to defer is communicated to a human when designing selective prediction systems, and that the composite accuracy of a human-AI team must be carefully evaluated using a human-in-the-loop framework.",10.1609/aaai.v36i5.20465,https://ojs.aaai.org/index.php/AAAI/article/view/20465,AAAI Conference on Artificial Intelligence,Elizabeth Bondi;Raphael Koster;Hannah Sheahan;Martin Chadwick;Yoram Bachrach;Taylan Cemgil;Ulrich Paquet;Krishnamurthy Dvijotham,2022,0,"@inproceedings{2-195,
  title={Role of Human-AI Interaction in Selective Prediction},
  author={Bondi, Elizabeth and Koster, Raphael and Sheahan, Hannah and Chadwick, Martin and Bachrach, Yoram and Cemgil, Taylan and Paquet, Ulrich and Dvijotham, Krishnamurthy},
  year={2022},
  doi={10.1609/aaai.v36i5.20465},
  booktitle={AAAI Conference on Artificial Intelligence}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research",Individual,"Forecasting, Collaborating",Decision-maker,Alter decision outcomes,no such info,"prediction of alternative, deferral status, system accuracy",NA,Visual,Yes,Yes
2-19531,ieee,Toward Human–AI Collaboration: A Recommender System to Support CS1 Instructors to Select Problems for Assignments and Exams,"Programming online judges (POJs) have been increasingly used in CS1 classes, as they allow students to practice and get quick feedback. For instructors, it is a useful tool for creating assignments and exams. However, selecting problems in POJs is time consuming. First, problems are generally not organized based on topics covered in the CS1 syllabus. Second, assessing whether problems require similar effort to be completed and map onto the same topic is a subjective and expert-dependent task. The difficulty increases if the instructor must create variations of these assessments, e.g., to avoid plagiarism. Thus, here, we research how to support CS1 instructors in the task of selecting problems, to compose one-size-fits-all or personalized assignments/exams. Our solution is to propose a novel intelligent recommender system, based on a fine-grained data-driven analysis of the students' effort on solving problems in the integrated development environment of a POJ system, and automatic detection of topics for CS1 problems, based on problem descriptions. Data collected from 2714 students are processed to support, via our artificial intelligence (AI) method recommendations, the instructors' decision-making process. We evaluated our method against the state of the art in a simple blind experiment with CS1 instructors (\nN=N =\n 35). Results show that our recommendations are 88% accurate, surpassing our baseline (\np<p<\n 0.05). Finally, our work paves the way for novel POJ smart learning environments, wherein instructors define learning tasks (assignments/exams) supported by AI.",10.1109/TLT.2022.3224121,https://ieeexplore.ieee.org/document/9961911,IEEE Transactions on Learning Technologies,Filipe Dwan Pereira;Luiz Rodrigues;Marcelo Henrique Oliveira Henklain;Hermino Freitas;David Fernandes Oliveira;Alexandra I. Cristea;Leandro Carvalho;Seiji Isotani;Aileen Benedict;Mohsen Dorodchi;Elaine Harada Teixeira de Oliveira,2023,0,"@article{2-19531,
  title={Toward Human--AI Collaboration: A Recommender System to Support CS1 Instructors to Select Problems for Assignments and Exams},
  author={Pereira, Filipe Dwan and Rodrigues, Luiz and Henklain, Marcelo Henrique Oliveira and Freitas, Hermino and Oliveira, David Fernandes and Cristea, Alexandra I. and Carvalho, Leandro and Isotani, Seiji and Benedict, Aileen and Dorodchi, Mohsen and de Oliveira, Elaine Harada Teixeira},
  year={2023},
  doi={10.1109/TLT.2022.3224121},
  journal={IEEE Transactions on Learning Technologies}
}",System/Artifact contributions,Education / Teaching / Research,Operational,"Advising, Analyzing, Forecasting","Decision-maker, Decision-subject, Knowledge provider","Change cognitive demands, Alter decision outcomes",Change AI responses,NA,NA,NA,Yes,Yes
2-19565,ieee,Towards Real-World Applications of Personalized Anesthesia Using Policy Constraint Q Learning for Propofol Infusion Control,"Automated anesthesia promises to enable more personalized and precise anesthetic administration and free anesthesiologists from repetitive tasks, allowing them to focus on the most critical aspects of surgical care for patients. Current research has typically focused on creating simulated environments from which agents can learn. These approaches have demonstrated good experimental results, but are still far from clinical application. In this paper, Policy Constraint Q-Learning (PCQL), a data-driven reinforcement learning algorithm for solving the problem of learning strategies on real world anesthesia data, is proposed. Conservative Q-Learning is first introduced to alleviate the problem of Q function overestimation in an offline context. A policy constraint term is then added to agent training to keep the action distribution of the agent and the anesthesiologist consistent, ensuring the agent makes safer decisions in anesthesia scenarios. The effectiveness of PCQL was validated by extensive experiments on a real clinical anesthesia dataset we collected. Experimental results show that PCQL is predicted to achieve higher gains than the baseline approaches while maintaining good agreement with the reference dose given by the anesthesiologist, using less total dose, and being more responsive to the patient's vital signs. In addition, the confidence intervals of the agent were investigated and were founded to cover most of the clinical decisions of the anesthesiologist. Finally, an interpretable method, SHAP, was used to analyze the contributing components of the model predictions, increasing the transparency of the model.",10.1109/JBHI.2023.3321099,https://ieeexplore.ieee.org/document/10268595,IEEE Journal of Biomedical and Health Informatics,Xiuding Cai;Jiao Chen;Yaoyao Zhu;Beimin Wang;Yu Yao,2024,18,"@article{2-19565,
  title={Towards Real-World Applications of Personalized Anesthesia Using Policy Constraint Q Learning for Propofol Infusion Control},
  author={Cai, Xiuding and Chen, Jiao and Zhu, Yaoyao and Wang, Beimin and Yao, Yu},
  year={2024},
  journal={IEEE Journal of Biomedical and Health Informatics},
  doi={10.1109/JBHI.2023.3321099}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Executing, Explaining","Decision-subject, Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-197,aaai,SafeAR: Safe Algorithmic Recourse by Risk-Aware Policies,"With the growing use of machine learning( ML) models in critical domains such as finance and healthcare, the need to offer recourse for those adversely affected by the decisions of ML models has become more important; individuals ought to be provided with recommendations on actions to take for improving their situation and thus receiving a favorable decision. Prior work on sequential algorithmic recourse---which recommends a series of changes---focuses on action feasibility and uses the proximity of feature changes to determine action costs. However, the uncertainties of feature changes and the risk of higher than average costs in recourse have not been considered. It is undesirable if a recourse could( with some probability) result in a worse situation from which recovery requires an extremely high cost. It is essential to incorporate risks when computing and evaluating recourse. We call the recourse computed with such risk considerations as Safe Algorithmic Recourse( SafeAR). The objective is to empower people to choose a recourse based on their risk tolerance. In this work, we discuss and show how existing recourse desiderata can fail to capture the risk of higher costs. We present a method to compute recourse policies that consider variability in cost and connect algorithmic recourse literature with risk-sensitive reinforcement learning. We also adopt measures Value at Risk and Conditional Value at Risk from the financial literature to summarize risk concisely. We apply our method to two real-world datasets and compare policies with different risk-aversion levels using risk measures and recourse desiderata( sparsity and proximity).",10.1609/aaai.v38i14.29522,https://ojs.aaai.org/index.php/AAAI/article/view/29522,AAAI Conference on Artificial Intelligence,Haochen Wu;Shubham Sharma;Sunandita Patra;Sriram Gopalakrishnan,2024,1,"@inproceedings{2-197,
  title = {SafeAR: Safe Algorithmic Recourse by Risk-Aware Policies},
  author = {Haochen Wu and Shubham Sharma and Sunandita Patra and Sriram Gopalakrishnan},
  year = {2024},
  doi = {10.1609/aaai.v38i14.29522},
  booktitle = {AAAI Conference on Artificial Intelligence}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Operational,"Analyzing, Advising","Stakeholder, Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-19701,ieee,Visualizing and Comparing Machine Learning Predictions to Improve Human-AI Teaming on the Example of Cell Lineage,"We visualize the predictions of multiple machine learning models to help biologists as they interactively make decisions about \ncell lineage\n—the development of a (plant) embryo from a single \novum cell\n. Based on a confocal microscopy dataset, traditionally biologists manually constructed the cell lineage, starting from this observation and reasoning backward in time to establish their inheritance. To speed up this tedious process, we make use of machine learning (ML) models trained on a database of manually established cell lineages to assist the biologist in cell assignment. Most biologists, however, are not familiar with ML, nor is it clear to them which model best predicts the embryo's development. We thus have developed a visualization system that is designed to support biologists in exploring and comparing ML models, checking the model predictions, detecting possible ML model mistakes, and deciding on the most likely embryo development. To evaluate our proposed system, we deployed our interface with six biologists in an observational study. Our results show that the visual representations of machine learning are easily understandable, and our tool, LineageD+, could potentially increase biologists’ working efficiency and enhance the understanding of embryos.",10.1109/TVCG.2023.3302308,https://ieeexplore.ieee.org/document/10239303,IEEE Transactions on Visualization and Computer Graphics,Jiayi Hong;Ross Maciejewski;Alain Trubuil;Tobias Isenberg,2024,0,"@article{2-19701,
  title={Visualizing and Comparing Machine Learning Predictions to Improve Human-AI Teaming on the Example of Cell Lineage},
  author={Jiayi Hong and Ross Maciejewski and Alain Trubuil and Tobias Isenberg},
  year={2024},
  doi={10.1109/TVCG.2023.3302308},
  journal={IEEE Transactions on Visualization and Computer Graphics}
}",System/Artifact contributions,Education / Teaching / Research,Operational,"Advising, Forecasting, Collaborating","Decision-maker, Knowledge provider","Change affective-perceptual, Alter decision outcomes, Change trust, Change cognitive demands","Update AI competence, Change AI responses","prediction of alternative, (continuous) support","corrective feedback, evaluation","Visual, Interactive interface",Yes,Yes
2-19744,ieee,A Machine Learning Decision-Support System Improves the Internet of Things’ Smart Meter Operations,An Internet of Things' (IoT) connected society and system represents a tremendous paradigm shift. We present a framework for a decision-support system (DSS) that operates within the IoT ecosystem. The DSS leverages advanced analytics of electric smart meter (ESM) network communication-quality data to improve cost predictions for smart meter field operations and provide actionable decision recommendations regarding whether to send a technician to a customer location to resolve an ESM issue. The model is empirically evaluated using data sets from a commercial network. We demonstrate the efficiency of our approach with a complete Bayesian network prediction model and compare with three machine learning prediction model classifiers: 1) Naïve Bayes; 2) random forest; and 3) decision tree. Results demonstrate that our approach generates statistically noteworthy estimations and that the DSS will improve the cost efficiency of ESM network operations and maintenance.,10.1109/JIOT.2017.2722358,https://ieeexplore.ieee.org/document/7964681,IEEE Internet of Things Journal,Joseph Siryani;Bereket Tanju;Timothy J. Eveleigh,2017,0,"@article{2-19744,
  title={A Machine Learning Decision-Support System Improves the Internet of Things’ Smart Meter Operations},
  author={Siryani, Joseph and Tanju, Bereket and Eveleigh, Timothy J.},
  year={2017},
  journal={IEEE Internet of Things Journal},
  doi={10.1109/JIOT.2017.2722358}
}",System/Artifact contributions,Software / Systems / Security,Organizational,"Advising, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19747,ieee,Driving Behavior Modeling Using Naturalistic Human Driving Data With Inverse Reinforcement Learning,"Driving behavior modeling is of great importance for designing safe, smart, and personalized autonomous driving systems. In this paper, an internal reward function-based driving model that emulates the human’s decision-making mechanism is utilized. To infer the reward function parameters from naturalistic human driving data, we propose a structural assumption about human driving behavior that focuses on discrete latent driving intentions. It converts the continuous behavior modeling problem to a discrete setting and thus makes maximum entropy inverse reinforcement learning (IRL) tractable to learn reward functions. Specifically, a polynomial trajectory sampler is adopted to generate candidate trajectories considering high-level intentions and approximate the partition function in the maximum entropy IRL framework. An environment model considering interactive behaviors among the ego and surrounding vehicles is built to better estimate the generated trajectories. The proposed method is applied to learn personalized reward functions for individual human drivers from the NGSIM highway driving dataset. The qualitative results demonstrate that the learned reward functions are able to explicitly express the preferences of different drivers and interpret their decisions. The quantitative results reveal that the learned reward functions are robust, which is manifested by only a marginal decline in proximity to the human driving trajectories when applying the reward function in the testing conditions. For the testing performance, the personalized modeling method outperforms the general modeling approach, significantly reducing the modeling errors in human likeness (a custom metric to gauge accuracy), and these two methods deliver better results compared to other baseline methods. Moreover, it is found that predicting the response actions of surrounding vehicles and incorporating their potential decelerations caused by the ego vehicle are critical in estimating the generated trajectories, and the accuracy of personalized planning using the learned reward functions relies on the accuracy of the forecasting model.",10.1109/TITS.2021.3088935,https://ieeexplore.ieee.org/document/9460807,IEEE Transactions on Intelligent Transportation Systems,Zhiyu Huang;Jingda Wu;Chen Lv,2022,254,"@article{2-19747,
  title={Driving Behavior Modeling Using Naturalistic Human Driving Data With Inverse Reinforcement Learning},
  author={Huang, Zhiyu and Wu, Jingda and Lv, Chen},
  year={2022},
  doi={10.1109/TITS.2021.3088935},
  journal={IEEE Transactions on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Analyzing, Forecasting","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-19749,ieee,Intelligent Cooperative Collision Avoidance at Overtaking and Lane Changing Maneuver in 6G-V2X Communications,"The rapid growth in Autonomous Vehicle (AV) technology endeavors increased attention towards road safety in recent days. Particularly, a higher number of road accidents occurs when the AV tries to overtake or change the lane. To cut down the number of accidents and improve traffic reliability, the AV should be capable of making intelligent decisions and communicating those to other AVs. Therefore, in this paper, a Cooperative Collision avoidance scheme for AVs at Overtaking and Lane Changing maneuver (CCAV-OLC) is proposed. The Inverse Reinforcement Learning (IRL) in the CCAV-OLC scheme, processes on the given number of expert demonstrations for automatically acquiring the reward function, and thereby imitating actual human driving strategy and decisions. However, the adaptability of IRL to a high-dimensional AV environment restricts the performance of the CCAV-OLC scheme. To overcome this, the IRL in CCAV-OLC leverages the Gaussian Process (GP) regression model (IRL-GP), which enables data-efficient Bayesian prediction even when the number of demonstrations is very low. After taking intelligent decisions in overtaking and lane changing maneuvers, the AVs cooperatively communicate and exchange the decisions with each other by 6th Generation Vehicle-to-Everything (6G-V2X) communications, which further improves the accuracy and lessens the time taken for making optimal decisions. The experimental results show that the AVs clone the expert's optimal driving strategy and avoid the collisions to a greater extent.",10.1109/TVT.2021.3127219,https://ieeexplore.ieee.org/document/9612041,IEEE Transactions on Vehicular Technology,Sahaya Beni Prathiba;Gunasekaran Raja;Neeraj Kumar,2022,58,"@article{2-19749,
  title={Intelligent Cooperative Collision Avoidance at Overtaking and Lane Changing Maneuver in 6G-V2X Communications},
  author={Sahaya Beni Prathiba and Gunasekaran Raja and Neeraj Kumar},
  year={2022},
  doi={10.1109/TVT.2021.3127219},
  journal={IEEE Transactions on Vehicular Technology}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Forecasting, Executing",Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-19751,ieee,Multimodal Vehicular Trajectory Prediction With Inverse Reinforcement Learning and Risk Aversion at Urban Unsignalized Intersections,"Understanding human drivers’ intentions and predicting their future motions are significant to connected and autonomous vehicles and traffic safety and surveillance systems. Predicting multimodal vehicular trajectories at urban unsignalized intersections remains challenging due to dynamic traffic flow and uncertainty of human drivers’ maneuvers. In this paper, we propose a comprehensive trajectory prediction framework that combines a multimodal trajectory generation network with inverse reinforcement learning (IRL) and risk aversion (RA) modules. Specifically, we first construct a multimodal spatial-temporal Transformer network (mmSTTN) to generate multiple trajectory candidates, using trajectory coordinates as inputs. Accounting for spatio-temporal features, we formulate the IRL reward function for evaluating all candidate trajectories. The optimal trajectory is then selected based on the computed rewards, a process that mimics human drivers’ decision-making. We further develop the RA module based on the driving risk field for optimal risk-averse trajectory prediction. We conduct experiments and ablation studies using the inD dataset at an urban unsignalized intersection, demonstrating impressive human trajectory alignment, prediction accuracy, and the ability to generate risk-averse trajectories. Our proposed framework reduces prediction errors and driving risks by 25% and 30% compared to baseline methods. Results validate vehicles’ human-like risk-averse diverging-and-concentrating behavior as they traverse the intersection. The proposed framework presents a novel approach for forecasting multimodal vehicular trajectories by imitating human drivers and incorporating physics-based risk information derived from the driving field. This research offers a promising direction for enhancing the safety and efficiency of connected and autonomous vehicles navigating urban environments.",10.1109/TITS.2023.3285891,https://ieeexplore.ieee.org/document/10164651,IEEE Transactions on Intelligent Transportation Systems,Maosi Geng;Zeen Cai;Yizhang Zhu;Xiqun Chen;Der-Horng Lee,2023,32,"@article{2-19751,
  title={Multimodal Vehicular Trajectory Prediction With Inverse Reinforcement Learning and Risk Aversion at Urban Unsignalized Intersections},
  author={Geng, Maosi and Cai, Zeen and Zhu, Yizhang and Chen, Xiqun and Lee, Der-Horng},
  year={2023},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  doi={10.1109/TITS.2023.3285891}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Analyzing, Executing, Forecasting","Decision-subject, Developer, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-19752,ieee,A Behavior Decision Method Based on Reinforcement Learning for Autonomous Driving,"Autonomous driving vehicles can reduce congestion and improve safety while increasing traffic efficiency. To reflect the quality of driving more comprehensively, the driving safety, efficiency, and occupant comfort should be jointly optimized for autonomous vehicles. Furthermore, in order to cope with complicated traffic environments and achieve satisfactory driving performance, a powerful behavior decision-making module is indispensable for autonomous vehicles. Toward this end, we study a reinforcement-learning (RL)-based method to intelligently make the behavior decision in this article. A Markov decision process (MDP) model is first formulated with a comprehensive reward function, including the effects of driving safety, efficiency, and comfort. The knowledge of the surrounding vehicles is also leveraged to exploit the behavior prediction of the target vehicle. We then propose a behavior decision strategy based on the actor–critic (AC) mechanism, which can efficiently learn both a Gaussian policy function and a linear value function. Finally, the real traffic data are used to build up the simulations for evaluating the performances of the proposed method thoroughly. Simulation results show that our proposed method can significantly reduce the collision rate for autonomous vehicles.",10.1109/JIOT.2022.3196639,https://ieeexplore.ieee.org/document/9851435,IEEE Internet of Things Journal,Kan Zheng;Haojun Yang;Shiwen Liu;Kuan Zhang;Lei Lei,2022,21,"@article{2-19752,
  title={A Behavior Decision Method Based on Reinforcement Learning for Autonomous Driving},
  author={Kan Zheng and Haojun Yang and Shiwen Liu and Kuan Zhang and Lei Lei},
  year={2022},
  doi={10.1109/JIOT.2022.3196639},
  journal={IEEE Internet of Things Journal}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Forecasting, Executing",Stakeholder,NA,NA,NA,NA,NA,Yes,No
2-19754,ieee,A Discrete Soft Actor-Critic Decision-Making Strategy With Sample Filter for Freeway Autonomous Driving,"Autonomous driving is a promising technology to reduce traffic accidents and improve driving efficiency. Although significant progress has been achieved, existing decision-making systems of autonomous vehicle still cannot meet the safety and driving efficiency requirements in highly dynamic environments. In this work, we design a discrete decision-making strategy based on the discrete soft actor-critic with sample filter algorithm (DSAC-SF) to improve driving efficiency and safety on freeways with dynamics traffic. Specifically, we first propose a sample filter method for discrete soft actor-critic, which improves the sample efficiency and stability of the algorithm via enhancing the utilization of effective samples. Subsequently, we construct the discrete decision-making strategy for autonomous driving based on the DSAC-SF algorithm, and further design the area observation method and the multi-objective reward function to improve the driving safety and efficiency. Finally, we carry out comparison and ablation experiments on the the scalable multi-agent reinforcement learning training school (SMARTS) simulation environment. Experimental results indicate that our strategy obtains a high success rate and a fast vehicle speed in the decision-making tasks on freeways. Moreover, our DSAC-SF algorithm also achieves improved performance in training efficiency and stability compared to the commonly used discrete reinforcement learning algorithm.",10.1109/TVT.2022.3212996,https://ieeexplore.ieee.org/document/9914655,IEEE Transactions on Vehicular Technology,Jiayi Guan;Guang Chen;Jin Huang;Zhijun Li;Lu Xiong;Jing Hou;Alois Knoll,2023,0,"@article{2-19754,
  title={A Discrete Soft Actor-Critic Decision-Making Strategy With Sample Filter for Freeway Autonomous Driving},
  author={Guan, Jiayi and Chen, Guang and Huang, Jin and Li, Zhijun and Xiong, Lu and Hou, Jing and Knoll, Alois},
  year={2023},
  doi={10.1109/TVT.2022.3212996},
  journal={IEEE Transactions on Vehicular Technology}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Analyzing",Developer,NA,NA,NA,NA,NA,Yes,No
2-19755,ieee,A Hybrid Driving Decision-Making System Integrating Markov Logic Networks and Connectionist AI,"Connectionist artificial intelligence (AI) can power many critical tasks for connected and autonomous vehicles (CAVs). However, connectionist AI lacks interpretability and usually needs large amount of data for learning. A Markov logic network (MLN), which combines first-order logic (FOL) with statistical learning, learns weighted FOL formulas for inference. MLNs can incorporate domain expert knowledge in the form of FOL formulas to achieve data-efficient learning and transparent decision process. In this paper, we propose a hybrid driving decision-making system, which integrates a MLN module and a deep Q-network (DQN) for enhanced driving safety. The MLN module evaluates the safety of ranked actions from DQN to reduce potential collisions. A collective MLN (Co-MLN) learning algorithm is proposed and it enables CAVs collectively learn a global MLN model for safe state transitions, given distributed small amount of noisy data. A hybrid DQN-MLN learning algorithm is also developed for CAVs to collectively learn to drive in new driving environments. Simulations performed using a highway driving simulator show that the proposed Co-MLN algorithm is highly data-efficient and the learned hybrid driving system can effectively reduce collisions. In addition, the learned MLN module provides transparency for safety-critical driving decisions.",10.1109/TITS.2022.3227122,https://ieeexplore.ieee.org/document/9990586,IEEE Transactions on Intelligent Transportation Systems,Mengyao Wu;F. Richard Yu;Peter Xiaoping Liu;Ying He,2023,19,"@article{2-19755,
  title={A Hybrid Driving Decision-Making System Integrating Markov Logic Networks and Connectionist AI},
  author={Mengyao Wu and F. Richard Yu and Peter Xiaoping Liu and Ying He},
  year={2023},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  doi={10.1109/TITS.2022.3227122}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Monitoring, Executing, Explaining",Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-19758,ieee,A Rear Anti-Collision Decision-Making Methodology Based on Deep Reinforcement Learning for Autonomous Commercial Vehicles,"Driving decision-making determines the safety and rationality of autonomous commercial vehicles. Aiming at the issue of safe driving decision-making, herein, a rear anti-collision decision-making methodology based on deep reinforcement learning (RAD-DRL) was creatively proposed. Firstly, aiming at the dynamic coupling of rear anti-collision factors for safe driving, a driving decision network based on an actor-critic framework was proposed for sensor data processing. Then, inspired by the idea of multi-objective optimization, a refined reward function is developed. It comprehensively considers the impact of backward target types, safety clearance, and vehicle roll stability on the rear collision. Finally, the RAD-DRL was trained (with different values of random seeds), tested, and verified in a simulation environment where road network and traffic situations were built-in SUMO (Simulation of Urban Mobility). After 30,000 training episodes, effective and reliable rear anti-collision driving decisions were achieved by our proposed RAD-DRL. The simulated results indicated that the RAD-DRL has remarkable superiority in generalization and effectiveness in expressway scenarios with different driving cases. Especially, it maintained good decision performance in the corner case in which the backward vehicle suddenly accelerates.",10.1109/JSEN.2022.3190302,https://ieeexplore.ieee.org/document/9832562,IEEE Sensors Journal,Weiming Hu;Xu Li;Jinchao Hu;Xiang Song;Xuan Dong;Dong Kong;Qimin Xu;Chunxiao Ren,2022,18,"@article{2-19758,
  title={A Rear Anti-Collision Decision-Making Methodology Based on Deep Reinforcement Learning for Autonomous Commercial Vehicles},
  author={Hu, Weiming and Li, Xu and Hu, Jinchao and Song, Xiang and Dong, Xuan and Kong, Dong and Xu, Qimin and Ren, Chunxiao},
  year={2022},
  journal={IEEE Sensors Journal},
  volume={},
  number={},
  pages={},
  doi={10.1109/JSEN.2022.3190302}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,Stakeholder,NA,NA,NA,NA,NA,Yes,No
2-19766,ieee,AI-Driven Synthetic Biology for Non-Small Cell Lung Cancer Drug Effectiveness-Cost Analysis in Intelligent Assisted Medical Systems,"According to statistics, in the 185 countries' 36 types of cancer, the morbidity and mortality of lung cancer take the first place, and non-small cell lung cancer (NSCLC) accounts for 85% of lung cancer (International Agency for Research on Cancer, 2018), (Bray \net al.\n, 2018). Significantly in many developing countries, limited medical resources and excess population seriously affect the diagnosis and treatment of alung cancer patients. The 21st century is an era of life medicine, big data, and information technology. Synthetic biology is known as the driving force of natural product innovation and research in this era. Based on the research of NSCLC targeted drugs, through the cross-fusion of synthetic biology and artificial intelligence, using the idea of bioengineering, we construct an artificial intelligence assisted medical system and propose a drug selection framework for the personalized selection of NSCLC patients. Under the premise of ensuring the efficacy, considering the economic cost of targeted drugs as an auxiliary decision-making factor, the system predicts the drug effectiveness-cost then. The experiment shows that our method can rely on the provided clinical data to screen drug treatment programs suitable for the patient's conditions and assist doctors in making an efficient diagnosis.",10.1109/JBHI.2021.3133455,https://ieeexplore.ieee.org/document/9640494,IEEE Journal of Biomedical and Health Informatics,Liu Chang;Jia Wu;Nour Moustafa;Ali Kashif Bashir;Keping Yu,2022,5,"@article{2-19766,
  title={AI-Driven Synthetic Biology for Non-Small Cell Lung Cancer Drug Effectiveness-Cost Analysis in Intelligent Assisted Medical Systems},
  author={Liu, Chang and Jia, Wu and Moustafa, Nour and Bashir, Ali Kashif and Yu, Keping},
  year={2022},
  journal={IEEE Journal of Biomedical and Health Informatics},
  doi={10.1109/JBHI.2021.3133455}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-19768,ieee,An Analysis of Fuzzy Group Decision Making to Adopt Emerging Technologies for Fashion Supply Chain Risk Management,"The dynamic and volatile nature of fashion supply chains (FSCs) has drawn increasing attention from academia and the corporate sector. Fashion products, characterized by short lifecycles, impulse buying, and an unpredictable demand, necessitate that FSC partners rapidly offer on-trend products to capture the real-time demand in the shortest time window. To achieve this, FSC partners must embrace technological innovations, collaborate, and establish partnering relations, and share real-time information. Failure to do so will result in obsolete inventory and financial markdowns. In this article, we focus on identifying risk categories in FSC, such as social, environmental, economic, operational, reputational, market, product, disruption, complexity, and workforce, along with relevant mitigation strategies. A survey questionnaire is distributed to six fashion companies in the U.K., employing the fuzzy group analytical hierarchy process for pairwise comparisons to assess the importance of each risk category. Fuzzy failure modes and effect analysis is used to analyze the impact of each risk mitigation strategy on the risk factors. This study supports the extant empirical research in which resource sharing is an effective risk mitigation strategy for fashion risk management. The study participants believe that designing resilient, flexible, agile, and responsive systems with increased levels of communication and information sharing with the help of emerging innovative technologies are the more robust mitigation strategies for fashion risk management. This study has evaluated the role of emerging technologies in risk management, confirming that information communication technology and artificial intelligence are the most effective technologies for managing potential risks in the fashion industry.",10.1109/TEM.2024.3354845,https://ieeexplore.ieee.org/document/10402555,IEEE Transactions on Engineering Management,Piyya Muhammad Rafi-Ul-Shan;Mahdi Bashiri;Muhammad Mustafa Kamal;Sachin Kumar Mangla;Benny Tjahjono,2024,38,"@article{2-19768,
  title={An Analysis of Fuzzy Group Decision Making to Adopt Emerging Technologies for Fashion Supply Chain Risk Management},
  author={Rafi-Ul-Shan, Piyya Muhammad and Bashiri, Mahdi and Kamal, Muhammad Mustafa and Mangla, Sachin Kumar and Tjahjono, Benny},
  year={2024},
  doi={10.1109/TEM.2024.3354845},
  journal={IEEE Transactions on Engineering Management}
}",Empirical contributions,Manufacturing / Industry / Automation,Organizational,"Forecasting, Advising, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19772,ieee,Artificial Intelligence and Information System Resilience to Cope With Supply Chain Disruption,"Artificial Intelligence (AI) as a technology has the potential to interpret and evaluate alternatives where multidimensional data are involved in dynamic situations such as supply chain disruption. This article aims to explore the role of resilient information systems in minimizing the risk magnitude in disruption situations in supply chain operations. The article is conducted in the qualitative mode through a semistructured interview schedule for professionals of supply chains. A thematic analysis has been used to create emerging categories. The findings of this article present critical gaps in current information systems and demonstrate how AI-oriented systems can facilitate the ecosystem of disrupted supply chains to save costs and drive efficiency on multiple parameters. The article also proposes a conceptual framework where organizational values and architectural components can be viewed jointly for quick and adequate business decisions in complex and uncertain disruptions. The framework presents the relationships among AI, information systems, and supply chain disruption. Installing appropriate AI-based data acquisition, processing, and self-training capabilities along with information system infrastructure can help organizations lessen the impact of supply chain disruption while aligning the transportation network and ensuring geographically suitable supply chains and cybersecurity. Finally, the implications for theory and practice with the limitations and scope for future research are described.",10.1109/TEM.2021.3116770,https://ieeexplore.ieee.org/document/9586738,IEEE Transactions on Engineering Management,Shivam Gupta;Sachin Modgil;Régis Meissonier;Yogesh K. Dwivedi,2024,93,"@article{2-19772,
  title={Artificial Intelligence and Information System Resilience to Cope With Supply Chain Disruption},
  author={Gupta, Shivam and Modgil, Sachin and Meissonier, Régis and Dwivedi, Yogesh K.},
  year={2024},
  journal={IEEE Transactions on Engineering Management},
  doi={10.1109/TEM.2021.3116770}
}",Methodological contributions,Manufacturing / Industry / Automation,Institutional,"Advising, Explaining, Monitoring",Decision-maker,"Change trust, Alter decision outcomes, Change cognitive demands, Change affective-perceptual, Shift responsibility, Shape ethical norms",Update AI competence,self-learning mechanisms,"experience and emotional quotient, lack of trained workforce",NA,Yes,Yes
2-19773,ieee,Artificial Intelligence Applications for Responsive Healthcare Supply Chains: A Decision-Making Framework,"Post-COVID-19, the healthcare sector is extensively digitalizing operations by applying emerging technologies such as artificial intelligence (AI). It is evident from previous research that the adoption of AI in the healthcare supply chain results in unmatched benefits. Therefore, this article identifies the enablers for adopting AI in healthcare supply chains and validates them using the Fuzzy–Delphi technique. Furthermore, enablers are prioritized, and the dyadic connections between them are investigated using the fuzzy–DEMATEL approach. In the last phase, the graph theory matrix approach was applied to assess the readiness of a case organization to adopt AI across various healthcare functions. The sensitivity analysis confirms the reliability of the results. Competitive and mimetic pressures, together with government policies and support, were the most influencing enablers. Furthermore, scalability and traceability of information flow across the healthcare supply chain are found to be the most influenced factor by other enablers.",10.1109/TEM.2024.3370377,https://ieeexplore.ieee.org/document/10470429,IEEE Transactions on Engineering Management,Naveen Virmani;Rajesh Kumar Singh;Vaishali Agarwal;Emel Aktas,2024,26,"@article{2-19773,
  title={Artificial Intelligence Applications for Responsive Healthcare Supply Chains: A Decision-Making Framework},
  author={Virmani, Naveen and Singh, Rajesh Kumar and Agarwal, Vaishali and Aktas, Emel},
  year={2024},
  journal={IEEE Transactions on Engineering Management},
  doi={10.1109/TEM.2024.3370377}
}",Methodological contributions,Healthcare / Medicine / Surgery,Organizational,Analyzing,"Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-19783,ieee,Cross-Insight Trader: A Trading Approach Integrating Policies with Diverse Investment Horizons for Portfolio Management,"Deep reinforcement learning (RL) has emerged as a promising approach for portfolio management due to its ability to make sequential decisions. However, applying RL techniques to this domain is still challenging due to the non-stationary nature of financial markets. Existing RL-based solutions fail to consider the intrinsic causes behind this non-stationary, which primarily stem from the involvement of diverse traders with distinct investment horizons and their varied investment strategies. In this paper, we tackle the non-stationary problem by examining its intrinsic causes and propose cross-insight trader, a novel two-step RL-based approach that integrates multiple trading policies with different investment horizons to adapt to the changing market conditions. In the first step, we learn multiple horizon-specific policies by providing each policy with tailored information specific to its investment horizon. This allows each policy to recognize dynamic patterns within its respective horizon and make insightful pre-decisions. In the second step, we learn a cross-insight policy to make the final trade decision by considering the investment pre-decisions made by multiple horizon-specific policies in the first step. To enable effective learning of two types of policies, our approach employs a centralized critic to evaluate the actions performed by both horizon-specific and cross-insight policies. By incorporating multiple insights from different investment horizons into the decision-making process, our approach enhances its adaptability to changing market conditions. Experimental results conducted on three stock markets demonstrate the superiority of our framework.",10.1109/ICDE60146.2024.00356,https://ieeexplore.ieee.org/document/10597980,IEEE International Conference on Data Engineering,Zetao Zheng;Jie Shao;Shilong Deng;Anjie Zhu;Heng Tao Shen;Xiaofang Zhou,2024,0,"@inproceedings{2-19783,
  title     = {Cross-Insight Trader: A Trading Approach Integrating Policies with Diverse Investment Horizons for Portfolio Management},
  author    = {Zheng, Zetao and Shao, Jie and Deng, Shilong and Zhu, Anjie and Shen, Heng Tao and Zhou, Xiaofang},
  year      = {2024},
  doi       = {10.1109/ICDE60146.2024.00356},
  booktitle = {IEEE International Conference on Data Engineering}
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Executing, Monitoring",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19785,ieee,Decentralized Semantic Traffic Control in AVs Using RL and DQN for Dynamic Roadblocks,"Autonomous Vehicles (AVs), furnished with sensors capable of capturing essential vehicle dynamics such as speed, acceleration, and precise location, possess the capacity to execute intelligent maneuvers, including lane changes, in anticipation of approaching roadblocks. Nevertheless, the sheer volume of sensory data and the processing necessary to derive informed decisions can often overwhelm the vehicles, rendering them unable to handle the task independently. Consequently, a common approach in traffic scenarios involves transmitting the data to servers for processing, a practice that introduces challenges, particularly in situations demanding real-time processing. In response to this challenge, we present a novel DL-based semantic traffic control system that entrusts semantic encoding responsibilities to the vehicles themselves. This system processes driving decisions obtained from a Reinforcement Learning (RL) agent, streamlining the decision-making process. Specifically, our framework envisions scenarios where abrupt roadblocks materialize due to factors such as road maintenance, accidents, or vehicle repairs, necessitating vehicles to make determinations concerning lane-keeping or lane-changing actions to navigate past these obstacles. To formulate this scenario mathematically, we employ a Markov Decision Process (MDP) and harness the Deep Q Learning (DQN) algorithm to unearth viable solutions.",10.1109/ICC51166.2024.10622833,https://ieeexplore.ieee.org/document/10622833,IEEE International Conference on Communications,Emanuel Figetakis;Yahuza Bello;Ahmed Refaey;Abdallah Shami,2024,0,"@inproceedings{2-19785,
  title = {Decentralized Semantic Traffic Control in AVs Using RL and DQN for Dynamic Roadblocks},
  author = {Emanuel Figetakis and Yahuza Bello and Ahmed Refaey and Abdallah Shami},
  year = {2024},
  doi = {10.1109/ICC51166.2024.10622833},
  booktitle = {IEEE International Conference on Communications}
}",System/Artifact contributions,Transportation / Mobility / Planning,Individual,Executing,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19789,ieee,Deep Reinforcement Learning-Based Energy-Efficient Decision-Making for Autonomous Electric Vehicle in Dynamic Traffic Environments,"Autonomous driving techniques are promising for improving the energy efficiency of electrified vehicles (EVs) by adjusting driving decisions and optimizing energy requirements. Conventional energy-efficient autonomous driving methods resort to longitudinal velocity planning and fixed-route scenes, which are not sufficient to achieve optimality. In this article, a novel decision-making strategy is proposed for autonomous EVs (AEVs) to maximize energy efficiency by simultaneously considering lane-change and car-following behaviors. Leveraging the deep reinforcement learning (RL) algorithm, the proposed strategy processes complex state information of visual spatial–temporal topology and physical variables to better comprehend surrounding environments. A rule-based safety checker system is developed and integrated downstream of the RL decision-making module to improve lane-change safety. The proposed strategy is trained and evaluated in dynamic driving scenarios with interactive surrounding traffic participants. Simulation results demonstrate that the proposed strategy remarkably improves the EV’s energy economy over state-of-the-art techniques without compromising driving safety or traffic efficiency. Moreover, the results suggest that integrating visual state variables into the RL decision-making strategy is more effective at saving energy in complicated traffic situations.",10.1109/TTE.2023.3290069,https://ieeexplore.ieee.org/document/10164154,IEEE Transactions on Transportation Electrification,Jingda Wu;Ziyou Song;Chen Lv,2024,3,"@article{2-19789,
  title={Deep Reinforcement Learning-Based Energy-Efficient Decision-Making for Autonomous Electric Vehicle in Dynamic Traffic Environments},
  author={Wu, Jingda and Song, Ziyou and Lv, Chen},
  year={2024},
  journal={IEEE Transactions on Transportation Electrification},
  doi={10.1109/TTE.2023.3290069}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,Executing,Developer,NA,NA,NA,NA,NA,Yes,No
2-19791,ieee,DITTO: A Visual Digital Twin for Interventions and Temporal Treatment Outcomes in Head and Neck Cancer,"Digital twin models are of high interest to Head and Neck Cancer (HNC) oncologists, who have to navigate a series of complex treatment decisions that weigh the efficacy of tumor control against toxicity and mortality risks. Evaluating individual risk profiles necessitates a deeper understanding of the interplay between different factors such as patient health, spatial tumor location and spread, and risk of subsequent toxicities that can not be adequately captured through simple heuristics. To support clinicians in better understanding tradeoffs when deciding on treatment courses, we developed DITTO, a digital-twin and visual computing system that allows clinicians to analyze detailed risk profiles for each patient, and decide on a treatment plan. DITTO relies on a sequential Deep Reinforcement Learning digital twin (DT) to deliver personalized risk of both long-term and short-term disease outcome and toxicity risk for HNC patients. Based on a participatory collaborative design alongside oncologists, we also implement several visual explainability methods to promote clinical trust and encourage healthy skepticism when using our system. We evaluate the efficacy of DITTO through quantitative evaluation of performance and case studies with qualitative feedback. Finally, we discuss design lessons for developing clinical visual XAI applications for clinical end users.",10.1109/TVCG.2024.3456160,https://ieeexplore.ieee.org/document/10670532,IEEE Transactions on Visualization and Computer Graphics,Andrew Wentzel;Serageldin Attia;Xinhua Zhang;Guadalupe Canahuate;Clifton David Fuller;G.Elisabeta Marai,2024,20,"@article{2-19791,
  title={DITTO: A Visual Digital Twin for Interventions and Temporal Treatment Outcomes in Head and Neck Cancer},
  author={Wentzel, Andrew and Attia, Serageldin and Zhang, Xinhua and Canahuate, Guadalupe and Fuller, Clifton David and Marai, G. Elisabeta},
  year={2024},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  doi={10.1109/TVCG.2024.3456160}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Explaining, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-19798,ieee,Explaining Autonomous Driving Actions with Visual Question Answering,"The end-to-end learning ability of self-driving vehicles has achieved significant milestones over the last decade owing to rapid advances in deep learning and computer vision algorithms. However, as autonomous driving technology is a safety-critical application of artificial intelligence (AI), road accidents and established regulatory principles necessitate the need for the explainability of intelligent action choices for self-driving vehicles. To facilitate interpretability of decisionmaking in autonomous driving, we present a Visual Question Answering (VQA) framework, which explains driving actions with question-answering-based causal reasoning. To do so, we first collect driving videos in a simulation environment using reinforcement learning (RL) and extract consecutive frames from this log data uniformly for five selected action categories. Further, we manually annotate the extracted frames using question-answer pairs as justifications for the actions chosen in each scenario. Finally, we evaluate the correctness of the VQA-predicted answers for actions on unseen driving scenes. The empirical results suggest that the VQA mechanism can provide support to interpret real-time decisions of autonomous vehicles and help enhance overall driving safety.",10.1109/ITSC57777.2023.10421901,https://ieeexplore.ieee.org/document/10421901,IEEE International Conference on Intelligent Transportation Systems,Shahin Atakishiyev;Mohammad Salameh;Housam Babiker;Randy Goebel,2023,14,"@inproceedings{2-19798,
  title={Explaining Autonomous Driving Actions with Visual Question Answering},
  author={Atakishiyev, Shahin and Salameh, Mohammad and Babiker, Housam and Goebel, Randy},
  year={2023},
  booktitle={IEEE International Conference on Intelligent Transportation Systems},
  doi={10.1109/ITSC57777.2023.10421901}
}",Empirical contributions,Transportation / Mobility / Planning,Individual,"Executing, Explaining",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19801,ieee,Fear-Neuro-Inspired Reinforcement Learning for Safe Autonomous Driving,"Ensuring safety and achieving human-level driving performance remain challenges for autonomous vehicles, especially in safety-critical situations. As a key component of artificial intelligence, reinforcement learning is promising and has shown great potential in many complex tasks; however, its lack of safety guarantees limits its real-world applicability. Hence, further advancing reinforcement learning, especially from the safety perspective, is of great importance for autonomous driving. As revealed by cognitive neuroscientists, the amygdala of the brain can elicit defensive responses against threats or hazards, which is crucial for survival in and adaptation to risky environments. Drawing inspiration from this scientific discovery, we present a fear-neuro-inspired reinforcement learning framework to realize safe autonomous driving through modeling the amygdala functionality. This new technique facilitates an agent to learn defensive behaviors and achieve safe decision making with fewer safety violations. Through experimental tests, we show that the proposed approach enables the autonomous driving agent to attain state-of-the-art performance compared to the baseline agents and perform comparably to 30 certified human drivers, across various safety-critical scenarios. The results demonstrate the feasibility and effectiveness of our framework while also shedding light on the crucial role of simulating the amygdala function in the application of reinforcement learning to safety-critical autonomous driving domains.",10.1109/TPAMI.2023.3322426,https://ieeexplore.ieee.org/document/10273631,IEEE Transactions on Pattern Analysis and Machine Intelligence,Xiangkun He;Jingda Wu;Zhiyu Huang;Zhongxu Hu;Jun Wang;Alberto Sangiovanni-Vincentelli;Chen Lv,2024,78,"@article{2-19801,
  title={Fear-Neuro-Inspired Reinforcement Learning for Safe Autonomous Driving},
  author={He, Xiangkun and Wu, Jingda and Huang, Zhiyu and Hu, Zhongxu and Wang, Jun and Sangiovanni-Vincentelli, Alberto and Lv, Chen},
  year={2024},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={},
  number={},
  pages={},
  doi={10.1109/TPAMI.2023.3322426}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Analyzing, Executing, Monitoring","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-19802,ieee,Fingerprint Networked Reinforcement Learning via Multiagent Modeling for Improving Decision Making in an Urban Food–Energy–Water Nexus,"Food–energy–water (FEW) nexus analyses are critical to sustainable development. Nexus analyses form a unique multiagent decision-making arena that requires using a system engineering approach to simultaneously improve food and water security as well as energy efficiency. To tackle the complexity in decision making within a FEW nexus with respect to dynamic behaviors and interactive logics, we model the FEW nexus as a multiagent system (MAS) under a mixed competitive and cooperative environment from the perspective of a Markov game. Then, we propose a fingerprint networked reinforcement learning (FNRL) framework for the collective learning of a MAS by following the logic flows of human decision making. FNRL can alleviate the problems caused by stationary issues in a MAS environment by integrating a long short-term-memory-driven neural network model into the context of multiagent reinforcement learning (RL) to extract fingerprint information via historical data. Numerical simulations for an urban FEW nexus analysis in Florida (USA) demonstrate that applying the FNRL framework can drive agents in the MAS toward achieving optimality via RL in a dynamic environment.",10.1109/TSMC.2023.3250620,https://ieeexplore.ieee.org/document/10070569,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",Wei Zhang;Andrea Valencia;Ni-Bin Chang,2023,3,"@article{2-19802,
  title={Fingerprint Networked Reinforcement Learning via Multiagent Modeling for Improving Decision Making in an Urban Food–Energy–Water Nexus},
  author={Zhang, Wei and Valencia, Andrea and Chang, Ni-Bin},
  year={2023},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  doi={10.1109/TSMC.2023.3250620}
}",Algorithmic contributions,Environment / Resources / Energy,Institutional,"Analyzing, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19804,ieee,Game-Theoretic Driver Modeling and Decision-Making for Autonomous Driving with Temporal-Spatial Attention-Based Deep Q-Learning,"The safe and efficient navigation of autonomous vehicles in complex traffic scenarios remains a significant challenge. One of the key impediments is the limited effectiveness of current methods in constructing driver models for surrounding vehicles (SVs) that capture diverse decision-making abilities. As a result, implementing decision-making algorithms for the ego vehicle (EV) based on such models could result in unfortunate accidents due to misinterpretation of the decision-making of SVs. To deal with this problem, this paper first introduces a novel decision-making framework that incorporates a game-theoretic driver model based on level-\nkk\n reasoning. For the SVs, driver models at different decision levels are established with an iterative level-\nkk\n reasoning algorithm. As supported by rigorous proof, the strategies of SVs corresponding to these driver models converge to the Nash equilibrium. Following this, utilizing these driver models characterized by mixed decision levels, we propose a temporal-spatial attention-based deep Q-learning (TSA-DQN) algorithm to approximate the optimal policy for the EV and generate respective decisions in various scenarios. It is noteworthy that the proposed algorithm leverages a temporal attention network to estimate the decision level of SVs and spatial attention mechanisms to generate decisions while capturing the interactions between vehicles. Simulations are conducted in representative traffic scenarios, and the results reveal that the proposed driver model based on level-\nkk\n game theory effectively captures the behaviors of diverse drivers. On this basis, the proposed TSA-DQN algorithm demonstrates superior performance over several deep reinforcement learning (DRL) baselines in terms of success rate, efficiency, and safety in driving tasks. Furthermore, the TSA-DQN algorithm is deployed to the real-world hardware platform to illustrate its effectiveness and practical feasibility.",10.1109/TIV.2024.3470910,https://ieeexplore.ieee.org/document/10700678,IEEE Transactions on Intelligent Vehicles,Xiao Zhou;Zengqi Peng;Yusen Xie;Ming Liu;Jun Ma,2024,0,"@article{2-19804,
  title={Game-Theoretic Driver Modeling and Decision-Making for Autonomous Driving with Temporal-Spatial Attention-Based Deep Q-Learning},
  author={Zhou, Xiao and Peng, Zengqi and Xie, Yusen and Liu, Ming and Ma, Jun},
  year={2024},
  journal={IEEE Transactions on Intelligent Vehicles},
  doi={10.1109/TIV.2024.3470910}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19810,ieee,Human-Like Decision Making and Planning for Autonomous Driving with Reinforcement Learning,"One of the main challenges faced by autonomous vehicles operating in mixed traffic scenarios pertains to ensuring safe and efficient navigation, particularly adhering to the implicit rules obeyed by human drivers. In this study, an Adaptive Socially-Compatible Hierarchical Behavior and Motion Planning (ASC-HBMP) framework is proposed to tackle the issue of socially-compatible navigation. ASC-HBMP comprehensively captures the attributes of other traffic participants to guide autonomous vehicles in devising human-like, safe, and efficient trajectories in a socially-compatible manner, striking a balance between safety and efficiency within complex multi-scenarios. Hierarchical Behavior and Motion Planning (HBMP) establishes driving tasks as high-level behavioral decision-making processes that emphasize efficiency, as well as low-level motion planning methods that prioritize safety. HBMP accepts the guidance provided by Adaptive Socially-Compatible Module (ASCM) to generate trajectories with diverse driving style characteristics. Finally, cross-platform simulation experiments are conducted on the SUMO and ROS simulators to validate the navigation performance and generalization capability of our approach in comparison to other baseline methods.",10.1109/ITSC57777.2023.10421908,https://ieeexplore.ieee.org/document/10421908,IEEE International Conference on Intelligent Transportation Systems,Ziqi Zong;Jiamin Shi;Runsheng Wang;Shitao Chen;Nanning Zheng,2023,3,"@inproceedings{2-19810,
  title={Human-Like Decision Making and Planning for Autonomous Driving with Reinforcement Learning},
  author={Zong, Ziqi and Shi, Jiamin and Wang, Runsheng and Chen, Shitao and Zheng, Nanning},
  year={2023},
  doi={10.1109/ITSC57777.2023.10421908},
  booktitle={IEEE International Conference on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Analyzing, Executing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19811,ieee,Hybrid Autonomous Driving Guidance Strategy Combining Deep Reinforcement Learning and Expert System,"The complex traffic and road environment pose considerable challenges to the accuracy, timeliness, and adaptive ability of connected and autonomous vehicles (CAVs) in making driving decisions. This paper uses vehicle collaboration and integrates the adaptive learning capabilities of machine learning and the interpretation capabilities of expert systems (ESs) in a unified architecture to form a hybrid autonomous driving guidance system, which not only solves the “bottleneck” of knowledge acquisition during the construction of expert systems but also solves the “black box” phenomenon of machine learning in the decision-making process. First, an autonomous driving strategy based on deep reinforcement learning (DRL) is proposed for CAVs to make decisions and extract corresponding rules. Next, we design an ES knowledge base expansion method including rule extraction, rule sharing, and rule test. Particularly, vehicular blockchain is adopted to ensure user privacy and data security during the rule-sharing process. Third, hybrid autonomous driving guidance combining ES and machine learning is proposed for CAVs to make accurate and efficient decisions in different driving environments. Once the strategy is well trained, it can effectively guide CAVs to cope with the complex traffic environment. Extensive simulations validate the performance of our proposal in terms of decision-making accuracy, effectiveness, and safety.",10.1109/TITS.2021.3102432,https://ieeexplore.ieee.org/document/9512524,IEEE Transactions on Intelligent Transportation Systems,Yuchuan Fu;Changle Li;F. Richard Yu;Tom H. Luan;Yao Zhang,2022,0,"@article{2-19811,
  title={Hybrid Autonomous Driving Guidance Strategy Combining Deep Reinforcement Learning and Expert System},
  author={Fu, Yuchuan and Li, Changle and Yu, F. Richard and Luan, Tom H. and Zhang, Yao},
  year={2022},
  doi={10.1109/TITS.2021.3102432},
  journal={IEEE Transactions on Intelligent Transportation Systems}
}",System/Artifact contributions,Transportation / Mobility / Planning,Individual,"Explaining, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19813,ieee,Impact-aware Maneuver Decision with Enhanced Perception for Autonomous Vehicle,"Autonomous driving is an emerging technology that has developed rapidly over the last decade. There have been numerous interdisciplinary challenges imposed on the current transportation system by autonomous vehicles. In this paper, we conduct an algorithmic study on the autonomous vehicle decision-making process, which is a fundamental problem in the vehicle automation field and the root cause of most traffic congestion. We propose a perception-and-decision framework, called HEAD, which consists of an enHanced pErception module and a mAneuver Decision module. HEAD aims to enable the autonomous vehicle to perform safe, efficient, and comfortable maneuvers with minimal impact on other vehicles. In the enhanced perception module, a graph-based state prediction model with a strategy of phantom vehicle construction is proposed to predict the one-step future states for multiple surrounding vehicles in parallel, which deals with sensor limitations such as limited detection range and poor detection accuracy under occlusions. Then in the maneuver decision module, a deep reinforcement learning-based model is designed to learn a policy for the autonomous vehicle to perform maneuvers in continuous action space w.r.t. a parameterized action Markov decision process. A hybrid reward function takes into account aspects of safety, efficiency, comfort, and impact to guide the autonomous vehicle to make optimal maneuver decisions. Extensive experiments offer evidence that HEAD can advance the state of the art in terms of both macroscopic and microscopic effectiveness.",10.1109/ICDE55515.2023.00250,https://ieeexplore.ieee.org/document/10184726,IEEE International Conference on Data Engineering,Shuncheng Liu;Yuyang Xia;Xu Chen;Jiandong Xie;Han Su;Kai Zheng,2023,9,"@inproceedings{2-19813,
  title={Impact-aware Maneuver Decision with Enhanced Perception for Autonomous Vehicle},
  author={Liu, Shuncheng and Xia, Yuyang and Chen, Xu and Xie, Jiandong and Su, Han and Zheng, Kai},
  year={2023},
  booktitle={IEEE International Conference on Data Engineering},
  doi={10.1109/ICDE55515.2023.00250}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Forecasting, Executing","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-19814,ieee,Improving Decision Making Skills through Business Simulation Gaming and Expert Systems,"Business simulations as experimental learning tools are common, but they usually train specific predetermined aspects. Research on artificial intelligence among business simulations is rare, and therefore, featured in this paper. The purpose of this research is to explore the use of business simulations games as an experimental learning tool through a contemporary, web-based application featuring artificial intelligence and mobile support. An expert system guides and advises the players, while they manage their virtual business in a competitive market against other participants. The core element is the design process of an artifact, based on the Design Science methodology. The training and learning effects on the participants are observed via the artifact itself in a series of experiments and an additional survey. Twenty-six students in Austria were chosen as the sample group to reveal and measure the improvements in decision making, experimental learning capabilities and the biasing ability of the artificial intelligence.",10.1109/HICSS.2016.107,https://ieeexplore.ieee.org/document/7427284,Hawaii International Conference on System Sciences,Alexander Fuchsberger,2016,15,"@inproceedings{2-19814,
  title={Improving Decision Making Skills through Business Simulation Gaming and Expert Systems},
  author={Fuchsberger, Alexander},
  booktitle={Proceedings of the Hawaii International Conference on System Sciences},
  year={2016},
  doi={10.1109/HICSS.2016.107}
}",System/Artifact contributions,Education / Teaching / Research,Individual,Advising,"Decision-maker, Decision-subject, Knowledge provider","Change trust, Alter decision outcomes, Change affective-perceptual","Update AI competence, Change AI responses","recommendations, visual explanations, textual explanations",NA,"Visual, Textual",Yes,Yes
2-1982,acl,Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections,"Language models will inevitably err in situations with which they are unfamiliar. However, by effectively communicating uncertainties, they can still guide humans toward making sound decisions in those contexts. We demonstrate this idea by developing HEAR, a system that can successfully guide humans in simulated residential environments despite generating potentially inaccurate instructions. Diverging from systems that provide users with only the instructions they generate, HEAR warns users of potential errors in its instructions and suggests corrections. This rich uncertainty information effectively prevents misguidance and reduces the search space for users. Evaluation with 80 users shows that HEAR achieves a 13% increase in success rate and a 29% reduction in final location error distance compared to only presenting instructions to users. Interestingly, we find that offering users possibilities to explore, HEAR motivates them to make more attempts at the task, ultimately leading to a higher success rate. To our best knowledge, this work is the first to show the practical benefits of uncertainty communication in a long-horizon sequential decision-making problem.",10.18653/v1/2024.emnlp-main.42,https://aclanthology.org/2024.emnlp-main.42,Empirical Methods in Natural Language Processing,"Zhao, Lingjun; Nguyen, Khanh Xuan; Daumé Iii, Hal",2024,5,"@inproceedings{2-1982,
  title = {Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections},
  author = {Zhao, Lingjun and Nguyen, Khanh Xuan and Daumé Iii, Hal},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-main.42},
  booktitle = {Empirical Methods in Natural Language Processing}
}",System/Artifact contributions,Generic / Abstract / Domain-agnostic,Individual,"Advising, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19824,ieee,Leveraging Driver Attention for an End-to-End Explainable Decision-Making From Frontal Images,"Explaining the decision made by end-to-end autonomous driving is a difficult task. These approaches take raw sensor data and compute the decision as a black box with large deep learning models. Understanding the output of deep learning is a complex challenge due to the complicated nature of explainability; as data passes through the network, it becomes untraceable, making it difficult to understand. Explainability increases confidence in the decision by making the black box that drives the vehicle transparent to the user inside. Achieving a Level 5 autonomous vehicle necessitates the resolution of that challenging task. In this work, we propose a model that leverages the driver’s attention to obtain explainable decisions based on an attention map and the scene context. Our novel architecture addresses the task of obtaining a decision and its explanation from a single RGB sequence of the driving scene ahead. We base this architecture on the Transformer architecture with some efficiency tricks in order to use it at a reasonable frame rate. Moreover, we integrate in this proposal our previous ARAGAN model, which obtains SOTA attention maps, to improve the performance of the model thanks to understand the sequence as a human does. We train and validate our proposal on the BDD-OIA dataset, achieving on-pair results or even better than other state-of-the-art methods. Additionally, we present a simulation-based proof of concept demonstrating the model’s performance as a copilot in a close-loop vehicle to driver interaction.",10.1109/TITS.2024.3350337,https://ieeexplore.ieee.org/document/10403545,IEEE Transactions on Intelligent Transportation Systems,Javier Araluce;Luis M. Bergasa;Manuel Ocaña;Ángel Llamazares;Elena López-Guillén,2024,0,"@article{2-19824,
  title={Leveraging Driver Attention for an End-to-End Explainable Decision-Making From Frontal Images},
  author={Araluce, Javier and Bergasa, Luis M. and Oca{\~n}a, Manuel and Llamazares, {\'A}ngel and L{\'o}pez-Guill{\'e}n, Elena},
  year={2024},
  doi={10.1109/TITS.2024.3350337},
  journal={IEEE Transactions on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Explaining","Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-19836,ieee,Modeling Human Driving Behavior Through Generative Adversarial Imitation Learning,"An open problem in autonomous vehicle safety validation is building reliable models of human driving behavior in simulation. This work presents an approach to learn neural driving policies from real world driving demonstration data. We model human driving as a sequential decision making problem that is characterized by non-linearity and stochasticity, and unknown underlying cost functions. Imitation learning is an approach for generating intelligent behavior when the cost function is unknown or difficult to specify. Building upon work in inverse reinforcement learning (IRL), Generative Adversarial Imitation Learning (GAIL) aims to provide effective imitation even for problems with large or continuous state and action spaces, such as modeling human driving. This article describes the use of GAIL for learning-based driver modeling. Because driver modeling is inherently a multi-agent problem, where the interaction between agents needs to be modeled, this paper describes a parameter-sharing extension of GAIL called PS-GAIL to tackle multi-agent driver modeling. In addition, GAIL is domain agnostic, making it difficult to encode specific knowledge relevant to driving in the learning process. This paper describes Reward Augmented Imitation Learning (RAIL), which modifies the reward signal to provide domain-specific knowledge to the agent. Finally, human demonstrations are dependent upon latent factors that may not be captured by GAIL. This paper describes Burn-InfoGAIL, which allows for disentanglement of latent variability in demonstrations. Imitation learning experiments are performed using NGSIM, a real-world highway driving dataset. Experiments show that these modifications to GAIL can successfully model highway driving behavior, accurately replicating human demonstrations and generating realistic, emergent behavior in the traffic flow arising from the interaction between driving agents.",10.1109/TITS.2022.3227738,https://ieeexplore.ieee.org/document/9990591,IEEE Transactions on Intelligent Transportation Systems,Raunak Bhattacharyya;Blake Wulfe;Derek J. Phillips;Alex Kuefler;Jeremy Morton;Ransalu Senanayake;Mykel J. Kochenderfer,2023,177,"@article{2-19836,
  title={Modeling Human Driving Behavior Through Generative Adversarial Imitation Learning},
  author={Bhattacharyya, Raunak and Wulfe, Blake and Phillips, Derek J. and Kuefler, Alex and Morton, Jeremy and Senanayake, Ransalu and Kochenderfer, Mykel J.},
  year={2023},
  doi={10.1109/TITS.2022.3227738},
  journal={IEEE Transactions on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Analyzing","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-19838,ieee,Multi-Modal GPT-4 Aided Action Planning and Reasoning for Self-driving Vehicles,"Explainable decision-making is critical for building trust in autonomous vehicles. We investigate the use of a pre-trained large language model (LLM) to derive comprehensible driving decisions from multi-modal time-series data captured by a monocular camera on an autonomous vehicle. Leveraging a graph-of-thought structure, the LLM learns policies that perform robustly while generating natural language rationales. We generate a novel multi-modal dataset with sequential images, scene labels, and driving actions. Results demonstrate our method produces human- understandable explanations for its driving choices, providing transparency. Our work indicates incorporating language-based reasoning enables accountable and transparent decision-making for self-driving cars, making LLM a potential solution for autonomous driving.",10.1109/ICASSP48485.2024.10446745,https://ieeexplore.ieee.org/document/10446745,"IEEE International Conference on Acoustics, Speech and Signal Processing",Fangyuan Chi;Yixiao Wang;Panos Nasiopoulos;Victor C.M. Leung,2024,0,"@inproceedings{2-19838,
  title={Multi-Modal GPT-4 Aided Action Planning and Reasoning for Self-driving Vehicles},
  author={Chi, Fangyuan and Wang, Yixiao and Nasiopoulos, Panos and Leung, Victor C.M.},
  year={2024},
  doi={10.1109/ICASSP48485.2024.10446745},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing}
}",Methodological contributions,Transportation / Mobility / Planning,Individual,"Executing, Explaining",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19842,ieee,Parameterized Decision-Making with Multi-Modality Perception for Autonomous Driving,"Autonomous driving is an emerging technology that has advanced rapidly over the last decade. Modern transportation is expected to benefit greatly from a wise decision-making framework of autonomous vehicles, including the improvement of mobility and the minimization of risks and travel time. However, existing methods either ignore the complexity of environments only fitting straight roads, or ignore the impact on surrounding vehicles during optimization phases, leading to weak environmental adaptability and incomplete optimization objectives. To address these limitations, we propose a pArameterized decision-making framework with mU lti-modality percepTiOn based on deep reinforcement learning, called AUTO. We conduct a comprehensive perception to capture the state features of various traffic participants around the autonomous vehicle, based on which we design a graph-based model to learn a state representation of the multi-modal semantic features. To distinguish between lane-following and lane-changing, we decompose an action of the autonomous vehicle into a parameterized action structure that first decides whether to change lanes and then computes an exact action to execute. A hybrid reward function takes into account aspects of safety, traffic efficiency, passenger comfort, and impact to guide the framework to generate optimal actions. In addition, we design a regularization term and a multi-worker paradigm to enhance the training. Extensive experiments offer evidence that AUTO can advance state-of-the-art in terms of both macroscopic and microscopic effectiveness.",10.1109/ICDE60146.2024.00340,https://ieeexplore.ieee.org/document/10597785,IEEE International Conference on Data Engineering,Yuyang Xia;Shuncheng Liu;Quanlin Yu;Liwei Deng;You Zhang;Han Su;Kai Zheng,2024,53,"@inproceedings{2-19842,
  title={Parameterized Decision-Making with Multi-Modality Perception for Autonomous Driving},
  author={Xia, Yuyang and Liu, Shuncheng and Yu, Quanlin and Deng, Liwei and Zhang, You and Su, Han and Zheng, Kai},
  year={2024},
  doi={10.1109/ICDE60146.2024.00340},
  booktitle={Proceedings of the IEEE International Conference on Data Engineering}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19846,ieee,Prescriptive Learning for Air-Cargo Revenue Management,"We propose RL-Cargo, a revenue management approach for air-cargo that combines machine learning prediction with decision-making using deep reinforcement learning. This approach addresses a problem that is unique to the air-cargo business, namely the wide discrepancy between the quantity (weight or volume) that a shipper will book and the actual amount received at departure time by the airline. The discrepancy results in sub-optimal and inefficient behavior by both the shipper and the airline resulting in an overall loss of potential revenue for the airline. A DQN method using uncertainty bounds from prediction is proposed for decision making under a prescriptive learning framework. Parts of RL-Cargo have been deployed in the production environment of a large commercial airline company. We have validated the benefits of RL-Cargo using a real dataset. More specifically, we have carried out simulations seeded with real data to compare classical Dynamic Programming and Deep Reinforcement Learning techniques on offloading costs and revenue generation. Our results suggest that prescriptive learning which combines prediction with decision-making provides a principled approach for managing the air cargo revenue ecosystem. Furthermore, the proposed approach can be abstracted to many other application domains where decision making needs to be carried out in face of both data and behavioral uncertainty.",10.1109/ICDM50108.2020.00055,https://ieeexplore.ieee.org/document/9338388,IEEE International Conference on Data Mining,Stefano Giovanni Rizzo;Yixian Chen;Linsey Pang;Ji Lucas;Zoi Kaoudi;Jorge Quiane;Sanjay Chawla,2020,7,"@inproceedings{2-19846,
  title={Prescriptive Learning for Air-Cargo Revenue Management},
  author={Rizzo, Stefano Giovanni and Chen, Yixian and Pang, Linsey and Lucas, Ji and Kaoudi, Zoi and Quiane, Jorge and Chawla, Sanjay},
  year={2020},
  doi={10.1109/ICDM50108.2020.00055},
  booktitle={IEEE International Conference on Data Mining}
}",Algorithmic contributions,Transportation / Mobility / Planning,Organizational,"Forecasting, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19850,ieee,"Risk Representation, Perception, and Propensity in an Integrated Human Lane-Change Decision Model","Lane-change decision models with enhanced human-likeness are increasingly important as they are integral in traffic simulations for training autonomous driving algorithms. This work proposes a computational model of driver lane-change decision-making by integrating relevant human features in perception, reasoning, emotion, and decision (PRED). The PRED model describes how drivers make lane-change decisions under collision risk. Here risk is represented by probabilities and outcomes of the possible consequences. The PRED model formulates drivers’ risk perception and risk propensity in its modules: the perception module is modeled with Bayesian inference; the reasoning module is modeled with Newtonian simulation; the emotion and decision module is modeled with the extended regret theory. The PRED model was fitted and tested with an empirical dataset from a naturalistic driving database. The prediction performance of the PRED model ranks higher than the selected benchmarks and is close to the state-of-the-art machine learning models. Moreover, the explicit modeling of risk propensity sheds light on an important question in transportation: what causes human drivers’ risk-taking behaviors? The results support the rationale that downplaying crash consequences is the main contributor.",10.1109/TITS.2022.3207182,https://ieeexplore.ieee.org/document/9901459,IEEE Transactions on Intelligent Transportation Systems,Longsheng Jiang;Dong Chen;Zhaojian Li;Yue Wang,2022,18,"@article{2-19850,
  title={Risk Representation, Perception, and Propensity in an Integrated Human Lane-Change Decision Model},
  author={Jiang, Longsheng and Chen, Dong and Li, Zhaojian and Wang, Yue},
  year={2022},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  doi={10.1109/TITS.2022.3207182}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Forecasting, Analyzing, Explaining",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19851,ieee,Risk-Anticipatory Autonomous Driving Strategies Considering Vehicles’ Weights Based on Hierarchical Deep Reinforcement Learning,"Autonomous vehicles (AVs) have the potential to prevent accidents caused by drivers’ errors and reduce road traffic risks. Due to the nature of heavy vehicles, whose collisions cause more serious crashes, the weights of vehicles need to be considered when making driving strategies aimed at reducing the potential risks and their consequences in the context of autonomous driving. This study develops an autonomous driving strategy based on risk anticipation, considering the weights of surrounding vehicles and using hierarchical deep reinforcement learning. A risk indicator integrating surrounding vehicles’ weights, based on the risk field theory, is proposed and incorporated into autonomous driving decisions. A hybrid action space is designed to allow for left lane changes, right lane changes and car-following, which enables AVs to act more freely and realistically whenever possible. To solve the above hybrid decision-making problem, a hierarchical proximal policy optimization (HPPO) algorithm with an attention mechanism (AT-HPPO) is developed, providing great advantages in maintaining stable performance with high robustness and generalization. An indicator, potential collision energy in conflicts (PCEC), is newly proposed to evaluate the performance of the developed AV driving strategy from the perspective of the consequences of potential accidents. The performance evaluation results in simulation and dataset demonstrate that our model provides driving strategies that reduce both the likelihood and consequences of potential accidents, at the same time maintaining driving efficiency. The developed method is especially meaningful for AVs driving on highways, where heavy vehicles make up a high proportion of the traffic.",10.1109/TITS.2024.3458439,https://ieeexplore.ieee.org/document/10688413,IEEE Transactions on Intelligent Transportation Systems,Di Chen;Hao Li;Zhicheng Jin;Huizhao Tu;Meixin Zhu,2024,13,"@article{2-19851,
  title={Risk-Anticipatory Autonomous Driving Strategies Considering Vehicles’ Weights Based on Hierarchical Deep Reinforcement Learning},
  author={Chen, Di and Li, Hao and Jin, Zhicheng and Tu, Huizhao and Zhu, Meixin},
  year={2024},
  doi={10.1109/TITS.2024.3458439},
  journal={IEEE Transactions on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,"Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-19853,ieee,Road Traffic Law Adaptive Decision-making for Self-Driving Vehicles,"Self-driving vehicles have their own intelligence to drive on open roads. However, vehicle managers, e.g., government or industrial companies, still need a way to tell these self-driving vehicles what behaviors are encouraged or forbidden. Unlike human drivers, current self-driving vehicles cannot understand the traffic laws, and thus rely on the programmers manually writing the corresponding principles into the driving systems. It would be less efficient and hard to adapt some temporary traffic laws, especially when the vehicles use data-driven decision-making algorithms. Besides, current self-driving vehicle systems rarely take traffic law modification into consideration. This work aims to design a road traffic law adaptive decision-making method. The decision-making algorithm is designed based on reinforcement learning, in which the traffic rules are usually implicitly coded in deep neural networks. The main idea is to supply the adaptability to traffic laws of self-driving vehicles by a law-adaptive backup policy. In this work, the natural language-based traffic laws are first translated into a logical expression by the Linear Temporal Logic method. Then, the system will try to monitor in advance whether the self-driving vehicle may break the traffic laws by designing a long-term RL action space. Finally, a sample-based planning method will re-plan the trajectory when the vehicle may break the traffic rules. The method is validated in a Beijing Winter Olympic Lane scenario and an overtaking case, built in CARLA simulator. The results show that by adopting this method, self-driving vehicles can comply with new issued or updated traffic laws effectively. This method helps self-driving vehicles governed by digital traffic laws, which is necessary for the wide adoption of autonomous driving.",10.1109/ITSC55140.2022.9922208,https://ieeexplore.ieee.org/document/9922208,IEEE International Conference on Intelligent Transportation Systems,Jiaxin Lin;Wenhui Zhou;Hong Wang;Zhong Cao;Wenhao Yu;Chengxiang Zhao;Ding Zhao;Diange Yang;Jun Li,2022,33,"@inproceedings{2-19853,
  title     = {Road Traffic Law Adaptive Decision-making for Self-Driving Vehicles},
  author    = {Jiaxin Lin and Wenhui Zhou and Hong Wang and Zhong Cao and Wenhao Yu and Chengxiang Zhao and Ding Zhao and Diange Yang and Jun Li},
  year      = {2022},
  doi       = {10.1109/ITSC55140.2022.9922208},
  booktitle = {IEEE International Conference on Intelligent Transportation Systems}
}",Algorithmic contributions,Transportation / Mobility / Planning,Institutional,"Analyzing, Executing",Decision-subject,NA,NA,NA,NA,NA,Yes,No
2-19857,ieee,Semantic Traffic Law Adaptive Decision-Making for Self-Driving Vehicles,"Facts proved that obeying traffic laws keeps the promise to promote the safety of self-driving vehicles. Current self-driving vehicles usually have fixed algorithms during autonomous driving, however the traffic laws may differ or change in different regions or times, e.g., tidal lanes. It raises a crucial requirement to make self-driving vehicles adapt to the newly received traffic laws. The challenges are that traffic laws are usually semantic and manually designed, but the original algorithms may not always contain the pre-designed interface to adapt to emerging laws. To this end, this work proposes a traffic law adaptive decision-making platform, which uses the linear temporal logic (LTL) formula to consistently describe the semantic traffic laws. Then, an LTL-based reinforcement learning framework is designed to estimate the probability of illegal behavior under different traffic laws. Finally, a law-specific backup policy is designed to maintain the performance threshold by monitoring the probability of illegal behavior. This work takes three typical scenarios where the traffic laws differ for instance to prove the effectiveness of the proposed approach, i.e., law amendment presented by the government, law difference between different regions, and temporary traffic control. The results show that the proposed method can help the original decision-making algorithms adapt to the traffic laws well without pre-defined interfaces. This method provides a way to administer on-road driving self-driving vehicles.",10.1109/TITS.2023.3294579,https://ieeexplore.ieee.org/document/10192480,IEEE Transactions on Intelligent Transportation Systems,Jiaxin Liu;Hong Wang;Zhong Cao;Wenhao Yu;Chengxiang Zhao;Ding Zhao;Diange Yang;Jun Li,2023,33,"@article{2-19857,
  title={Semantic Traffic Law Adaptive Decision-Making for Self-Driving Vehicles},
  author={Liu, Jiaxin and Wang, Hong and Cao, Zhong and Yu, Wenhao and Zhao, Chengxiang and Zhao, Ding and Yang, Diange and Li, Jun},
  year={2023},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  doi={10.1109/TITS.2023.3294579}
}",Methodological contributions,Transportation / Mobility / Planning,Institutional,"Executing, Monitoring",Guardian,NA,NA,NA,NA,NA,Yes,No
2-19859,ieee,Suicidal Pedestrian: Generation of Safety-Critical Scenarios for Autonomous Vehicles,"Developing reliable autonomous driving algorithms poses challenges in testing, particularly when it comes to safety-critical traffic scenarios involving pedestrians. An open question is how to simulate rare events, not necessarily found in autonomous driving datasets or scripted simulations, but which can occur in testing, and, in the end may lead to severe pedestrian related accidents. This paper presents a method for designing a suicidal pedestrian agent within the CARLA simulator, enabling the automatic generation of traffic scenarios for testing safety of autonomous vehicles (AVs) in dangerous situations with pedestrians. The pedestrian is modeled as a reinforcement learning (RL) agent with two custom reward functions that allow the agent to either arbitrarily or with high velocity to collide with the AV. Instead of significantly constraining the initial locations and the pedestrian behavior, we allow the pedestrian and autonomous car to be placed anywhere in the environment and the pedestrian to roam freely to generate diverse scenarios. To assess the performance of the suicidal pedestrian and the target vehicle during testing, we propose three collision-oriented evaluation metrics. Experimental results involving two state-of-the-art autonomous driving algorithms trained end-to-end with imitation learning from sensor data demonstrate the effectiveness of the suicidal pedestrian in identifying decision errors made by autonomous vehicles controlled by the algorithms.",10.1109/ITSC57777.2023.10422034,https://ieeexplore.ieee.org/document/10422034,IEEE International Conference on Intelligent Transportation Systems,Yuhang Yang;Kalle Kujanpää;I Amin Babadi;Joni Pajarinen;Alexander Ilin,2023,0,"@inproceedings{2-19859,
  title     = {Suicidal Pedestrian: Generation of Safety-Critical Scenarios for Autonomous Vehicles},
  author    = {Yuhang Yang and Kalle Kujanpää and I Amin Babadi and Joni Pajarinen and Alexander Ilin},
  year      = {2023},
  doi       = {10.1109/ITSC57777.2023.10422034},
  booktitle = {IEEE International Conference on Intelligent Transportation Systems}
}",Methodological contributions,Transportation / Mobility / Planning,Individual,Executing,"Knowledge provider, Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-19862,ieee,Team Formation for Human-Artificial Intelligence Collaboration in the Workplace: A Goal Programming Model to Foster Organizational Change,"The need for preparing for digital transformation is a recurrent theme in the recent public and academic debate. Artificial Intelligence (AI) has the potential to reduce operational costs, increase efficiency, and improve customer experience. Thus, it is crucial to forming project teams in an organization, in such a way that they will welcome AI in the decision-making process. The current technological revolution is demanding a rapid pace of change to companies and has increased the attention to the role of teams in fostering innovation adoption. We propose an innovative multicriteria model based on the goal programming approach for solving the optimal allocation of individuals to different groups. The model copes with human resources’ cost and human–machine trust. Indeed, we propose an aggregated measure of the attitude towards AI tools to be employed to support tasks in an organization: more precisely our index is based on three dimensions: technology acceptance, technology self-efficacy, and source credibility. By incorporating this index in a team formation model, each team can be guaranteed to have less resistance to change in adopting machine-based decisions, a scenario that will characterize the years to come. The proposed index can also be integrated into more complex and comprehensive models to support business transformation.",10.1109/TEM.2021.3077195,https://ieeexplore.ieee.org/document/9442337,IEEE Transactions on Engineering Management,Davide La Torre;Cinzia Colapinto;Ilaria Durosini;Stefano Triberti,2023,128,"@article{2-19862,
  title={Team Formation for Human-Artificial Intelligence Collaboration in the Workplace: A Goal Programming Model to Foster Organizational Change},
  author={La Torre, Davide and Colapinto, Cinzia and Durosini, Ilaria and Triberti, Stefano},
  year={2023},
  journal={IEEE Transactions on Engineering Management},
  doi={10.1109/TEM.2021.3077195}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy, Everyday / Employment / Public Service",Organizational,"Executing, Collaborating","Decision-subject, Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-19863,ieee,The SMART Framework: Selection of Machine Learning Algorithms With ReplicaTions—A Case Study on the Microvascular Complications of Diabetes,"Over 34 million people in the US have diabetes, a major cause of blindness, renal failure, and amputations. Machine learning (ML) models can predict high-risk patients to help prevent adverse outcomes. Selecting the ‘best’ prediction model for a given disease, population, and clinical application is challenging due to the hundreds of health-related ML models in the literature and the increasing availability of ML methodologies. To support this decision process, we developed the Selection of Machine-learning Algorithms with ReplicaTions (SMART) Framework that integrates building and selecting ML models with decision theory. We build ML models and estimate performance for multiple plausible future populations with a replicated nested cross-validation technique. We rank ML models by simulating decision-maker priorities, using a range of accuracy measures (e.g., AUC) and robustness metrics from decision theory (e.g., minimax Regret). We present the SMART Framework through a case study on the microvascular complications of diabetes using data from the ACCORD clinical trial. We compare selections made by risk-averse, -neutral, and -seeking decision-makers, finding agreement in 80% of the risk-averse and risk-neutral selections, with the risk-averse selections showing consistency for a given complication. We also found that the models that best predicted outcomes in the validation set were those with low performance variance on the testing set, indicating a risk-averse approach in model selection is ideal when there is a potential for high population feature variability. The SMART Framework is a powerful, interactive tool that incorporates various ML algorithms and stakeholder preferences, generalizable to new data and technological advancements.",10.1109/JBHI.2021.3094777,https://ieeexplore.ieee.org/document/9477105,IEEE Journal of Biomedical and Health Informatics,Breanna P. Swan;Maria E. Mayorga;Julie S. Ivy,2022,0,"@article{2-19863,
  title={The SMART Framework: Selection of Machine Learning Algorithms With ReplicaTions—A Case Study on the Microvascular Complications of Diabetes},
  author={Swan, Breanna P. and Mayorga, Maria E. and Ivy, Julie S.},
  year={2022},
  journal={IEEE Journal of Biomedical and Health Informatics},
  volume={},
  number={},
  pages={},
  doi={10.1109/JBHI.2021.3094777}
}",Methodological contributions,Healthcare / Medicine / Surgery,Individual,"Advising, Forecasting","Decision-maker, Decision-subject, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-19865,ieee,Towards Safe Decision-Making for Autonomous Vehicles at Unsignalized Intersections,"Urban autonomous driving decision-making poses a significant challenge, particularly when navigating unsignalized intersections. This complexity mainly stems from the stochastic interactions between various traffic participants. While reinforcement learning (RL)-based decision-making has shown promise, there are valid concerns regarding safety and adaptability. In particular, current RL-based models lack safeguards to prevent issuing potentially unsafe commands in unfamiliar scenarios that are not covered during training. To mitigate this issue, this paper proposes a safe decision-making framework to improve driving safety at unsignalized intersections. First, the RL-based policy is constructed based on the soft actor-critic (SAC) that maps environmental observations into actions directly. Subsequently, the reliability of the SAC policy is measured at run-time via epistemic uncertainty quantification. Furthermore, the risky actions of the RL policy are filtered based on the estimated reliability with integrating a risk-adaptive model predictive control (RAMPC) backup policy. Finally, an unsignalized intersection with occlusion is built via Simulation of Urban Mobility (SUMO). More importantly, several cases are carried out to simulate scenario data distribution shifts, i.e., traffic flow density variation, observation with sensor noise, and observation range decrease, which are not included in the RL policy training process. The results suggest that the proposed method can reduce risk and enhance the safety of autonomous driving at unsignalized intersections",10.1109/TVT.2024.3488749,https://ieeexplore.ieee.org/document/10740674,IEEE Transactions on Vehicular Technology,Kai Yang;Shen Li;Yongli Chen;Dongpu Cao;Xiaolin Tang,2024,3,"@article{2-19865,
  title={Towards Safe Decision-Making for Autonomous Vehicles at Unsignalized Intersections},
  author={Yang, Kai and Li, Shen and Chen, Yongli and Cao, Dongpu and Tang, Xiaolin},
  journal={IEEE Transactions on Vehicular Technology},
  year={2024},
  doi={10.1109/TVT.2024.3488749}
}",Methodological contributions,Transportation / Mobility / Planning,Individual,"Executing, Advising, Monitoring",Developer,NA,NA,NA,NA,NA,Yes,No
2-19866,ieee,Towards Socially Responsive Autonomous Vehicles: A Reinforcement Learning Framework With Driving Priors and Coordination Awareness,"The advent of autonomous vehicles (AVs) alongside human-driven vehicles (HVs) has ushered in an era of mixed traffic flow, presenting a significant challenge: the intricate interaction between these entities within complex driving environments. AVs are expected to have human-like driving behavior to seamlessly integrate into human-dominated traffic systems. To address this issue, we propose a reinforcement learning framework that considers driving priors and Social Coordination Awareness (SCA) to optimize the behavior of AVs. The framework integrates a driving prior learning (DPL) model based on a variational autoencoder to infer the driver's driving priors from human drivers' trajectories. A policy network based on a multi-head attention mechanism is designed to effectively capture the interactive dependencies between AVs and other traffic participants to improve decision-making quality. The introduction of SCA into the autonomous driving decision-making system, and the use of Coordination Tendency (CT) to quantify the willingness of AVs to coordinate the traffic system is explored. The unsignalized intersection serves as a representative experimental scenario. Simulation results show that the proposed framework can not only improve the decision-making quality of AVs but also motivate them to produce social behaviors, with potential benefits for the safety and traffic efficiency of the entire transportation system.",10.1109/TIV.2023.3332080,https://ieeexplore.ieee.org/document/10315232,IEEE Transactions on Intelligent Vehicles,Jiaqi Liu;Donghao Zhou;Peng Hang;Ying Ni;Jian Sun,2024,42,"@article{2-19866,
  title={Towards Socially Responsive Autonomous Vehicles: A Reinforcement Learning Framework With Driving Priors and Coordination Awareness},
  author={Liu, Jiaqi and Zhou, Donghao and Hang, Peng and Ni, Ying and Sun, Jian},
  year={2024},
  doi={10.1109/TIV.2023.3332080},
  journal={IEEE Transactions on Intelligent Vehicles}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Collaborating","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-19872,ieee,Uncertainty-Aware Portfolio Management With Risk-Sensitive Multiagent Network,"As deep neural networks (DNNs) have gained considerable attention in recent years, there have been several cases applying DNNs to portfolio management (PM). Although some researchers have experimentally demonstrated its ability to make a profit, it is still insufficient to use in real situations because existing studies have failed to answer how risky investment decisions are. Furthermore, even though the objective of PM is to maximize returns within a risk tolerance, they overlook the predictive uncertainty of DNNs in the process of risk management. To overcome these limitations, we propose a novel framework called risk-sensitive multiagent network (RSMAN), which includes risk-sensitive agents (RSAs) and a risk adaptive portfolio generator (RAPG). Standard DNNs do not understand the risks of their decision, whereas RSA can take risk-sensitive decisions by estimating market uncertainty and parameter uncertainty. Acting as a trader, this agent is trained via reinforcement learning from dynamic trading simulations to estimate the distribution of reward and via unsupervised learning to assess parameter uncertainty without labeled data. We also present an RAPG that can generate a portfolio fitting the user’s risk appetite without retraining by exploiting the estimated information from the RSAs. We tested our framework on the U.S. and Korean real financial markets to demonstrate the practicality of the RSMAN.",10.1109/TNNLS.2022.3174642,https://ieeexplore.ieee.org/document/9779871,IEEE Transactions on Neural Networks and Learning Systems,Kidon Park;Hong-Gyu Jung;Tae-San Eom;Seong-Whan Lee,2024,19,"@article{2-19872,
  title={Uncertainty-Aware Portfolio Management With Risk-Sensitive Multiagent Network},
  author={Park, Kidon and Jung, Hong-Gyu and Eom, Tae-San and Lee, Seong-Whan},
  year={2024},
  doi={10.1109/TNNLS.2022.3174642},
  journal={IEEE Transactions on Neural Networks and Learning Systems}
}",Algorithmic contributions,Finance / Business / Economy,Operational,Executing,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19874,ieee,Where Shall We Log? Studying and Suggesting Logging Locations in Code Blocks,"Developers write logging statements to generate logs and record system execution behaviors to assist in debugging and software maintenance. However, deciding where to insert logging statements is a crucial yet challenging task. On one hand, logging too little may increase the maintenance difficulty due to missing important system execution information. On the other hand, logging too much may introduce excessive logs that mask the real problems and cause significant performance overhead. Prior studies provide recommendations on logging locations, but such recommendations are only for limited situations( e. g. , exception logging) or at a coarse-grained level( e. g. , method level). Thus, properly helping developers decide finer-grained logging locations for different situations remains an unsolved challenge. In this paper, we tackle the challenge by first conducting a comprehensive manual study on the characteristics of logging locations in seven open-source systems. We uncover six categories of logging locations and find that developers usually insert logging statements to record execution information in various types of code blocks. Based on the observed patterns, we then propose a deep learning framework to automatically suggest logging locations at the block level. We model the source code at the code block level using the syntactic and semantic information. We find that: 1) our models achieve an average of 80. 1% balanced accuracy when suggesting logging locations in blocks; 2) our cross-system logging suggestion results reveal that there might be an implicit logging guideline across systems. Our results show that we may accurately provide finer-grained suggestions on logging locations, and such suggestions may be shared across systems.",NA,https://ieeexplore.ieee.org/document/9286119,IEEE/ACM International Conference on Automated Software Engineering,Zhenhao Li;Tse-Hsun Chen;Weiyi Shang,2020,16,"@inproceedings{2-19874,
  title={Where Shall We Log? Studying and Suggesting Logging Locations in Code Blocks},
  author={Li, Zhenhao and Chen, Tse-Hsun and Shang, Weiyi},
  year={2020},
  booktitle={IEEE/ACM International Conference on Automated Software Engineering}
}",Empirical contributions,Software / Systems / Security,Individual,Advising,"Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-19906,ijcai,"A virtual environment with multi-robot navigation, analytics, and decision support for critical incident investigation","Accidents and attacks that involve chemical, biological, radiological/nuclear or explosive( CBRNE) substances are rare, but can be of high consequence. Since the investigation of such events is not anybodys routine work, a range of AI techniques can reduce investigators cognitive load and support decision-making, including: planning the assessment of the scene; ongoing evaluation and updating of risks; control of autonomous vehicles for collecting images and sensor data; reviewing images/videos for items of interest; identification of anomalies; and retrieval of relevant documentation. Because of the rare and high-risk nature of these events, realistic simulations can support the development and evaluation of AI-based tools. We have developed realistic models of CBRNE scenarios and implemented an initial set of tools.",https://doi.org/10.24963/ijcai.2018/863,https://www.ijcai.org/proceedings/2018/863,International Joint Conference on Artificial Intelligence,David L. Smyth;James Fennell;Sai Abinesh;Nazli B. Karimi;Frank G. Glavin;Ihsan Ullah;Brett Drury;Michael G. Madden,2018,6,"@inproceedings{2-19906,
  title     = {A virtual environment with multi-robot navigation, analytics, and decision support for critical incident investigation},
  author    = {David L. Smyth and James Fennell and Sai Abinesh and Nazli B. Karimi and Frank G. Glavin and Ihsan Ullah and Brett Drury and Michael G. Madden},
  year      = {2018},
  booktitle = {International Joint Conference on Artificial Intelligence},
  doi       = {10.24963/ijcai.2018/863}
}",System/Artifact contributions,"Healthcare / Medicine / Surgery, Defense / Military / Emergency",Organizational,"Monitoring, Advising, Analyzing, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19907,ijcai,Achieving outcome fairness in machine learning models for social decision problems,"Effective complements to human judgment, artificial intelligence techniques have started to aid human decisions in complicated social decision problems across the world. Automated machine learning/deep learning( ML/DL) classification models, through quantitative modeling, have the potential to improve upon human decisions in a wide range of decision problems on social resource allocation such as Medicaid and Supplemental Nutrition Assistance Program( SNAP, commonly referred to as Food Stamps). However, given the limitations in ML/DL model design, these algorithms may fail to leverage various factors for decision making, resulting in improper decisions that allocate resources to individuals who may not be in the most need of such resource. In view of such an issue, we propose in this paper the strategy of fairgroups, based on the legal doctrine of disparate impact, to improve fairness in prediction outcomes. Experiments on various datasets demonstrate that our fairgroup construction method effectively boosts the fairness in automated decision making, while maintaining high prediction accuracy.",https://doi.org/10.24963/ijcai.2020/62,https://www.ijcai.org/proceedings/2020/62,International Joint Conference on Artificial Intelligence,Boli Fang;Miao Jiang;Pei-yi Cheng;Jerry Shen;Yi Fang,2020,32,"@inproceedings{2-19907,
  title={Achieving outcome fairness in machine learning models for social decision problems},
  author={Fang, Boli and Jiang, Miao and Cheng, Pei-yi and Shen, Jerry and Fang, Yi},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2020},
  doi={10.24963/ijcai.2020/62}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service, Healthcare / Medicine / Surgery",Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-19908,ijcai,Acta 2. 0: a modular architecture for multi-layer argumentative analysis of clinical trials,"Evidence-based medicine aims at making decisions about the care of individual patients based on the explicit use of the best available evidence in the patient clinical history and the medical literature results. Argumentation represents a natural way of addressing this task by( i) identifying evidence and claims in text, and( ii) reasoning upon the extracted arguments and their relations to make a decision. ACTA 2. 0 is an automated tool which relies on Argument Mining methods to analyse the abstracts of clinical trials to extract argument components and relations to support evidence-based clinical decision making. ACTA 2. 0 allows also for the identification of PICO( Patient, Intervention, Comparison, Outcome) elements, and the analysis of the effects of an intervention on the outcomes of the study. A REST API is also provided to exploit the tool’s functionalities.",https://doi.org/10.24963/ijcai.2022/859,https://www.ijcai.org/proceedings/2022/859,International Joint Conference on Artificial Intelligence,Benjamin Molinet;Santiago Marro;Elena Cabrio;Serena Villata;Tobias Mayer,2022,1,"@inproceedings{2-19908,
  title = {Acta 2.0: A Modular Architecture for Multi-Layer Argumentative Analysis of Clinical Trials},
  author = {Benjamin Molinet and Santiago Marro and Elena Cabrio and Serena Villata and Tobias Mayer},
  year = {2022},
  booktitle = {International Joint Conference on Artificial Intelligence},
  doi = {https://doi.org/10.24963/ijcai.2022/859}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-19911,ijcai,Adesse: advice explanations in complex repeated decision-making environments,"In the evolving landscape of human-centered AI, fostering a synergistic relationship between humans and AI agents in decision-making processes stands as a paramount challenge. This work considers a problem setup where an intelligent agent comprising a neural network-based prediction component and a deep reinforcement learning component provides advice to a human decision-maker in complex repeated decision-making environments. Whether the human decision-maker would follow the agents advice depends on their beliefs and trust in the agent and on their understanding of the advice itself. To this end, we developed an approach named ADESSE to generate explanations about the adviser agent to improve human trust and decision-making. Computational experiments on a range of environments with varying model sizes demonstrate the applicability and scalability of ADESSE. Furthermore, an interactive game-based user study shows that participants were significantly more satisfied, achieved a higher reward in the game, and took less time to select an action when presented with explanations generated by ADESSE. These findings illuminate the critical role of tailored, human-centered explanations in AI-assisted decision-making.",https://doi.org/10.24963/ijcai.2024/875,https://www.ijcai.org/proceedings/2024/875,International Joint Conference on Artificial Intelligence,Sören Schleibaum;Lu Feng;Sarit Kraus;Jörg P. Müller,2024,5,"@inproceedings{2-19911,
  title={Adesse: advice explanations in complex repeated decision-making environments},
  author={Schleibaum, S{\""o}ren and Feng, Lu and Kraus, Sarit and M{\""u}ller, J{\""o}rg P.},
  year={2024},
  doi={10.24963/ijcai.2024/875},
  booktitle={International Joint Conference on Artificial Intelligence}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Media / Communication / Entertainment",Operational,"Advising, Executing, Explaining",Decision-maker,"Alter decision outcomes, Change affective-perceptual, Change trust, Change cognitive demands",no such info,visual explanations,NA,"Interactive interface, Visual",Yes,Yes
2-19912,ijcai,Agent-based decision support for pain management in primary care settings,"The lack of systematic pain management training and support among primary care physicians( PCPs) limits their ability to provide quality care for patients with pain. Here, we demonstrate an Agent-based Clinical Decision Support System to empower PCPs to leverage knowledge from pain specialists. The system learns a general-purpose representation space on patients, automatically diagnoses pain, recommends therapy and medicine, and suggests a referral program to PCPs in their decision-making tasks.",https://doi.org/10.24963/ijcai.2019/943,https://www.ijcai.org/proceedings/2019/943,International Joint Conference on Artificial Intelligence,Xu Guo;Han Yu;Chunyan Miao;Yiqiang Chen,2019,3,"@inproceedings{2-19912,
  title = {Agent-based decision support for pain management in primary care settings},
  author = {Xu Guo and Han Yu and Chunyan Miao and Yiqiang Chen},
  year = {2019},
  booktitle = {International Joint Conference on Artificial Intelligence},
  doi = {https://doi.org/10.24963/ijcai.2019/943}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Explaining","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-19913,ijcai,Aid-em: adaptive decision support for electricity markets negotiations,"This paper presents the Adaptive Decision Support for Electricity Markets Negotiations( AiD-EM) system. AiD-EM is a multi-agent system that provides decision support to market players by incorporating multiple sub-( agent-based) systems, directed to the decision support of specific problems. These sub-systems make use of different artificial intelligence methodologies, such as machine learning and evolutionary computing, to enable players adaptation in the planning phase and in actual negotiations in auction-based markets and bilateral negotiations. AiD-EM demonstration is enabled by its connection to MASCEM( Multi-Agent Simulator of Competitive Electricity Markets).",https://doi.org/10.24963/ijcai.2019/957,https://www.ijcai.org/proceedings/2019/957,International Joint Conference on Artificial Intelligence,Tiago Pinto;Zita Vale,2019,22,"@inproceedings{2-19913,
  title = {Aid-em: adaptive decision support for electricity markets negotiations},
  author = {Tiago Pinto and Zita Vale},
  year = {2019},
  doi = {https://doi.org/10.24963/ijcai.2019/957},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",System/Artifact contributions,Environment / Resources / Energy,Operational,"Executing, Advising",Decision-maker,no such info,Update AI competence,"prediction of alternative, recommendations",NA,Autonomous System,Yes,Yes
2-19915,ijcai,Automated reasoning for city infrastructure maintenance decision support,"We present an interactive decision support system for assisting city infrastructure inter-asset management. It combines real-time site specific data retrieval, a knowledge base co-created with domain experts and an inference engine capable of predicting potential consequences and risks resulting from the available data and knowledge. The system can give explanations of each consequence, cope with incomplete and uncertain data by making assumptions about what might be the worst case scenario, and making suggestions for further investigation. This demo presents multiple real-world scenarios, and demonstrates how modifying assumptions( parameter values) can lead to different consequences.",https://doi.org/10.24963/ijcai.2018/868,https://www.ijcai.org/proceedings/2018/868,International Joint Conference on Artificial Intelligence,Lijun Wei;Derek R. Magee;Vania Dimitrova;Barry Clarke;Heshan Du;Quratul-ain Mahesar;Kareem Al Ammari;Anthony G. Cohn,2018,5,"@inproceedings{2-19915,
  title={Automated reasoning for city infrastructure maintenance decision support},
  author={Wei, Lijun and Magee, Derek R. and Dimitrova, Vania and Clarke, Barry and Du, Heshan and Mahesar, Quratul-ain and Al Ammari, Kareem and Cohn, Anthony G.},
  year={2018},
  booktitle={International Joint Conference on Artificial Intelligence},
  doi={10.24963/ijcai.2018/868}
}",System/Artifact contributions,Transportation / Mobility / Planning,Institutional,"Explaining, Advising, Forecasting","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-19916,ijcai,Bayesian case-exclusion and personalized explanations for sustainable dairy farming( extended abstract),"Smart agriculture( SmartAg) has emerged as a rich domain for AI-driven decision support systems( DSS) ; however, it is often challenged by user-adoption issues. This paper reports a case-based reasoning( CBR) system, PBI-CBR, that predicts grass growth for dairy farmers, that combines predictive accuracy and explanations to improve user adoption. PBI-CBR’s key novelty is its use of Bayesian methods for case-base maintenance in a regression domain. Experiments report the tradeoff between predictive accuracy and explanatory capability for different variants of PBI-CBR, and how updating Bayesian priors each year improves performance.",https://doi.org/10.24963/ijcai.2020/657,https://www.ijcai.org/proceedings/2020/657,International Joint Conference on Artificial Intelligence,Eoin M. Kenny;Elodie Ruelle;Anne Geoghegan;Laurence Shalloo;Micheál O'Leary;Michael O'Donovan;Mohammed Temraz;Mark T. Keane,2020,0,"@inproceedings{2-19916,
  title = {Bayesian case-exclusion and personalized explanations for sustainable dairy farming (extended abstract)},
  author = {Kenny, Eoin M. and Ruelle, Elodie and Geoghegan, Anne and Shalloo, Laurence and O'Leary, Micheál and O'Donovan, Michael and Temraz, Mohammed and Keane, Mark T.},
  year = {2020},
  doi = {10.24963/ijcai.2020/657},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",System/Artifact contributions,Environment / Resources / Energy,Operational,"Forecasting, Advising, Explaining",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19917,ijcai,Bias on demand: investigating bias with a synthetic data generator,"Machine Learning( ML) systems are increasingly being adopted to make decisions that might have a significant impact on peoples lives. Because these decision-making systems rely on data-driven learning, the risk is that they will systematically propagate the bias embedded in the data. To prevent harmful consequences, it is essential to comprehend how and where bias is introduced and possibly how to mitigate it. We demonstrate Bias on Demand, a framework to generate synthetic datasets with different types of bias, which is available as an open-source toolkit and as a pip package. We include a demo of our proposed synthetic data generator, in which we illustrate experiments on different scenarios to showcase the interconnection between biases and their effect on performance and fairness evaluations. We encourage readers to explore the full paper for a more detailed analysis.",https://doi.org/10.24963/ijcai.2023/828,https://www.ijcai.org/proceedings/2023/828,International Joint Conference on Artificial Intelligence,Joachim Baumann;Alessandro Castelnovo;Andrea Cosentini;Riccardo Crupi;Nicole Inverardi;Daniele Regoli,2023,0,"@inproceedings{2-19917,
  title = {Bias on demand: investigating bias with a synthetic data generator},
  author = {Baumann, Joachim and Castelnovo, Alessandro and Cosentini, Andrea and Crupi, Riccardo and Inverardi, Nicole and Regoli, Daniele},
  year = {2023},
  booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence},
  doi = {10.24963/ijcai.2023/828}
}",System/Artifact contributions,Generic / Abstract / Domain-agnostic,Organizational,"Executing, Auditing",Decision-subject,NA,NA,NA,NA,NA,Yes,No
2-19920,ijcai,Decision making for improving maritime traffic safety using constraint programming,"Maritime navigational safety is of utmost importance to prevent vessel collisions in heavily trafficked ports, and avoid environmental costs. In case of a likely near miss among vessels, port traffic controllers provide assistance for safely navigating the waters, often at very short lead times. A better strategy is to avoid such situations from even happening. To achieve this, we a) formalize the decision model for traffic hotspot mitigation including realistic maritime navigational features and constraints through consultations with domain experts; and b) develop a constraint programming based scheduling approach to mitigate hotspots. We model the problem as a variant of the resource constrained project scheduling problem to adjust vessel movement schedules such that the average delay is minimized and navigational safety constraints are also satisfied. We conduct a thorough evaluation on key performance indicators using real world data, and demonstrate the effectiveness of our approach in mitigating high-risk situations.",https://doi.org/10.24963/ijcai.2019/803,https://www.ijcai.org/proceedings/2019/803,International Joint Conference on Artificial Intelligence,Saumya Bhatnagar;Akshat Kumar;Hoong Chuin Lau,2019,16,"@inproceedings{2-19920,
  title = {Decision Making for Improving Maritime Traffic Safety Using Constraint Programming},
  author = {Bhatnagar, Saumya and Kumar, Akshat and Lau, Hoong Chuin},
  year = {2019},
  doi = {10.24963/ijcai.2019/803},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,"Executing, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-19923,ijcai,Designing behavior-aware ai to improve the human-ai team performance in ai-assisted decision making,"With the rapid development of decision aids that are driven by AI models, the practice of AI-assisted decision making has become increasingly prevalent. To improve the human-AI team performance in decision making, earlier studies mostly focus on enhancing humans capability in better utilizing a given AI-driven decision aid. In this paper, we tackle this challenge through a complementary approach—we aim to train behavior-aware AI by adjusting the AI model underlying the decision aid to account for humans behavior in adopting AI advice. In particular, as humans are observed to accept AI advice more when their confidence in their own judgement is low, we propose to train AI models with a human-confidence-based instance weighting strategy, instead of solving the standard empirical risk minimization problem. Under an assumed, threshold-based model characterizing when humans will adopt the AI advice, we first derive the optimal instance weighting strategy for training AI models. We then validate the efficacy and robustness of our proposed method in improving the human-AI joint decision making performance through systematic experimentation on synthetic datasets. Finally, via randomized experiments with real human subjects along with their actual behavior in adopting the AI advice, we demonstrate that our method can significantly improve the decision making performance of the human-AI team in practice.",https://doi.org/10.24963/ijcai.2024/344,https://www.ijcai.org/proceedings/2024/344,International Joint Conference on Artificial Intelligence,Syed Hasan Amin Mahmood;Zhuoran Lu;Ming Yin,2024,7,"@inproceedings{2-19923,
  title={Designing behavior-aware AI to improve the human-AI team performance in AI-assisted decision making},
  author={Mahmood, Syed Hasan Amin and Lu, Zhuoran and Yin, Ming},
  year={2024},
  doi={10.24963/ijcai.2024/344},
  booktitle={International Joint Conference on Artificial Intelligence}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research",Operational,"Advising, Explaining, Collaborating","Decision-subject, Decision-maker",Alter decision outcomes,Update AI competence,"confidence score, AI advice",NA,"Textual, Visual, Conversational/Natural Language",Yes,Yes
2-19927,ijcai,Faht: an adaptive fairness-aware decision tree classifier,"Automated data-driven decision-making systems are ubiquitous across a wide spread of online as well as offline services. These systems, depend on sophisticated learning algorithms and available data, to optimize the service function for decision support assistance. However, there is a growing concern about the accountability and fairness of the employed models by the fact that often the available historic data is intrinsically discriminatory, i. e. , the proportion of members sharing one or more sensitive attributes is higher than the proportion in the population as a whole when receiving positive classification, which leads to a lack of fairness in decision support system. A number of fairness-aware learning methods have been proposed to handle this concern. However, these methods tackle fairness as a static problem and do not take the evolution of the underlying stream population into consideration. In this paper, we introduce a learning mechanism to design a fair classifier for online stream based decision-making. Our learning model, FAHT( Fairness-Aware Hoeffding Tree) , is an extension of the well-known Hoeffding Tree algorithm for decision tree induction over streams, that also accounts for fairness. Our experiments show that our algorithm is able to deal with discrimination in streaming environments, while maintaining a moderate predictive performance over the stream.",https://doi.org/10.24963/ijcai.2019/205,https://www.ijcai.org/proceedings/2019/205,International Joint Conference on Artificial Intelligence,Wenbin Zhang;Eirini Ntoutsi,2019,148,"@inproceedings{2-19927,
  title     = {Faht: an adaptive fairness-aware decision tree classifier},
  author    = {Wenbin Zhang and Eirini Ntoutsi},
  year      = {2019},
  doi       = {10.24963/ijcai.2019/205},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Institutional,"Analyzing, Executing",Decision-subject,NA,NA,NA,NA,NA,Yes,No
2-19929,ijcai,Forecasting patient outcomes in kidney exchange,"Kidney exchanges allow patients with end-stage renal disease to find a lifesaving living donor by way of an organized market. However, not all patients are equally easy to match, nor are all donor organs of equal quality---some patients are matched within weeks, while others may wait for years with no match offers at all. We propose the first decision-support tool for kidney exchange that takes as input the biological features of a patient-donor pair, and returns( i) the probability of being matched prior to expiry, and( conditioned on a match outcome) , ( ii) the waiting time for and( iii) the organ quality of the matched transplant. This information may be used to inform medical and insurance decisions. We predict all quantities( i, ii, iii) exclusively from match records that are readily available in any kidney exchange using a quantile random forest approach. To evaluate our approach, we developed two state-of-the-art realistic simulators based on data from the United Network for Organ Sharing that sample from the training and test distribution for these learning tasks---in our application these distributions are distinct. We analyze distributional shift through a theoretical lens, and show that the two distributions converge as the kidney exchange nears steady-state. We then show that our approach produces clinically-promising estimates using simulated data. Finally, we show how our approach, in conjunction with tools from the model explainability literature, can be used to calibrate and detect bias in matching policies.",https://doi.org/10.24963/ijcai.2022/701,https://www.ijcai.org/proceedings/2022/701,International Joint Conference on Artificial Intelligence,Naveen Durvasula;Aravind Srinivasan;John Dickerson,2022,2,"@inproceedings{2-19929,
  title = {Forecasting Patient Outcomes in Kidney Exchange},
  author = {Naveen Durvasula and Aravind Srinivasan and John Dickerson},
  year = {2022},
  doi = {10.24963/ijcai.2022/701},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Auditing, Forecasting, Advising, Explaining","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-19932,ijcai,Glass-box: explaining ai decisions with counterfactual statements through conversation with a voice-enabled virtual assistant,"The prevalence of automated decision making, influencing important aspects of our lives -- e. g. , school admission, job market, insurance and banking -- has resulted in increasing pressure from society and regulators to make this process more transparent and ensure its explainability, accountability and fairness. We demonstrate a prototype voice-enabled device, called Glass-Box, which users can question to understand automated decisions and identify the underlying models biases and errors. Our system explains algorithmic predictions with class-contrastive counterfactual statements( e. g. , ``Had a number of conditions been different:... the prediction would change... ) , which show a difference in a particular scenario that causes an algorithm to ``change its mind. Such explanations do not require any prior technical knowledge to understand, hence are suitable for a lay audience, who interact with the system in a natural way -- through an interactive dialogue. We demonstrate the capabilities of the device by allowing users to impersonate a loan applicant who can question the system to understand the automated decision that he received.",https://doi.org/10.24963/ijcai.2018/865,https://www.ijcai.org/proceedings/2018/865,International Joint Conference on Artificial Intelligence,Kacper Sokol;Peter Flach,2018,101,"@inproceedings{2-19932,
  title        = {Glass-box: explaining AI decisions with counterfactual statements through conversation with a voice-enabled virtual assistant},
  author       = {Kacper Sokol and Peter Flach},
  year         = {2018},
  doi          = {10.24963/ijcai.2018/865},
  booktitle    = {International Joint Conference on Artificial Intelligence}
}",System/Artifact contributions,Finance / Business / Economy,Operational,"Explaining, Collaborating","Decision-maker, Decision-subject",no such info,Change AI responses,"counterfactual explanations, important factors",question,"Auditory, Interactive interface",Yes,Yes
2-19934,ijcai,Hierarchical approach to transfer of control in semi-autonomous systems,"Semi-Autonomous Systems( SAS) encapsulate a stochastic decision process explicitly controlled by both an agent and a human, in order to leverage the distinct capabilities of each actor. Planning in SAS must address the challenge of transferring control quickly, safely, and smoothly back-and-forth between the agent and the human. We formally define SAS and the requirements to guarantee that the controlling entities are always able to act competently. We then consider applying the model to Semi-Autonomous VEhicles( SAVE) , using a hierarchical approach in which micro-level transfer-of-control actions are governed by a high-fidelity POMDP model. Macro-level path planning in our hierarchical approach is performed by solving a Stochastic Shortest Path( SSP) problem. We analyze the integrated model and show that it provides the required guarantees. Finally, we test the SAVE model using real-world road data from Open Street Map( OSM) within 10 cities, showing the benefits of the collaboration between the agent and human.",NA,https://www.ijcai.org/Abstract/16/080,International Joint Conference on Artificial Intelligence,Kyle Hollins Wray;Luis Pineda;Shlomo Zilberstein,2016,42,"@inproceedings{2-19934,
  title={Hierarchical approach to transfer of control in semi-autonomous systems},
  author={Wray, Kyle Hollins and Pineda, Luis and Zilberstein, Shlomo},
  year={2016},
  booktitle={Proceedings of the International Joint Conference on Artificial Intelligence}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Analyzing, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19935,ijcai,Human-ai collaboration with bandit feedback,"Human-machine complementarity is important when neither the algorithm nor the human yield dominant performance across all instances in a given domain. Most research on algorithmic decision-making solely centers on the algorithms performance, while recent work that explores human-machine collaboration has framed the decision-making problems as classification tasks. In this paper, we first propose and then develop a solution for a novel human-machine collaboration problem in a bandit feedback setting. Our solution aims to exploit the human-machine complementarity to maximize decision rewards. We then extend our approach to settings with multiple human decision makers. We demonstrate the effectiveness of our proposed methods using both synthetic and real human responses, and find that our methods outperform both the algorithm and the human when they each make decisions on their own. We also show how personalized routing in the presence of multiple human decision-makers can further improve the human-machine team performance.",https://doi.org/10.24963/ijcai.2021/237,https://www.ijcai.org/proceedings/2021/237,International Joint Conference on Artificial Intelligence,Ruijiang Gao;Maytal Saar-Tsechansky;Maria De-Arteaga;Ligong Han;Min Kyung Lee;Matthew Lease,2021,50,"@inproceedings{2-19935,
  title={Human-ai collaboration with bandit feedback},
  author={Gao, Ruijiang and Saar-Tsechansky, Maytal and De-Arteaga, Maria and Han, Ligong and Lee, Min Kyung and Lease, Matthew},
  year={2021},
  doi={10.24963/ijcai.2021/237},
  booktitle={International Joint Conference on Artificial Intelligence}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research",Operational,"Executing, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19936,ijcai,Human-centric justification of machine learning predictions,"Human decision makers in many domains can make use of predictions made by machine learning models in their decision making process, but the usability of these predictions is limited if the human is unable to justify his or her trust in the prediction. We propose a novel approach to producing justifications that is geared towards users without machine learning expertise, focusing on domain knowledge and on human reasoning, and utilizing natural language generation. Through a task-based experiment, we show that our approach significantly helps humans to correctly decide whether or not predictions are accurate, and significantly increases their satisfaction with the justification.",https://doi.org/10.24963/ijcai.2017/202,https://www.ijcai.org/proceedings/2017/202,International Joint Conference on Artificial Intelligence,Or Biran;Kathleen McKeown,2017,0,"@inproceedings{2-19936,
  title = {Human-centric justification of machine learning predictions},
  author = {Or Biran and Kathleen McKeown},
  year = {2017},
  doi = {10.24963/ijcai.2017/202},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",no such info,"Explaining, Forecasting",Decision-maker,"Alter decision outcomes, Change affective-perceptual",no such info,"prediction of alternative, textual explanations",NA,"Textual, Visual, Conversational/Natural Language",Yes,Yes
2-19939,ijcai,Intelligent decision support for improving power management,"With the development and adoption of the electricity information tracking system in China, real-time electricity consumption big data have become available to enable artificial intelligence( AI) to help power companies and the urban management departments to make demand side management decisions. We demonstrate the Power Intelligent Decision Support( PIDS) platform, which can generate Orderly Power Utilization( OPU) decision recommendations and perform Demand Response( DR) implementation management based on a short-term load forecasting model. It can also provide different users with query and application functions to facilitate explainable decision support.",https://doi.org/10.24963/ijcai.2019/965,https://www.ijcai.org/proceedings/2019/965,International Joint Conference on Artificial Intelligence,Yongqing Zheng;Han Yu;Kun Zhang;Yuliang Shi;Cyril Leung;Chunyan Miao,2019,3,"@inproceedings{2-19939,
  title     = {Intelligent decision support for improving power management},
  author    = {Zheng, Yongqing and Yu, Han and Zhang, Kun and Shi, Yuliang and Leung, Cyril and Miao, Chunyan},
  booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence},
  year      = {2019},
  doi       = {10.24963/ijcai.2019/965}
}",System/Artifact contributions,Environment / Resources / Energy,Organizational,"Forecasting, Advising, Explaining","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-19942,ijcai,Learning when to advise human decision makers,"Artificial intelligence( AI) systems are increasingly used for providing advice to facilitate human decision making in a wide range of domains, such as healthcare, criminal justice, and finance. Motivated by limitations of the current practice where algorithmic advice is provided to human users as a constant element in the decision-making pipeline, in this paper we raise the question of when should algorithms provide advice? We propose a novel design of AI systems in which the algorithm interacts with the human user in a two-sided manner and aims to provide advice only when it is likely to be beneficial for the user in making their decision. The results of a large-scale experiment show that our advising approach manages to provide advice at times of need and to significantly improve human decision making compared to fixed, non-interactive, advising approaches. This approach has additional advantages in facilitating human learning, preserving complementary strengths of human decision makers, and leading to more positive responsiveness to the advice.",https://doi.org/10.24963/ijcai.2023/339,https://www.ijcai.org/proceedings/2023/339,International Joint Conference on Artificial Intelligence,Gali Noti;Yiling Chen,2023,3,"@inproceedings{2-19942,
  title = {Learning when to advise human decision makers},
  author = {Noti, Gali and Chen, Yiling},
  year = {2023},
  doi = {10.24963/ijcai.2023/339},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery, Law / Policy / Governance",Operational,"Advising, Collaborating",Decision-maker,"Alter decision outcomes, Change trust, Shape ethical norms","Update AI competence, Change AI responses",AI advice,NA,"Textual, Conversational/Natural Language",Yes,Yes
2-19944,ijcai,Mdfa: multi-differential fairness auditor for black box classifiers,"Machine learning algorithms are increasingly involved in sensitive decision-making processes with adversarial implications on individuals. This paper presents a new tool, mdfa that identifies the characteristics of the victims of a classifiers discrimination. We measure discrimination as a violation of multi-differential fairness. Multi-differential fairness is a guarantee that a black box classifiers outcomes do not leak information on the sensitive attributes of a small group of individuals. We reduce the problem of identifying worst-case violations to matching distributions and predicting where sensitive attributes and classifiers outcomes coincide. We apply mdfa to a recidivism risk assessment classifier widely used in the United States and demonstrate that for individuals with little criminal history, identified African-Americans are three-times more likely to be considered at high risk of violent recidivism than similar non-African-Americans.",https://doi.org/10.24963/ijcai.2019/814,https://www.ijcai.org/proceedings/2019/814,International Joint Conference on Artificial Intelligence,Xavier Gitiaux;Huzefa Rangwala,2019,14,"@inproceedings{2-19944,
  title={Mdfa: multi-differential fairness auditor for black box classifiers},
  author={Xavier Gitiaux and Huzefa Rangwala},
  year={2019},
  doi={10.24963/ijcai.2019/814},
  booktitle={International Joint Conference on Artificial Intelligence}
}",System/Artifact contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance",Institutional,"Auditing, Executing, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19947,ijcai,Neural representation and learning of hierarchical 2-additive choquet integrals,"Multi-Criteria Decision Making( MCDM) aims at modelling expert preferences and assisting decision makers in identifying options best accommodating expert criteria. An instance of MCDM model, the Choquet integral is widely used in real-world applications, due to its ability to capture interactions between criteria while retaining interpretability. Aimed at a better scalability and modularity, hierarchical Choquet integrals involve intermediate aggregations of the interacting criteria, at the cost of a more complex elicitation. The paper presents a machine learning-based approach for the automatic identification of hierarchical MCDM models, composed of 2-additive Choquet integral aggregators and of marginal utility functions on the raw features from data reflecting expert preferences. The proposed NEUR-HCI framework relies on a specific neural architecture, enforcing by design the Choquet model constraints and supporting its end-to-end training. The empirical validation of NEUR-HCI on real-world and artificial benchmarks demonstrates the merits of the approach compared to state-of-art baselines.",https://doi.org/10.24963/ijcai.2020/275,https://www.ijcai.org/proceedings/2020/275,International Joint Conference on Artificial Intelligence,Roman Bresson;Johanne Cohen;Eyke Hüllermeier;Christophe Labreuche;Michèle Sebag,2020,3,"@inproceedings{2-19947,
  title     = {Neural Representation and Learning of Hierarchical 2-Additive Choquet Integrals},
  author    = {Bresson, Roman and Cohen, Johanne and H{\""u}llermeier, Eyke and Labreuche, Christophe and Sebag, Mich{\`e}le},
  year      = {2020},
  doi       = {10.24963/ijcai.2020/275},
  booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Media / Communication / Entertainment",no such info,"Advising, Analyzing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-19952,ijcai,Reassessing evaluation functions in algorithmic recourse: an empirical study from a human-centered perspective,"In this study, we critically examine the foundational premise of algorithmic recourse - a process of generating counterfactual action plans( i. e. , recourses) assisting individuals to reverse adverse decisions made by AI systems. The assumption underlying algorithmic recourse is that individuals accept and act on recourses that minimize the gap between their current and desired states. This assumption, however, remains empirically unverified. To address this issue, we conducted a user study with 362 participants and assessed whether minimizing the distance function, a metric of the gap between the current and desired states, indeed prompts them to accept and act upon suggested recourses. Our findings reveal a nuanced landscape: participants acceptance of recourses did not correlate with the recourse distance. Moreover, participants willingness to act upon recourses peaked at the minimal recourse distance but was otherwise constant. These findings cast doubt on the prevailing assumption of algorithmic recourse research and signal the need to rethink the evaluation functions to pave the way for human-centered recourse generation.",https://doi.org/10.24963/ijcai.2024/876,https://www.ijcai.org/proceedings/2024/876,International Joint Conference on Artificial Intelligence,Tomu Tominaga;Naomi Yamashita;Takeshi Kurashima,2024,1,"@inproceedings{2-19952,
  title     = {Reassessing evaluation functions in algorithmic recourse: an empirical study from a human-centered perspective},
  author    = {Tomu Tominaga and Naomi Yamashita and Takeshi Kurashima},
  year      = {2024},
  booktitle = {International Joint Conference on Artificial Intelligence},
  doi       = {https://doi.org/10.24963/ijcai.2024/876}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Operational,"Advising, Executing",Decision-maker,"Change cognitive demands, Change trust, Alter decision outcomes",no such info,distance metrics,NA,Textual,Yes,Yes
2-19956,ijcai,Spotlight news driven quantitative trading based on trajectory optimization,"News-driven quantitative trading( NQT) has been popularly studied in recent years. Most existing NQT methods are performed in a two-step paradigm, i. e. , first analyzing markets by a financial prediction task and then making trading decisions, which is doomed to failure due to the nearly futile financial prediction task. To bypass the financial prediction task, in this paper, we focus on reinforcement learning( RL) based NQT paradigm, which leverages news to make profitable trading decisions directly. In this paper, we propose a novel NQT framework SpotlightTrader based on decision trajectory optimization, which can effectively stitch together a continuous and flexible sequence of trading decisions to maximize profits. In addition, we enhance this framework by constructing a spotlight-driven state trajectory that obeys a stochastic process with irregular abrupt jumps caused by spotlight news. Furthermore, in order to adapt to non-stationary financial markets, we propose an effective training pipeline for this framework, which blends offline pretraining with online finetuning to balance exploration and exploitation effectively during online tradings. Extensive experiments on three real-world datasets demonstrate our proposed model’s superiority over the state-of-the-art NQT methods.",https://doi.org/10.24963/ijcai.2023/548,https://www.ijcai.org/proceedings/2023/548,International Joint Conference on Artificial Intelligence,Mengyuan Yang;Mengying Zhu;Qianqiao Liang;Xiaolin Zheng;MengHan Wang,2023,0,"@inproceedings{2-19956,
  title = {Spotlight News Driven Quantitative Trading Based on Trajectory Optimization},
  author = {Yang, Mengyuan and Zhu, Mengying and Liang, Qianqiao and Zheng, Xiaolin and Wang, MengHan},
  booktitle = {International Joint Conference on Artificial Intelligence},
  year = {2023},
  doi = {10.24963/ijcai.2023/548}
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Executing, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-19962,ijcai,Towards reducing biases in combining multiple experts online,"In many real life situations, including job and loan applications, gatekeepers must make justified and fair real-time decisions about a person’s fitness for a particular opportunity. In this paper, we aim to accomplish approximate group fairness in an online stochastic decision-making process, where the fairness metric we consider is equalized odds. Our work follows from the classical learning-from-experts scheme, assuming a finite set of classifiers( human experts, rules, options, etc) that cannot be modified. We run separate instances of the algorithm for each label class as well as sensitive groups, where the probability of choosing each instance is optimized for both fairness and regret. Our theoretical results show that approximately equalized odds can be achieved without sacrificing much regret. We also demonstrate the performance of the algorithm on real data sets commonly used by the fairness community.",https://doi.org/10.24963/ijcai.2021/416,https://www.ijcai.org/proceedings/2021/416,International Joint Conference on Artificial Intelligence,Yi Sun;Iván Ramírez Díaz;Alfredo Cuesta Infante;Kalyan Veeramachaneni,2021,2,"@inproceedings{2-19962,
  title={Towards reducing biases in combining multiple experts online},
  author={Sun, Yi and Ram{\'\i}rez D{\'\i}az, Iv{\'a}n and Cuesta Infante, Alfredo and Veeramachaneni, Kalyan},
  year={2021},
  booktitle={Proceedings of the International Joint Conference on Artificial Intelligence},
  doi={10.24963/ijcai.2021/416}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy, Everyday / Employment / Public Service, Law / Policy / Governance",Operational,"Advising, Analyzing","Knowledge provider, Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-19965,ijcai,“My nose is running. ” “Are you also coughing?”: building a medical diagnosis agent with interpretable inquiry logics,"With the rise of telemedicine, the task of developing Dialogue Systems for Medical Diagnosis( DSMD) has received much attention in recent years. Different from early researches that needed to rely on extra human resources and expertise to build the system, recent researches focused on how to build DSMD in a data-driven manner. However, the previous data-driven DSMD methods largely overlooked the system interpretability, which is critical for a medical application, and they also suffered from the data sparsity issue at the same time. In this paper, we explore how to bring interpretability to data-driven DSMD. Specifically, we propose a more interpretable decision process to implement the dialogue manager of DSMD by reasonably mimicking real doctors inquiry logics, and we devise a model with highly transparent components to conduct the inference. Moreover, we collect a new DSMD dataset, which has a much larger scale, more diverse patterns, and is of higher quality than the existing ones. The experiments show that our method obtains 7. 7%, 10. 0%, 3. 0% absolute improvement in diagnosis accuracy respectively on three datasets, demonstrating the effectiveness of its rational decision process and model design. Our codes and the GMD-12 dataset are available at https://github. com/lwgkzl/BR-Agent.",https://doi.org/10.24963/ijcai.2022/592,https://www.ijcai.org/proceedings/2022/592,International Joint Conference on Artificial Intelligence,Wenge Liu;Yi Cheng;Hao Wang;Jianheng Tang;Yafei Liu;Ruihui Zhao;Wenjie Li;Yefeng Zheng;Xiaodan Liang,2022,16,"@inproceedings{2-19965,
  title = {``My nose is running.'' ``Are you also coughing?'': Building a Medical Diagnosis Agent with Interpretable Inquiry Logics},
  author = {Wenge Liu and Yi Cheng and Hao Wang and Jianheng Tang and Yafei Liu and Ruihui Zhao and Wenjie Li and Yefeng Zheng and Xiaodan Liang},
  year = {2022},
  doi = {10.24963/ijcai.2022/592},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",Dataset/Benchmark contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Explaining","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-20005,ijcai,Addressing the long-term impact of ml decisions via policy regret,"Machine Learning( ML) increasingly informs the allocation of opportunities to individuals and communities in areas such as lending, education, employment, and beyond. Such decisions often impact their subjects future characteristics and capabilities in an a priori unknown fashion. The decision-maker, therefore, faces exploration-exploitation dilemmas akin to those in multi-armed bandits. Following prior work, we model communities as arms. To capture the long-term effects of ML-based allocation decisions, we study a setting in which the reward from each arm evolves every time the decision-maker pulls that arm. We focus on reward functions that are initially increasing in the number of pulls but may become( and remain) decreasing after a certain point. We argue that an acceptable sequential allocation of opportunities must take an arms potential for growth into account. We capture these considerations through the notion of policy regret, a much stronger notion than the often-studied external regret, and present an algorithm with provably sub-linear policy regret for sufficiently long time horizons. We empirically compare our algorithm with several baselines and find that it consistently outperforms them, in particular for long time horizons.",https://doi.org/10.24963/ijcai.2021/75,https://www.ijcai.org/proceedings/2021/75,International Joint Conference on Artificial Intelligence,David Lindner;Hoda Heidari;Andreas Krause,2021,3,"@inproceedings{2-20005,
  title     = {Addressing the long-term impact of ML decisions via policy regret},
  author    = {David Lindner and Hoda Heidari and Andreas Krause},
  year      = {2021},
  booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence},
  doi       = {10.24963/ijcai.2021/75}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Operational,"Advising, Executing",Decision-subject,NA,NA,NA,NA,NA,Yes,No
2-20016,ijcai,An exact single-agent task selection algorithm for the crowdsourced logistics,"The trend of moving online in the retail industry has created great pressure for the logistics industry to catch up both in terms of volume and response time. On one hand, volume is fluctuating at greater magnitude, making peaks higher; on the other hand, customers are also expecting shorter response time. As a result, logistics service providers are pressured to expand and keep up with the demands. Expanding fleet capacity, however, is not sustainable as capacity built for the peak seasons would be mostly vacant during ordinary days. One promising solution is to engage crowdsourced workers, who are not employed full-time but would be willing to help with the deliveries if their schedules permit. The challenge, however, is to choose appropriate sets of tasks that would not cause too much disruption from their intended routes, while satisfying each delivery tasks delivery time window requirement. In this paper, we propose a decision-support algorithm to select delivery tasks for a single crowdsourced worker that best fit his/her upcoming route both in terms of additional travel time and the time window requirements at all stops along his/her route, while at the same time satisfies tasks delivery time windows. Our major contributions are in the formulation of the problem and the design of an efficient exact algorithm based on the branch-and-cut approach. The major innovation we introduce is the efficient generation of promising valid inequalities via our separation heuristics. In all numerical instances we study, our approach manages to reach optimality yet with much fewer computational resource requirement than the plain integer linear programming formulation. The greedy heuristic, while efficient in time, only achieves around 40-60% of the optimum in all cases. To illustrate how our solver could help in advancing the sustainability objective, we also quantify the reduction in the carbon footprint.",https://doi.org/10.24963/ijcai.2020/600,https://www.ijcai.org/proceedings/2020/600,International Joint Conference on Artificial Intelligence,Chung-Kyun Han;Shih-Fen Cheng,2020,5,"@inproceedings{2-20016,
  title = {An exact single-agent task selection algorithm for the crowdsourced logistics},
  author = {Chung-Kyun Han and Shih-Fen Cheng},
  year = {2020},
  doi = {10.24963/ijcai.2020/600},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,"Executing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-20018,ijcai,An online learning approach towards far-sighted emergency relief planning under intentional attacks in conflict areas,"A large number of emergency humanitarian rescue demands in conflict areas around the world are accompanied by intentional, persistent and unpredictable attacks on rescuers and supplies. Unfortunately, existing work on humanitarian relief planning mostly ignores this challenge in reality resulting a parlous and short-sighted relief distribution plan to a large extent. To address this, we first propose an offline multi-stage optimization problem of emergency relief planning under intentional attacks, in which all parameters in the game between the rescuer and attacker are supposed to be known or predictable. Then, an online version of this problem is introduced to meet the need of online and irrevocable decision making when those parameters are revealed in an online fashion. To achieve a far-sighted emergency relief planning under attacks, we design an online learning approach which is proven to obtain a near-optimal solution of the offline problem when those online reveled parameters are i. i. d. sampled from an unknown distribution. Finally, extensive experiments on a real anti-Ebola relief planning case based on the data of Ebola outbreak and armed attacks in DRC Congo show the scalability and effectiveness of our approach.",https://doi.org/10.24963/ijcai.2022/649,https://www.ijcai.org/proceedings/2022/649,International Joint Conference on Artificial Intelligence,Haoyu Yang;Kaiming Xiao;Lihua Liu;Hongbin Huang;Weiming Zhang,2022,313,"@inproceedings{2-20018,
  title     = {An online learning approach towards far-sighted emergency relief planning under intentional attacks in conflict areas},
  author    = {Haoyu Yang and Kaiming Xiao and Lihua Liu and Hongbin Huang and Weiming Zhang},
  year      = {2022},
  doi       = {10.24963/ijcai.2022/649},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",Algorithmic contributions,Defense / Military / Emergency,Organizational,"Executing, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-20020,ijcai,Answering binary causal questions through large-scale text mining: an evaluation using cause-effect pairs from human experts,"In this paper, we study the problem of answering questions of type Could X cause Y? where X and Y are general phrases without any constraints. Answering such questions will assist with various decision analysis tasks such as verifying and extending presumed causal associations used for decision making. Our goal is to analyze the ability of an AI agent built using state-of-the-art unsupervised methods in answering causal questions derived from collections of cause-effect pairs from human experts. We focus only on unsupervised and weakly supervised methods due to the difficulty of creating a large enough training set with a reasonable quality and coverage. The methods we examine rely on a large corpus of text derived from news articles, and include methods ranging from large-scale application of classic NLP techniques and statistical analysis to the use of neural network based phrase embeddings and state-of-the-art neural language models.",https://doi.org/10.24963/ijcai.2019/695,https://www.ijcai.org/proceedings/2019/695,International Joint Conference on Artificial Intelligence,Oktie Hassanzadeh;Debarun Bhattacharjya;Mark Feblowitz;Kavitha Srinivas;Michael Perrone;Shirin Sohrabi;Michael Katz,2019,67,"@inproceedings{2-20020,
  title     = {Answering binary causal questions through large-scale text mining: an evaluation using cause-effect pairs from human experts},
  author    = {Hassanzadeh, Oktie and Bhattacharjya, Debarun and Feblowitz, Mark and Srinivas, Kavitha and Perrone, Michael and Sohrabi, Shirin and Katz, Michael},
  year      = {2019},
  booktitle = {International Joint Conference on Artificial Intelligence},
  doi       = {https://doi.org/10.24963/ijcai.2019/695}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,no such info,Executing,"Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-20028,ijcai,Attain: attention-based time-aware lstm networks for disease progression modeling,"Modeling patient disease progression using Electronic Health Records( EHRs) is critical to assist clinical decision making. Long-Short Term Memory( LSTM) is an effective model to handle sequential data, such as EHRs, but it encounters two major limitations when applied to EHRs: it is unable to interpret the prediction results and it ignores the irregular time intervals between consecutive events. To tackle these limitations, we propose an attention-based time-aware LSTM Networks( ATTAIN) , to improve the interpretability of LSTM and to identify the critical previous events for current diagnosis by modeling the inherent time irregularity. We validate ATTAIN on modeling the progression of an extremely challenging disease, septic shock, by using real-world EHRs. Our results demonstrate that the proposed framework outperforms the state-of-the-art models such as RETAIN and T-LSTM. Also, the generated interpretative time-aware attention weights shed some lights on the progression behaviors of septic shock.",https://doi.org/10.24963/ijcai.2019/607,https://www.ijcai.org/proceedings/2019/607,International Joint Conference on Artificial Intelligence,Yuan Zhang;Xi Yang;Julie Ivy;Min Chi,2019,139,"@inproceedings{2-20028,
  title={Attain: attention-based time-aware LSTM networks for disease progression modeling},
  author={Zhang, Yuan and Yang, Xi and Ivy, Julie and Chi, Min},
  year={2019},
  booktitle={International Joint Conference on Artificial Intelligence},
  doi={10.24963/ijcai.2019/607}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Forecasting, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-20032,ijcai,Automated negotiation with gaussian process-based utility models,"Designing agents that can efficiently learn and integrate users preferences into decision making processes is a key challenge in automated negotiation. While accurate knowledge of user preferences is highly desirable, eliciting the necessary information might be rather costly, since frequent user interactions may cause inconvenience. Therefore, efficient elicitation strategies( minimizing elicitation costs) for inferring relevant information are critical. We introduce a stochastic, inverse-ranking utility model compatible with the Gaussian Process preference learning framework and integrate it into a( belief) Markov Decision Process paradigm which formalizes automated negotiation processes with incomplete information. Our utility model, which naturally maps ordinal preferences( inferred from the user) into( random) utility values( with the randomness reflecting the underlying uncertainty) , provides the basic quantitative modeling ingredient for automated( agent-based) negotiation.",https://doi.org/10.24963/ijcai.2019/60,https://www.ijcai.org/proceedings/2019/60,International Joint Conference on Artificial Intelligence,Haralambie Leahu;Michael Kaisers;Tim Baarslag,2019,12,"@inproceedings{2-20032,
  title = {Automated negotiation with gaussian process-based utility models},
  author = {Haralambie Leahu and Michael Kaisers and Tim Baarslag},
  year = {2019},
  doi = {10.24963/ijcai.2019/60},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Operational,"Executing, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-20052,ijcai,Beyond polarity: interpretable financial sentiment analysis with hierarchical query-driven attention,"Sentiment analysis has played a significant role in financial applications in recent years. The informational and emotive aspects of news texts may affect the prices, volatilities, volume of trades, and even potential risks of financial subjects. Previous studies in this field mainly focused on identifying polarity~( e. g. positive or negative). However, as financial decisions broadly require justifications, only plausible polarity cannot provide enough evidence during the decision making processes of humanity. Hence an explainable solution is in urgent demand. In this paper, we present an interpretable neural net framework for financial sentiment analysis. First, we design a hierarchical model to learn the representation of a document from multiple granularities. In addition, we propose a query-driven attention mechanism to satisfy the unique characteristics of financial documents. With the domain specified questions provided by the financial analysts, we can discover different spotlights for queries from different aspects. We conduct extensive experiments on a real-world dataset. The results demonstrate that our framework can learn better representation of the document and unearth meaningful clues on replying different users? preferences. It also outperforms the state-of-the-art methods on sentiment prediction of financial documents.",https://doi.org/10.24963/ijcai.2018/590,https://www.ijcai.org/proceedings/2018/590,International Joint Conference on Artificial Intelligence,Ling Luo;Xiang Ao;Feiyang Pan;Jin Wang;Tong Zhao;Ningzi Yu;Qing He,2018,82,"@inproceedings{2-20052,
  title     = {Beyond Polarity: Interpretable Financial Sentiment Analysis with Hierarchical Query-driven Attention},
  author    = {Ling Luo and Xiang Ao and Feiyang Pan and Jin Wang and Tong Zhao and Ningzi Yu and Qing He},
  year      = {2018},
  booktitle = {International Joint Conference on Artificial Intelligence},
  doi       = {10.24963/ijcai.2018/590}
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Explaining, Collaborating","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-20067,ijcai,Charge prediction by constitutive elements matching of crimes,"Charge prediction is to automatically predict the judgemental charges for legal cases. To convict a person/unit of a charge, the case description must contain matching instances of the constitutive elements( CEs) of that charge. This knowledge of CEs is a valuable guide for the judge in making final decisions. However, it is far from fully exploited for charge prediction in the literature. In this paper we propose a novel method named Constitutive Elements-guided Charge Prediction( CECP). CECP mimics humans charge identification process to extract potential instances of CEs and generate predictions accordingly. It avoids laborious labeling of matching instances of CEs by a novel reinforcement learning module which progressively selects potentially matching sentences for CEs and evaluates their relevance. The final prediction is generated based on the selected sentences and their relevant CEs. Experiments on two real-world datasets show the superiority of CECP over competitive baselines.",https://doi.org/10.24963/ijcai.2022/627,https://www.ijcai.org/proceedings/2022/627,International Joint Conference on Artificial Intelligence,Jie Zhao;Ziyu Guan;Cai Xu;Wei Zhao;Enze Chen,2022,26,"@inproceedings{2-20067,
  title={Charge prediction by constitutive elements matching of crimes},
  author={Zhao, Jie and Guan, Ziyu and Xu, Cai and Zhao, Wei and Chen, Enze},
  year={2022},
  booktitle={International Joint Conference on Artificial Intelligence},
  doi={10.24963/ijcai.2022/627}
}",Algorithmic contributions,Law / Policy / Governance,Operational,"Analyzing, Forecasting, Advising","Knowledge provider, Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-20074,ijcai,Cognitive-inspired conversational-strategy reasoner for socially-aware agents,"In this work we propose a novel module for a dialogue system that allows a conversational agent to utter phrases that do not just meet the systems task intentions, but also work towards achieving the systems social intentions. The module - a Social Reasoner - takes the task goals the system must achieve and decides the appropriate conversational style and strategy with which the dialogue system describes the information the user desires so as to boost the strength of the relationship between the user and system( rapport) , and therefore the users engagement and willingness to divulge the information the agent needs to efficiently and effectively achieve the users goals. Our Social Reasoner is inspired both by analysis of empirical data of friends and stranger dyads engaged in a task, and by prior literature in fields as diverse as reasoning processes in cognitive and social psychology, decision-making, sociolinguistics and conversational analysis. Our experiments demonstrated that, when using the Social Reasoner in a Dialogue System, the rapport level between the user and system increases in more than 35% in comparison with those cases where no Social Reasoner is used.",https://doi.org/10.24963/ijcai.2017/532,https://www.ijcai.org/proceedings/2017/532,International Joint Conference on Artificial Intelligence,Oscar J. Romero;Ran Zhao;Justine Cassell,2017,31,"@inproceedings{2-20074,
  title = {Cognitive-inspired conversational-strategy reasoner for socially-aware agents},
  author = {Oscar J. Romero and Ran Zhao and Justine Cassell},
  year = {2017},
  booktitle = {International Joint Conference on Artificial Intelligence},
  doi = {https://doi.org/10.24963/ijcai.2017/532}
}",Algorithmic contributions,"Media / Communication / Entertainment, Everyday / Employment / Public Service",Individual,"Executing, Explaining, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-20091,ijcai,Comvas: contextual moral values alignment system,"In contemporary society, the integration of artificial intelligence( AI) systems into various aspects of daily life raises significant ethical concerns. One critical aspect is to ensure that AI systems align with the moral values of the endusers. To that end, we introduce the Contextual Moral Value Alignment System, ComVas. Unlike traditional AI systems which have moral values predefined, ComVas empowers users to dynamically select and customize the desired moral values thereby guiding the system’s decision-making process. Through a user-friendly interface, individuals can specify their preferred morals, allowing the system to steer the model’s responses and actions accordingly. ComVas utilizes advanced natural language processing techniques to engage with the users in a meaningful dialogue, understanding their preferences, and reasoning about moral dilemmas in diverse contexts. This demo article showcases the functionality of ComVas, illustrating its potential to foster ethical decision-making in AI systems while respecting individual autonomy and promoting user-centric design principles.",https://doi.org/10.24963/ijcai.2024/1026,https://www.ijcai.org/proceedings/2024/1026,International Joint Conference on Artificial Intelligence,Inkit Padhi;Pierre Dognin;Jesus Rios;Ronny Luss;Swapnaja Achintalwar;Matthew Riemer;Miao Liu;Prasanna Sattigeri;Manish Nagireddy;Kush R. Varshney;Djallel Bouneffouf,2024,12,"@inproceedings{2-20091,
  title     = {Comvas: Contextual Moral Values Alignment System},
  author    = {Inkit Padhi and Pierre Dognin and Jesus Rios and Ronny Luss and Swapnaja Achintalwar and Matthew Riemer and Miao Liu and Prasanna Sattigeri and Manish Nagireddy and Kush R. Varshney and Djallel Bouneffouf},
  year      = {2024},
  booktitle = {International Joint Conference on Artificial Intelligence},
  doi       = {10.24963/ijcai.2024/1026},
}",System/Artifact contributions,Generic / Abstract / Domain-agnostic,Individual,"Explaining, Executing",Decision-maker,Change cognitive demands,"Update AI competence, Shape AI for accountability",information prompts,personalized settings,Interactive interface,Yes,Yes
2-201,aaai,Sustaining Fairness via Incremental Learning,"Machine learning systems are often deployed for making critical decisions like credit lending, hiring, etc. While making decisions, such systems often encode the users demographic information( like gender, age) in their intermediate representations. This can lead to decisions that are biased towards specific demographics. Prior work has focused on debiasing intermediate representations to ensure fair decisions. However, these approaches fail to remain fair with changes in the task or demographic distribution. To ensure fairness in the wild, it is important for a system to adapt to such changes as it accesses new data in an incremental fashion. In this work, we propose to address this issue by introducing the problem of learning fair representations in an incremental learning setting. To this end, we present Fairness-aware Incremental Representation Learning( FaIRL) , a representation learning system that can sustain fairness while incrementally learning new tasks. FaIRL is able to achieve fairness and learn new tasks by controlling the rate-distortion function of the learned representations. Our empirical evaluations show that FaIRL is able to make fair decisions while achieving high performance on the target task, outperforming several baselines.",10.1609/aaai.v37i6.25833,https://ojs.aaai.org/index.php/AAAI/article/view/25833,AAAI Conference on Artificial Intelligence,Somnath Basu Roy Chowdhury;Snigdha Chaturvedi,2023,8,"@inproceedings{2-201,
  title = {Sustaining Fairness via Incremental Learning},
  author = {Somnath Basu Roy Chowdhury and Snigdha Chaturvedi},
  year = {2023},
  doi = {10.1609/aaai.v37i6.25833},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence}
}",Algorithmic contributions,"Everyday / Employment / Public Service, Generic / Abstract / Domain-agnostic, Education / Teaching / Research",Institutional,"Analyzing, Collaborating","Stakeholder, Developer, Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-20122,ijcai,Decision platform for pattern discovery and causal effect estimation in contraceptive discontinuation,"Contraceptive use improves the health of women and children in several ways, yet data shows high rates of discontinuation which is not well understood. We introduce an AI-based decision platform capable of analyzing event data to identify patterns of contraceptive uptake that are unique to a subpopulation of interest. These discriminatory patterns provide valuable, interpretable insights to policy-makers. The sequences then serve as a hypothesis for downstream causal analysis to estimate the effect of specific variables on discontinuation outcomes. Our platform presents a way to visualize, stratify, compare, and perform a causal analysis on covariates that determine contraceptive uptake behavior, and yet is general enough to be extended to a variety of applications.",https://doi.org/10.24963/ijcai.2020/772,https://www.ijcai.org/proceedings/2020/772,International Joint Conference on Artificial Intelligence,Celia Cintas;Ramya Raghavendra;Victor Akinwande;Aisha Walcott-Bryant;Charity Wayua;Komminist Weldemariam,2020,3,"@inproceedings{2-20122,
  title={Decision platform for pattern discovery and causal effect estimation in contraceptive discontinuation},
  author={Cintas, Celia and Raghavendra, Ramya and Akinwande, Victor and Walcott-Bryant, Aisha and Wayua, Charity and Weldemariam, Komminist},
  year={2020},
  booktitle={Proceedings of the International Joint Conference on Artificial Intelligence},
  doi={10.24963/ijcai.2020/772}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Institutional,"Advising, Auditing","Guardian, Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-20151,ijcai,Dynamic task allocation algorithm for hiring workers that learn,"The automation of hiring decisions is a well-studied topic in crowdsourcing. Existing hiring algorithms make a common assumption — that each worker has a level of task competence that is static and does not vary over time. In this work, we explore the question of how to hire workers who can learn over time. Using a medical time series classification task as a case study, we conducted experiments to show that workers performance does improve with experience and that it is possible to model and predict their learning rate. Furthermore, we propose a dynamic hiring mechanism that accounts for workers learning potential. Through both simulation and real-world crowdsourcing data, we show that our hiring procedure can lead to high-accuracy outcomes at lower cost compared to other mechanisms.",NA,https://www.ijcai.org/Abstract/16/538,International Joint Conference on Artificial Intelligence,Shengying Pan;Kate Larson;Josh Bradshaw;Edith Law,2016,22,"@inproceedings{2-20151,
  title={Dynamic task allocation algorithm for hiring workers that learn},
  author={Pan, Shengying and Larson, Kate and Bradshaw, Josh and Law, Edith},
  year={2016},
  booktitle={International Joint Conference on Artificial Intelligence}
}",Algorithmic contributions,Everyday / Employment / Public Service,Operational,"Forecasting, Executing, Analyzing","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-20181,ijcai,Explainable fashion recommendation: a semantic attribute region guided approach,"In fashion recommender systems, each product usually consists of multiple semantic attributes( e. g. , sleeves, collar, etc). When making cloth decisions, people usually show preferences for different semantic attributes( e. g. , the clothes with v-neck collar). Nevertheless, most previous fashion recommendation models comprehend the clothing images with a global content representation and lack detailed understanding of users semantic preferences, which usually leads to inferior recommendation performance. To bridge this gap, we propose a novel Semantic Attribute Explainable Recommender System( SAERS). Specifically, we first introduce a fine-grained interpretable semantic space. We then develop a Semantic Extraction Network( SEN) and Fine-grained Preferences Attention( FPA) module to project users and items into this space, respectively. With SAERS, we are capable of not only providing cloth recommendations for users, but also explaining the reason why we recommend the cloth through intuitive visual attribute semantic highlights in a personalized manner. Extensive experiments conducted on real-world datasets clearly demonstrate the effectiveness of our approach compared with the state-of-the-art methods.",https://doi.org/10.24963/ijcai.2019/650,https://www.ijcai.org/proceedings/2019/650,International Joint Conference on Artificial Intelligence,Min Hou;Le Wu;Enhong Chen;Zhi Li;Vincent W. Zheng;Qi Liu,2019,114,"@inproceedings{2-20181,
  title = {Explainable fashion recommendation: a semantic attribute region guided approach},
  author = {Min Hou and Le Wu and Enhong Chen and Zhi Li and Vincent W. Zheng and Qi Liu},
  year = {2019},
  doi = {10.24963/ijcai.2019/650},
  booktitle = {International Joint Conference on Artificial Intelligence}
}","Algorithmic contributions, System/Artifact contributions",Design / Creativity / Architecture,Individual,"Advising, Explaining",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-20193,ijcai,Fairness and representation in satellite-based poverty maps: evidence of urban-rural disparities and their impacts on downstream policy,"Poverty maps derived from satellite imagery are increasingly used to inform high-stakes policy decisions, such as the allocation of humanitarian aid and the distribution of government resources. Such poverty maps are typically constructed by training machine learning algorithms on a relatively modest amount of ``ground truth data from surveys, and then predicting poverty levels in areas where imagery exists but surveys do not. Using survey and satellite data from ten countries, this paper investigates disparities in representation, systematic biases in prediction errors, and fairness concerns in satellite-based poverty mapping across urban and rural lines, and shows how these phenomena affect the validity of policies based on predicted maps. Our findings highlight the importance of careful error and bias analysis before using satellite-based poverty maps in real-world policy decisions.",https://doi.org/10.24963/ijcai.2023/653,https://www.ijcai.org/proceedings/2023/653,International Joint Conference on Artificial Intelligence,Emily Aiken;Esther Rolf;Joshua Blumenstock,2023,21,"@inproceedings{2-20193,
  title     = {Fairness and representation in satellite-based poverty maps: evidence of urban-rural disparities and their impacts on downstream policy},
  author    = {Emily Aiken and Esther Rolf and Joshua Blumenstock},
  year      = {2023},
  doi       = {10.24963/ijcai.2023/653},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",Empirical contributions,Law / Policy / Governance,Institutional,"Analyzing, Forecasting","Guardian, Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-2020,acl,This Patient Looks Like That Patient: Prototypical Networks for Interpretable Diagnosis Prediction from Clinical Text,"The use of deep neural models for diagnosis prediction from clinical text has shown promising results. However, in clinical practice such models must not only be accurate, but provide doctors with interpretable and helpful results. We introduce ProtoPatient, a novel method based on prototypical networks and label-wise attention with both of these abilities. ProtoPatient makes predictions based on parts of the text that are similar to prototypical patients—providing justifications that doctors understand. We evaluate the model on two publicly available clinical datasets and show that it outperforms existing baselines. Quantitative and qualitative evaluations with medical doctors further demonstrate that the model provides valuable explanations for clinical decision support.",10.18653/v1/2022.aacl-main.14,https://aclanthology.org/2022.aacl-main.14,AACL-IJCNLP,"van Aken, Betty; Papaioannou, Jens-Michalis; Naik, Marcel; Eleftheriadis, Georgios; Nejdl, Wolfgang; Gers, Felix; Loeser, Alexander",2022,22,"@inproceedings{2-2020,
  title = {This Patient Looks Like That Patient: Prototypical Networks for Interpretable Diagnosis Prediction from Clinical Text},
  author = {van Aken, Betty and Papaioannou, Jens-Michalis and Naik, Marcel and Eleftheriadis, Georgios and Nejdl, Wolfgang and Gers, Felix and Loeser, Alexander},
  year = {2022},
  doi = {10.18653/v1/2022.aacl-main.14},
  booktitle = {Proceedings of the AACL-IJCNLP}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Forecasting","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-20205,ijcai,Formalisation and evaluation of properties for consequentialist machine ethics,"As artificial intelligence( AI) technologies continue to influence our daily lives, there has been a growing need to ensure that AI enabled decision making systems adhere to principles expected of human decision makers. This need has given rise to the area of Machine Ethics. We formalise several ethical principles from the philosophical literature in the situation calculus framework to verify the ethical permissibility of a plan. Moreover, we propose several important properties, including some of our own that are intuitively appealing, and a number derived from the social choice literature that would appear to be relevant in evaluating the various approaches. Finally we provide an assessment of how our various situation calculus models of Machine Ethics that we examine satisfy the important properties we have identified.",https://doi.org/10.24963/ijcai.2024/49,https://www.ijcai.org/proceedings/2024/49,International Joint Conference on Artificial Intelligence,Raynaldio Limarga;Yang Song;Abhaya Nayak;David Rajaratnam;Maurice Pagnucco,2024,1,"@inproceedings{2-20205,
  title={Formalisation and evaluation of properties for consequentialist machine ethics},
  author={Limarga, Raynaldio and Song, Yang and Nayak, Abhaya and Rajaratnam, David and Pagnucco, Maurice},
  year={2024},
  booktitle={International Joint Conference on Artificial Intelligence},
  doi={10.24963/ijcai.2024/49}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,no such info,"Executing, Auditing","Guardian, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-2022,acl,"Through the Lens of Split Vote: Exploring Disagreement, Difficulty and Calibration in Legal Case Outcome Classification","In legal decisions, split votes (SV) occur when judges cannot reach a unanimous decision, posing a difficulty for lawyers who must navigate diverse legal arguments and opinions. In high-stakes domains, %as human-AI interaction systems become increasingly important, understanding the alignment of perceived difficulty between humans and AI systems is crucial to build trust. However, existing NLP calibration methods focus on a classifier's awareness of predictive performance, measured against the human majority class, overlooking inherent human label variation (HLV). This paper explores split votes as naturally observable human disagreement and value pluralism. We collect judges' vote distributions from the European Court of Human Rights (ECHR), and present SV-ECHR, a case outcome classification (COC) dataset with SV information. We build a taxonomy of disagreement with SV-specific subcategories. We further assess the alignment of perceived difficulty between models and humans, as well as confidence- and human-calibration of COC models. We observe limited alignment with the judge vote distribution. To our knowledge, this is the first systematic exploration of calibration to human judgements in legal NLP. Our study underscores the necessity for further research on measuring and enhancing model calibration considering HLV in legal decision tasks.",10.18653/v1/2024.acl-long.13,https://aclanthology.org/2024.acl-long.13,Annual Meeting of the Association for Computational Linguistics (ACL),"Xu, Shanshan; T.y.s.s, Santosh; Ichim, Oana; Plank, Barbara; Grabmair, Matthias",2024,7,"@inproceedings{2-2022,
  title = {Through the Lens of Split Vote: Exploring Disagreement, Difficulty and Calibration in Legal Case Outcome Classification},
  author = {Xu, Shanshan and T. y. s., Santosh and Ichim, Oana and Plank, Barbara and Grabmair, Matthias},
  year = {2024},
  doi = {10.18653/v1/2024.acl-long.13},
  booktitle = {Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)}
}",Dataset/Benchmark contributions,Law / Policy / Governance,Organizational,"Forecasting, Analyzing","Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-20234,ijcai,Hierarchical reinforcement learning for pedagogical policy induction( extended abstract),"In interactive e-learning environments such as Intelligent Tutoring Systems, there are pedagogical decisions to make at two main levels of granularity: whole problems and single steps. In recent years, there is growing interest in applying data-driven techniques for adaptive decision making that can dynamically tailor students learning experiences. Most existing data-driven approaches, however, treat these pedagogical decisions equally, or independently, disregarding the long-term impact that tutor decisions may have across these two levels of granularity. In this paper, we propose and apply an offline Gaussian Processes based Hierarchical Reinforcement Learning( HRL) framework to induce a hierarchical pedagogical policy that makes decisions at both problem and step levels. An empirical classroom study shows that the HRL policy is significantly more effective than a Deep Q-Network( DQN) induced policy and a random yet reasonable baseline policy.",https://doi.org/10.24963/ijcai.2020/647,https://www.ijcai.org/proceedings/2020/647,International Joint Conference on Artificial Intelligence,Guojing Zhou;Hamoon Azizsoltani;Markel Sanz Ausin;Tiffany Barnes;Min Chi,2020,58,"@inproceedings{2-20234,
  title     = {Hierarchical Reinforcement Learning for Pedagogical Policy Induction (Extended Abstract)},
  author    = {Guojing Zhou and Hamoon Azizsoltani and Markel Sanz Ausin and Tiffany Barnes and Min Chi},
  booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence},
  year      = {2020},
  doi       = {10.24963/ijcai.2020/647}
}",Algorithmic contributions,Education / Teaching / Research,Operational,Executing,"Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-20251,ijcai,Ibm scenario planning advisor: plan recognition as ai planning in practice,"We present the IBM Research Scenario Planning Advisor( SPA) , a decision support system that allows users to generate diverse alternate scenarios of the future and enhance their ability to imagine the different possible outcomes, including unlikely but potentially impactful futures. The system includes tooling for experts to intuitively encode their domain knowledge, and uses AI Planning to reason about this knowledge and the current state of the world, including news and social media, when generating scenarios.",https://doi.org/10.24963/ijcai.2018/864,https://www.ijcai.org/proceedings/2018/864,International Joint Conference on Artificial Intelligence,Shirin Sohrabi;Michael Katz;Oktie Hassanzadeh;Octavian Udrea;Mark D. Feblowitz,2018,67,"@inproceedings{2-20251,
  title={{IBM Scenario Planning Advisor: Plan Recognition as AI Planning in Practice}},
  author={Shirin Sohrabi and Michael Katz and Oktie Hassanzadeh and Octavian Udrea and Mark D. Feblowitz},
  year={2018},
  booktitle={International Joint Conference on Artificial Intelligence},
  doi={10.24963/ijcai.2018/864}
}",System/Artifact contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service",Organizational,"Analyzing, Advising, Forecasting","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-20262,ijcai,Incentivizing recourse through auditing in strategic classification,"The increasing automation of high-stakes decisions with direct impact on the lives and well-being of individuals raises a number of important considerations. Prominent among these is strategic behavior by individuals hoping to achieve a more desirable outcome. Two forms of such behavior are commonly studied: 1) misreporting of individual attributes, and 2) recourse, or actions that truly change such attributes. The former involves deception, and is inherently undesirable, whereas the latter may well be a desirable goal insofar as it changes true individual qualification. We study misreporting and recourse as strategic choices by individuals within a unified framework. In particular, we propose auditing as a means to incentivize recourse actions over attribute manipulation, and characterize optimal audit policies for two types of principals, utility-maximizing and recourse-maximizing. Additionally, we consider subsidies as an incentive for recourse over manipulation, and show that even a utility-maximizing principal would be willing to devote a considerable amount of audit budget to providing such subsidies. Finally, we consider the problem of optimizing fines for failed audits, and bound the total cost incurred by the population as a result of audits.",https://doi.org/10.24963/ijcai.2023/45,https://www.ijcai.org/proceedings/2023/45,International Joint Conference on Artificial Intelligence,Andrew Estornell;Yatong Chen;Sanmay Das;Yang Liu;Yevgeniy Vorobeychik,2023,17,"@inproceedings{2-20262,
  title = {Incentivizing Recourse through Auditing in Strategic Classification},
  author = {Andrew Estornell and Yatong Chen and Sanmay Das and Yang Liu and Yevgeniy Vorobeychik},
  booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence},
  year = {2023},
  doi = {10.24963/ijcai.2023/45}
}",Theoretical contributions,"Law / Policy / Governance, Generic / Abstract / Domain-agnostic",Operational,"Auditing, Forecasting","Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-20263,ijcai,Incorporating failure events in agents’ decision making to improve user satisfaction,"This paper suggests a new paradigm for the design of collaborative autonomous agents engaged in executing a joint task alongside a human user. In particular, we focus on the way an agents failures should affect its decision making, as far as user satisfaction measures are concerned. Unlike the common practice that considers agent( and more broadly, system) failures solely in the prism of their influence over the agents contribution to the execution of the joint task, we argue that there is an additional, direct, influence which cannot be fully captured by the above measure. Through two series of large-scale controlled experiments with 450 human subjects, recruited through Amazon Mechanical Turk, we show that, indeed, such direct influence holds. Furthermore, we show that the use of a simple agent design that takes into account the direct influence of failures in its decision making yields considerably better user satisfaction, compared to an agent that focuses exclusively on maximizing its absolute contribution to the joint task.",https://doi.org/10.24963/ijcai.2020/215,https://www.ijcai.org/proceedings/2020/215,International Joint Conference on Artificial Intelligence,David Sarne;Chen Rozenshtein,2020,30,"@inproceedings{2-20263,
  title     = {Incorporating failure events in agents' decision making to improve user satisfaction},
  author    = {David Sarne and Chen Rozenshtein},
  year      = {2020},
  doi       = {10.24963/ijcai.2020/215},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Media / Communication / Entertainment",Individual,"Executing, Collaborating","Decision-maker, Decision-subject","Change affective-perceptual, Change trust, Alter decision outcomes",no such info,"absolute score contribution, the number of agent failures",NA,"Textual, Visual, Physical / Embodiment, Autonomous System",Yes,Yes
2-2035,acl,Towards Interactivity and Interpretability: A Rationale-based Legal Judgment Prediction Framework,"Legal judgment prediction (LJP) is a fundamental task in legal AI, which aims to assist the judge to hear the case and determine the judgment. The legal judgment usually consists of the law article, charge, and term of penalty. In the real trial scenario, the judge usually makes the decision step-by-step: first concludes the rationale according to the case's facts and then determines the judgment. Recently, many models have been proposed and made tremendous progress in LJP, but most of them adopt an end-to-end manner that cannot be manually intervened by the judge for practical use. Moreover, existing models lack interpretability due to the neglect of rationale in the prediction process. Following the judge's real trial logic, in this paper, we propose a novel Rationale-based Legal Judgment Prediction (RLJP) framework. In the RLJP framework, the LJP process is split into two steps. In the first phase, the model generates the rationales according to the fact description. Then it predicts the judgment based on the fact and the generated rationales. Extensive experiments on a real-world dataset show RLJP achieves the best results compared to the state-of-the-art models. Meanwhile, the proposed framework provides good interactivity and interpretability which enables practical use.",10.18653/v1/2022.emnlp-main.316,https://aclanthology.org/2022.emnlp-main.316,Empirical Methods in Natural Language Processing,"Wu, Yiquan; Liu, Yifei; Lu, Weiming; Zhang, Yating; Feng, Jun; Sun, Changlong; Wu, Fei; Kuang, Kun",2022,11,"@inproceedings{2-2035,
  title={Towards Interactivity and Interpretability: A Rationale-based Legal Judgment Prediction Framework},
  author={Wu, Yiquan and Liu, Yifei and Lu, Weiming and Zhang, Yating and Feng, Jun and Sun, Changlong and Wu, Fei and Kuang, Kun},
  year={2022},
  booktitle={Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2022.emnlp-main.316}
}",Methodological contributions,Law / Policy / Governance,Operational,"Explaining, Analyzing, Advising","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-20353,ijcai,Model-free model reconciliation,"Designing agents capable of explaining complex sequential decisions remains a significant open problem in human-AI interaction. Recently, there has been a lot of interest in developing approaches for generating such explanations for various decision-making paradigms. One such approach has been the idea of explanation as model-reconciliation. The framework hypothesizes that one of the common reasons for a users confusion could be the mismatch between the users model of the agents task model and the model used by the agent to generate the decisions. While this is a general framework, most works that have been explicitly built on this explanatory philosophy have focused on classical planning settings where the model of users knowledge is available in a declarative form. Our goal in this paper is to adapt the model reconciliation approach to a more general planning paradigm and discuss how such methods could be used when user models are no longer explicitly available. Specifically, we present a simple and easy to learn labeling model that can help an explainer decide what information could help achieve model reconciliation between the user and the agent with in the context of planning with MDPs.",https://doi.org/10.24963/ijcai.2019/83,https://www.ijcai.org/proceedings/2019/83,International Joint Conference on Artificial Intelligence,Sarath Sreedharan;Alberto Olmo Hernandez;Aditya Prasad Mishra;Subbarao Kambhampati,2019,48,"@inproceedings{2-20353,
  title={Model-free model reconciliation},
  author={Sreedharan, Sarath and Hernandez, Alberto Olmo and Mishra, Aditya Prasad and Kambhampati, Subbarao},
  year={2019},
  booktitle={International Joint Conference on Artificial Intelligence},
  doi={10.24963/ijcai.2019/83}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Manufacturing / Industry / Automation",Operational,"Executing, Explaining",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-20372,ijcai,No pizza for you: value-based plan selection in bdi agents,"Autonomous agents are increasingly required to be able to make moral decisions. In these situations, the agent should be able to reason about the ethical bases of the decision and explain its decision in terms of the moral values involved. This is of special importance when the agent is interacting with a user and should understand the value priorities of the user in order to provide adequate support. This paper presents a model of agent behavior that takes into account user preferences and moral values.",https://doi.org/10.24963/ijcai.2017/26,https://www.ijcai.org/proceedings/2017/26,International Joint Conference on Artificial Intelligence,Stephen Cranefield;Michael Winikoff;Virginia Dignum;Frank Dignum,2017,0,"@inproceedings{2-20372,
  title     = {No pizza for you: value-based plan selection in BDI agents},
  author    = {Stephen Cranefield and Michael Winikoff and Virginia Dignum and Frank Dignum},
  year      = {2017},
  doi       = {10.24963/ijcai.2017/26},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,no such info,"Explaining, Analyzing, Executing, Advising","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-20388,ijcai,On the utility of prediction sets in human-ai teams,"Research on human-AI teams usually provides experts with a single label, which ignores the uncertainty in a models recommendation. Conformal prediction( CP) is a well established line of research that focuses on building a theoretically grounded, calibrated prediction set, which may contain multiple labels. We explore how such prediction sets impact expert decision-making in human-AI teams. Our evaluation on human subjects finds that set valued predictions positively impact experts. However, we notice that the predictive sets provided by CP can be very large, which leads to unhelpful AI assistants. To mitigate this, we introduce D-CP, a method to perform CP on some examples and defer to experts. We prove that D-CP can reduce the prediction set size of non-deferred examples. We show how D-CP performs in quantitative and in human subject experiments( n=120). Our results suggest that CP prediction sets improve human-AI team performance over showing the top-1 prediction alone, and that experts find D-CP prediction sets are more useful than CP prediction sets.",https://doi.org/10.24963/ijcai.2022/341,https://www.ijcai.org/proceedings/2022/341,International Joint Conference on Artificial Intelligence,Varun Babbar;Umang Bhatt;Adrian Weller,2022,0,"@inproceedings{2-20388,
  title = {On the utility of prediction sets in human-ai teams},
  author = {Babbar, Varun and Bhatt, Umang and Weller, Adrian},
  year = {2022},
  doi = {10.24963/ijcai.2022/341},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",Methodological contributions,"Education / Teaching / Research, Generic / Abstract / Domain-agnostic",Operational,"Advising, Collaborating","Decision-subject, Decision-maker, Knowledge provider","Alter decision outcomes, Change trust, Change affective-perceptual",no such info,set valued predictions,NA,"Visual, Textual",Yes,Yes
2-204,aaai,The Role of Heuristics and Biases during Complex Choices with an AI Teammate,"Behavioral scientists have classically documented aversion to algorithmic decision aids, from simple linear models to AI. Sentiment, however, is changing and possibly accelerating AI helper usage. AI assistance is, arguably, most valuable when humans must make complex choices. We argue that classic experimental methods used to study heuristics and biases are insufficient for studying complex choices made with AI helpers. We adapted an experimental paradigm designed for studying complex choices in such contexts. We show that framing and anchoring effects impact how people work with an AI helper and are predictive of choice outcomes. The evidence suggests that some participants, particularly those in a loss frame, put too much faith in the AI helper and experienced worse choice outcomes by doing so. The paradigm also generates computational modeling-friendly data allowing future studies of human-AI decision making.",10.1609/aaai.v37i5.25741,https://ojs.aaai.org/index.php/AAAI/article/view/25741,AAAI Conference on Artificial Intelligence,Nikolos Gurney;John H. Miller;David V. Pynadath,2023,0,"@inproceedings{2-204,
  title     = {The Role of Heuristics and Biases during Complex Choices with an AI Teammate},
  author    = {Nikolos Gurney and John H. Miller and David V. Pynadath},
  year      = {2023},
  booktitle = {AAAI Conference on Artificial Intelligence},
  doi       = {10.1609/aaai.v37i5.25741}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research",Individual,"Auditing, Advising, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-20413,ijcai,Optimal prosumer decision-making using factored mdps,"Tackling the decision-making problem faced by a prosumer( i. e. , a producer that is simultaneously a consumer) when selling and buying energy in the emerging smart electricity grid, is of utmost importance for the economic profitability of such a business entity. In this work, we model, for the first time, this problem as a factored Markov Decision Process. By so doing, we are able to rep-resent the problem compactly, and provide an ex-act optimal solution via dynamic programming — notwithstanding its large size. Our model success-fully captures the main aspects of the business decisions of a prosumer corresponding to a community microgrid of any size. Moreover, it includes appropriate sub-models for prosumer production and consumption prediction. Experimental simulations verify the effectiveness of our approach; and show that our exact value iteration solution matches that of a state-of-the-art method for stochastic planning in very large environments, while outperforming it in terms of computation time.",NA,https://www.ijcai.org/Abstract/16/612,International Joint Conference on Artificial Intelligence,Angelos Angelidakis;Georgios Chalkiadakis,2016,0,"@inproceedings{2-20413,
  title     = {Optimal Prosumer Decision-Making Using Factored MDPs},
  author    = {Angelos Angelidakis and Georgios Chalkiadakis},
  year      = {2016},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",Algorithmic contributions,Environment / Resources / Energy,Operational,"Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-20454,ijcai,Profet: predicting the risk of firms from event transcripts,"Financial risk, defined as the chance to deviate from return expectations, is most commonly measured with volatility. Due to its value for investment decision making, volatility prediction is probably among the most important tasks in finance and risk management. Although evidence exists that enriching purely financial models with natural language information can improve predictions of volatility, this task is still comparably underexplored. We introduce PRoFET, the first neural model for volatility prediction jointly exploiting both semantic language representations and a comprehensive set of financial features. As language data, we use transcripts from quarterly recurring events, so-called earnings calls; in these calls, the performance of publicly traded companies is summarized and prognosticated by their management. We show that our proposed architecture, which models verbal context with an attention mechanism, significantly outperforms the previous state-of-the-art and other strong baselines. Finally, we visualize this attention mechanism on the token-level, thus aiding interpretability and providing a use case of PRoFET as a tool for investment decision support.",https://doi.org/10.24963/ijcai.2019/724,https://www.ijcai.org/proceedings/2019/724,International Joint Conference on Artificial Intelligence,Christoph Kilian Theil;Samuel Broscheit;Heiner Stuckenschmidt,2019,6,"@inproceedings{2-20454,
  title = {Profet: Predicting the Risk of Firms from Event Transcripts},
  author = {Christoph Kilian Theil and Samuel Broscheit and Heiner Stuckenschmidt},
  year = {2019},
  booktitle = {International Joint Conference on Artificial Intelligence},
  doi = {10.24963/ijcai.2019/724}
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Forecasting, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-205,aaai,The Value of AI Guidance in Human Examination of Synthetically-Generated Faces,"Face image synthesis has progressed beyond the point at which humans can effectively distinguish authentic faces from synthetically-generated ones. Recently developed synthetic face image detectors boast ``better-than-human discriminative ability, especially those guided by human perceptual intelligence during the models training process. In this paper, we investigate whether these human-guided synthetic face detectors can assist non-expert human operators in the task of synthetic image detection when compared to models trained without human-guidance. We conducted a large-scale experiment with more than 1, 560 subjects classifying whether an image shows an authentic or synthetically-generated face, and annotating regions supporting their decisions. In total, 56, 015 annotations across 3, 780 unique face images were collected. All subjects first examined samples without any AI support, followed by samples given( a) the AIs decision( ``synthetic or ``authentic) , ( b) class activation maps illustrating where the model deems salient for its decision, or( c) both the AIs decision and AIs saliency map. Synthetic faces were generated with six modern Generative Adversarial Networks. Interesting observations from this experiment include:( 1) models trained with human-guidance, which are also more accurate in our experiments, offer better support to human examination of face images when compared to models trained traditionally using cross-entropy loss, ( 2) binary decisions presented to humans results in their better performance than when saliency maps are presented, ( 3) understanding the AIs accuracy helps humans to increase trust in a given model and thus increase their overall accuracy. This work demonstrates that although humans supported by machines achieve better-than-random accuracy of synthetic face detection, the approaches of supplying humans with AI support and of building trust are key factors determining high effectiveness of the human-AI tandem.",10.1609/aaai.v37i5.25734,https://ojs.aaai.org/index.php/AAAI/article/view/25734,AAAI Conference on Artificial Intelligence,Aidan Boyd;Patrick Tinsley;Kevin Bowyer;Adam Czajka,2023,2,"@article{2-205,
  title = {The Value of AI Guidance in Human Examination of Synthetically-Generated Faces},
  author = {Aidan Boyd and Patrick Tinsley and Kevin Bowyer and Adam Czajka},
  year = {2023},
  doi = {10.1609/aaai.v37i5.25734},
  journal = {AAAI Conference on Artificial Intelligence}
}",Empirical contributions,Media / Communication / Entertainment,Operational,"Advising, Explaining, Forecasting","Knowledge provider, Decision-maker","Alter decision outcomes, Change affective-perceptual, Change trust",Update AI competence,"classification, visual analysis",NA,"Textual, Visual",Yes,Yes
2-20511,ijcai,Shared autonomy systems with stochastic operator models,"We consider shared autonomy systems where multiple operators( AI and human) , can interact with the environment, e. g. by controlling a robot. The decision problem for the shared autonomy system is to select which operator takes control at each timestep, such that a reward specifying the intended system behaviour is maximised. The performance of the human operator is influenced by unobserved factors, such as fatigue or skill level. Therefore, the system must reason over stochastic models of operator performance. We present a framework for stochastic operators in shared autonomy systems( SO-SAS) , where we represent operators using rich, partially observable models. We formalise SO-SAS as a mixed-observability Markov decision process, where environment states are fully observable and internal operator states are hidden. We test SO-SAS on a simulated domain and a computer game, empirically showing it results in better performance compared to traditional formulations of shared autonomy systems.",https://doi.org/10.24963/ijcai.2022/640,https://www.ijcai.org/proceedings/2022/640,International Joint Conference on Artificial Intelligence,Clarissa Costen;Marc Rigter;Bruno Lacerda;Nick Hawes,2022,0,"@inproceedings{2-20511,
  title = {Shared autonomy systems with stochastic operator models},
  author = {Clarissa Costen and Marc Rigter and Bruno Lacerda and Nick Hawes},
  year = {2022},
  doi = {10.24963/ijcai.2022/640},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Operational,"Executing, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-20567,ijcai,Toward policy explanations for multi-agent reinforcement learning,"Advances in multi-agent reinforcement learning( MARL) enable sequential decision making for a range of exciting multi-agent applications such as cooperative AI and autonomous driving. Explaining agent decisions is crucial for improving system transparency, increasing user satisfaction, and facilitating human-agent collaboration. However, existing works on explainable reinforcement learning mostly focus on the single-agent setting and are not suitable for addressing challenges posed by multi-agent environments. We present novel methods to generate two types of policy explanations for MARL:( i) policy summarization about the agent cooperation and task sequence, and( ii) language explanations to answer queries about agent behavior. Experimental results on three MARL domains demonstrate the scalability of our methods. A user study shows that the generated explanations significantly improve user performance and increase subjective ratings on metrics such as user satisfaction.",https://doi.org/10.24963/ijcai.2022/16,https://www.ijcai.org/proceedings/2022/16,International Joint Conference on Artificial Intelligence,Kayla Boggess;Sarit Kraus;Lu Feng,2022,2,"@inproceedings{2-20567,
  title = {Toward Policy Explanations for Multi-Agent Reinforcement Learning},
  author = {Boggess, Kayla and Kraus, Sarit and Feng, Lu},
  year = {2022},
  doi = {10.24963/ijcai.2022/16},
  booktitle = {International Joint Conference on Artificial Intelligence}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,no such info,"Executing, Explaining, Advising","Decision-maker, Developer","Alter decision outcomes, Change affective-perceptual",Update AI competence,"policy summarization about the agent cooperation and task sequence, language explanations to answer queries about agent behavior",NA,"Textual, Visual, Conversational/Natural Language",Yes,Yes
2-20619,ijcai,Strategic adversarial attacks in ai-assisted decision making to reduce human trust and reliance,"With the increased integration of AI technologies in human decision making processes, adversarial attacks on AI models become a greater concern than ever before as they may significantly hurt humans’ trust in AI models and decrease the effectiveness of human-AI collaboration. While many adversarial attack methods have been proposed to decrease the performance of an AI model, limited attention has been paid on understanding how these attacks will impact the human decision makers interacting with the model, and accordingly, how to strategically deploy adversarial attacks to maximize the reduction of human trust and reliance. In this paper, through a human-subject experiment, we first show that in AI-assisted decision making, the timing of the attacks largely influences how much humans decrease their trust in and reliance on AI—the decrease is particularly salient when attacks occur on decision making tasks that humans are highly confident themselves. Based on these insights, we next propose an algorithmic framework to infer the human decision maker’s hidden trust in the AI model and dynamically decide when the attacker should launch an attack to the model. Our evaluations show that following the proposed approach, attackers deploy more efficient attacks and achieve higher utility than adopting other baseline strategies.",https://doi.org/10.24963/ijcai.2023/337,https://www.ijcai.org/proceedings/2023/337,International Joint Conference on Artificial Intelligence,Zhuoran Lu;Zhuoyan Li;Chun-Wei Chiang;Ming Yin,2023,0,"@inproceedings{2-20619,
  title = {Strategic adversarial attacks in AI-assisted decision making to reduce human trust and reliance},
  author = {Zhuoran Lu and Zhuoyan Li and Chun-Wei Chiang and Ming Yin},
  year = {2023},
  booktitle = {International Joint Conference on Artificial Intelligence},
  doi = {https://doi.org/10.24963/ijcai.2023/337}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research",Individual,"Advising, Collaborating",Decision-maker,"Change trust, Alter decision outcomes, Change affective-perceptual",Update AI competence,"attack, prediction of alternative",NA,Visual,Yes,Yes
2-20622,ijcai,The effects of ai biases and explanations on human decision fairness: a case study of bidding in rental housing markets,"The use of AI-based decision aids in diverse domains has inspired many empirical investigations into how AI models’ decision recommendations impact humans’ decision accuracy in AI-assisted decision making, while explorations on the impacts on humans’ decision fairness are largely lacking despite their clear importance. In this paper, using a real-world business decision making scenario—bidding in rental housing markets—as our testbed, we present an experimental study on understanding how the bias level of the AI-based decision aid as well as the provision of AI explanations affect the fairness level of humans’ decisions, both during and after their usage of the decision aid. Our results suggest that when people are assisted by an AI-based decision aid, both the higher level of racial biases the decision aid exhibits and surprisingly, the presence of AI explanations, result in more unfair human decisions across racial groups. Moreover, these impacts are partly made through triggering humans’ “disparate interactions” with AI. However, regardless of the AI bias level and the presence of AI explanations, when people return to make independent decisions after their usage of the AI-based decision aid, their decisions no longer exhibit significant unfairness across racial groups.",https://doi.org/10.24963/ijcai.2023/343,https://www.ijcai.org/proceedings/2023/343,International Joint Conference on Artificial Intelligence,Xinru Wang;Chen Liang;Ming Yin,2023,0,"@inproceedings{2-20622,
  title = {The Effects of AI Biases and Explanations on Human Decision Fairness: A Case Study of Bidding in Rental Housing Markets},
  author = {Xinru Wang and Chen Liang and Ming Yin},
  year = {2023},
  booktitle = {International Joint Conference on Artificial Intelligence},
  doi = {https://doi.org/10.24963/ijcai.2023/343}
}",Empirical contributions,Finance / Business / Economy,Operational,"Explaining, Executing, Advising","Decision-maker, Decision-subject","Shape ethical norms, Alter decision outcomes",no such info,explanations,NA,"Conversational/Natural Language, Textual, Visual",Yes,Yes
2-20626,ijcai,Randomized adversarial imitation learning for autonomous driving,"With the evolution of various advanced driver assistance system( ADAS) platforms, the design of autonomous driving system is becoming more complex and safety-critical. The autonomous driving system simultaneously activates multiple ADAS functions; and thus it is essential to coordinate various ADAS functions. This paper proposes a randomized adversarial imitation learning( RAIL) method that imitates the coordination of autonomous vehicle equipped with advanced sensors. The RAIL policies are trained through derivative-free optimization for the decision maker that coordinates the proper ADAS functions, e. g. , smart cruise control and lane keeping system. Especially, the proposed method is also able to deal with the LIDAR data and makes decisions in complex multi-lane highways and multi-agent environments.",https://doi.org/10.24963/ijcai.2019/638,https://www.ijcai.org/proceedings/2019/638,International Joint Conference on Artificial Intelligence,MyungJae Shin;Joongheon Kim,2019,39,"@inproceedings{2-20626,
  title = {Randomized adversarial imitation learning for autonomous driving},
  author = {Shin, MyungJae and Kim, Joongheon},
  year = {2019},
  booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence},
  doi = {10.24963/ijcai.2019/638}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,"Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-20702,jstor,A right to a human decision,"Recent advances in computational technologies have spurred anxiety about a shift of power from human to machine decision makers. From welfare and employment to bail and other risk assessments, state actors increasingly lean on machine-learning tools to directly allocate goods and coercion among individuals. Machine-learning tools are perceived to be eclipsing, even extinguishing, human agency in ways that compromise important individual interests. An emerging legal response to such worries is to assert a novel right to a human decision. European law embraced the idea in the General Data Protection Regulation. American law, especially in the criminal justice domain, is moving in the same direction. But no jurisdiction has defined with precision what that right entails, furnished a clear justification for its creation, or defined its appropriate domain. This Article investigates the legal possibilities and normative appeal of a right to a human decision. I begin by sketching its conditions of technological plausibility. This requires the specification of both a feasible domain of machine decisions and the margins along which machine decisions are distinct from human ones. With this technological accounting in hand, I analyze the normative stakes of a right to a human decision. I consider four potential normative justifications: (a) a concern with population-wide accuracy; (b) a grounding in individual subjects' interests in participation and reason giving; (c) arguments about the insufficiently reasoned or individuated quality of state action; and (d) objections grounded in negative externalities. None of these yields a general justification for a right to a human decision. Instead of being derived from normative first principles, limits to machine decision making are appropriately found in the technical constraints on predictive instruments. Within that domain, concerns about due process, privacy, and discrimination in machine decisions are typically best addressed through a justiciable right to a well-calibrated machine decision.",10.2307/27074704,http://www.jstor.org/stable/27074704,Virginia Law Review,Aziz Z. Huq,2020,250,"@article{2-20702,
  title={A right to a human decision},
  author={Huq, Aziz Z.},
  year={2020},
  doi={10.2307/27074704},
  journal={Virginia Law Review}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Institutional,Executing,"Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-20733,jstor,Accountable algorithms,"Many important decisions historically made by people are now made by computers. Algorithms count votes, approve loan and credit card applications, target citizens or neighborhoods for police scrutiny, select taxpayers for IRS audit, grant or deny immigration visas, and more. The accountability mechanisms and legal standards that govern such decision processes have not kept pace with technology. The tools currently available to policymakers, legislators, and courts were developed to oversee human decisionmakers and often fail when applied to computers instead. For example, how do you judge the intent of a piece of software? Because automated decision systems can return potentially incorrect, unjustified, or unfair results, additional approaches are needed to make such systems accountable and governable. This Article reveals a new technological toolkit to verify that automated decisions comply with key standards of legal fairness. We challenge the dominant position in the legal literature that transparency will solve these problems. Disclosure of source code is often neither necessary (because of alternative techniques from computer science) nor sufficient (because of the issues analyzing code) to demonstrate the fairness of a process. Furthermore, transparency may be undesirable, such as when it discloses private information or permits tax cheats or terrorists to game the systems determining audits or security screening. The central issue is how to assure the interests of citizens, and society as a whole, in making these processes more accountable. This Article argues that technology is creating new opportunities—subtler and more flexible than total transparency—to design decisionmaking algorithms so that they better align with legal and policy objectives. Doing so will improve not only the current governance of automated decisions, but also—in certain cases—the governance of decisionmaking in general. The implicit (or explicit) biases of human decisionmakers can be difficult to find and root out, but we can peer into the brain of an algorithm: computational processes and purpose specifications can be declared prior to use and verified afterward. The technological tools introduced in this Article apply widely. They can be used in designing decisionmaking processes from both the private and public sectors, and they can be tailored to verify different characteristics as desired by decisionmakers, regulators, or the public. By forcing a more careful consideration of the effects of decision rules, they also engender policy discussions and closer looks at legal standards. As such, these tools have far-reaching implications throughout law and society. Part I of this Article provides an accessible and concise introduction to foundational computer science techniques that can be used to verify and demonstrate compliance with key standards of legal fairness for automated decisions without revealing key attributes of the decisions or the processes by which the decisions were reached. Part II then describes how these techniques can assure that decisions are made with the key governance attribute of procedural regularity, meaning that decisions are made under an announced set of rules consistently applied in each case. We demonstrate how this approach could be used to redesign and resolve issues with the State Department's diversity visa lottery. In Part III, we go further and explore how other computational techniques can assure that automated decisions preserve fidelity to substantive legal and policy choices. We show how these tools may be used to assure that certain kinds of unjust discrimination are avoided and that automated decision processes behave in ways that comport with the social or legal standards that govern the decision. We also show how automated decisionmaking may even complicate existing doctrines of disparate treatment and disparate impact, and we discuss some recent computer science work on detecting and removing discrimination in algorithms, especially in the context of big data and machine learning. And lastly, in Part IV, we propose an agenda to further synergistic collaboration between computer science, law, and policy to advance the design of automated decision processes for accountability.",10.2307/26600576,http://www.jstor.org/stable/26600576,University of Pennsylvania Law Review,Joshua A. Kroll;Joanna Huey;Solon Barocas;Edward W. Felten;Joel R. Reidenberg;David G. Robinson;Harlan Yu,2017,1979,"@article{2-20733,
  title={Accountable algorithms},
  author={Kroll, Joshua A. and Huey, Joanna and Barocas, Solon and Felten, Edward W. and Reidenberg, Joel R. and Robinson, David G. and Yu, Harlan},
  year={2017},
  doi={10.2307/26600576},
  journal={University of Pennsylvania Law Review}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance",Institutional,"Executing, Analyzing","Guardian, Decision-subject, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-208,aaai,Updates in Human-AI Teams: Understanding and Addressing the Performance/Compatibility Tradeoff,"AI systems are being deployed to support human decision making in high-stakes domains such as healthcare and criminal justice. In many cases, the human and AI form a team, in which the human makes decisions after reviewing the AI’s inferences. A successful partnership requires that the human develops insights into the performance of the AI system, including its failures. We study the influence of updates to an AI system in this setting. While updates can increase the AI’s predictive performance, they may also lead to behavioral changes that are at odds with the user’s prior experiences and confidence in the AI’s inferences. We show that updates that increase AI performance may actually hurt team performance. We introduce the notion of the compatibility of an AI update with prior user experience and present methods for studying the role of compatibility in human-AI teams. Empirical results on three high-stakes classification tasks show that current machine learning algorithms do not produce compatible updates. We propose a re-training objective to improve the compatibility of an update by penalizing new errors. The objective offers full leverage of the performance/compatibility tradeoff across different datasets, enabling more compatible yet accurate updates.",10.1609/aaai.v33i01.33012429,https://ojs.aaai.org/index.php/AAAI/article/view/4087,AAAI Conference on Artificial Intelligence,Gagan Bansal;Besmira Nushi;Ece Kamar;Daniel S. Weld;Walter S. Lasecki;Eric Horvitz,2019,18,"@inproceedings{2-208,
  title={Updates in Human-AI Teams: Understanding and Addressing the Performance/Compatibility Tradeoff},
  author={Bansal, Gagan and Nushi, Besmira and Kamar, Ece and Weld, Daniel S. and Lasecki, Walter S. and Horvitz, Eric},
  year={2019},
  doi={10.1609/aaai.v33i01.33012429},
  booktitle={AAAI Conference on Artificial Intelligence}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,"Forecasting, Advising",Decision-maker,"Change trust, Alter decision outcomes, Change cognitive demands",no such info,"confidence score, recommendations",trust,Textual,Yes,Yes
2-20832,jstor,Artificial intelligence and role-reversible judgment,"Intelligent machines increasingly outperform human experts, raising the question of when (and why) humans should remain ‘in the loop’ of decision-making. One common answer focuses on outcomes: relying on intuition and experience, humans are capable of identifying interpretive errors—sometimes disastrous errors—that elude machines. Though plausible today, this argument will wear thin as technology evolves. In this Article, we seek out sturdier ground: a defense of human judgment that focuses on the normative integrity of decision-making. Specifically, we propose an account of democratic equality as ‘role-reversibility.’ In a democracy, those tasked with making decisions should be susceptible, reciprocally, to the impact of decisions; there ought to be a meaningful sense in which the participants’ roles in the decisional process could always be inverted. Role-reversibility infuses the act of judgment with a ‘there but for the grace of god’ dynamic and, in doing so, casts judgment as the result of self-rule. After defending role-reversibility in concept, we show how it bears out in the paradigm case of criminal jury trials. Although it was not the historical impetus behind the jury trial—at least, not in any strong sense—we argue that role-reversibility explains some of the institution’s core features and stands among the best reasons for its preservation. Finally, for the sci-fi enthusiasts among us, role-reversibility offers a prescription as to when the legal system will be ready for robo-jurors and robo-judges: when it incorporates robo-defendants.",10.2307/48572776,http://www.jstor.org/stable/48572776,The Journal of Criminal Law and Criminology,KIEL BRENNAN-MARQUEZ;STEPHEN E. HENDERSON,2019,127,"@article{2-20832,
  title={Artificial intelligence and role-reversible judgment},
  author={Brennan-Marquez, Kiel and Henderson, Stephen E.},
  year={2019},
  journal={The Journal of Criminal Law and Criminology},
  doi={10.2307/48572776}
}",Theoretical contributions,Law / Policy / Governance,Organizational,"Forecasting, Collaborating, Advising","Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-20855,jstor,Automated decision support technologies and the legal profession,"A quiet revolution is afoot in the field of law. Technical systems employing algorithms are shaping and displacing professional decision making, and they are disrupting and restructuring relationships between law firms, lawyers, and clients. Decision-support systems marketed to legal professionals to support e-discovery—generally referred to as “technologyassisted review” (TAR)—increasingly rely on “predictive coding”: machine-learning techniques to classify and predict which of the voluminous electronic documents subject to litigation should be withheld or produced to the opposing side. These systems and the companies offering them are reshaping relationships between lawyers and clients, introducing new kinds of professionals into legal practice, altering the discovery process, and shaping how lawyers construct knowledge about their cases and professional obligations. In the midst of these shifting relationships—and the ways in which these systems are shaping the construction and presentation of knowledge—lawyers are grappling with their professional obligations, ethical duties, and what it means for the future of legal practice. Through in-depth, semi-structured interviews of experts in the e-discovery technology space—the technology company representatives who develop and sell such systems to law firms and the legal professionals who decide whether and how to use them in practice—we shed light on the organizational structures, professional rules and norms, and technical system properties that are shaping and being reshaped by predictive coding systems. Our findings show that AI-supported decision systems such as these are reconfiguring professional work practices. In particular, they highlight concerns about potential loss of professional agency and skill, limited understanding and thereby both over- and underreliance on decision-support systems, and confusion about responsibility and accountability as new kinds of technical professionals and technologies are brought into legal practice. The introduction of predictive coding systems and the new professional and organizational arrangements they are ushering into legal practice compound general concerns over the opacity of technical systems with specific concerns about encroachments on the construction of expert knowledge, liability frameworks, and the potential (mis)alignment of machine reasoning with professional logic and ethics. Based on our findings, we conclude that predictive coding tools—and likely other algorithmic systems lawyers use to construct knowledge and reason about legal practice— challenge the current model for evaluating whether and how tools are appropriate for legal practice. As tools become both more complex and more consequential, it is unreasonable to rely solely on legal professionals—judges, law firms, and lawyers—to determine which technologies are appropriate for use. The legal professionals we interviewed report relying on the evaluation and judgment of a range of new technical experts within law firms and, increasingly, third-party vendors and their technical experts. This system for choosing technical systems upon which lawyers rely to make professional decisions—e.g., whether documents are responsive, or whether the standard of proportionality has been met—is no longer sufficient. As the tools of medicine are reviewed by appropriate experts before they are put out for consideration and adoption by medical professionals, we argue that the legal profession must develop new processes for determining which algorithmic tools are fit to support lawyers’ decision making. Relatedly, because predictive coding systems are used to produce lawyers’ professional judgment, we argue they must be designed for contestability— providing greater transparency, interaction, and configurability around embedded choices to ensure decisions about how to embed core professional judgments, such as relevance and proportionality, remain salient and demand engagement from lawyers, not just their technical experts.",10.2307/26954398,http://www.jstor.org/stable/26954398,Berkeley Technology Law Journal,Daniel N. Kluttz;Deirdre K. Mulligan,2019,140,"@article{2-20855,
  title={Automated decision support technologies and the legal profession},
  author={Kluttz, Daniel N. and Mulligan, Deirdre K.},
  year={2019},
  journal={Berkeley Technology Law Journal},
  doi={10.2307/26954398}
}",Empirical contributions,Law / Policy / Governance,Operational,"Advising, Analyzing, Forecasting","Knowledge provider, Guardian, Decision-maker","Change cognitive demands, Restrict human agency, Change trust, Alter decision outcomes","Change AI responses, Update AI competence",NA,alignment with professional knowledge,Autonomous System,Yes,Yes
2-209,aaai,Using Small Business Banking Data for Explainable Credit Risk Scoring,Machine learning applied to financial transaction records can predict how likely a small business is to repay a loan. For this purpose we compared a traditional scorecard credit risk model against various machine learning models and found that XGBoost with monotonic constraints outperformed scorecard model by 7% in K-S statistic. To deploy such a machine learning model in production for loan application risk scoring it must comply with lending industry regulations that require lenders to provide understandable and specific reasons for credit decisions. Thus we also developed a loan decision explanation technique based on the ideas of WoE and SHAP. Our research was carried out using a historical dataset of tens of thousands of loans and millions of associated financial transactions. The credit risk scoring model based on XGBoost with monotonic constraints and SHAP explanations described in this paper have been deployed by QuickBooks Capital to assess incoming loan applications since July 2019.,10.1609/aaai.v34i08.7055,https://ojs.aaai.org/index.php/AAAI/article/view/7055,AAAI Conference on Artificial Intelligence,Wei Wang;Christopher Lesner;Alexander Ran;Marko Rukonic;Jason Xue;Eric Shiu,2020,32,"@inproceedings{2-209,
  title     = {Using Small Business Banking Data for Explainable Credit Risk Scoring},
  author    = {Wei Wang and Christopher Lesner and Alexander Ran and Marko Rukonic and Jason Xue and Eric Shiu},
  year      = {2020},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi       = {10.1609/aaai.v34i08.7055}
}",System/Artifact contributions,Finance / Business / Economy,Operational,"Explaining, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-210,aaai,"Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties","Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another( e. g. , when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction. We introduce ValuePrism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. ValuePrism’s contextualized values are generated by GPT-4 and deemed high-quality by human annotators 91% of the time. We conduct a large-scale study with annotators across diverse social and demographic backgrounds to try to understand whose values are represented. With ValuePrism, we build Value Kaleidoscope( or Kaleido) , an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence( i. e. , support or oppose) of human values, rights, and duties within a specific context. Humans prefer the sets of values output by our system over the teacher GPT4, finding them more accurate and with broader coverage. In addition, we demonstrate that Kaleido can help explain variability in human decision-making by outputting contrasting values. Finally, we show that Kaleido’s representations transfer to other philosophical frameworks and datasets, confirming the benefit of an explicit, modular, and interpretable approach to value pluralism. We hope that our work will serve as a step to making more explicit the implicit values behind human decision-making and to steering AI systems to make decisions that are more in accordance with them.",10.1609/aaai.v38i18.29970,https://ojs.aaai.org/index.php/AAAI/article/view/29970,AAAI Conference on Artificial Intelligence,Taylor Sorensen;Liwei Jiang;Jena D. Hwang;Sydney Levine;Valentina Pyatkin;Peter West;Nouha Dziri;Ximing Lu;Kavel Rao;Chandra Bhagavatula;Maarten Sap;John Tasioulas;Yejin Choi,2024,2,"@inproceedings{2-210,
  title = {Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties},
  author = {Taylor Sorensen and Liwei Jiang and Jena D. Hwang and Sydney Levine and Valentina Pyatkin and Peter West and Nouha Dziri and Ximing Lu and Kavel Rao and Chandra Bhagavatula and Maarten Sap and John Tasioulas and Yejin Choi},
  year = {2024},
  doi = {10.1609/aaai.v38i18.29970},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence}
}","Empirical contributions, Dataset/Benchmark contributions",Generic / Abstract / Domain-agnostic,no such info,"Analyzing, Explaining","Decision-maker, Knowledge provider","Alter decision outcomes, Change cognitive demands, Change affective-perceptual","Update AI competence, Shape AI for accountability, Change AI responses",contrasting values,NA,Textual,Yes,Yes
2-2104,acl,ExcavatorCovid: Extracting Events and Relations from Text Corpora for Temporal and Causal Analysis for COVID-19,"Timely responses from policy makers to mitigate the impact of the COVID-19 pandemic rely on a comprehensive grasp of events, their causes, and their impacts. These events are reported at such a speed and scale as to be overwhelming. In this paper, we present ExcavatorCovid, a machine reading system that ingests open-source text documents (e.g., news and scientific publications), extracts COVID-19 related events and relations between them, and builds a Temporal and Causal Analysis Graph (TCAG). Excavator will help government agencies alleviate the information overload, understand likely downstream effects of political and economic decisions and events related to the pandemic, and respond in a timely manner to mitigate the impact of COVID-19. We expect the utility of Excavator to outlive the COVID-19 pandemic: analysts and decision makers will be empowered by Excavator to better understand and solve complex problems in the future. A demonstration video is available at https://vimeo.com/528619007.",10.18653/v1/2021.emnlp-demo.8,https://aclanthology.org/2021.emnlp-demo.8,Empirical Methods in Natural Language Processing (EMNLP),"Min, Bonan; Rozonoyer, Benjamin; Qiu, Haoling; Zamanian, Alexander; Xue, Nianwen; MacBride, Jessica",2021,14,"@inproceedings{2-2104,
  title={ExcavatorCovid: Extracting Events and Relations from Text Corpora for Temporal and Causal Analysis for COVID-19},
  author={Min, Bonan and Rozonoyer, Benjamin and Qiu, Haoling and Zamanian, Alexander and Xue, Nianwen and MacBride, Jessica},
  booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  year={2021},
  doi={10.18653/v1/2021.emnlp-demo.8}
}",System/Artifact contributions,"Healthcare / Medicine / Surgery, Law / Policy / Governance",Organizational,Analyzing,"Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-2108,acl,SHIELD: LLM-Driven Schema Induction for Predictive Analytics in EV Battery Supply Chain Disruptions,"The electric vehicle (EV) battery supply chain's vulnerability to disruptions necessitates advanced predictive analytics. We present SHIELD (Schema-based Hierarchical Induction for EV supply chain Disruption), a system integrating Large Language Models (LLMs) with domain expertise for EV battery supply chain risk assessment. SHIELD combines: (1) LLM-driven schema learning to construct a comprehensive knowledge library, (2) a disruption analysis system utilizing fine-tuned language models for event extraction, multi-dimensional similarity matching for schema matching, and Graph Convolutional Networks (GCNs) with logical constraints for prediction, and (3) an interactive interface for visualizing results and incorporating expert feedback to enhance decision-making. Evaluated on 12,070 paragraphs from 365 sources (2022-2023), SHIELD outperforms baseline GCNs and LLM+prompt methods (e.g. GPT-4o) in disruption prediction. These results demonstrate SHIELD's effectiveness in combining LLM capabilities with domain expertise for enhanced supply chain risk assessment.",10.18653/v1/2024.emnlp-industry.24,https://aclanthology.org/2024.emnlp-industry.24,Empirical Methods in Natural Language Processing (EMNLP),"Cheng, Zhi-Qi; Dong, Yifei; Shi, Aike; Liu, Wei; Hu, Yuzhi; O'Connor, Jason; Hauptmann, Alexander G; Whitefoot, Kate",2024,9,"@inproceedings{2-2108,
  title     = {SHIELD: LLM-Driven Schema Induction for Predictive Analytics in EV Battery Supply Chain Disruptions},
  author    = {Cheng, Zhi-Qi and Dong, Yifei and Shi, Aike and Liu, Wei and Hu, Yuzhi and O'Connor, Jason and Hauptmann, Alexander G and Whitefoot, Kate},
  year      = {2024},
  doi       = {10.18653/v1/2024.emnlp-industry.24},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)}
}",System/Artifact contributions,Manufacturing / Industry / Automation,Organizational,"Forecasting, Analyzing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-2109,acl,Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs,"Large Language Models (LLMs) have shown remarkable capabilities in tasks such as summarization, arithmetic reasoning, and question answering. However, they encounter significant challenges in the domain of moral reasoning and ethical decision-making, especially in complex scenarios with multiple stakeholders. This paper introduces the Skin-in-the-Game (SKIG) framework, aimed at enhancing moral reasoning in LLMs by exploring decisions' consequences from multiple stakeholder perspectives. The core components of the framework consist of simulating accountability for decisions, conducting empathy exercises on different stakeholders, and evaluating the risks associated with the impacts of potential actions. We study SKIG's performance across various moral reasoning benchmarks with proprietary and open-source LLMs, and investigate its crucial components through extensive ablation analyses. Our framework exhibits marked improvements in performance compared to baselines across different language models and benchmarks.",10.18653/v1/2024.acl-long.751,https://aclanthology.org/2024.acl-long.751,Annual Meeting of the Association for Computational Linguistics (ACL),"Sel, Bilgehan; Shanmugasundaram, Priya; Kachuee, Mohammad; Zhou, Kun; Jia, Ruoxi; Jin, Ming",2024,0,"@inproceedings{2-2109,
  title = {Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs},
  author = {Sel, Bilgehan and Shanmugasundaram, Priya and Kachuee, Mohammad and Zhou, Kun and Jia, Ruoxi and Jin, Ming},
  year = {2024},
  booktitle = {Annual Meeting of the Association for Computational Linguistics (ACL)},
  doi = {10.18653/v1/2024.acl-long.751}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,no such info,"Explaining, Executing",Stakeholder,NA,NA,NA,NA,NA,Yes,No
2-21106,jstor,Democracy & distrust in an era of artificial intelligence,"Our legal system has historically operated under the general view that courts should defer to the legislature. There is one significant exception to this view: cases in which it appears that the political process has failed to recognize the rights or interests of minorities. This basic approach provides much of the foundational justifications for the role of judicial review in protecting minorities from discrimination by the legislature. Today, the rise of AI decision-making poses a similar challenge to democracy’s basic framework. As I argue in this essay, the rise of three trends – privatization, prediction, and automation in AI – have combined to pose similar risks to minorities. In this essay, I outline what a theory of judicial review would look like in an era of artificial intelligence, analyzing both the limitations and the possibilities of judicial review of AI. Here, I draw on cases in which AI decision-making has been challenged in courts, to show how concepts of due process and equal protection can be recuperated in a modern AI era, and even integrated into AI, to provide for better oversight and accountability.",10.2307/48662045,http://www.jstor.org/stable/48662045,Daedalus,Sonia K. Katyal,2022,16,"@article{2-21106,
  title={Democracy \& distrust in an era of artificial intelligence},
  author={Katyal, Sonia K.},
  year={2022},
  doi={10.2307/48662045},
  journal={Daedalus}
}",Theoretical contributions,Law / Policy / Governance,Institutional,"Executing, Analyzing, Advising","Decision-maker, Knowledge provider, Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-2111,acl,Towards Reducing Diagnostic Errors with Interpretable Risk Prediction,"Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual “true” diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refine this set of evidence according to correlations learned by the model. We conduct an in-depth evaluation of the usefulness of our approach by simulating how it might be used by a clinician to decide between a pre-defined list of differential diagnoses.",10.18653/v1/2024.naacl-long.399,https://aclanthology.org/2024.naacl-long.399,NAACL HLT,"McInerney, Denis; Dickinson, William; Flynn, Lucy; Young, Andrea; Young, Geoffrey; van de Meent, Jan-Willem; Wallace, Byron",2024,6,"@inproceedings{2-2111,
  title = {Towards Reducing Diagnostic Errors with Interpretable Risk Prediction},
  author = {McInerney, Denis and Dickinson, William and Flynn, Lucy and Young, Andrea and Young, Geoffrey and van de Meent, Jan-Willem and Wallace, Byron},
  year = {2024},
  doi = {10.18653/v1/2024.naacl-long.399},
  booktitle = {Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT)},
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Explaining, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-2116,acl,Natural Language Generation enhances human decision-making with uncertain information,"Decision-making is often dependent on
uncertain data, e.g. data associated with
confidence scores or probabilities. We
present a comparison of different information presentations for uncertain data and,
for the first time, measure their effects
on human decision-making. We show
that the use of Natural Language Generation (NLG) improves decision-making under uncertainty, compared to state-of-theart graphical-based representation methods. In a task-based study with 442 adults,
we found that presentations using NLG
lead to 24% better decision-making on average than the graphical presentations, and
to 44% better decision-making when NLG
is combined with graphics. We also show
that women achieve significantly better results when presented with NLG output
(an 87% increase on average compared to
graphical presentations).",10.18653/v1/P16-2043,https://aclanthology.org/P16-2043,Annual Meeting of the Association for Computational Linguistics,"Gkatzia, Dimitra; Lemon, Oliver; Rieser, Verena",2016,0,"@inproceedings{2-2116,
  title={Natural Language Generation enhances human decision-making with uncertain information},
  author={Gkatzia, Dimitra and Lemon, Oliver and Rieser, Verena},
  year={2016},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/P16-2043}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Advising, Explaining",Decision-maker,"Change affective-perceptual, Alter decision outcomes",no such info,"visual explanations, textual explanations",NA,"Visual, Textual, Conversational/Natural Language",Yes,Yes
2-2123,acm,The Medical Authority of AI: A Study of AI-enabled Consumer-Facing Health Technology,"Recently, consumer-facing health technologies such as Artificial Intelligence (AI)-based symptom checkers (AISCs) have sprung up in everyday healthcare practice. AISCs solicit symptom information from users and provide medical suggestions and possible diagnoses, a responsibility that people usually entrust with real-person authorities such as physicians and expert patients. Thus, the advent of AISCs begs a question of whether and how they transform the notion of medical authority in people's everyday healthcare practice. To answer this question, we conducted an interview study with thirty AISC users. We found that users assess the medical authority of AISCs using various factors including AISCs’ automated decisions and interaction design patterns, associations with established medical authorities like hospitals, and comparisons with other health technologies. We reveal how AISCs are used in healthcare delivery, discuss how AI transforms conventional understandings of medical authority, and derive implications for designing AI-enabled health technology.",10.1145/3411764.3445657,https://doi.org/10.1145/3411764.3445657,CHI Conference on Human Factors in Computing Systems,"You, Yue; Kou, Yubo; Ding, Xianghua(Sharon); Gui, Xinning",2021,2,"@inproceedings{2-2123,
  title = {The Medical Authority of AI: A Study of AI-enabled Consumer-Facing Health Technology},
  author = {You, Yue and Kou, Yubo and Ding, Xianghua and Gui, Xinning},
  year = {2021},
  doi = {10.1145/3411764.3445657},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Individual,"Advising, Analyzing","Decision-maker, Decision-subject","Change trust, Alter decision outcomes, Change affective-perceptual",no such info,preliminary diagnoses,NA,"Textual, Conversational/Natural Language",Yes,Yes
2-2124,acm,Crowdsourcing Perceptions of Fair Predictors for Machine Learning: A Recidivism Case Study,"The increased reliance on algorithmic decision-making in socially impactful processes has intensified the calls for algorithms that are unbiased and procedurally fair. Identifying fair predictors is an essential step in the construction of equitable algorithms, but the lack of ground-truth in fair predictor selection makes this a challenging task. In our study, we recruit 90 crowdworkers to judge the inclusion of various predictors for recidivism. We divide participants across three conditions with varying group composition. Our results show that participants were able to make informed decisions on predictor selection. We find that agreement with the majority vote is higher when participants are part of a more diverse group. The presented workflow, which provides a scalable and practical approach to reach a diverse audience, allows researchers to capture participants' perceptions of fairness in private while simultaneously allowing for structured participant discussion.",10.1145/3359130,https://doi.org/10.1145/3359130,Proceedings of the ACM on Human-Computer Interaction,"van Berkel, Niels; Goncalves, Jorge; Hettiachchi, Danula; Wijenayake, Senuri; Kelly, Ryan M.; Kostakos, Vassilis",2019,91,"@article{2-2124,
  title = {Crowdsourcing Perceptions of Fair Predictors for Machine Learning: A Recidivism Case Study},
  author = {van Berkel, Niels and Goncalves, Jorge and Hettiachchi, Danula and Wijenayake, Senuri and Kelly, Ryan M. and Kostakos, Vassilis},
  year = {2019},
  doi = {10.1145/3359130},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Law / Policy / Governance,Operational,"Forecasting, Auditing","Decision-maker, Stakeholder, Knowledge provider",no such info,"Update AI competence, Change AI responses, Shape AI for accountability",textual explanations,NA,"Autonomous System, Physical / Embodiment",Yes,Yes
2-21360,jstor,Fragile algorithms and fallible decision-makers,"Algorithms (in some form) are already widely used in the criminal justice system. We draw lessons from this experience for what is to come for the rest of society as machine learning diffuses. We find economists and other social scientists have a key role to play in shaping the impact of algorithms, in part through improving the tools used to build them.",10.2307/27074126,http://www.jstor.org/stable/27074126,The Journal of Economic Perspectives,Jens Ludwig;Sendhil Mullainathan,2021,0,"@article{2-21360,
  title={Fragile algorithms and fallible decision-makers},
  author={Ludwig, Jens and Mullainathan, Sendhil},
  year={2021},
  journal={The Journal of Economic Perspectives},
  doi={10.2307/27074126}
}",Theoretical contributions,Law / Policy / Governance,Institutional,"Advising, Forecasting, Auditing","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-21454,jstor,Human decisions and machine predictions,"Can machine learning improve human decision making? Bail decisions provide a good test case. Millions of times each year, judges make jail-or-release decisions that hinge on a prediction of what a defendant would do if released. The concreteness of the prediction task combined with the volume of data available makes this a promising machine-learning application. Yet comparing the algorithm to judges proves complicated. First, the available data are generated by prior judge decisions. We only observe crime outcomes for released defendants, not for those judges detained. This makes it hard to evaluate counterfactual decision rules based on algorithmic predictions. Second, judges may have a broader set of preferences than the variable the algorithm predicts; for instance, judges may care specifically about violent crimes or about racial inequities. We deal with these problems using different econometric strategies, such as quasi-random assignment of cases to judges. Even accounting for these concerns, our results suggest potentially large welfare gains: one policy simulation shows crime reductions up to 24.7% with no change in jailing rates, or jailing rate reductions up to 41.9% with no increase in crime rates. Moreover, all categories of crime, including violent crimes, show reductions; these gains can be achieved while simultaneously reducing racial disparities. These results suggest that while machine learning can be valuable, realizing this value requires integrating these tools into an economic framework: being clear about the link between predictions and decisions; specifying the scope of payoff functions; and constructing unbiased decision counterfactuals.",10.2307/26495162,http://www.jstor.org/stable/26495162,The Quarterly Journal of Economics,Jon Kleinberg;Himabindu Lakkaraju;Jure Leskovec;Jens Ludwig;Sendhil Mullainathan,2018,99,"@article{2-21454,
  title={Human decisions and machine predictions},
  author={Kleinberg, Jon and Lakkaraju, Himabindu and Leskovec, Jure and Ludwig, Jens and Mullainathan, Sendhil},
  year={2018},
  journal={The Quarterly Journal of Economics},
  doi={10.2307/26495162}
}",Empirical contributions,Law / Policy / Governance,Operational,"Forecasting, Monitoring, Auditing","Decision-maker, Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-2175,acm,Target-Oriented Maneuver Decision for Autonomous Vehicle: A Rule-Aided Reinforcement Learning Framework,"Autonomous driving systems (ADSs) have the potential to revolutionize transportation by improving traffic safety and efficiency. As the core component of ADSs, maneuver decision aims to make tactical decisions to accomplish road following, obstacle avoidance, and efficient driving. In this work, we consider a typical but rarely studied task, called Target-Lane-Entering (TLE), where an autonomous vehicle should enter a target lane before reaching an intersection to ensure a smooth transition to another road. For navigation-assisted autonomous driving, a maneuver decision module chooses the optimal timing to enter the target lane in each road section, thus avoiding rerouting and reducing travel time. To achieve the TLE task, we propose a ruLe-aided reINforcement lEarning framework, called LINE, which combines the advantages of RL-based policy and rule-based strategy, allowing the autonomous vehicle to make target-oriented maneuver decisions. Specifically, an RL-based policy with a hybrid reward function is able to make safe, efficient, and comfortable decisions while considering the factors of target lanes. Then a strategy of rule revision aims to help the policy learn from intervention and block the risk of missing target lanes. Extensive experiments based on the SUMO simulator confirm the effectiveness of our framework. The results show that LINE achieves state-of-the-art driving performance with over 95% task success rate.",10.1145/3583780.3615072,https://doi.org/10.1145/3583780.3615072,ACM International Conference on Information and Knowledge Management (CIKM),"Zeng, Ximu; Yu, Quanlin; Liu, Shuncheng; Xia, Yuyang; Su, Han; Zheng, Kai",2023,7,"@inproceedings{2-2175,
  title = {Target-Oriented Maneuver Decision for Autonomous Vehicle: A Rule-Aided Reinforcement Learning Framework},
  author = {Zeng, Ximu and Yu, Quanlin and Liu, Shuncheng and Xia, Yuyang and Su, Han and Zheng, Kai},
  year = {2023},
  doi = {10.1145/3583780.3615072},
  booktitle = {Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM)}
}",Methodological contributions,Transportation / Mobility / Planning,Individual,"Executing, Advising",Developer,NA,NA,NA,NA,NA,Yes,No
2-2180,acm,Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem,"Algorithmic audits (or ‘AI audits’) are an increasingly popular mechanism for algorithmic accountability; however, they remain poorly defined. Without a clear understanding of audit practices, let alone widely used standards or regulatory guidance, claims that an AI product or system has been audited, whether by first-, second-, or third-party auditors, are difficult to verify and may potentially exacerbate, rather than mitigate, bias and harm. To address this knowledge gap, we provide the first comprehensive field scan of the AI audit ecosystem. We share a catalog of individuals (N=438) and organizations (N=189) who engage in algorithmic audits or whose work is directly relevant to algorithmic audits; conduct an anonymous survey of the group (N=152); and interview industry leaders (N=10). We identify emerging best practices as well as methods and tools that are becoming commonplace, and enumerate common barriers to leveraging algorithmic audits as effective accountability mechanisms. We outline policy recommendations to improve the quality and impact of these audits, and highlight proposals with wide support from algorithmic auditors as well as areas of debate. Our recommendations have implications for lawmakers, regulators, internal company policymakers, and standards-setting bodies, as well as for auditors. They are: 1) require the owners and operators of AI systems to engage in independent algorithmic audits against clearly defined standards; 2) notify individuals when they are subject to algorithmic decision-making systems; 3) mandate disclosure of key components of audit findings for peer review; 4) consider real-world harm in the audit process, including through standardized harm incident reporting and response mechanisms; 5) directly involve the stakeholders most likely to be harmed by AI systems in the algorithmic audit process; and 6) formalize evaluation and, potentially, accreditation of algorithmic auditors.",10.1145/3531146.3533213,https://doi.org/10.1145/3531146.3533213,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Costanza-Chock, Sasha; Raji, Inioluwa Deborah; Buolamwini, Joy",2022,26,"@inproceedings{2-2180,
  author    = {Sasha Costanza-Chock and Inioluwa Deborah Raji and Joy Buolamwini},
  title     = {Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem},
  booktitle = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)},
  year      = {2022},
  doi       = {10.1145/3531146.3533213}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Institutional,"Advising, Auditing","Decision-maker, Stakeholder, Guardian",NA,NA,NA,NA,NA,Yes,No
2-2182,acm,"""It depends"": Configuring AI to Improve Clinical Usefulness Across Contexts","Artificial Intelligence (AI) repeatedly match or outperform radiologists in lab experiments. However, real-world implementations of radiological AI-based systems are found to provide little to no clinical value. This paper explores how to design AI for clinical usefulness in different contexts. We conducted 19 design sessions and design interventions with 13 radiologists from 7 clinical sites in Denmark and Kenya, based on three iterations of a functional AI-based prototype. Ten sociotechnical dependencies were identified as crucial for the design of AI in radiology. We conceptualised four technical dimensions that must be configured to the intended clinical context of use: AI functionality, AI medical focus, AI decision threshold, and AI Explainability. We present four design recommendations on how to address dependencies pertaining to the medical knowledge, clinic type, user expertise level, patient context, and user situation that condition the configuration of these technical dimensions.",10.1145/3643834.3660707,https://doi.org/10.1145/3643834.3660707,ACM Designing Interactive Systems Conference,"Zając, Hubert Dariusz; Ribeiro, Jorge Miguel Neves; Ingala, Silvia; Gentile, Simona; Wanjohi, Ruth; Gitau, Samuel Nguku; Carlsen, Jonathan Frederik; Nielsen, Michael Bachmann; Andersen, Tariq Osman",2024,0,"@inproceedings{2-2182,
  title = {``It depends'': Configuring AI to Improve Clinical Usefulness Across Contexts},
  author = {Zając, Hubert Dariusz and Ribeiro, Jorge Miguel Neves and Ingala, Silvia and Gentile, Simona and Wanjohi, Ruth and Gitau, Samuel Nguku and Carlsen, Jonathan Frederik and Nielsen, Michael Bachmann and Andersen, Tariq Osman},
  year = {2024},
  doi = {10.1145/3643834.3660707},
  booktitle = {ACM Designing Interactive Systems Conference}
}","Methodological contributions, Empirical contributions",Healthcare / Medicine / Surgery,Operational,"Explaining, Advising, Analyzing","Decision-maker, Knowledge provider","Change trust, Alter decision outcomes, Change cognitive demands","Update AI competence, Change AI responses",(continuous) support,"refinement, domain knowledge","Interactive interface, Textual, Conversational/Natural Language",Yes,Yes
2-2184,acm,"It’s Complicated: The Relationship between User Trust, Model Accuracy and Explanations in AI","Automated decision-making systems become increasingly powerful due to higher model complexity. While powerful in prediction accuracy, Deep Learning models are black boxes by nature, preventing users from making informed judgments about the correctness and fairness of such an automated system. Explanations have been proposed as a general remedy to the black box problem. However, it remains unclear if effects of explanations on user trust generalise over varying accuracy levels. In an online user study with 959 participants, we examined the practical consequences of adding explanations for user trust: We evaluated trust for three explanation types on three classifiers of varying accuracy. We find that the influence of our explanations on trust differs depending on the classifier’s accuracy. Thus, the interplay between trust and explanations is more complex than previously reported. Our findings also reveal discrepancies between self-reported and behavioural trust, showing that the choice of trust measure impacts the results.",10.1145/3495013,https://doi.org/10.1145/3495013,ACM Transactions on Computer-Human Interaction,"Papenmeier, Andrea; Kern, Dagmar; Englebienne, Gwenn; Seifert, Christin",2022,259,"@article{2-2184,
  title = {It’s Complicated: The Relationship between User Trust, Model Accuracy and Explanations in AI},
  author = {Papenmeier, Andrea and Kern, Dagmar and Englebienne, Gwenn and Seifert, Christin},
  year = {2022},
  journal = {ACM Transactions on Computer-Human Interaction},
  doi = {10.1145/3495013}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Executing, Explaining, Forecasting",Decision-maker,"Change cognitive demands, Change affective-perceptual, Alter decision outcomes, Change trust",no such info,"textual explanations, highlighting and extraction",NA,"Interactive interface, Textual, Conversational/Natural Language, Autonomous System",Yes,Yes
2-2187,acm,"I Know This Looks Bad, But I Can Explain: Understanding When AI Should Explain Actions In Human-AI Teams","Explanation of artificial intelligence (AI) decision-making has become an important research area in human–computer interaction (HCI) and computer-supported teamwork research. While plenty of research has investigated AI explanations with an intent to improve AI transparency and human trust in AI, how AI explanations function in teaming environments remains unclear. Given that a major benefit of AI giving explanations is to increase human trust understanding how AI explanations impact human trust is crucial to effective human-AI teamwork. An online experiment was conducted with 156 participants to explore this question by examining how a teammate’s explanations impact the perceived trust of the teammate and the effectiveness of the team and how these impacts vary based on whether the teammate is a human or an AI. This study shows that explanations facilitate trust in AI teammates when explaining why AI disobeyed humans’ orders but hindered trust when explaining why an AI lied to humans. In addition, participants’ personal characteristics (e.g., their gender and the individual’s ethical framework) impacted their perceptions of AI teammates both directly and indirectly in different scenarios. Our study contributes to interactive intelligent systems and HCI by shedding light on how an AI teammate’s actions and corresponding explanations are perceived by humans while identifying factors that impact trust and perceived effectiveness. This work provides an initial understanding of AI explanations in human-AI teams, which can be used for future research to build upon in exploring AI explanation implementation in collaborative environments.",10.1145/3635474,https://doi.org/10.1145/3635474,ACM Transactions on Interactive Intelligent Systems,"Zhang, Rui; Flathmann, Christopher; Musick, Geoff; Schelble, Beau; McNeese, Nathan J.; Knijnenburg, Bart; Duan, Wen",2024,1,"@article{2-2187,
  title = {I Know This Looks Bad, But I Can Explain: Understanding When AI Should Explain Actions In Human-AI Teams},
  author = {Zhang, Rui and Flathmann, Christopher and Musick, Geoff and Schelble, Beau and McNeese, Nathan J. and Knijnenburg, Bart and Duan, Wen},
  year = {2024},
  doi = {10.1145/3635474},
  journal = {ACM Transactions on Interactive Intelligent Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Institutional,"Explaining, Executing, Collaborating",Decision-maker,"Change affective-perceptual, Change trust",no such info,textual explanations,NA,Textual,Yes,Yes
2-2191,acm,LuPe: A System for Personalized and Transparent Data-driven Decisions,"Machine learning models are commonly used for decision support even though they are far from perfect, e.g., due to bias introduced by imperfect training data or wrong feature selection. While efforts are made and should continue to be put into developing better models, we will likely continue to rely on imperfect models in many applications. In these settings, how could we at least use the ""best"" model for an individual or a group of users and transparently communicate the risks and weaknesses that apply?We demonstrate LuPe, a system that addresses these questions. LuPe allows to optimize the choice of the applied model for subgroups of the population or individuals, thereby personalizing the model choice to best fit users' profiles, which improves fairness. LuPe further captures data to explain the choices made and the results of the model. We showcase how such data enable users to understand the system performance they can expect. This transparency helps users in making informed decisions or providing informed consent when such systems are used. Our demonstration will focus on several real-world applications showcasing the behavior of LuPe, including credit scoring and income prediction.",10.1145/3357384.3357857,https://doi.org/10.1145/3357384.3357857,ACM International Conference on Information and Knowledge Management (CIKM),"Oppold, Sarah; Herschel, Melanie",2019,12,"@inproceedings{2-2191,
  title = {LuPe: A System for Personalized and Transparent Data-driven Decisions},
  author = {Oppold, Sarah and Herschel, Melanie},
  year = {2019},
  booktitle = {Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM)},
  doi = {10.1145/3357384.3357857}
}",System/Artifact contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Individual,"Advising, Explaining",Decision-maker,"Shape ethical norms, Change cognitive demands, Alter decision outcomes","Update AI competence, Shape AI for accountability",NA,personalized settings,Interactive interface,Yes,Yes
2-2198,acm,MetaWriter: Exploring the Potential and Perils of AI Writing Support in Scientific Peer Review,"Recent advances in Large Language Models (LLMs) show the potential to significantly augment or even replace complex human writing activities. However, for complex tasks where people need to make decisions as well as write a justification, the trade offs between making work efficient and hindering decisions remain unclear. In this paper, we explore this question in the context of designing intelligent scaffolding for writing meta-reviews for an academic peer review process. We prototyped a system called ""MetaWriter” trained on five years of open peer review data to support meta-reviewing. The system highlights common topics in the original peer reviews, extracts key points by each reviewer, and on request, provides a preliminary draft of a meta-review that can be further edited. To understand how novice and experienced meta-reviewers use MetaWriter, we conducted a within-subject study with 32 participants. Each participant wrote meta-reviews for two papers: one with and one without MetaWriter. We found that MetaWriter significantly expedited the authoring process and improved the coverage of meta-reviews, as rated by experts, compared to the baseline. While participants recognized the efficiency benefits, they raised concerns around trust, over-reliance, and agency. We also interviewed six paper authors to understand their opinions of using machine intelligence to support the peer review process and reported critical reflections. We discuss implications for future interactive AI writing tools to support complex synthesis work.",10.1145/3637371,https://doi.org/10.1145/3637371,Proceedings of the ACM on Human-Computer Interaction,"Sun, Lu; Tao, Stone; Hu, Junjie; Dow, Steven P.",2024,10,"@article{2-2198,
  title={MetaWriter: Exploring the Potential and Perils of AI Writing Support in Scientific Peer Review},
  author={Sun, Lu and Tao, Stone and Hu, Junjie and Dow, Steven P.},
  year={2024},
  doi={10.1145/3637371},
  journal={Proceedings of the ACM on Human-Computer Interaction}
}",System/Artifact contributions,Education / Teaching / Research,Operational,"Analyzing, Advising","Decision-maker, Knowledge provider","Alter decision outcomes, Change cognitive demands, Change affective-perceptual, Restrict human agency, Change trust",no such info,"highlighting and extraction, preliminary draft of a meta-review",NA,Interactive interface,Yes,Yes
2-2200,acm,Categorical and Continuous Features in Counterfactual Explanations of AI Systems,"Recently, eXplainable AI (XAI) research has focused on the use of counterfactual explanations to address interpretability, algorithmic recourse, and bias in AI system decision-making. The developers of these algorithms claim they meet user requirements in generating counterfactual explanations with “plausible”, “actionable” or “causally important” features. However, few of these claims have been tested in controlled psychological studies. Hence, we know very little about which aspects of counterfactual explanations really help users understand the decisions of AI systems. Nor do we know whether counterfactual explanations are an advance on more traditional causal explanations that have a longer history in AI (e.g., in expert systems). Accordingly, we carried out three user studies to (i) test a fundamental distinction in feature-types, between categorical and continuous features, and (ii) compare the relative effectiveness of counterfactual and causal explanations. The studies used a simulated, automated decision-making app that determined safe driving limits after drinking alcohol, based on predicted blood alcohol content, where users’ responses were measured objectively (using predictive accuracy) and subjectively (using satisfaction and trust judgments). Study 1 (N = 127) showed that users understand explanations referring to categorical features more readily than those referring to continuous features. It also discovered a dissociation between objective and subjective measures: counterfactual explanations elicited higher accuracy than no-explanation controls but elicited no more accuracy than causal explanations, yet counterfactual explanations elicited greater satisfaction and trust than causal explanations. In Study 2 (N = 136) we transformed the continuous features of presented items to be categorical (i.e., binary) and found that these converted features led to highly accurate responding. Study 3 (N = 211) explicitly compared matched items involving either mixed features (i.e., a mix of categorical and continuous features) or categorical features (i.e., categorical and categorically-transformed continuous features), and found that users were more accurate when categorically-transformed features were used instead of continuous ones. It also replicated the dissociation between objective and subjective effects of explanations. The findings delineate important boundary conditions for current and future counterfactual explanation methods in XAI.",10.1145/3673907,https://doi.org/10.1145/3673907,ACM Transactions on Interactive Intelligent Systems,"Warren, Greta; Byrne, Ruth M.J.; Keane, Mark T.",2024,2,"@article{2-2200,
  title={Categorical and Continuous Features in Counterfactual Explanations of AI Systems},
  author={Warren, Greta and Byrne, Ruth M.J. and Keane, Mark T.},
  year={2024},
  journal={ACM Transactions on Interactive Intelligent Systems},
  doi={10.1145/3673907}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Explaining, Forecasting, Advising","Decision-maker, Decision-subject","Alter decision outcomes, Change affective-perceptual, Shape ethical norms, Change trust",no such info,"counterfactual explanations, causal explanations",NA,Autonomous System,Yes,Yes
2-2205,acm,Amplifying Human Capabilities in Prostate Cancer Diagnosis: An Empirical Study of Current Practices and AI Potentials in Radiology,"This paper examines the potential of Human-Centered AI (HCAI) solutions to support radiologists in diagnosing prostate cancer. Prostate cancer is one of the most prevalent and increasing cancers among men. The scarcity of radiologists raises concerns about their ability to address the growing demand for prostate cancer diagnosis, leading to a significant surge in the workload of radiologists. Drawing on an HCAI approach, we sought to understand the current practices concerning radiologists’ work on detecting and diagnosing prostate cancer, as well as the challenges they face. The findings from our empirical studies point toward the potential that AI has to expedite informed decision-making and enhance accuracy, efficiency, and consistency. This is particularly beneficial for collaborative prostate cancer diagnosis processes. We discuss these results and introduce design recommendations and HCAI concepts for the domain of prostate cancer diagnosis, with the aim of amplifying the professional capabilities of radiologists.",10.1145/3613904.3642362,https://doi.org/10.1145/3613904.3642362,CHI Conference on Human Factors in Computing Systems,"Saßmannshausen, Sheree May; Ontika, Nazmun Nisat; Pinatti De Carvalho, Aparecido Fabiano; Rouncefield, Mark; Pipek, Volkmar",2024,6,"@inproceedings{2-2205,
  title = {Amplifying Human Capabilities in Prostate Cancer Diagnosis: An Empirical Study of Current Practices and AI Potentials in Radiology},
  author = {Saßmannshausen, Sheree May and Ontika, Nazmun Nisat and Pinatti De Carvalho, Aparecido Fabiano and Rouncefield, Mark and Pipek, Volkmar},
  year = {2024},
  doi = {10.1145/3613904.3642362},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising, Analyzing","Decision-maker, Knowledge provider","Alter decision outcomes, Change trust","Change AI responses, Update AI competence",preliminary diagnoses,"corrective feedback, trust",Semi-Autonomous System,Yes,Yes
2-2207,acm,The Neutrality Fallacy: When Algorithmic Fairness Interventions are (Not) Positive Action,"Various metrics and interventions have been developed to identify and mitigate unfair outputs of machine learning systems. While individuals and organizations have an obligation to avoid discrimination, the use of fairness-aware machine learning interventions has also been described as amounting to ‘algorithmic positive action’ under European Union (EU) non-discrimination law. As the Court of Justice of the European Union has been strict when it comes to assessing the lawfulness of positive action, this would impose a significant legal burden on those wishing to implement fair-ml interventions. In this paper, we propose that algorithmic fairness interventions often should be interpreted as a means to prevent discrimination, rather than a measure of positive action. Specifically, we suggest that this category mistake can often be attributed to neutrality fallacies: faulty assumptions regarding the neutrality of (fairness-aware) algorithmic decision-making. Our findings raise the question of whether a negative obligation to refrain from discrimination is sufficient in the context of algorithmic decision-making. Consequently, we suggest moving away from a duty to ‘not do harm’ towards a positive obligation to actively ‘do no harm’ as a more adequate framework for algorithmic decision-making and fair ml-interventions.",10.1145/3630106.3659025,https://doi.org/10.1145/3630106.3659025,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Weerts, Hilde; Xenidis, Raphaële; Tarissan, Fabien; Olsen, Henrik Palmer; Pechenizkiy, Mykola",2024,4,"@inproceedings{2-2207,
  title = {The Neutrality Fallacy: When Algorithmic Fairness Interventions are (Not) Positive Action},
  author = {Weerts, Hilde and Xenidis, Rapha{\""e}le and Tarissan, Fabien and Olsen, Henrik Palmer and Pechenizkiy, Mykola},
  year = {2024},
  doi = {10.1145/3630106.3659025},
  booktitle = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}",Theoretical contributions,Law / Policy / Governance,Institutional,"Advising, Monitoring","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-2208,acm,DeepGroup: Group Recommendation with Implicit Feedback,"We focus on making recommendations for a new group of users whose preferences are unknown, but we are given the decisions of other groups. By formulating this problem as group recommendation from group implicit feedback, we focus on two of its practical instances: group decision prediction and reverse social choice. Given a set of groups and their observed decisions, group decision prediction intends to predict the decision of a new group of users, whereas reverse social choice aims to infer the preferences of those users involved in observed group decisions. These two problems are of interest to not only group recommendation, but also to personal privacy when the users intend to conceal their personal preferences but have participated in group decisions. To tackle these two problems, we propose and study DeepGroup—a deep learning approach for group recommendation with group implicit data. We empirically assess the predictive power of DeepGroup on various real-world datasets and group decision rules. Our extensive experiments not only demonstrate the efficacy of DeepGroup but also shed light on the privacy-leakage concerns of some decision-making processes.",10.1145/3459637.3482081,https://doi.org/10.1145/3459637.3482081,ACM International Conference on Information and Knowledge Management (CIKM),"Sajjadi Ghaemmaghami, Sarina; Salehi-Abari, Amirali",2021,22,"@inproceedings{2-2208,
  title = {DeepGroup: Group Recommendation with Implicit Feedback},
  author = {Sajjadi Ghaemmaghami, Sarina and Salehi-Abari, Amirali},
  year = {2021},
  doi = {10.1145/3459637.3482081},
  booktitle = {Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM)}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Individual,"Forecasting, Advising",Decision-maker,Alter decision outcomes,Update AI competence,NA,NA,"Textual, Conversational/Natural Language",Yes,Yes
2-2209,acm,AI Opacity and Explainability in Tort Litigation,"A spate of recent accidents and a lawsuit involving Tesla's ‘self-driving’ cars highlights the growing need for meaningful accountability when harms are caused by AI systems. Tort (or civil liability) lawsuits are one important way for victims to redress such harms. The prospect of tort liability may also prompt AI developers to take better precautions against safety risks. Tort claims of all kinds will be hindered by AI opacity: the difficulty of determining how and why complex AI systems make decisions. We address this problem by formulating and evaluating several options for mitigating AI opacity that combine expert evidence, legal argumentation, civil procedure, and Explainable AI approaches. We emphasise the need for explanations of AI systems in tort litigation to be attuned to the elements of legal ‘causes of action’ – the specific facts that must be proven to succeed in a lawsuit. We take a recent Australian case involving explainable AI evidence as a starting point from which to map contemporary Explainable AI approaches to elements of tortious causes of action, focusing on misleading conduct, negligence, and product liability for safety defects. Our work synthesizes law, legal procedure, and computer science to provide greater clarity on the opportunities and challenges for Explainable AI in civil litigation, and may prove helpful to potential litigants, to courts, and to illuminate key targets for regulatory intervention.",10.1145/3531146.3533084,https://doi.org/10.1145/3531146.3533084,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Fraser, Henry; Simcock, Rhyle; Snoswell, Aaron J.",2022,24,"@inproceedings{2-2209,
  title = {AI Opacity and Explainability in Tort Litigation},
  author = {Fraser, Henry and Simcock, Rhyle and Snoswell, Aaron J.},
  year = {2022},
  doi = {10.1145/3531146.3533084},
  booktitle = {ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}","Theoretical contributions, Empirical contributions",Law / Policy / Governance,Institutional,"Explaining, Executing","Decision-maker, Developer, Guardian",NA,NA,NA,NA,NA,Yes,No
2-22097,jstor,Rulemaking and inscrutable automated decision tools,"Complex machine learning models derived from personal data are increasingly used in making decisions important to peoples’ lives. These automated decision tools are controversial, in part because their operation is difficult for humans to grasp or explain. While scholars and policymakers have begun grappling with these explainability concerns, the debate has focused on explanations to decision subjects. This Essay argues that explainability has equally important normative and practical ramifications for decision-system design. Automated decision tools are particularly attractive when decisionmaking responsibility is delegated and distributed across multiple actors to handle large numbers of cases. Such decision systems depend on explanatory flows among those responsible for setting goals, developing decision criteria, and applying those criteria to particular cases. Inscrutable automated decision tools can disrupt all of these flows. This Essay focuses on explanation’s role in decision-criteria development, which it analogizes to rulemaking. It analyzes whether, and how, decision tool inscrutability undermines the traditional functions of explanation in rulemaking. It concludes that providing information about the many aspects of decision tool design, function, and use that can be explained can perform many of those traditional functions. Nonetheless, the technical inscrutability of machine learning models has significant ramifications for some decision contexts. Decision tool inscrutability makes it harder, for example, to assess whether decision criteria will generalize to unusual cases or new situations and heightens communication and coordination barriers between data scientists and subject matter experts. The Essay concludes with some suggested approaches for facilitating explanatory flows for decision-system design.",10.2307/26810852,http://www.jstor.org/stable/26810852,Columbia Law Review,Katherine J. Strandburg,2019,118,"@article{2-22097,
  title={Rulemaking and inscrutable automated decision tools},
  author={Strandburg, Katherine J.},
  year={2019},
  journal={Columbia Law Review},
  doi={10.2307/26810852}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance",Institutional,"Executing, Explaining, Advising","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-2212,acm,Fine-tuning Large Language Models to Improve Accuracy and Comprehensibility of Automated Code Review,"As code review is a tedious and costly software quality practice, researchers have proposed several machine learning-based methods to automate the process. The primary focus has been on accuracy, that is, how accurately the algorithms are able to detect issues in the code under review. However, human intervention still remains inevitable since results produced by automated code review are not 100% correct. To assist human reviewers in making their final decisions on automatically generated review comments, the comprehensibility of the comments underpinned by accurate localization and relevant explanations for the detected issues with repair suggestions is paramount. However, this has largely been neglected in the existing research. Large language models (LLMs) have the potential to generate code review comments that are more readable and comprehensible by humans thanks to their remarkable processing and reasoning capabilities. However, even mainstream LLMs perform poorly in detecting the presence of code issues because they have not been specifically trained for this binary classification task required in code review. In this paper, we contribute Carllm (Comprehensibility of Automated Code Review using Large Language Models), a novel fine-tuned LLM that has the ability to improve not only the accuracy but, more importantly, the comprehensibility of automated code review, as compared to state-of-the-art pre-trained models and general LLMs.",10.1145/3695993,https://doi.org/10.1145/3695993,ACM Transactions on Software Engineering and Methodology,"Yu, Yongda; Rong, Guoping; Shen, Haifeng; Zhang, He; Shao, Dong; Wang, Min; Wei, Zhao; Xu, Yong; Wang, Juhong",2024,2,"@article{2-2212,
  title={Fine-tuning Large Language Models to Improve Accuracy and Comprehensibility of Automated Code Review},
  author={Yu, Yongda and Rong, Guoping and Shen, Haifeng and Zhang, He and Shao, Dong and Wang, Min and Wei, Zhao and Xu, Yong and Wang, Juhong},
  year={2024},
  journal={ACM Transactions on Software Engineering and Methodology},
  doi={10.1145/3695993}
}",System/Artifact contributions,Software / Systems / Security,Organizational,"Explaining, Advising, Analyzing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-2215,acm,Globally Optimized Matchmaking in Online Games,"As one of the core components of online games, matchmaking is the process of arranging multiple players into matches, where the quality of matchmaking systems directly determines player satisfaction and further affects the life cycle of game products. With the number of candidate players increases, the number of possible match combinations grows exponentially, which makes the current implementation for multiplayer matchmaking can only obtain locally optimal arrangement in an inefficient fashion. In this paper, we focus on the globally optimized matchmaking problem, in which the objective is to decide an optimal matching sequence for the queuing players. To tackle this challenging problem, we propose a novel data-driven matchmaking framework, called GloMatch, based on machine learning principles. Through transforming the matchmaking problem into a sequential decision problem, we solve it with the help of an effective policy-based deep reinforcement learning algorithm. Quantitative experiments on simulation and online game environments demonstrate the effectiveness of the presented framework.",10.1145/3447548.3467074,https://doi.org/10.1145/3447548.3467074,ACM SIGKDD Conference on Knowledge Discovery and Data Mining,"Deng, Qilin; Li, Hao; Wang, Kai; Hu, Zhipeng; Wu, Runze; Gong, Linxia; Tao, Jianrong; Fan, Changjie; Cui, Peng",2021,21,"@inproceedings{2-2215,
  title     = {Globally Optimized Matchmaking in Online Games},
  author    = {Deng, Qilin and Li, Hao and Wang, Kai and Hu, Zhipeng and Wu, Runze and Gong, Linxia and Tao, Jianrong and Fan, Changjie and Cui, Peng},
  year      = {2021},
  booktitle = {Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3447548.3467074}
}",Algorithmic contributions,Media / Communication / Entertainment,Organizational,Executing,Decision-subject,NA,NA,NA,NA,NA,Yes,No
2-2216,acm,On the Distinction between Implicit and Explicit Ethical Agency,"With recent advances in artificial intelligence and the rapidly increasing importance of autonomous intelligent systems in society, it is becoming clear that artificial agents will have to be designed to comply with complex ethical standards. As we work to develop moral machines, we also push the boundaries of existing legal categories. The most pressing question is what kind of ethical decision-making our machines are actually able to engage in. Both in law and in ethics, the concept of agency forms a basis for further legal and ethical categorisations, pertaining to decision-making ability. Hence, without a cross-disciplinary understanding of what we mean by ethical agency in machines, the question of responsibility and liability cannot be clearly addressed. Here we make first steps towards a comprehensive definition, by suggesting ways to distinguish between implicit and explicit forms of ethical agency.",10.1145/3278721.3278769,https://doi.org/10.1145/3278721.3278769,"AAAI/ACM Conference on AI, Ethics, and Society","Dyrkolbotn, Sjur; Pedersen, Truls; Slavkovik, Marija",2018,0,"@inproceedings{2-2216,
  title        = {On the Distinction between Implicit and Explicit Ethical Agency},
  author       = {Dyrkolbotn, Sjur and Pedersen, Truls and Slavkovik, Marija},
  year         = {2018},
  doi          = {10.1145/3278721.3278769},
  booktitle    = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,no such info,"Advising, Executing",Guardian,NA,NA,NA,NA,NA,Yes,No
2-2228,acm,SenseMate: An Accessible and Beginner-Friendly Human-AI Platform for Qualitative Data Analysis,"Community organizations face challenges in harnessing the power of qualitative data analysis, or sensemaking, to understand the diverse perspectives and needs brought up by their constituents. One of the most time-consuming and tedious parts of sensemaking is qualitative coding, or the process of identifying themes across a large and unstructured corpus of community input. A challenge in qualitative coding is attaining high intercoder reliability, especially between expert and beginner sensemakers. In this work, we present SenseMate, a novel human-AI system designed to help with qualitative coding. SenseMate leverages rationale extraction models, a new machine learning strategy to semi-automate sensemaking, which produces theme recommendations and human-interpretable explanations. The models were trained on a dataset of people’s experiences living in Boston, which was annotated for themes by expert sensemakers. We integrated rationale extraction models into SenseMate through an iterative, human-centered design process revolving around four key design principles derived from an extensive literature review. The design process consisted of three iterations with continuous feedback from seven people associated with community organizations. Through an online experiment involving 180 novice sensemakers, we aimed to determine whether AI-generated recommendations and rationales would decrease coding time, increase intercoder reliability (i.e. Cohen’s kappa), and minimize differences between novice and expert coding decisions (i.e. F-score of participant answers compared to expert gold labels). We found that though the model recommendations and explanations increased coding time by 49 seconds per unit of analysis, they raised intercoder reliability by 29% and coding F-score by 10%. Regarding the effectiveness of SenseMate’s design, participants reported that the platform was generally easy to use. In summary, Sensemate is (1) built for beginner sensemakers without a technical background, a user group that prior work doesn’t focus on, (2) implements rationale extraction models to recommend themes and generate explanations, which has advantages over large language models in terms of user privacy and control, and (3) contains original and intuitive features created from user feedback that can be applied to future QDA systems.",10.1145/3640543.3645194,https://doi.org/10.1145/3640543.3645194,International Conference on Intelligent User Interfaces (IUI),"Overney, Cassandra; Saldías, Belén; Dimitrakopoulou, Dimitra; Roy, Deb",2024,25,"@inproceedings{2-2228,
  title = {SenseMate: An Accessible and Beginner-Friendly Human-AI Platform for Qualitative Data Analysis},
  author = {Overney, Cassandra and Sald{\'i}as, Bel{\'e}n and Dimitrakopoulou, Dimitra and Roy, Deb},
  year = {2024},
  doi = {10.1145/3640543.3645194},
  booktitle = {International Conference on Intelligent User Interfaces (IUI)}
}",System/Artifact contributions,Education / Teaching / Research,Operational,"Advising, Analyzing","Decision-maker, Knowledge provider","Change cognitive demands, Alter decision outcomes, Change trust",Change AI responses,"theme recommendations, rationales",NA,Interactive interface,Yes,Yes
2-2231,acm,Visualizing Uncertainty and Alternatives in Event Sequence Predictions,"Data analysts apply machine learning and statistical methods to timestamped event sequences to tackle various problems but face unique challenges when interpreting the results. Especially in event sequence prediction, it is difficult to convey uncertainty and possible alternative paths or outcomes. In this work, informed by interviews with five machine learning practitioners, we iteratively designed a novel visualization for exploring event sequence predictions of multiple records where users are able to review the most probable predictions and possible alternatives alongside uncertainty information. Through a controlled study with 18 participants, we found that users are more confident in making decisions when alternative predictions are displayed and they consider the alternatives more when deciding between two options with similar top predictions.",10.1145/3290605.3300803,https://doi.org/10.1145/3290605.3300803,Conference on Human Factors in Computing Systems,"Guo, Shunan; Du, Fan; Malik, Sana; Koh, Eunyee; Kim, Sungchul; Liu, Zhicheng; Kim, Donghyun; Zha, Hongyuan; Cao, Nan",2019,5,"@inproceedings{2-2231,
  title = {Visualizing Uncertainty and Alternatives in Event Sequence Predictions},
  author = {Guo, Shunan and Du, Fan and Malik, Sana and Koh, Eunyee and Kim, Sungchul and Liu, Zhicheng and Kim, Donghyun and Zha, Hongyuan and Cao, Nan},
  year = {2019},
  doi = {10.1145/3290605.3300803},
  booktitle = {Conference on Human Factors in Computing Systems}
}","Empirical contributions, System/Artifact contributions",Generic / Abstract / Domain-agnostic,Operational,"Forecasting, Advising","Decision-maker, Knowledge provider","Alter decision outcomes, Change affective-perceptual",no such info,"prediction of alternative, uncertainty",NA,Visual,Yes,Yes
2-22351,jstor,"The importance of imagination (or lack thereof) in artificial, human and quantum decision making","Enlarging upon experiments and analysis that I did jointly some years ago, in which artificial (symbolic, neural-net and pattern) learning and generalization were compared with that of humans, I will emphasize the role of imagination (or lack thereof) in artificial, human and quantum cognition and decision-making processes. Then I will look in more detail at some of the 'engineering details' of its implementation (or lack thereof) in each of these settings. In other words, the question posed is: What is actually happening? For example, we previously found that humans overwhelmingly seek, create or imagine context in order to provide meaning when presented with abstract, apparently incomplete, contradictory or otherwise untenable decision-making situations. Humans are intolerant of contradiction and will greatly simplify to avoid it. They can partially correlate but do not average. Human learning is not Boolean. These and other human reasoning properties will then be taken to critique how well artificial intelligence methods and quantum mechanical modelling might compete with them in decision-making tasks within psychology and economics.",10.2307/24758908,http://www.jstor.org/stable/24758908,"Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",Karl Gustafson,2016,13,"@article{2-22351,
  title={The importance of imagination (or lack thereof) in artificial, human and quantum decision making},
  author={Gustafson, Karl},
  year={2016},
  doi={10.2307/24758908},
  journal={Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy, Education / Teaching / Research",no such info,"Analyzing, Explaining","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-2238,acm,Values in Emotion Artificial Intelligence Hiring Services: Technosolutions to Organizational Problems,"Despite debates about emotion artificial intelligence's (EAI) validity, legality, and social consequences, EAI is increasingly present in the high stakes context of hiring, with potential to shape the future of work and the workforce. The values laden in technology play a significant role in its societal impact.We conducted qualitative content analysis on the public-facing websites (N=229) of EAI hiring services. We identify the organizational problems that EAI hiring services claim to solve and reveal the values emerging in desired EAI uses as promoted by EAI hiring services to solve organizational problems. Our findings show that EAI hiring services market their technologies as technosolutions to three purported organizational hiring problems: 1) hiring (in)accuracy, 2) hiring (mis)fit, and 3) hiring (in)authenticity. We unpack these problems to expose how these desired uses of EAI are legitimized by the corporate ideals of data-driven decision making, continuous improvement, precision, loyalty, and stability. We identify the unfair and deceptive mechanisms by which EAI hiring services claim to solve the purported organizational hiring problems, suggesting that they unfairly exclude and exploit job candidates through EAI's creation, extraction, and affective commodification of a candidate's affective value through pseudoscientific approaches. Lastly, we interrogate EAI hiring service claims to reveal the core values that underpin their stated desired use: techno-omnipresence, techno-omnipotence, and techno-omniscience. We show how EAI hiring services position desired use of their technology as a moral imperative for hiring organizations with supreme capabilities to solve organizational hiring problems, then discuss implications for fairness, ethics, and policy in EAI-enabled hiring within the US policy landscape.",10.1145/3579543,https://doi.org/10.1145/3579543,Proceedings of the ACM on Human-Computer Interaction,"Roemmich, Kat; Rosenberg, Tillie; Fan, Serena; Andalibi, Nazanin",2023,44,"@article{2-2238,
  title = {Values in Emotion Artificial Intelligence Hiring Services: Technosolutions to Organizational Problems},
  author = {Roemmich, Kat and Rosenberg, Tillie and Fan, Serena and Andalibi, Nazanin},
  year = {2023},
  doi = {10.1145/3579543},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Everyday / Employment / Public Service,Operational,"Analyzing, Advising","Decision-subject, Decision-maker","Change affective-perceptual, Alter decision outcomes, Shape ethical norms",no such info,"inferences, filtering, data storage, recommendations, causal explanations","emotion expression, fairness constraints",Autonomous System,Yes,Yes
2-2240,acm,Crystalline: Lowering the Cost for Developers to Collect and Organize Information for Decision Making,"Developers perform online sensemaking on a daily basis, such as researching and choosing libraries and APIs. Prior research has introduced tools that help developers capture information from various sources and organize it into structures useful for subsequent decision-making. However, it remains a laborious process for developers to manually identify and clip content, maintaining its provenance and synthesizing it with other content. In this work, we introduce a new system called Crystalline that automatically collects and organizes information into tabular structures as the user searches and browses the web. It leverages natural language processing to automatically group similar criteria together to reduce clutter, and uses passive behavioral signals such as mouse movement and dwell time to infer what information to collect and how to visualize and prioritize it. Our user study suggests that developers are able to create comparison tables about 20% faster with a 60% reduction in operational cost without sacrificing the quality of the tables.",10.1145/3491102.3501968,https://doi.org/10.1145/3491102.3501968,ACM CHI Conference on Human Factors in Computing Systems,"Liu, Michael Xieyang; Kittur, Aniket; Myers, Brad A.",2022,0,"@inproceedings{2-2240,
  title={Crystalline: Lowering the Cost for Developers to Collect and Organize Information for Decision Making},
  author={Liu, Michael Xieyang and Kittur, Aniket and Myers, Brad A.},
  year={2022},
  doi={10.1145/3491102.3501968},
  booktitle={ACM CHI Conference on Human Factors in Computing Systems}
}",System/Artifact contributions,Software / Systems / Security,Individual,Analyzing,"Decision-maker, Developer","Alter decision outcomes, Change cognitive demands, Change affective-perceptual, Restrict human agency",no such info,collected information,domain knowledge,Interactive interface,Yes,Yes
2-2244,acm,Designing Fair AI in Human Resource Management: Understanding Tensions Surrounding Algorithmic Evaluation and Envisioning Stakeholder-Centered Solutions,"Enterprises have recently adopted AI to human resource management (HRM) to evaluate employees’ work performance evaluation. However, in such an HRM context where multiple stakeholders are complexly intertwined with different incentives, it is problematic to design AI reflecting one stakeholder group's needs (e.g., enterprises, HR managers). Our research aims to investigate what tensions surrounding AI in HRM exist among stakeholders and explore design solutions to balance the tensions. By conducting stakeholder-centered participatory workshops with diverse stakeholders (including employees, employers/HR teams, and AI/business experts), we identified five major tensions: 1)&nbsp;divergent perspectives on fairness, 2)&nbsp;the accuracy of AI, 3)&nbsp;the transparency of the algorithm and its decision process, 4)&nbsp;the interpretability of algorithmic decisions, and 5)&nbsp;the trade-off between productivity and inhumanity. We present stakeholder-centered design ideas for solutions to mitigate these tensions and further discuss how to promote harmony among various stakeholders at the workplace.",10.1145/3491102.3517672,https://doi.org/10.1145/3491102.3517672,ACM CHI Conference on Human Factors in Computing Systems,"Park, Hyanghee; Ahn, Daehwan; Hosanagar, Kartik; Lee, Joonhwan",2022,85,"@inproceedings{2-2244,
  title = {Designing Fair AI in Human Resource Management: Understanding Tensions Surrounding Algorithmic Evaluation and Envisioning Stakeholder-Centered Solutions},
  author = {Park, Hyanghee and Ahn, Daehwan and Hosanagar, Kartik and Lee, Joonhwan},
  year = {2022},
  doi = {10.1145/3491102.3517672},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Everyday / Employment / Public Service,Organizational,Advising,"Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-2245,acm,"A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist","Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets. While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks. To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading. FinAgent's market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market. Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retrieval system, enhancing the agent's ability to learn from historical data and improve decision-making processes. The agent's emphasis on reasoning for actions fosters trust in its financial decisions. Moreover, FinAgent integrates established trading strategies and expert insights, ensuring that its trading approaches are both data-driven and rooted in sound financial principles. With comprehensive experiments on 6 financial datasets, including stocks and Crypto, FinAgent significantly outperforms 12 state-of-the-art baselines in terms of 6 financial metrics with over 36% average improvement on profit. Specifically, a 92.27% return (a 84.39% relative improvement) is achieved on one dataset. Notably, FinAgent is the first advanced multimodal foundation agent designed for financial trading tasks.",10.1145/3637528.3671801,https://doi.org/10.1145/3637528.3671801,ACM SIGKDD Conference on Knowledge Discovery and Data Mining,"Zhang, Wentao; Zhao, Lingxuan; Xia, Haochong; Sun, Shuo; Sun, Jiaze; Qin, Molei; Li, Xinyi; Zhao, Yuqing; Zhao, Yilei; Cai, Xinyu; Zheng, Longtao; Wang, Xinrun; An, Bo",2024,123,"@inproceedings{2-2245,
  title     = {A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist},
  author    = {Zhang, Wentao and Zhao, Lingxuan and Xia, Haochong and Sun, Shuo and Sun, Jiaze and Qin, Molei and Li, Xinyi and Zhao, Yuqing and Zhao, Yilei and Cai, Xinyu and Zheng, Longtao and Wang, Xinrun and An, Bo},
  year      = {2024},
  doi       = {10.1145/3637528.3671801},
  booktitle = {ACM SIGKDD Conference on Knowledge Discovery and Data Mining}
}","System/Artifact contributions, Algorithmic contributions",Finance / Business / Economy,Operational,"Executing, Forecasting, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-2248,acm,Investigating Explainability of Generative AI for Code through Scenario-based Design,"What does it mean for a generative AI model to be explainable? The emergent discipline of explainable AI (XAI) has made great strides in helping people understand discriminative models. Less attention has been paid to generative models that produce artifacts, rather than decisions, as output. Meanwhile, generative AI (GenAI) technologies are maturing and being applied to application domains such as software engineering. Using scenario-based design and question-driven XAI design approaches, we explore users’ explainability needs for GenAI in three software engineering use cases: natural language to code, code translation, and code auto-completion. We conducted 9 workshops with 43 software engineers in which real examples from state-of-the-art generative AI models were used to elicit users’ explainability needs. Drawing from prior work, we also propose 4 types of XAI features for GenAI for code and gathered additional design ideas from participants. Our work explores explainability needs for GenAI for code and demonstrates how human-centered approaches can drive the technical development of XAI in novel domains.",10.1145/3490099.3511119,https://doi.org/10.1145/3490099.3511119,International Conference on Intelligent User Interfaces (IUI),"Sun, Jiao; Liao, Q. Vera; Muller, Michael; Agarwal, Mayank; Houde, Stephanie; Talamadupula, Kartik; Weisz, Justin D.",2022,1,"@inproceedings{2-2248,
  title={Investigating Explainability of Generative AI for Code through Scenario-based Design},
  author={Sun, Jiao and Liao, Q. Vera and Muller, Michael and Agarwal, Mayank and Houde, Stephanie and Talamadupula, Kartik and Weisz, Justin D.},
  year={2022},
  doi={10.1145/3490099.3511119},
  booktitle={International Conference on Intelligent User Interfaces (IUI)}
}",Empirical contributions,Software / Systems / Security,Individual,"Analyzing, Advising, Explaining","Decision-maker, Knowledge provider","Change cognitive demands, Change affective-perceptual","Change AI responses, Shape AI for accountability, Update AI competence","""more information like suggesting alternative outputs,
providing uncertainty explanations, and giving more fine-grained
uncertainty"", more options or alternatives with specific characteristics, uncertainty explanation, more detailed information about uncertainty threshold, attention distribution, local explanations","clarification, confirmation, preferences",Interactive interface,Yes,Yes
2-2250,acm,How Is the Stroke? Inferring Shot Influence in Badminton Matches via Long Short-term Dependencies,"Identifying significant shots in a rally is important for evaluating players’ performance in badminton matches. While there are several studies that have quantified player performance in other sports, analyzing badminton data has remained untouched. In this article, we introduce a badminton language to fully describe the process of the shot, and we propose a deep-learning model composed of a novel short-term extractor and a long-term encoder for capturing a shot-by-shot sequence in a badminton rally by framing the problem as predicting a rally result. Our model incorporates an attention mechanism to enable the transparency between the action sequence and the rally result, which is essential for badminton experts to gain interpretable predictions. Experimental evaluation based on a real-world dataset demonstrates that our proposed model outperforms the strong baselines. We also conducted case studies to show the ability to enhance players’ decision-making confidence and to provide advanced insights for coaching, which benefits the badminton analysis community and bridges the gap between the field of badminton and computer science.",10.1145/3551391,https://doi.org/10.1145/3551391,ACM Transactions on Intelligent Systems and Technology,"Wang, Wei-Yao; Chan, Teng-Fong; Peng, Wen-Chih; Yang, Hui-Kuo; Wang, Chih-Chuan; Fan, Yao-Chung",2022,19,"@article{2-2250,
  title={How Is the Stroke? Inferring Shot Influence in Badminton Matches via Long Short-term Dependencies},
  author={Wang, Wei-Yao and Chan, Teng-Fong and Peng, Wen-Chih and Yang, Hui-Kuo and Wang, Chih-Chuan and Fan, Yao-Chung},
  year={2022},
  doi={10.1145/3551391},
  journal={ACM Transactions on Intelligent Systems and Technology}
}",Algorithmic contributions,Media / Communication / Entertainment,Individual,"Forecasting, Analyzing, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-22508,jstor,"Uncertainty in learning, choice, and visual fixation","Uncertainty plays a critical role in reinforcement learning and decision making. However, exactly how it influences behavior remains unclear. Multiarmed-bandit tasks offer an ideal test bed, since computational tools such as approximate Kalman filters can closely characterize the interplay between trial-by-trial values, uncertainty, learning, and choice. To gain additional insight into learning and choice processes, we obtained data from subjects’ overt allocation of gaze. The estimated value and estimation uncertainty of options influenced what subjects looked at before choosing; these same quantities also influenced choice, as additionally did fixation itself. A momentary measure of uncertainty in the form of absolute prediction errors determined how long participants looked at the obtained outcomes. These findings affirm the importance of uncertainty in multiple facets of behavior and help delineate its effects on decision making.",10.2307/26928779,http://www.jstor.org/stable/26928779,Proceedings of the National Academy of Sciences of the United States of America (PNAS),Hrvoje Stojić;Jacob L. Orquin;Peter Dayan;Raymond J. Dolan;Maarten Speekenbrink,2020,47,"@article{2-22508,
  title={Uncertainty in learning, choice, and visual fixation},
  author={Stojić, Hrvoje and Orquin, Jacob L. and Dayan, Peter and Dolan, Raymond J. and Speekenbrink, Maarten},
  year={2020},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  volume={117},
  number={6},
  pages={2791--2805},
  doi={10.2307/26928779}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research",Individual,Analyzing,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-2252,acm,How do visual explanations foster end users' appropriate trust in machine learning?,"We investigated the effects of example-based explanations for a machine learning classifier on end users' appropriate trust. We explored the effects of spatial layout and visual representation in an in-person user study with 33 participants. We measured participants' appropriate trust in the classifier, quantified the effects of different spatial layouts and visual representations, and observed changes in users' trust over time. The results show that each explanation improved users' trust in the classifier, and the combination of explanation, human, and classification algorithm yielded much better decisions than the human and classification algorithm separately. Yet these visual explanations lead to different levels of trust and may cause inappropriate trust if an explanation is difficult to understand. Visual representation and performance feedback strongly affect users' trust, and spatial layout shows a moderate effect. Our results do not support that individual differences (e.g., propensity to trust) affect users' trust in the classifier. This work advances the state-of-the-art in trust-able machine learning and informs the design and appropriate use of automated systems.",10.1145/3377325.3377480,https://doi.org/10.1145/3377325.3377480,International Conference on Intelligent User Interfaces (IUI),"Yang, Fumeng; Huang, Zhuanyi; Scholtz, Jean; Arendt, Dustin L.",2020,281,"@inproceedings{2-2252,
  title = {How do visual explanations foster end users' appropriate trust in machine learning?},
  author = {Yang, Fumeng and Huang, Zhuanyi and Scholtz, Jean and Arendt, Dustin L.},
  year = {2020},
  doi = {10.1145/3377325.3377480},
  booktitle = {International Conference on Intelligent User Interfaces (IUI)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research",Individual,"Explaining, Forecasting",Decision-maker,"Change cognitive demands, Change affective-perceptual, Change trust, Alter decision outcomes",no such info,"visual explanations, spatial layouts",NA,Visual,Yes,Yes
2-2256,acm,Face Value? Exploring the Effects of Embodiment for a Group Facilitation Agent,"We are interested in increasing the ability of groups to collaborate efficiently by leveraging new advances in AI and Conversational Agent (CA) technology. Given the longstanding debate on the necessity of embodiment for CAs, bringing them to groups requires answering the questions of whether and how providing a CA with a face affects its interaction with the humans in a group. We explored these questions by comparing group decision-making sessions facilitated by an embodied agent, versus a voice-only agent. Results of an experiment with 20 user groups revealed that while the embodiment improved various aspects of group's social perception of the agent (e.g., rapport, trust, intelligence, and power), its impact on the group-decision process and outcome was nuanced. Drawing on both quantitative and qualitative findings, we discuss the pros and cons of embodiment, argue that the value of having a face depends on the types of assistance the agent provides, and lay out directions for future research.",10.1145/3173574.3173965,https://doi.org/10.1145/3173574.3173965,ACM CHI Conference on Human Factors in Computing Systems,"Shamekhi, Ameneh; Liao, Q. Vera; Wang, Dakuo; Bellamy, Rachel K. E.; Erickson, Thomas",2018,177,"@inproceedings{2-2256,
  title     = {Face Value? Exploring the Effects of Embodiment for a Group Facilitation Agent},
  author    = {Shamekhi, Ameneh and Liao, Q. Vera and Wang, Dakuo and Bellamy, Rachel K. E. and Erickson, Thomas},
  year      = {2018},
  doi       = {10.1145/3173574.3173965},
  booktitle = {Proceedings of the ACM CHI Conference on Human Factors in Computing Systems},
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Organizational,"Executing, Advising",Decision-maker,"Change cognitive demands, Change affective-perceptual, Change trust",no such info,facial expressions,NA,"Auditory, Physical / Embodiment",Yes,Yes
2-2258,acm,Aggregating E-commerce Search Results from Heterogeneous Sources via Hierarchical Reinforcement Learning,"In this paper, we investigate the task of aggregating search results from heterogeneous sources in an E-commerce environment. First, unlike traditional aggregated web search that merely presents multi-sourced results in the first page, this new task may present aggregated results in all pages and has to dynamically decide which source should be presented in the current page. Second, as pointed out by many existing studies, it is not trivial to rank items from heterogeneous sources because the relevance scores from different source systems are not directly comparable. To address these two issues, we decompose the task into two subtasks in a hierarchical structure: a high-level task for source selection where we model the sequential patterns of user behaviors onto aggregated results in different pages so as to understand user intents and select the relevant sources properly; and a low-level task for item presentation where we formulate a slot filling process to sequentially present the items instead of giving each item a relevance score when deciding the presentation order of heterogeneous items. Since both subtasks can be naturally formulated as sequential decision problems and learn from the future user feedback on search results, we build our model with hierarchical reinforcement learning. Extensive experiments demonstrate that our model obtains remarkable improvements in search performance metrics, and achieves a higher user satisfaction.",10.1145/3308558.3313455,https://doi.org/10.1145/3308558.3313455,The Web Conference (formerly known as the World Wide Web Conference),"Takanobu, Ryuichi; Zhuang, Tao; Huang, Minlie; Feng, Jun; Tang, Haihong; Zheng, Bo",2019,27,"@inproceedings{2-2258,
  title={Aggregating E-commerce Search Results from Heterogeneous Sources via Hierarchical Reinforcement Learning},
  author={Takanobu, Ryuichi and Zhuang, Tao and Huang, Minlie and Feng, Jun and Tang, Haihong and Zheng, Bo},
  year={2019},
  doi={10.1145/3308558.3313455},
  booktitle={The Web Conference (formerly known as the World Wide Web Conference)}
}",Algorithmic contributions,Finance / Business / Economy,Operational,Executing,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-22654,mlr,Mathematical framework for online social media auditing,"Social media platforms (SMPs) leverage algorithmic filtering (AF) as a means of selecting the content that constitutes a user's feed with the aim of maximizing their rewards. Selectively choosing the contents to be shown on the user's feed may yield a certain extent of influence, either minor or major, on the user's decision-making, compared to what it would have been under a natural/fair content selection. As we have witnessed over the past decade, algorithmic filtering can cause detrimental side effects, ranging from biasing individual decisions to shaping those of society as a whole, for example, diverting users' attention from whether to get the COVID-19 vaccine or inducing the public to choose a presidential candidate. The government's constant attempts to regulate the adverse effects of AF are often complicated, due to bureaucracy, legal affairs, and financial considerations. On the other hand SMPs seek to monitor their own algorithmic activities to avoid being fined for exceeding the allowable threshold. In this paper, we mathematically formalize this framework and utilize it to construct a data-driven statistical auditing procedure to regulate AF from deflecting users' beliefs over time, along with sample complexity guarantees. This state-of-the-art algorithm can be used either by authorities acting as external regulators or by SMPs for self-auditing.",NA,https://www.jmlr.org/papers/v25/22-1112.html,Journal of Machine Learning Research,Wasim Huleihel;Yehonathan Refael,2024,4,"@article{2-22654,
  title={Mathematical framework for online social media auditing},
  author={Huleihel, Wasim and Refael, Yehonathan},
  year={2024},
  journal={Journal of Machine Learning Research}
}",Methodological contributions,Media / Communication / Entertainment,Institutional,Auditing,"Guardian, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-22657,mlr,Learning-to-defer for sequential medical decision-making under uncertainty,"Learning-to-defer is a framework to automatically defer decision-making to a human expert when ML-based decisions are deemed unreliable. Existing learning-to-defer frameworks are not designed for sequential settings. That is, they defer at every instance independently, based on immediate predictions, while ignoring the potential long-term impact of these interventions. As a result, existing frameworks are myopic. Further, they do not defer adaptively, which is crucial when human interventions are costly. In this work, we propose Sequential Learning-to-Defer (SLTD), a framework for learning-to-defer to a domain expert in sequential decision-making settings. Contrary to existing literature, we pose the problem of learning-to-defer as model-based reinforcement learning (RL) to i) account for long-term consequences of ML-based actions using RL and ii) adaptively defer based on the dynamics (model-based). Our proposed framework determines whether to defer (at each time step) by quantifying whether a deferral now will improve the value compared to delaying deferral to the next time step. To quantify the improvement, we account for potential future deferrals. As a result, we learn a pre-emptive deferral policy (i.e. a policy that defers early if using the ML-based policy could worsen long-term outcomes). Our deferral policy is adaptive to the non-stationarity in the dynamics. We demonstrate that adaptive deferral via SLTD provides an improved trade-off between long-term outcomes and deferral frequency on synthetic, semi-synthetic, and real-world data with non-stationary dynamics. Finally, we interpret the deferral decision by decomposing the propagated (long-term) uncertainty around the outcome, to justify the deferral decision.",NA,https://openreview.net/forum?id=0pn3KnbH5F,Transactions on Machine Learning Research,Shalmali Joshi;Sonali Parbhoo;Finale Doshi-Velez,2024,0,"@article{2-22657,
  title={Learning-to-defer for sequential medical decision-making under uncertainty},
  author={Joshi, Shalmali and Parbhoo, Sonali and Doshi-Velez, Finale},
  year={2024},
  journal={Transactions on Machine Learning Research}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,Executing,"Decision-maker, Knowledge provider, Decision-subject",NA,NA,system accuracy,NA,NA,Yes,No
2-2268,acm,MAIDR Meets AI: Exploring Multimodal LLM-Based Data Visualization Interpretation by and with Blind and Low-Vision Users,"This paper investigates how blind and low-vision (BLV) users interact with multimodal large language models (LLMs) to interpret data visualizations. Building upon our previous work on the multimodal access and interactive data representation (MAIDR) framework, our mixed-visual-ability team co-designed maidrAI, an LLM extension providing multiple AI responses to users’ visual queries. To explore generative AI-based data representation, we conducted user studies with 8 BLV participants, tasking them with interpreting box plots using our system. We examined how participants personalize LLMs through prompt engineering, their preferences for data visualization descriptions, and strategies for verifying LLM responses. Our findings highlight three dimensions affecting BLV users’ decision-making process: modal preference, LLM customization, and multimodal data representation. This research contributes to designing more accessible data visualization tools for BLV users and advances the understanding of inclusive generative AI applications.",10.1145/3663548.3675660,https://doi.org/10.1145/3663548.3675660,International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS),"Seo, JooYoung; Kamath, Sanchita S.; Zeidieh, Aziz; Venkatesh, Saairam; McCurry, Sean",2024,14,"@inproceedings{2-2268,
  title = {MAIDR Meets AI: Exploring Multimodal LLM-Based Data Visualization Interpretation by and with Blind and Low-Vision Users},
  author = {Seo, JooYoung and Kamath, Sanchita S. and Zeidieh, Aziz and Venkatesh, Saairam and McCurry, Sean},
  year = {2024},
  doi = {10.1145/3663548.3675660},
  booktitle = {Proceedings of the International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS)}
}",Empirical contributions,Education / Teaching / Research,Individual,"Explaining, Advising",Decision-maker,Alter decision outcomes,Change AI responses,"information prompts, illustrative figure, accessibility interactions","simple feedback, language proficiency, personalized settings","Textual, Conversational/Natural Language, Visual",Yes,Yes
2-2271,acm,Human-computer Coalition Formation in Weighted Voting Games,"This article proposes a negotiation game, based on the weighted voting paradigm in cooperative game theory, where agents need to form coalitions and agree on how to share the gains. Despite the prevalence of weighted voting in the real world, there has been little work studying people’s behavior in such settings. This work addresses this gap by combining game-theoretic solution concepts with machine learning models for predicting human behavior in such domains. We present a five-player online version of a weighted voting game in which people negotiate to create coalitions. We provide an equilibrium analysis of this game and collect hundreds of instances of people’s play in the game. We show that a machine learning model with features based on solution concepts from cooperative game theory (in particular, an extension of the Deegan-Packel Index) provide a good prediction of people’s decisions to join coalitions in the game. We designed an agent that uses the prediction model to make offers to people in this game and was able to outperform other people in an extensive empirical study. These results demonstrate the benefit of incorporating concepts from cooperative game theory in the design of agents that interact with people in group decision-making settings.",10.1145/3408294,https://doi.org/10.1145/3408294,ACM Transactions on Intelligent Systems and Technology,"Mash, Moshe; Fairstein, Roy; Bachrach, Yoram; Gal, Kobi; Zick, Yair",2020,0,"@article{2-2271,
  title={Human-computer Coalition Formation in Weighted Voting Games},
  author={Mash, Moshe and Fairstein, Roy and Bachrach, Yoram and Gal, Kobi and Zick, Yair},
  year={2020},
  doi={10.1145/3408294},
  journal={ACM Transactions on Intelligent Systems and Technology}
}",System/Artifact contributions,Generic / Abstract / Domain-agnostic,Operational,"Forecasting, Advising, Analyzing, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-22731,mlr,Adjusting machine learning decisions for equal opportunity and counterfactual fairness,"Machine learning (ML) methods have the potential to automate high-stakes decisions, such as bail admissions or credit lending, by analyzing and learning from historical data. But these algorithmic decisions may be unfair: in learning from historical data, they may replicate discriminatory practices from the past. In this paper, we propose two algorithms that adjust fitted ML predictors to produce decisions that are fair. Our methods provide post-hoc adjustments to the predictors, without requiring that they be retrained. We consider a causal model of the ML decisions, define fairness through counterfactual decisions within the model, and then form algorithmic decisions that capture the historical data as well as possible but are provably fair. In particular, we consider two definitions of fairness. The first is ``equal counterfactual opportunity,'' where the counterfactual distribution of the decision is the same regardless of the protected attribute; the second is counterfactual fairness. We evaluate the algorithms, and the trade-off between accuracy and fairness, on datasets about admissions, income, credit, and recidivism.",NA,https://openreview.net/forum?id=P6NcRPb13w,Transactions on Machine Learning Research,Yixin Wang;Dhanya Sridhar;David Blei,2024,148,"@article{2-22731,
  title={Adjusting machine learning decisions for equal opportunity and counterfactual fairness},
  author={Wang, Yixin and Sridhar, Dhanya and Blei, David},
  year={2024},
  journal={Transactions on Machine Learning Research}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance, Finance / Business / Economy",Operational,"Forecasting, Advising, Executing","Guardian, Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-2276,acm,Multi-objective framework for quantile forecasting in financial time series using transformers,"The uncertainty associated with predictions is vital for decision making in financial time series forecasting. For the sake of enriching forecasts, quantile interval predictions are generated and their quality is evaluated using two conflicting indicators: quantile coverage error and quantile estimation error, which are later optimized using multi-objective evolutionary algorithms (MOEAs). The high performance quantile multi-horizon predictions are computed by an attention-based deep learning model based on transformers and the weights of its last layer are optimized using NSGA-II and NSGA-III. The Pareto fronts obtained in this work show that evolutionary algorithms can find a wide range of solutions that allow the decision maker to efficiently fine-tune the quantile forecasts without need of retraining the neural network. The results show that the decision maker can choose solutions whose risks' ranges variation is as high as 169% with only an increase of 5% in the original loss function. The dataset used in this work consists of S&amp;P 500 Futures from Jan-2015 to Jun-2021 with one-hour frequency.",10.1145/3512290.3528740,https://doi.org/10.1145/3512290.3528740,Genetic and Evolutionary Computation Conference (GECCO),"López-Ruiz, Samuel; Hernández-Castellanos, Carlos Ignacio; Rodríguez-Vázquez, Katya",2022,9,"@inproceedings{2-2276,
  title = {Multi-objective framework for quantile forecasting in financial time series using transformers},
  author = {López-Ruiz, Samuel and Hernández-Castellanos, Carlos Ignacio and Rodríguez-Vázquez, Katya},
  year = {2022},
  doi = {10.1145/3512290.3528740},
  booktitle = {Genetic and Evolutionary Computation Conference (GECCO)}
}",Methodological contributions,Finance / Business / Economy,Operational,"Forecasting, Advising, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-22763,mlr,Machine explanations and human understanding,"Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. To address this question, we first identify three core concepts that cover most existing quantitative measures of understanding: task decision boundary, model decision boundary, and model error. Using adapted causal diagrams, we provide a formal characterization of the relationship between these concepts and human approximations (i.e., understanding) of them. The relationship varies by the level of human intuition in different task types, such as emulation and discovery, which are often ignored when building or evaluating explanation methods. Our key result is that human intuitions are necessary for generating and evaluating machine explanations in human-AI decision making: without assumptions about human intuitions, explanations may improve human understanding of model decision boundary, but cannot improve human understanding of task decision boundary or model error. To validate our theoretical claims, we conduct human subject studies to show the importance of human intuitions. Together with our theoretical contributions, we provide a new paradigm for designing behavioral studies towards a rigorous view of the role of machine explanations across different tasks of human-AI decision making.",NA,https://openreview.net/forum?id=y4CGF1A8VG,Transactions on Machine Learning Research,Chacha Chen;Shi Feng;Amit Sharma;Chenhao Tan,2024,5,"@article{2-22763,
  title={Machine explanations and human understanding},
  author={Chen, Chacha and Feng, Shi and Sharma, Amit and Tan, Chenhao},
  year={2024},
  journal={Transactions on Machine Learning Research}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,no such info,Explaining,"Decision-maker, Knowledge provider",Alter decision outcomes,Update AI competence,"natural language explanations, visual explanations","domain knowledge, intuition","Textual, Visual",Yes,Yes
2-22809,mlr,Fairness constraints: a flexible approach for fair classification,"Algorithmic decision making is employed in an increasing number of real-world applicationstions to aid human decision making. While it has shown considerable promise in terms of improved decision accuracy, in some scenarios, its outcomes have been also shown to impose an unfair (dis)advantage on people from certain social groups (e.g., women, blacks). In this context, there is a need for computational techniques to limit unfairness in algorithmic decision making. In this work, we take a step forward to fulfill that need and introduce a flexible constraint-based framework to enable the design of fair margin-based classifiers. The main technical innovation of our framework is a general and intuitive measure of decision boundary unfairness, which serves as a tractable proxy to several of the most popular computational definitions of unfairness from the literature. Leveraging our measure, we can reduce the design of fair margin-based classifiers to adding tractable constraints on their decision boundaries. Experiments on multiple synthetic and real-world datasets show that our framework is able to successfully limit unfairness, often at a small cost in terms of accuracy.",NA,https://www.jmlr.org/papers/v20/18-262.html,Journal of Machine Learning Research,Muhammad Bilal Zafar;Isabel Valera;Manuel Gomez-Rodriguez;Krishna P. Gummadi,2019,525,"@article{2-22809,
  title={Fairness constraints: a flexible approach for fair classification},
  author={Zafar, Muhammad Bilal and Valera, Isabel and Gomez-Rodriguez, Manuel and Gummadi, Krishna P.},
  year={2019},
  journal={Journal of Machine Learning Research}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Institutional,"Analyzing, Forecasting, Executing","Developer, Guardian",NA,NA,NA,NA,NA,Yes,No
2-2284,acm,Dynamic Agent Affiliation: Who Should the AI Agent Work for in the Older Adult's Care Network?,"The population of older adults experiencing cognitive decline is growing faster than the number of workers who can care for them. Artificially intelligent (AI) agents could assist these older adults, keeping them in their homes longer. For this to happen, older adults must be willing to adopt and rely on agents. Would they trust an agent that might need to report their decline to others? We conducted a speed dating study exploring the impact of agent affiliation (i.e., who the agent should work for). Our healthy and declining participants reacted positively to the idea of agents supporting them. They particularly recognized how the agent would reduce the burden placed on their family caregivers. They viewed affiliation to be dynamic, shifting from the declining older adult and orienting more to their caregivers over the course of cognitive decline. They envisioned the agent modifying its decision-making process to be like their caregivers’.",10.1145/3643834.3661500,https://doi.org/10.1145/3643834.3661500,ACM Designing Interactive Systems Conference,"Chang, Mai Lee; Lee, Alicia (Hyun Jin); Han, Nara; Huang, Anna; Simão, Hugo; Reig, Samantha; Mohammad Ali, Abdullah Ubed; Martinez, Rebekah; Khanuja, Neeta M; Zimmerman, John; Forlizzi, Jodi; Steinfeld, Aaron",2024,7,"@inproceedings{2-2284,
  title = {Dynamic Agent Affiliation: Who Should the AI Agent Work for in the Older Adult's Care Network?},
  author = {Chang, Mai Lee and Lee, Alicia (Hyun Jin) and Han, Nara and Huang, Anna and Sim{\~a}o, Hugo and Reig, Samantha and Mohammad Ali, Abdullah Ubed and Martinez, Rebekah and Khanuja, Neeta M and Zimmerman, John and Forlizzi, Jodi and Steinfeld, Aaron},
  year = {2024},
  doi = {10.1145/3643834.3661500},
  booktitle = {Proceedings of the ACM Designing Interactive Systems Conference}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Collaborating, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,Autonomous System,Yes,No
2-2288,acm,Dynamic Graph-based Deep Reinforcement Learning with Long and Short-term Relation Modeling for Portfolio Optimization,"Portfolio optimization is a significant concern in finance. Existing research on portfolio optimization fails to adequately learn from the long and short-term relationships among equities, which inevitably leads to suboptimal performance. In this paper, we propose a Dynamic Graph-based Deep Reinforcement Learning (DGDRL) for optimal portfolio decisions. We achieve this goal by devising two mechanisms for naturally modeling the financial market. Firstly, we utilize the static and dynamic graphs to represent the long and short-term relations, which are then naturally represented by the proposed multi-channel graph attention neural network. Secondly, compared with the traditional two-phase approach, forecasting equity's trend and then weighting them by combinatorial optimization, we naturally optimize the portfolio decisions, which could directly guide the model to converge to optimal rewards. Through extensive experiments on three real-world datasets, we have demonstrated that our method significantly outperforms state-of-the-art benchmark methods in portfolio management. Furthermore, the evaluation of the industrial trading system has shown the applicability of our model to real-world financial markets.",10.1145/3627673.3680039,https://doi.org/10.1145/3627673.3680039,ACM International Conference on Information and Knowledge Management (CIKM),"Sun, Haoyu; Bian, Yuxuan; Han, Li; Zhu, Peng; Cheng, Dawei; Liang, Yuqi",2024,0,"@inproceedings{2-2288,
  title = {Dynamic Graph-based Deep Reinforcement Learning with Long and Short-term Relation Modeling for Portfolio Optimization},
  author = {Sun, Haoyu and Bian, Yuxuan and Han, Li and Zhu, Peng and Cheng, Dawei and Liang, Yuqi},
  booktitle = {Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM)},
  year = {2024},
  doi = {10.1145/3627673.3680039}
}",Algorithmic contributions,Finance / Business / Economy,Individual,"Executing, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-22884,mlr,Steering social activity: a stochastic optimal control point of view,"User engagement in online social networking depends critically on the level of social activity in the corresponding platform---the number of online actions, such as posts, shares or replies, taken by their users. Can we design data-driven algorithms to increase social activity? At a user level, such algorithms may increase activity by helping users decide when to take an action to be more likely to be noticed by their peers. At a network level, they may increase activity by incentivizing a few influential users to take more actions, which in turn will trigger additional actions by other users. In this paper, we model social activity using the framework of marked temporal point processes, derive an alternate representation of these processes using stochastic differential equations (SDEs) with jumps and, exploiting this alternate representation, develop two efficient online algorithms with provable guarantees to steer social activity both at a user and at a network level. In doing so, we establish a previously unexplored connection between optimal control of jump SDEs and doubly stochastic marked temporal point processes, which is of independent interest. Finally, we experiment both with synthetic and real data gathered from Twitter and show that our algorithms consistently steer social activity more effectively than the state of the art.",NA,https://www.jmlr.org/papers/v18/17-416.html,Journal of Machine Learning Research,Ali Zarezade;Abir De;Utkarsh Upadhyay;Hamid R. Rabiee;Manuel Gomez-Rodriguez,2018,33,"@article{2-22884,
  title={Steering social activity: a stochastic optimal control point of view},
  author={Zarezade, Ali and De, Abir and Upadhyay, Utkarsh and Rabiee, Hamid R. and Gomez-Rodriguez, Manuel},
  year={2018},
  journal={Journal of Machine Learning Research}
}",Algorithmic contributions,Media / Communication / Entertainment,Organizational,"Advising, Analyzing","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-22913,mlr,Estimation and optimization of composite outcomes,"There is tremendous interest in precision medicine as a means to improve patient outcomes by tailoring treatment to individual characteristics. An individualized treatment rule formalizes precision medicine as a map from patient information to a recommended treatment. A treatment rule is defined to be optimal if it maximizes the mean of a scalar outcome in a population of interest, e.g., symptom reduction. However, clinical and intervention scientists often seek to balance multiple and possibly competing outcomes, e.g., symptom reduction and the risk of an adverse event. One approach to precision medicine in this setting is to elicit a composite outcome which balances all competing outcomes; unfortunately, eliciting a composite outcome directly from patients is difficult without a high-quality instrument, and an expert-derived composite outcome may not account for heterogeneity in patient preferences. We propose a new paradigm for the study of precision medicine using observational data that relies solely on the assumption that clinicians are approximately (i.e., imperfectly) making decisions to maximize individual patient utility. Estimated composite outcomes are subsequently used to construct an estimator of an individualized treatment rule which maximizes the mean of patient-specific composite outcomes. The estimated composite outcomes and estimated optimal individualized treatment rule provide new insights into patient preference heterogeneity, clinician behavior, and the value of precision medicine in a given domain. We derive inference procedures for the proposed estimators under mild conditions and demonstrate their finite sample performance through a suite of simulation experiments and an illustrative application to data from a study of bipolar depression.",NA,https://www.jmlr.org/papers/v22/20-429.html,Journal of Machine Learning Research,Daniel J. Luckett;Eric B. Laber;Siyeon Kim;Michael R. Kosorok,2021,29,"@article{2-22913,
  title={Estimation and optimization of composite outcomes},
  author={Luckett, Daniel J. and Laber, Eric B. and Kim, Siyeon and Kosorok, Michael R.},
  year={2021},
  journal={Journal of Machine Learning Research}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-22928,mlr,Selection by prediction with conformal p-values,"Decision making or scientific discovery pipelines such as job hiring and drug discovery often involve multiple stages: before any resource-intensive step, there is often an initial screening that uses predictions from a machine learning model to shortlist a few candidates from a large pool. We study screening procedures that aim to select candidates whose unobserved outcomes exceed user-specified values. We develop a method that wraps around any prediction model to produce a subset of candidates while controlling the proportion of falsely selected units. Building upon the conformal inference framework, our method first constructs p-values that quantify the statistical evidence for large outcomes; it then determines the shortlist by comparing the p-values to a threshold introduced in the multiple testing literature. In many cases, the procedure selects candidates whose predictions are above a data-dependent threshold. Our theoretical guarantee holds under mild exchangeability conditions on the samples, generalizing existing results on multiple conformal p-values. We demonstrate the empirical performance of our method via simulations, and apply it to job hiring and drug discovery datasets.",NA,https://www.jmlr.org/papers/v24/22-1176.html,Journal of Machine Learning Research,Ying Jin;Emmanuel J. Candes,2023,70,"@article{2-22928,
  title={Selection by prediction with conformal p-values},
  author={Jin, Ying and Candes, Emmanuel J.},
  year={2023},
  journal={Journal of Machine Learning Research}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery, Everyday / Employment / Public Service",Operational,"Forecasting, Advising, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-22930,mlr,The disagreement problem in explainable machine learning: a practitioner’s perspective,"As various post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to develop a deeper understanding of if and when the explanations output by these methods disagree with each other, and how such disagreements are resolved in practice. However, there is little to no research that provides answers to these critical questions. In this work, we introduce and study the disagreement problem in explainable machine learning. More specifically, we formalize the notion of disagreement between explanations, analyze how often such disagreements occur in practice, and how practitioners resolve these disagreements. We first conduct interviews with data scientists to understand what constitutes disagreement between explanations generated by different methods for the same model prediction and introduce a novel quantitative framework to formalize this understanding. We then leverage this framework to carry out a rigorous empirical analysis with four real-world datasets, six state-of-the-art post hoc explanation methods, and six different predictive models, to measure the extent of disagreement between the explanations generated by various popular explanation methods. In addition, we carry out an online user study with data scientists to understand how they resolve the aforementioned disagreements. Our results indicate that (1) state-of-the-art explanation methods often disagree in terms of the explanations they output, and (2) machine learning practitioners often employ ad hoc heuristics when resolving such disagreements. These findings suggest that practitioners may be relying on misleading explanations when making consequential decisions. They also underscore the importance of developing principled frameworks for effectively evaluating and comparing explanations output by various explanation techniques.",NA,https://openreview.net/forum?id=jESY2WTZCe,Transactions on Machine Learning Research,Satyapriya Krishna;Tessa Han;Alex Gu;Steven Wu;Shahin Jabbari;Himabindu Lakkaraju,2024,2,"@article{2-22930,
  title = {The disagreement problem in explainable machine learning: a practitioner’s perspective},
  author = {Satyapriya Krishna and Tessa Han and Alex Gu and Steven Wu and Shahin Jabbari and Himabindu Lakkaraju},
  year = {2024},
  journal = {Transactions on Machine Learning Research}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,no such info,"Explaining, Forecasting","Decision-maker, Guardian",Change trust,no such info,post hoc explanations,NA,"Visual, Textual",Yes,Yes
2-22933,mlr,When does uncertainty matter?: understanding the impact of predictive uncertainty in ml assisted decision making,"As machine learning (ML) models are increasingly being employed to assist human decision makers, it becomes critical to provide these decision makers with relevant inputs which can help them decide if and how to incorporate model predictions into their decision making. For instance, communicating the uncertainty associated with model predictions could potentially be helpful in this regard. In this work, we carry out user studies (1,330 responses from 190 participants) to systematically assess how people with differing levels of expertise respond to different types of predictive uncertainty (i.e., posterior predictive distributions with different shapes and variances) in the context of ML assisted decision making for predicting apartment rental prices. We found that showing posterior predictive distributions led to smaller disagreements with the ML model's predictions, regardless of the shapes and variances of the posterior predictive distributions we considered, and that these effects may be sensitive to expertise in both ML and the domain. This suggests that posterior predictive distributions can potentially serve as useful decision aids which should be used with caution and take into account the type of distribution and the expertise of the human.",NA,https://openreview.net/forum?id=pbs22kJmEO,Transactions on Machine Learning Research,Sean McGrath;Parth Mehta;Alexandra Zytek;Isaac Lage;Himabindu Lakkaraju,2024,2,"@article{2-22933,
  title={When does uncertainty matter?: understanding the impact of predictive uncertainty in ML assisted decision making},
  author={McGrath, Sean and Mehta, Parth and Zytek, Alexandra and Lage, Isaac and Lakkaraju, Himabindu},
  year={2024},
  journal={Transactions on Machine Learning Research}
}",Empirical contributions,"Everyday / Employment / Public Service, Generic / Abstract / Domain-agnostic",Individual,Advising,Decision-maker,"Alter decision outcomes, Change trust",no such info,"prediction of alternative, posterior predictive distributions, uncertainty distributions","domain knowledge, expertise","Textual, Visual",Yes,Yes
2-22936,mlr,Interpretable mixture of experts,"The need for reliable model explanations is prominent for many machine learning applications, particularly for tabular and time-series data as their use cases often involve high-stakes decision making. Towards this goal, we introduce a novel interpretable modeling framework, Interpretable Mixture of Experts (IME), that yields high accuracy, comparable to `black-box' Deep Neural Networks (DNNs) in many cases, along with useful interpretability capabilities. IME consists of an assignment module and a mixture of experts, with each sample being assigned to a single expert for prediction. We introduce multiple options for IME based on the assignment and experts being interpretable. When the experts are chosen to be interpretable such as linear models, IME yields an inherently-interpretable architecture where the explanations produced by IME are the exact descriptions of how the prediction is computed. In addition to constituting a standalone inherently-interpretable architecture, IME has the premise of being integrated with existing DNNs to offer interpretability to a subset of samples while maintaining the accuracy of the DNNs. Through extensive experiments on 15 tabular and time-series datasets, IME is demonstrated to be more accurate than single interpretable models and perform comparably with existing state-of-the-art DNNs in accuracy. On most datasets, IME even outperforms DNNs, while providing faithful explanations. Lastly, IME's explanations are compared to commonly-used post-hoc explanations methods through a user study -- participants are able to better predict the model behavior when given IME explanations, while finding IME's explanations more faithful and trustworthy.",NA,https://openreview.net/forum?id=DdZoPUPm0a,Transactions on Machine Learning Research,Aya Abdelsalam Ismail;Sercan O Arik;Jinsung Yoon;Ankur Taly;Soheil Feizi;Tomas Pfister,2024,21,"@article{2-22936,
  title={Interpretable Mixture of Experts},
  author={Ismail, Aya Abdelsalam and Arik, Sercan O and Yoon, Jinsung and Taly, Ankur and Feizi, Soheil and Pfister, Tomas},
  year={2024},
  journal={Transactions on Machine Learning Research}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,no such info,"Explaining, Forecasting","Decision-maker, Knowledge provider","Alter decision outcomes, Change trust",Update AI competence,"textual explanations, local explanations",NA,Textual,Yes,Yes
2-22939,mlr,The measure and mismeasure of fairness,"The field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last decade, several formal, mathematical definitions of fairness have gained prominence. Here we first assemble and categorize these definitions into two broad families: (1) those that constrain the effects of decisions on disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions typically result in strongly Pareto dominated decision policies. For example, in the case of college admissions, adhering to popular formal conceptions of fairness would simultaneously result in lower student-body diversity and a less academically prepared class, relative to what one could achieve by explicitly tailoring admissions policies to achieve desired outcomes. In this sense, requiring that these fairness definitions hold can, perversely, harm the very groups they were designed to protect. In contrast to axiomatic notions of fairness, we argue that the equitable design of algorithms requires grappling with their context-specific consequences, akin to the equitable design of policy. We conclude by listing several open challenges in fair machine learning and offering strategies to ensure algorithms are better aligned with policy goals.",NA,https://www.jmlr.org/papers/v24/22-1511.html,Journal of Machine Learning Research,Sam Corbett-Davies;Johann D. Gaebler;Hamed Nilforoshan;Ravi Shroff;Sharad Goel,2023,1195,"@article{2-22939,
  title={The measure and mismeasure of fairness},
  author={Corbett-Davies, Sam and Gaebler, Johann D. and Nilforoshan, Hamed and Shroff, Ravi and Goel, Sharad},
  year={2023},
  journal={Journal of Machine Learning Research}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Institutional,"Advising, Auditing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-22951,neurips,"Ask not what ai can do, but what ai should do: towards a framework of task delegability","While artificial intelligence (AI) holds promise for addressing societal challenges, issues of exactly which tasks to automate and to what extent to do so remain understudied. We approach this problem of task delegability from a human-centered perspective by developing a framework on human perception of task delegation to AI. We consider four high-level factors that can contribute to a delegation decision: motivation, difficulty, risk, and trust. To obtain an empirical understanding of human preferences in different tasks, we build a dataset of 100 tasks from academic papers, popular media portrayal of AI, and everyday life, and administer a survey based on our proposed framework. We find little preference for full AI control and a strong preference for machine-in-the-loop designs, in which humans play the leading role. Among the four factors, trust is the most correlated with human preferences of optimal human-machine delegation. This framework represents a first step towards characterizing human preferences of AI automation across tasks. We hope this work encourages future efforts towards understanding such individual attitudes; our goal is to inform the public and the AI research community rather than dictating any direction in technology development.",NA,https://proceedings.neurips.cc/paper_files/paper/2019/hash/d67d8ab4f4c10bf22aa353e27879133c-Abstract.html,Advances in Neural Information Processing Systems,Brian Lubars;Chenhao Tan,2019,0,"@inproceedings{2-22951,
  title = {Ask Not What AI Can Do, But What AI Should Do: Towards a Framework of Task Delegability},
  author = {Brian Lubars and Chenhao Tan},
  year = {2019},
  booktitle = {Advances in Neural Information Processing Systems}
}","Methodological contributions, Empirical contributions",Generic / Abstract / Domain-agnostic,Institutional,"Executing, Collaborating",Decision-maker,"Restrict human agency, Change trust",no such info,"delegation, system accuracy",power,Textual,Yes,Yes
2-22952,neurips,Auditing for human expertise,"High-stakes prediction tasks (e.g., patient diagnosis) are often handled by trained human experts. A common source of concern about automation in these settings is that experts may exercise intuition that is difficult to model and/or have access to information (e.g., conversations with a patient) that is simply unavailable to a would-be algorithm. This raises a natural question whether human experts add value which could not be captured by an algorithmic predictor.We develop a statistical framework under which we can pose this question as a natural hypothesis test. Indeed, as our framework highlights, detecting human expertise is more subtle than simply comparing the accuracy of expert predictions to those made by a particular learning algorithm. Instead, we propose a simple procedure which tests whether expert predictions are statistically independent from the outcomes of interest after conditioning on the available inputs (‘features’). A rejection of our test thus suggests that human experts may add value to any algorithm trained on the available data, and has direct implications for whether human-AI ‘complementarity’ is achievable in a given prediction task.We highlight the utility of our procedure using admissions data collected from the emergency department of a large academic hospital system, where we show that physicians’ admit/discharge decisions for patients with acute gastrointestinal bleeding (AGIB) appear to be incorporating information that is not available to a standard algorithmic screening tool. This is despite the fact that the screening tool is arguably more accurate than physicians’ discretionary decisions, highlighting that – even absent normative concerns about accountability or interpretability – accuracy is insufficient to justify algorithmic automation.",NA,https://proceedings.neurips.cc/paper_files/paper/2023/hash/fb44a668c2d4bc984e9d6ca261262cbb-Abstract-Conference.html,Advances in Neural Information Processing Systems,Rohan Alur;Loren Laine;Darrick Li;Manish Raghavan;Devavrat Shah;Dennis Shung,2023,30,"@inproceedings{2-22952,
  title={Auditing for human expertise},
  author={Alur, Rohan and Laine, Loren and Li, Darrick and Raghavan, Manish and Shah, Devavrat and Shung, Dennis},
  year={2023},
  booktitle={Advances in Neural Information Processing Systems}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,Auditing,"Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-22958,neurips,Causal fairness for outcome control,"As society transitions towards an AI-based decision-making infrastructure, an ever-increasing number of decisions once under control of humans are now delegated to automated systems. Even though such developments make various parts of society more efficient, a large body of evidence suggests that a great deal of care needs to be taken to make such automated decision-making systems fair and equitable, namely, taking into account sensitive attributes such as gender, race, and religion. In this paper, we study a specific decision-making task called outcome control in which an automated system aims to optimize an outcome variable YYY while being fair and equitable. The interest in such a setting ranges from interventions related to criminal justice and welfare, all the way to clinical decision-making and public health. In this paper, we first analyze through causal lenses the notion of benefit, which captures how much a specific individual would benefit from a positive decision, counterfactually speaking, when contrasted with an alternative, negative one. We introduce the notion of benefit fairness, which can be seen as the minimal fairness requirement in decision-making, and develop an algorithm for satisfying it. We then note that the benefit itself may be influenced by the protected attribute, and propose causal tools which can be used to analyze this. Finally, if some of the variations of the protected attribute in the benefit are considered as discriminatory, the notion of benefit fairness may need to be strengthened, which leads us to articulating a notion of causal benefit fairness. Using this notion, we develop a new optimization procedure capable of maximizing YYY while ascertaining causal fairness in the decision process.",NA,https://proceedings.neurips.cc/paper_files/paper/2023/hash/948552777302d3abf92415b1d7e9de70-Abstract-Conference.html,Advances in Neural Information Processing Systems,Drago Plecko;Elias Bareinboim,2023,1,"@inproceedings{2-22958,
  title = {Causal fairness for outcome control},
  author = {Plecko, Drago and Bareinboim, Elias},
  year = {2023},
  booktitle = {Advances in Neural Information Processing Systems}
}","Algorithmic contributions, Methodological contributions","Healthcare / Medicine / Surgery, Law / Policy / Governance, Generic / Abstract / Domain-agnostic",Institutional,"Executing, Advising","Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-22960,neurips,Closing the loop in medical decision support by understanding clinical decision-making: a case study on organ transplantation,"Significant effort has been placed on developing decision support tools to improve patient care. However, drivers of real-world clinical decisions in complex medical scenarios are not yet well-understood, resulting in substantial gaps between these tools and practical applications. In light of this, we highlight that more attention on understanding clinical decision-making is required both to elucidate current clinical practices and to enable effective human-machine interactions. This is imperative in high-stakes scenarios with scarce available resources. Using organ transplantation as a case study, we formalize the desiderata of methods for understanding clinical decision-making. We show that most existing machine learning methods are insufficient to meet these requirements and propose iTransplant, a novel data-driven framework to learn the factors affecting decisions on organ offers in an instance-wise fashion directly from clinical data, as a possible solution. Through experiments on real-world liver transplantation data from OPTN, we demonstrate the use of iTransplant to: (1) discover which criteria are most important to clinicians for organ offer acceptance; (2) identify patient-specific organ preferences of clinicians allowing automatic patient stratification; and (3) explore variations in transplantation practices between different transplant centers. Finally, we emphasize that the insights gained by iTransplant can be used to inform the development of future decision support tools.",NA,https://proceedings.neurips.cc/paper_files/paper/2021/hash/c344336196d5ec19bd54fd14befdde87-Abstract.html,Advances in Neural Information Processing Systems,Yuchao Qin;Fergus Imrie;Alihan Hüyük;Daniel Jarrett;alexander gimson;Mihaela van der Schaar,2021,0,"@inproceedings{2-22960,
  title={Closing the loop in medical decision support by understanding clinical decision-making: a case study on organ transplantation},
  author={Qin, Yuchao and Imrie, Fergus and H{\""u}y{\""u}k, Alihan and Jarrett, Daniel and Gimson, Alexander and van der Schaar, Mihaela},
  year={2021},
  booktitle={Advances in Neural Information Processing Systems}
}","Algorithmic contributions, Empirical contributions",Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-22961,neurips,Confusions over time: an interpretable bayesian model to characterize trends in decision making,"We propose Confusions over Time (CoT), a novel generative framework which facilitates a multi-granular analysis of the decision making process. The CoT not only models the confusions or error properties of individual decision makers and their evolution over time, but also allows us to obtain diagnostic insights into the collective decision making process in an interpretable manner. To this end, the CoT models the confusions of the decision makers and their evolution over time via time-dependent confusion matrices. Interpretable insights are obtained by grouping similar decision makers (and items being judged) into clusters and representing each such cluster with an appropriate prototype and identifying the most important features characterizing the cluster via a subspace feature indicator vector. Experimentation with real world data on bail decisions, asthma treatments, and insurance policy approval decisions demonstrates that CoT can accurately model and explain the confusions of decision makers and their evolution over time.",NA,https://proceedings.neurips.cc/paper_files/paper/2016/hash/97d0145823aeb8ed80617be62e08bdcc-Abstract.html,Advances in Neural Information Processing Systems,Himabindu Lakkaraju;Jure Leskovec,2016,16,"@inproceedings{2-22961,
  title={Confusions over time: an interpretable bayesian model to characterize trends in decision making},
  author={Lakkaraju, Himabindu and Leskovec, Jure},
  year={2016},
  booktitle={Advances in Neural Information Processing Systems}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery, Finance / Business / Economy, Law / Policy / Governance",Operational,Analyzing,"Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-22963,neurips,Controlling counterfactual harm in decision support systems based on prediction sets,"Decision support systems based on prediction sets help humans solve multiclass classification tasks by narrowing down the set of potential label values to a subset of them, namely a prediction set, and asking them to always predict label values from the prediction sets. While this type of systems have been proven to be effective at improving the average accuracy of the predictions made by humans, by restricting human agency, they may cause harm---a human who has succeeded at predicting the ground-truth label of an instance on their own may have failed had they used these systems. In this paper, our goal is to control how frequently a decision support system based on prediction sets may cause harm, by design. To this end, we start by characterizing the above notion of harm using the theoretical framework of structural causal models. Then, we show that, under a natural, albeit unverifiable, monotonicity assumption, we can estimate how frequently a system may cause harm using only predictions made by humans on their own. Further, we also show that, under a weaker monotonicity assumption, which can be verified experimentally, we can bound how frequently a system may cause harm again using only predictions made by humans on their own. Building upon these assumptions, we introduce a computational framework to design decision support systems based on prediction sets that are guaranteed to cause harm less frequently than a user-specified value using conformal risk control. We validate our framework using real human predictions from two different human subject studies and show that, in decision support systems based on prediction sets, there is a trade-off between accuracy and counterfactual harm.",NA,https://openreview.net/forum?id=PyTkA6HkzX,Advances in Neural Information Processing Systems,Eleni Straitouri;Suhas Thejaswi;Manuel Gomez Rodriguez,2024,4,"@inproceedings{2-22963,
  title={Controlling counterfactual harm in decision support systems based on prediction sets},
  author={Straitouri, Eleni and Thejaswi, Suhas and Gomez Rodriguez, Manuel},
  year={2024},
  booktitle={Advances in Neural Information Processing Systems}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Operational,"Advising, Forecasting","Decision-maker, Decision-subject","Alter decision outcomes, Restrict human agency",no such info,counterfactual harm,prediction set,"Interactive interface, Textual",Yes,Yes
2-22964,neurips,Counterfactual fairness,"Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.",NA,https://proceedings.neurips.cc/paper_files/paper/2017/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html,Advances in Neural Information Processing Systems,Matt J. Kusner;Joshua Loftus;Chris Russell;Ricardo Silva,2017,2649,"@inproceedings{2-22964,
  title={Counterfactual fairness},
  author={Kusner, Matt J. and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
  year={2017},
  booktitle={Advances in Neural Information Processing Systems}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service",Organizational,Forecasting,"Decision-maker, Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-22968,neurips,"Decisions, counterfactual explanations and strategic behavior","As data-driven predictive models are increasingly used to inform decisions, it has been argued that decision makers should provide explanations that help individuals understand what would have to change for these decisions to be beneficial ones. However, there has been little discussion on the possibility that individuals may use the above counterfactual explanations to invest effort strategically and maximize their chances of receiving a beneficial decision. In this paper, our goal is to find policies and counterfactual explanations that are optimal in terms of utility in such a strategic setting. We first show that, given a pre-defined policy, the problem of finding the optimal set of counterfactual explanations is NP-hard. Then, we show that the corresponding objective is nondecreasing and satisfies submodularity and this allows a standard greedy algorithm to enjoy approximation guarantees. In addition, we further show that the problem of jointly finding both the optimal policy and set of counterfactual explanations reduces to maximizing a non-monotone submodular function. As a result, we can use a recent randomized algorithm to solve the problem, which also offers approximation guarantees. Finally, we demonstrate that, by incorporating a matroid constraint into the problem formulation, we can increase the diversity of the optimal set of counterfactual explanations and incentivize individuals across the whole spectrum of the population to self improve. Experiments on synthetic and real lending and credit card data illustrate our theoretical findings and show that the counterfactual explanations and decision policies found by our algorithms achieve higher utility than several competitive baselines.",NA,https://proceedings.neurips.cc/paper_files/paper/2020/hash/c2ba1bc54b239208cb37b901c0d3b363-Abstract.html,Advances in Neural Information Processing Systems,Stratis Tsirtsis;Manuel Gomez Rodriguez,2020,82,"@inproceedings{2-22968,
  title     = {Decisions, counterfactual explanations and strategic behavior},
  author    = {Stratis Tsirtsis and Manuel Gomez Rodriguez},
  year      = {2020},
  booktitle = {Advances in Neural Information Processing Systems},
}","Algorithmic contributions, Theoretical contributions","Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Operational,"Executing, Explaining, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-22969,neurips,Detecting individual decision-making style: exploring behavioral stylometry in chess,"The advent of machine learning models that surpass human decision-making ability in complex domains has initiated a movement towards building AI systems that interact with humans. Many building blocks are essential for this activity, with a central one being the algorithmic characterization of human behavior. While much of the existing work focuses on aggregate human behavior, an important long-range goal is to develop behavioral models that specialize to individual people and can differentiate among them.To formalize this process, we study the problem of behavioral stylometry, in which the task is to identify a decision-maker from their decisions alone. We present a transformer-based approach to behavioral stylometry in the context of chess, where one attempts to identify the player who played a set of games. Our method operates in a few-shot classification framework, and can correctly identify a player from among thousands of candidate players with 98% accuracy given only 100 labeled games. Even when trained on amateur play, our method generalises to out-of-distribution samples of Grandmaster players, despite the dramatic differences between amateur and world-class players. Finally, we consider more broadly what our resulting embeddings reveal about human style in chess, as well as the potential ethical implications of powerful methods for identifying individuals from behavioral data.",NA,https://proceedings.neurips.cc/paper_files/paper/2021/hash/ccf8111910291ba472b385e9c5f59099-Abstract.html,Advances in Neural Information Processing Systems,Reid McIlroy-Young;Yu Wang;Siddhartha Sen;Jon Kleinberg;Ashton Anderson,2021,45,"@inproceedings{2-22969,
  title = {Detecting individual decision-making style: exploring behavioral stylometry in chess},
  author = {McIlroy-Young, Reid and Wang, Yu and Sen, Siddhartha and Kleinberg, Jon and Anderson, Ashton},
  year = {2021},
  booktitle = {Advances in Neural Information Processing Systems}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Media / Communication / Entertainment",Individual,"Analyzing, Forecasting","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-22971,neurips,Enhancing the accuracy and fairness of human decision making,"Societies often rely on human experts to take a wide variety of decisions affecting their members, from jail-or-release decisions taken by judges and stop-and-frisk decisions taken by police officers to accept-or-reject decisions taken by academics. In this context, each decision is taken by an expert who is typically chosen uniformly at random from a pool of experts. However, these decisions may be imperfect due to limited experience, implicit biases, or faulty probabilistic reasoning. Can we improve the accuracy and fairness of the overall decision making process by optimizing the assignment between experts and decisions?",NA,https://proceedings.neurips.cc/paper_files/paper/2018/hash/0a113ef6b61820daa5611c870ed8d5ee-Abstract.html,Advances in Neural Information Processing Systems,Isabel Valera;Adish Singla;Manuel Gomez Rodriguez,2018,0,"@inproceedings{2-22971,
  title={Enhancing the accuracy and fairness of human decision making},
  author={Valera, Isabel and Singla, Adish and Gomez Rodriguez, Manuel},
  year={2018},
  booktitle={Advances in Neural Information Processing Systems}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance",Operational,Executing,"Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-22977,neurips,Fincon: a synthesized llm multi-agent system with conceptual verbal reinforcement for enhanced financial decision making,"Large language models (LLMs) have demonstrated notable potential in conducting complex tasks and are increasingly utilized in various financial applications. However, high-quality sequential financial investment decision-making remains challenging. These tasks require multiple interactions with a volatile environment for every decision, demanding sufficient intelligence to maximize returns and manage risks. Although LLMs have been used to develop agent systems that surpass human teams and yield impressive investment returns, opportunities to enhance multi-source information synthesis and optimize decision-making outcomes through timely experience refinement remain unexplored. Here, we introduce FinCon, an LLM-based multi-agent framework tailored for diverse financial tasks. Inspired by effective real-world investment firm organizational structures, FinCon utilizes a manager-analyst communication hierarchy. This structure allows for synchronized cross-functional agent collaboration towards unified goals through natural language interactions and equips each agent with greater memory capacity than humans. Additionally, a risk-control component in FinCon enhances decision quality by episodically initiating a self-critiquing mechanism to update systematic investment beliefs. The conceptualized beliefs serve as verbal reinforcement for the future agent’s behavior and can be selectively propagated to the appropriate node that requires knowledge updates. This feature significantly improves performance while reducing unnecessary peer-to-peer communication costs. Moreover, FinCon demonstrates strong generalization capabilities in various financial tasks, including stock trading and portfolio management.",NA,https://openreview.net/forum?id=dG1HwKMYbC,Advances in Neural Information Processing Systems,Yangyang Yu;Zhiyuan Yao;Haohang Li;Zhiyang Deng;Yuechen Jiang;Yupeng Cao;Zhi Chen;Jordan W. Suchow;Zhenyu Cui;Rong Liu;Zhaozhuo Xu;Denghui Zhang;Koduvayur Subbalakshmi;GUOJUN XIONG;Yueru He;Jimin Huang;Dong Li;Qianqian Xie,2024,145,"@inproceedings{2-22977,
  title = {Fincon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making},
  author = {Yangyang Yu and Zhiyuan Yao and Haohang Li and Zhiyang Deng and Yuechen Jiang and Yupeng Cao and Zhi Chen and Jordan W. Suchow and Zhenyu Cui and Rong Liu and Zhaozhuo Xu and Denghui Zhang and Koduvayur Subbalakshmi and Guojun Xiong and Yueru He and Jimin Huang and Dong Li and Qianqian Xie},
  year = {2024},
  booktitle = {Advances in Neural Information Processing Systems}
}",System/Artifact contributions,Finance / Business / Economy,Operational,"Analyzing, Executing, Advising","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-22978,neurips,Finding regions of heterogeneity in decision-making via expected conditional covariance,"Individuals often make different decisions when faced with the same context, due to personal preferences and background. For instance, judges may vary in their leniency towards certain drug-related offenses, and doctors may vary in their preference for how to start treatment for certain types of patients. With these examples in mind, we present an algorithm for identifying types of contexts (e.g., types of cases or patients) with high inter-decision-maker disagreement. We formalize this as a causal inference problem, seeking a region where the assignment of decision-maker has a large causal effect on the decision. Our algorithm finds such a region by maximizing an empirical objective, and we give a generalization bound for its performance. In a semi-synthetic experiment, we show that our algorithm recovers the correct region of heterogeneity accurately compared to baselines. Finally, we apply our algorithm to real-world healthcare datasets, recovering variation that aligns with existing clinical knowledge.",NA,https://proceedings.neurips.cc/paper_files/paper/2021/hash/81930c54e08b6d26d9638dd2e4656dc1-Abstract.html,Advances in Neural Information Processing Systems,Justin Lim;Christina X Ji;Michael Oberst;Saul Blecker;Leora Horwitz;David Sontag,2021,27,"@inproceedings{2-22978,
  title={Finding regions of heterogeneity in decision-making via expected conditional covariance},
  author={Lim, Justin and Ji, Christina X and Oberst, Michael and Blecker, Saul and Horwitz, Leora and Sontag, David},
  year={2021},
  booktitle={Advances in Neural Information Processing Systems}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance",Operational,Auditing,"Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-22979,neurips,From parity to preference-based notions of fairness in classification,"The adoption of automated, data-driven decision making in an ever expanding range of applications has raised concerns about its potential unfairness towards certain social groups. In this context, a number of recent studies have focused on defining, detecting, and removing unfairness from data-driven decision systems. However, the existing notions of fairness, based on parity (equality) in treatment or outcomes for different social groups, tend to be quite stringent, limiting the overall decision making accuracy. In this paper, we draw inspiration from the fair-division and envy-freeness literature in economics and game theory and propose preference-based notions of fairness -- given the choice between various sets of decision treatments or outcomes, any group of users would collectively prefer its treatment or outcomes, regardless of the (dis)parity as compared to the other groups. Then, we introduce tractable proxies to design margin-based classifiers that satisfy these preference-based notions of fairness. Finally, we experiment with a variety of synthetic and real-world datasets and show that preference-based fairness allows for greater decision accuracy than parity-based fairness.",NA,https://proceedings.neurips.cc/paper_files/paper/2017/hash/82161242827b703e6acf9c726942a1e4-Abstract.html,Advances in Neural Information Processing Systems,Muhammad Bilal Zafar;Isabel Valera;Manuel Rodriguez;Krishna Gummadi;Adrian Weller,2017,5,"@inproceedings{2-22979,
  title={From parity to preference-based notions of fairness in classification},
  author={Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel and Gummadi, Krishna and Weller, Adrian},
  year={2017},
  booktitle={Advances in Neural Information Processing Systems}
}","Methodological contributions, Theoretical contributions","Generic / Abstract / Domain-agnostic, Finance / Business / Economy, Law / Policy / Governance",Operational,"Forecasting, Advising, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-22980,neurips,Guide: real-time human-shaped agents,"The recent rapid advancement of machine learning has been driven by increasingly powerful models with the growing availability of training data and computational resources. However, real-time decision-making tasks with limited time and sparse learning signals remain challenging. One way of improving the learning speed and performance of these agents is to leverage human guidance. In this work, we introduce GUIDE, a framework for real-time human-guided reinforcement learning by enabling continuous human feedback and grounding such feedback into dense rewards to accelerate policy learning. Additionally, our method features a simulated feedback module that learns and replicates human feedback patterns in an online fashion, effectively reducing the need for human input while allowing continual training. We demonstrate the performance of our framework on challenging tasks with sparse rewards and visual observations. Our human study involving 50 subjects offers strong quantitative and qualitative evidence of the effectiveness of our approach. With only 10 minutes of human feedback, our algorithm achieves up to 30\\% increase in success rate compared to its RL baseline.",NA,https://openreview.net/forum?id=KrHFICMPjm,Advances in Neural Information Processing Systems,Lingyu Zhang;Zhengran Ji;Nicholas R Waytowich;Boyuan Chen,2024,10,"@inproceedings{2-22980,
  title={Guide: real-time human-shaped agents},
  author={Zhang, Lingyu and Ji, Zhengran and Waytowich, Nicholas R and Chen, Boyuan},
  year={2024},
  booktitle={Advances in Neural Information Processing Systems}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Operational,"Executing, Collaborating","Knowledge provider, Decision-maker",no such info,"Update AI competence, Change AI responses",NA,"domain knowledge, corrective feedback, simple feedback","Interactive interface, Autonomous System",Yes,Yes
2-22982,neurips,Human-aligned calibration for ai-assisted decision making,"Whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. Then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. In this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. However, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. In this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. We first argue that, for a broad class of utility functions, there exists data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values—an optimal decision maker would need to sometimes place more (less) trust on predictions with lower (higher) confidence values. However, we then show that, if the confidence values satisfy a natural alignment property with respect to the decision maker’s confidence on her own predictions, there always exists an optimal decision policy under which the level of trust the decision maker would need to place on predictions is monotone on the confidence values, facilitating its discoverability. Further, we show that multicalibration with respect to the decision maker’s confidence on her own prediction is a sufficient condition for alignment. Experiments on a real AI-assisted decision making scenario where a classifier provides decision support to human decision makers validate our theoretical results and suggest that alignment may lead to better decisions.",NA,https://proceedings.neurips.cc/paper_files/paper/2023/hash/2f1d1196426ba84f47d115cac3dcb9d8-Abstract-Conference.html,Advances in Neural Information Processing Systems,Nina Corvelo Benz;Manuel Rodriguez,2023,2,"@inproceedings{2-22982,
  title = {Human-aligned calibration for AI-assisted decision making},
  author = {Nina Corvelo Benz and Manuel Rodriguez},
  year = {2023},
  booktitle = {Advances in Neural Information Processing Systems}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Operational,"Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-22988,neurips,Maia-2: a unified model for human-ai alignment in chess,"There are an increasing number of domains in which artificial intelligence (AI) systems both surpass human ability and accurately model human behavior. This introduces the possibility of algorithmically-informed teaching in these domains through more relatable AI partners and deeper insights into human decision-making. Critical to achieving this goal, however, is coherently modeling human behavior at various skill levels. Chess is an ideal model system for conducting research into this kind of human-AI alignment, with its rich history as a pivotal testbed for AI research, mature superhuman AI systems like AlphaZero, and precise measurements of skill via chess rating systems. Previous work in modeling human decision-making in chess uses completely independent models to capture human style at different skill levels, meaning they lack coherence in their ability to adapt to the full spectrum of human improvement and are ultimately limited in their effectiveness as AI partners and teaching tools. In this work, we propose a unified modeling approach for human-AI alignment in chess that coherently captures human style across different skill levels and directly captures how people improve. Recognizing the complex, non-linear nature of human learning, we introduce a skill-aware attention mechanism to dynamically integrate players’ strengths with encoded chess positions, enabling our model to be sensitive to evolving player skill. Our experimental results demonstrate that this unified framework significantly enhances the alignment between AI and human players across a diverse range of expertise levels, paving the way for deeper insights into human decision-making and AI-guided teaching tools.",NA,https://openreview.net/forum?id=XWlkhRn14K,Advances in Neural Information Processing Systems,Zhenwei Tang;Difan Jiao;Reid McIlroy-Young;Jon Kleinberg;Siddhartha Sen;Ashton Anderson,2024,14,"@inproceedings{2-22988,
  title={Maia-2: a unified model for human-ai alignment in chess},
  author={Tang, Zhenwei and Jiao, Difan and McIlroy-Young, Reid and Kleinberg, Jon and Sen, Siddhartha and Anderson, Ashton},
  year={2024},
  booktitle={Advances in Neural Information Processing Systems}
}",Algorithmic contributions,Media / Communication / Entertainment,Individual,"Analyzing, Collaborating, Executing","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-22989,neurips,Mdagents: an adaptive collaboration of llms for medical decision-making,"Foundation models are becoming valuable tools in medicine. Yet despite their promise, the best way to leverage Large Language Models (LLMs) in complex medical tasks remains an open question. We introduce a novel multi-agent framework, named **M**edical **D**ecision-making **Agents** (**MDAgents**) that helps to address this gap by automatically assigning a collaboration structure to a team of LLMs. The assigned solo or group collaboration structure is tailored to the medical task at hand, a simple emulation inspired by the way real-world medical decision-making processes are adapted to tasks of different complexities. We evaluate our framework and baseline methods using state-of-the-art LLMs across a suite of real-world medical knowledge and clinical diagnosis benchmarks, including a comparison of LLMs’ medical complexity classification against human physicians. MDAgents achieved the **best performance in seven out of ten** benchmarks on tasks requiring an understanding of medical knowledge and multi-modal reasoning, showing a significant **improvement of up to 4.2\\%** ($p$ < 0.05) compared to previous methods' best performances. Ablation studies reveal that MDAgents effectively determines medical complexity to optimize for efficiency and accuracy across diverse medical tasks. Notably, the combination of moderator review and external medical knowledge in group collaboration resulted in an average accuracy **improvement of 11.8\\%**. Our code can be found at https://github.com/mitmedialab/MDAgents.",NA,https://openreview.net/forum?id=EKdk4vxKO4,Advances in Neural Information Processing Systems,Yubin Kim;Chanwoo Park;Hyewon Jeong;Yik Siu Chan;Xuhai Xu;Daniel McDuff;Hyeonhoon Lee;Marzyeh Ghassemi;Cynthia Breazeal;Hae Won Park,2024,168,"@inproceedings{2-22989,
  title={Mdagents: An adaptive collaboration of LLMs for medical decision-making},
  author={Kim, Yubin and Park, Chanwoo and Jeong, Hyewon and Chan, Yik Siu and Xu, Xuhai and McDuff, Daniel and Lee, Hyeonhoon and Ghassemi, Marzyeh and Breazeal, Cynthia and Park, Hae Won},
  year={2024},
  booktitle={Advances in Neural Information Processing Systems}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Executing, Forecasting, Collaborating","Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-22990,neurips,Mind the gap: a causal perspective on bias amplification in prediction & decision-making,"As society increasingly relies on AI-based tools for decision-making in socially sensitive domains, investigating fairness and equity of such automated systems has become a critical field of inquiry. Most of the literature in fair machine learning focuses on defining and achieving fairness criteria in the context of prediction, while not explicitly focusing on how these predictions may be used later on in the pipeline. For instance, if commonly used criteria, such as independence or sufficiency, are satisfied for a prediction score $S$ used for binary classification, they need not be satisfied after an application of a simple thresholding operation on $S$ (as commonly used in practice). In this paper, we take an important step to address this issue in numerous statistical and causal notions of fairness. We introduce the notion of a margin complement, which measures how much a prediction score $S$ changes due to a thresholding operation. We then demonstrate that the marginal difference in the optimal 0/1 predictor $\\widehat Y$ between groups, written $P(\\hat y \\mid x_1) - P(\\hat y \\mid x_0)$, can be causally decomposed into the influences of $X$ on the $L_2$-optimal prediction score $S$ and the influences of $X$ on the margin complement $M$, along different causal pathways (direct, indirect, spurious). We then show that under suitable causal assumptions, the influences of $X$ on the prediction score $S$ are equal to the influences of $X$ on the true outcome $Y$. This yields a new decomposition of the disparity in the predictor $\\widehat Y$ that allows us to disentangle causal differences inherited from the true outcome $Y$ that exists in the real world vs. those coming from the optimization procedure itself. This observation highlights the need for more regulatory oversight due to the potential for bias amplification, and to address this issue we introduce new notions of weak and strong business necessity, together with an algorithm for assessing whether these notions are satisfied. We apply our method to three real-world datasets and derive new insights on bias amplification in prediction and decision-making.",NA,https://openreview.net/forum?id=aXYL24yhjN,Advances in Neural Information Processing Systems,Drago Plecko;Elias Bareinboim,2024,3,"@inproceedings{2-22990,
  title={Mind the Gap: A Causal Perspective on Bias Amplification in Prediction \& Decision-Making},
  author={Drago Plecko and Elias Bareinboim},
  year={2024},
  booktitle={Advances in Neural Information Processing Systems}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance, Healthcare / Medicine / Surgery, Everyday / Employment / Public Service",Institutional,"Forecasting, Analyzing, Executing","Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-22992,neurips,Off-policy policy evaluation for sequential decisions under unobserved confounding,"When observed decisions depend only on observed features, off-policy policy evaluation (OPE) methods for sequential decision problems can estimate the performance of evaluation policies before deploying them. However, this assumption is frequently violated due to unobserved confounders, unrecorded variables that impact both the decisions and their outcomes. We assess robustness of OPE methods under unobserved confounding by developing worst-case bounds on the performance of an evaluation policy. When unobserved confounders can affect every decision in an episode, we demonstrate that even small amounts of per-decision confounding can heavily bias OPE methods. Fortunately, in a number of important settings found in healthcare, policy-making, and technology, unobserved confounders may directly affect only one of the many decisions made, and influence future decisions/rewards only through the directly affected decision. Under this less pessimistic model of one-decision confounding, we propose an efficient loss-minimization-based procedure for computing worst-case bounds, and prove its statistical consistency. On simulated healthcare examples---management of sepsis and interventions for autistic children---where this is a reasonable model, we demonstrate that our method invalidates non-robust results and provides meaningful certificates of robustness, allowing reliable selection of policies under unobserved confounding.",NA,https://proceedings.neurips.cc/paper_files/paper/2020/hash/da21bae82c02d1e2b8168d57cd3fbab7-Abstract.html,Advances in Neural Information Processing Systems,Hongseok Namkoong;Ramtin Keramati;Steve Yadlowsky;Emma Brunskill,2020,103,"@inproceedings{2-22992,
  title={Off-policy policy evaluation for sequential decisions under unobserved confounding},
  author={Namkoong, Hongseok and Keramati, Ramtin and Yadlowsky, Steve and Brunskill, Emma},
  year={2020},
  booktitle={Advances in Neural Information Processing Systems}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery, Law / Policy / Governance, Software / Systems / Security",Operational,"Monitoring, Advising","Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-22995,neurips,Online decision mediation,"Consider learning a decision support assistant to serve as an intermediary between (oracle) expert behavior and (imperfect) human behavior: At each time, the algorithm observes an action chosen by a fallible agent, and decides whether to accept that agent's decision, intervene with an alternative, or request the expert's opinion. For instance, in clinical diagnosis, fully-autonomous machine behavior is often beyond ethical affordances, thus real-world decision support is often limited to monitoring and forecasting. Instead, such an intermediary would strike a prudent balance between the former (purely prescriptive) and latter (purely descriptive) approaches, while providing an efficient interface between human mistakes and expert feedback. In this work, we first formalize the sequential problem of online decision mediation---that is, of simultaneously learning and evaluating mediator policies from scratch with abstentive feedback: In each round, deferring to the oracle obviates the risk of error, but incurs an upfront penalty, and reveals the otherwise hidden expert action as a new training data point. Second, we motivate and propose a solution that seeks to trade off (immediate) loss terms against (future) improvements in generalization error; in doing so, we identify why conventional bandit algorithms may fail. Finally, through experiments and sensitivities on a variety of datasets, we illustrate consistent gains over applicable benchmarks on performance measures with respect to the mediator policy, the learned model, and the decision-making system as a whole.",NA,https://proceedings.neurips.cc/paper_files/paper/2022/hash/0bc795afae289ed465a65a3b4b1f4eb7-Abstract-Conference.html,Advances in Neural Information Processing Systems,Daniel Jarrett;Alihan Hüyük;Mihaela van der Schaar,2022,7,"@inproceedings{2-22995,
  title={Online decision mediation},
  author={Jarrett, Daniel and Hüyük, Alihan and van der Schaar, Mihaela},
  year={2022},
  booktitle={Advances in Neural Information Processing Systems}
}","Algorithmic contributions, Theoretical contributions","Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,"Advising, Monitoring","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-23004,neurips,Policy optimization with advantage regularization for long-term fairness in decision systems,"Long-term fairness is an important factor of consideration in designing and deploying learning-based decision systems in high-stake decision-making contexts. Recent work has proposed the use of Markov Decision Processes (MDPs) to formulate decision-making with long-term fairness requirements in dynamically changing environments, and demonstrated major challenges in directly deploying heuristic and rule-based policies that worked well in static environments. We show that policy optimization methods from deep reinforcement learning can be used to find strictly better decision policies that can often achieve both higher overall utility and less violation of the fairness requirements, compared to previously-known strategies. In particular, we propose new methods for imposing fairness requirements in policy optimization by regularizing the advantage evaluation of different actions. Our proposed methods make it easy to impose fairness constraints without reward engineering or sacrificing training efficiency. We perform detailed analyses in three established case studies, including attention allocation in incident monitoring, bank loan approval, and vaccine distribution in population networks.",NA,https://proceedings.neurips.cc/paper_files/paper/2022/hash/36b76e1f69bbba80d3463f7d6c02bc3d-Abstract-Conference.html,Advances in Neural Information Processing Systems,Eric Yu;Zhizhen Qin;Min Kyung Lee;Sicun Gao,2022,1,"@inproceedings{2-23004,
  title={Policy optimization with advantage regularization for long-term fairness in decision systems},
  author={Yu, Eric and Qin, Zhizhen and Lee, Min Kyung and Gao, Sicun},
  year={2022},
  booktitle={Advances in Neural Information Processing Systems}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery, Finance / Business / Economy, Transportation / Mobility / Planning",Operational,"Forecasting, Executing",Decision-subject,NA,NA,NA,NA,NA,Yes,No
2-23010,neurips,Reliable decision support using counterfactual models,"Decision-makers are faced with the challenge of estimating what is likely to happen when they take an action. For instance, if I choose not to treat this patient, are they likely to die? Practitioners commonly use supervised learning algorithms to fit predictive models that help decision-makers reason about likely future outcomes, but we show that this approach is unreliable, and sometimes even dangerous. The key issue is that supervised learning algorithms are highly sensitive to the policy used to choose actions in the training data, which causes the model to capture relationships that do not generalize. We propose using a different learning objective that predicts counterfactuals instead of predicting outcomes under an existing action policy as in supervised learning. To support decision-making in temporal settings, we introduce the Counterfactual Gaussian Process (CGP) to predict the counterfactual future progression of continuous-time trajectories under sequences of future actions. We demonstrate the benefits of the CGP on two important decision-support tasks: risk prediction and “what if?” reasoning for individualized treatment planning.",NA,https://proceedings.neurips.cc/paper_files/paper/2017/hash/299a23a2291e2126b91d54f3601ec162-Abstract.html,Advances in Neural Information Processing Systems,Peter Schulam;Suchi Saria,2017,278,"@inproceedings{2-23010,
  title={Reliable decision support using counterfactual models},
  author={Schulam, Peter and Saria, Suchi},
  year={2017},
  booktitle={Advances in Neural Information Processing Systems}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-23011,neurips,Rise: robust individualized decision learning with sensitive variables,"This paper introduces RISE, a robust individualized decision learning framework with sensitive variables, where sensitive variables are collectible data and important to the intervention decision, but their inclusion in decision making is prohibited due to reasons such as delayed availability or fairness concerns. A naive baseline is to ignore these sensitive variables in learning decision rules, leading to significant uncertainty and bias. To address this, we propose a decision learning framework to incorporate sensitive variables during offline training but not include them in the input of the learned decision rule during model deployment. Specifically, from a causal perspective, the proposed framework intends to improve the worst-case outcomes of individuals caused by sensitive variables that are unavailable at the time of decision. Unlike most existing literature that uses mean-optimal objectives, we propose a robust learning framework by finding a newly defined quantile- or infimum-optimal decision rule. The reliable performance of the proposed method is demonstrated through synthetic experiments and three real-world applications.",NA,https://proceedings.neurips.cc/paper_files/paper/2022/hash/7b2f0758334389b8ad0665a9bd165463-Abstract-Conference.html,Advances in Neural Information Processing Systems,Xiaoqing Tan;Zhengling Qi;Christopher Seymour;Lu Tang,2022,18,"@inproceedings{2-23011,
  title={Rise: robust individualized decision learning with sensitive variables},
  author={Tan, Xiaoqing and Qi, Zhengling and Seymour, Christopher and Tang, Lu},
  year={2022},
  booktitle={Advances in Neural Information Processing Systems}
}","Algorithmic contributions, Methodological contributions",Generic / Abstract / Domain-agnostic,Institutional,Executing,"Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-23012,neurips,Sel-bald: deep bayesian active learning for selective labeling with instance rejection,"Machine learning systems are widely used in many high-stakes contexts in which experimental designs for assigning treatments are infeasible. When evaluating a decision instance is costly, such as investigating a fraud case, or evaluating a biopsy decision, a sample-efficient strategy is needed. However, while existing active learning methods assume humans will always label the instances selected by the machine learning model, in many critical applications, humans may decline to label instances selected by the machine learning model due to reasons such as regulation constraint, domain knowledge, or algorithmic aversion, thus not sample efficient. In this paper, we propose the Active Learning with Instance Rejection (ALIR) problem, which is a new active learning problem that considers the human discretion behavior for high-stakes decision making problems. We propose new active learning algorithms under deep Bayesian active learning for selective labeling (SEL-BALD) to address the ALIR problem. Our algorithms consider how to acquire information for both the machine learning model and the human discretion model. We conduct experiments on both synthetic and real-world datasets to demonstrate the effectiveness of our proposed algorithms.",NA,https://openreview.net/forum?id=tDMTwto6jv,Advances in Neural Information Processing Systems,Ruijiang Gao;Mingzhang Yin;Maytal Saar-Tsechansky,2024,1,"@inproceedings{2-23012,
  title     = {Sel-BALD: Deep Bayesian Active Learning for Selective Labeling with Instance Rejection},
  author    = {Ruijiang Gao and Mingzhang Yin and Maytal Saar-Tsechansky},
  year      = {2024},
  booktitle = {Advances in Neural Information Processing Systems}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Operational,Analyzing,"Decision-maker, Guardian, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-23013,neurips,Sequential decision making with expert demonstrations under unobserved heterogeneity,"We study the problem of online sequential decision-making given auxiliary demonstrations from _experts_ who made their decisions based on unobserved contextual information. These demonstrations can be viewed as solving related but slightly different tasks than what the learner faces. This setting arises in many application domains, such as self-driving cars, healthcare, and finance, where expert demonstrations are made using contextual information, which is not recorded in the data available to the learning agent. We model the problem as a zero-shot meta-reinforcement learning setting with an unknown task distribution and a Bayesian regret minimization objective, where the unobserved tasks are encoded as parameters with an unknown prior. We propose the Experts-as-Priors algorithm (ExPerior), an empirical Bayes approach that utilizes expert data to establish an informative prior distribution over the learner's decision-making problem. This prior enables the application of any Bayesian approach for online decision-making, such as posterior sampling. We demonstrate that our strategy surpasses existing behaviour cloning and online algorithms, as well as online-offline baselines for multi-armed bandits, Markov decision processes (MDPs), and partially observable MDPs, showcasing the broad reach and utility of ExPerior in using expert demonstrations across different decision-making setups.",NA,https://openreview.net/forum?id=c8cpMlPUbI,Advances in Neural Information Processing Systems,Vahid Balazadeh;Keertana Chidambaram;Viet Nguyen;Rahul Krishnan;Vasilis Syrgkanis,2024,0,"@inproceedings{2-23013,
  title={Sequential decision making with expert demonstrations under unobserved heterogeneity},
  author={Balazadeh, Vahid and Chidambaram, Keertana and Nguyen, Viet and Krishnan, Rahul and Syrgkanis, Vasilis},
  year={2024},
  booktitle={Advances in Neural Information Processing Systems}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery, Finance / Business / Economy, Transportation / Mobility / Planning",Operational,"Executing, Analyzing",Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-23016,neurips,Teachable reinforcement learning via advice distillation,"Training automated agents to complete complex tasks in interactive environments is challenging: reinforcement learning requires careful hand-engineering of reward functions, imitation learning requires specialized infrastructure and access to a human expert, and learning from intermediate forms of supervision (like binary preferences) is time-consuming and extracts little information from each human intervention. Can we overcome these challenges by building agents that learn from rich, interactive feedback instead? We propose a new supervision paradigm for interactive learning based on \\teachable\\ decision-making systems that learn from structured advice provided by an external teacher. We begin by formalizing a class of human-in-the-loop decision making problems in which multiple forms of teacher-provided advice are available to a learner. We then describe a simple learning algorithm for these problems that first learns to interpret advice, then learns from advice to complete tasks even in the absence of human supervision. In puzzle-solving, navigation, and locomotion domains, we show that agents that learn from advice can acquire new skills with significantly less human supervision than standard reinforcement learning algorithms and often less than imitation learning.",NA,https://proceedings.neurips.cc/paper_files/paper/2021/hash/37cfff3c04f95b22bcf166df586cd7a9-Abstract.html,Advances in Neural Information Processing Systems,Olivia Watkins;Abhishek Gupta;Trevor Darrell;Pieter Abbeel;Jacob Andreas,2021,5,"@inproceedings{2-23016,
  title={Teachable reinforcement learning via advice distillation},
  author={Watkins, Olivia and Gupta, Abhishek and Darrell, Trevor and Abbeel, Pieter and Andreas, Jacob},
  year={2021},
  booktitle={Advances in Neural Information Processing Systems}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,no such info,Executing,"Guardian, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-23018,neurips,Uncalibrated models can improve human-ai collaboration,"In many practical applications of AI, an AI model is used as a decision aid for human users. The AI provides advice that a human (sometimes) incorporates into their decision-making process. The AI advice is often presented with some measure of \\confidence\\ that the human can use to calibrate how much they depend on or trust the advice. In this paper, we present an initial exploration that suggests showing AI models as more confident than they actually are, even when the original AI is well-calibrated, can improve human-AI performance (measured as the accuracy and confidence of the human's final prediction after seeing the AI advice). We first train a model to predict human incorporation of AI advice using data from thousands of human-AI interactions. This enables us to explicitly estimate how to transform the AI's prediction confidence, making the AI uncalibrated, in order to improve the final human prediction. We empirically validate our results across four different tasks---dealing with images, text and tabular data---involving hundreds of human participants. We further support our findings with simulation analysis. Our findings suggest the importance of jointly optimizing the human-AI system as opposed to the standard paradigm of optimizing the AI model alone.",NA,https://proceedings.neurips.cc/paper_files/paper/2022/hash/1968ea7d985aa377e3a610b05fc79be0-Abstract-Conference.html,Advances in Neural Information Processing Systems,Kailas Vodrahalli;Tobias Gerstenberg;James Y. Zou,2022,68,"@inproceedings{2-23018,
  title = {Uncalibrated models can improve human-ai collaboration},
  author = {Vodrahalli, Kailas and Gerstenberg, Tobias and Zou, James Y.},
  year = {2022},
  booktitle = {Advances in Neural Information Processing Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Forecasting, Advising",Decision-maker,"Alter decision outcomes, Change affective-perceptual",no such info,confidence score,corrective feedback,"Visual, Interactive interface",Yes,Yes
2-23020,neurips,"Utilizing human behavior modeling to manipulate explanations in ai-assisted decision making: the good, the bad, and the scary","Recent advances in AI models have increased the integration of AI-based decision aids into the human decision making process. To fully unlock the potential of AI-assisted decision making, researchers have computationally modeled how humans incorporate AI recommendations into their final decisions, and utilized these models to improve human-AI team performance. Meanwhile, due to the ``black-box'' nature of AI models, providing AI explanations to human decision makers to help them rely on AI recommendations more appropriately has become a common practice. In this paper, we explore whether we can quantitatively model how humans integrate both AI recommendations and explanations into their decision process, and whether this quantitative understanding of human behavior from the learned model can be utilized to manipulate AI explanations, thereby nudging individuals towards making targeted decisions. Our extensive human experiments across various tasks demonstrate that human behavior can be easily influenced by these manipulated explanations towards targeted outcomes, regardless of the intent being adversarial or benign. Furthermore, individuals often fail to detect any anomalies in these explanations, despite their decisions being affected by them.",NA,https://openreview.net/forum?id=7XkwzaPMvX,Advances in Neural Information Processing Systems,Zhuoyan Li;Ming Yin,2024,7,"@inproceedings{2-23020,
  title={Utilizing human behavior modeling to manipulate explanations in AI-assisted decision making: the good, the bad, and the scary},
  author={Li, Zhuoyan and Yin, Ming},
  year={2024},
  booktitle={Advances in Neural Information Processing Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Analyzing, Explaining, Advising",Decision-maker,"Alter decision outcomes, Shape ethical norms, Change trust",no such info,"recommendations, textual explanations, highlighting and extraction",NA,"Textual, Visual, Interactive interface",Yes,Yes
2-2307,acm,Reliable Decision from Multiple Subtasks through Threshold Optimization: Content Moderation in the Wild,"Social media platforms struggle to protect users from harmful content through content moderation. These platforms have recently leveraged machine learning models to cope with the vast amount of user-generated content daily. Since moderation policies vary depending on countries and types of products, it is common to train and deploy the models per policy. However, this approach is highly inefficient, especially when the policies change, requiring dataset re-labeling and model re-training on the shifted data distribution. To alleviate this cost inefficiency, social media platforms often employ third-party content moderation services that provide prediction scores of multiple subtasks, such as predicting the existence of underage personnel, rude gestures, or weapons, instead of directly providing final moderation decisions. However, making a reliable automated moderation decision from the prediction scores of the multiple subtasks for a specific target policy has not been widely explored yet. In this study, we formulate real-world scenarios of content moderation and introduce a simple yet effective threshold optimization method that searches the optimal thresholds of the multiple subtasks to make a reliable moderation decision in a cost-effective way. Extensive experiments demonstrate that our approach shows better performance in content moderation compared to existing threshold optimization methods and heuristics.",10.1145/3539597.3570439,https://doi.org/10.1145/3539597.3570439,ACM International Conference on Web Search and Data Mining (WSDM),"Son, Donghyun; Lew, Byounggyu; Choi, Kwanghee; Baek, Yongsu; Choi, Seungwoo; Shin, Beomjun; Ha, Sungjoo; Chang, Buru",2023,0,"@inproceedings{2-2307,
  title = {Reliable Decision from Multiple Subtasks through Threshold Optimization: Content Moderation in the Wild},
  author = {Son, Donghyun and Lew, Byounggyu and Choi, Kwanghee and Baek, Yongsu and Choi, Seungwoo and Shin, Beomjun and Ha, Sungjoo and Chang, Buru},
  year = {2023},
  doi = {10.1145/3539597.3570439},
  booktitle = {Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM)}
}",Methodological contributions,Media / Communication / Entertainment,Operational,"Forecasting, Advising, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-23079,neurips,Action gaps and advantages in continuous-time distributional reinforcement learning,"When decisions are made at high frequency, traditional reinforcement learning (RL) methods struggle to accurately estimate action values. In turn, their performance is inconsistent and often poor. Whether the performance of distributional RL (DRL) agents suffers similarly, however, is unknown. In this work, we establish that DRL agents *are* sensitive to the decision frequency. We prove that action-conditioned return distributions collapse to their underlying policy's return distribution as the decision frequency increases. We quantify the rate of collapse of these return distributions and exhibit that their statistics collapse at different rates. Moreover, we define distributional perspectives on action gaps and advantages. In particular, we introduce the *superiority* as a probabilistic generalization of the advantage---the core object of approaches to mitigating performance issues in high-frequency value-based RL. In addition, we build a superiority-based DRL algorithm. Through simulations in an option-trading domain, we validate that proper modeling of the superiority distribution produces improved controllers at high decision frequencies.",NA,https://openreview.net/forum?id=BRW0MKJ7Rr,Advances in Neural Information Processing Systems,Harley Wiltzer;Marc G Bellemare;David Meger;Patrick Shafto;Yash Jhaveri,2024,0,"@inproceedings{2-23079,
  title={Action gaps and advantages in continuous-time distributional reinforcement learning},
  author={Wiltzer, Harley and Bellemare, Marc G and Meger, David and Shafto, Patrick and Jhaveri, Yash},
  year={2024},
  booktitle={Advances in Neural Information Processing Systems}
}",Empirical contributions,Finance / Business / Economy,no such info,Executing,"Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-23082,neurips,Active learning with safety constraints,"Active learning methods have shown great promise in reducing the number of samples necessary for learning. As automated learning systems are adopted into real-time, real-world decision-making pipelines, it is increasingly important that such algorithms are designed with safety in mind. In this work we investigate the complexity of learning the best safe decision in interactive environments. We reduce this problem to a safe linear bandits problem, where our goal is to find the best arm satisfying certain (unknown) safety constraints. We propose an adaptive experimental design-based algorithm, which we show efficiently trades off between the difficulty of showing an arm is unsafe vs suboptimal. To our knowledge, our results are the first on best-arm identification in linear bandits with safety constraints. In practice, we demonstrate that this approach performs well on synthetic and real world datasets.",NA,https://proceedings.neurips.cc/paper_files/paper/2022/hash/d6929af3791b2cec21c136b573aa87f2-Abstract-Conference.html,Advances in Neural Information Processing Systems,Romain Camilleri;Andrew Wagenmaker;Jamie H. Morgenstern;Lalit Jain;Kevin G. Jamieson,2022,2,"@inproceedings{2-23082,
  title={Active learning with safety constraints},
  author={Camilleri, Romain and Wagenmaker, Andrew and Morgenstern, Jamie H. and Jain, Lalit and Jamieson, Kevin G.},
  year={2022},
  booktitle={Advances in Neural Information Processing Systems}
}",Algorithmic contributions,"Finance / Business / Economy, Generic / Abstract / Domain-agnostic",no such info,Executing,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-23091,neurips,Adaptive data debiasing through bounded exploration,"Biases in existing datasets used to train algorithmic decision rules can raise ethical and economic concerns due to the resulting disparate treatment of different groups. We propose an algorithm for sequentially debiasing such datasets through adaptive and bounded exploration in a classification problem with costly and censored feedback. Exploration in this context means that at times, and to a judiciously-chosen extent, the decision maker deviates from its (current) loss-minimizing rule, and instead accepts some individuals that would otherwise be rejected, so as to reduce statistical data biases. Our proposed algorithm includes parameters that can be used to balance between the ultimate goal of removing data biases -- which will in turn lead to more accurate and fair decisions, and the exploration risks incurred to achieve this goal. We analytically show that such exploration can help debias data in certain distributions. We further investigate how fairness criteria can work in conjunction with our data debiasing algorithm. We illustrate the performance of our algorithm using experiments on synthetic and real-world datasets.",NA,https://proceedings.neurips.cc/paper_files/paper/2022/hash/0a166a3d98720697d9028bbe592fa177-Abstract-Conference.html,Advances in Neural Information Processing Systems,Yifan Yang;Yang Liu;Parinaz Naghizadeh,2022,11,"@inproceedings{2-23091,
  title={Adaptive data debiasing through bounded exploration},
  author={Yang, Yifan and Liu, Yang and Naghizadeh, Parinaz},
  year={2022},
  booktitle={Advances in Neural Information Processing Systems}
}",Algorithmic contributions,"Finance / Business / Economy, Generic / Abstract / Domain-agnostic",Operational,"Forecasting, Executing","Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-2312,acm,Real-Time Bidding by Reinforcement Learning in Display Advertising,"The majority of online display ads are served through real-time bidding (RTB) — each ad display impression is auctioned off in real-time when it is just being generated from a user visit. To place an ad automatically and optimally, it is critical for advertisers to devise a learning algorithm to cleverly bid an ad impression in real-time. Most previous works consider the bid decision as a static optimization problem of either treating the value of each impression independently or setting a bid price to each segment of ad volume. However, the bidding for a given ad campaign would repeatedly happen during its life span before the budget runs out. As such, each bid is strategically correlated by the constrained budget and the overall effectiveness of the campaign (e.g., the rewards from generated clicks), which is only observed after the campaign has completed. Thus, it is of great interest to devise an optimal bidding strategy sequentially so that the campaign budget can be dynamically allocated across all the available impressions on the basis of both the immediate and future rewards. In this paper, we formulate the bid decision process as a reinforcement learning problem, where the state space is represented by the auction information and the campaign's real-time parameters, while an action is the bid price to set. By modeling the state transition via auction competition, we build a Markov Decision Process framework for learning the optimal bidding policy to optimize the advertising performance in the dynamic real-time bidding environment. Furthermore, the scalability problem from the large real-world auction volume and campaign budget is well handled by state value approximation using neural networks. The empirical study on two large-scale real-world datasets and the live A/B testing on a commercial platform have demonstrated the superior performance and high efficiency compared to state-of-the-art methods.",10.1145/3018661.3018702,https://doi.org/10.1145/3018661.3018702,ACM International Conference on Web Search and Data Mining (WSDM),"Cai, Han; Ren, Kan; Zhang, Weinan; Malialis, Kleanthis; Wang, Jun; Yu, Yong; Guo, Defeng",2017,361,"@inproceedings{2-2312,
  title = {Real-Time Bidding by Reinforcement Learning in Display Advertising},
  author = {Cai, Han and Ren, Kan and Zhang, Weinan and Malialis, Kleanthis and Wang, Jun and Yu, Yong and Guo, Defeng},
  year = {2017},
  booktitle = {Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM)},
  doi = {10.1145/3018661.3018702}
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Executing, Monitoring","Decision-maker, Stakeholder, Developer",NA,NA,NA,NA,NA,Yes,No
2-23212,neurips,Calibrating predictions to decisions: a novel approach to multi-class calibration,"When facing uncertainty, decision-makers want predictions they can trust. A machine learning provider can convey confidence to decision-makers by guaranteeing their predictions are distribution calibrated--- amongst the inputs that receive a predicted vector of class probabilities q, the actual distribution over classes is given by q. For multi-class prediction problems, however, directly optimizing predictions under distribution calibration tends to be infeasible, requiring sample complexity that grows exponentially in the number of classes C. In this work, we introduce a new notion---decision calibration---that requires the predicted distribution and true distribution over classes to be indistinguishable'' to downstream decision-makers. This perspective gives a new characterization of distribution calibration: a predictor is distribution calibrated if and only if it is decision calibrated with respect to all decision-makers. Our main result shows that under a mild restriction, unlike distribution calibration, decision calibration is actually feasible. We design a recalibration algorithm that provably achieves decision calibration efficiently, provided that the decision-makers have a bounded number of actions (e.g., polynomial in C). We validate our recalibration algorithm empirically: compared to existing methods, decision calibration improves decision-making on skin lesion and ImageNet classification with modern neural network predictors.",NA,https://proceedings.neurips.cc/paper_files/paper/2021/hash/bbc92a647199b832ec90d7cf57074e9e-Abstract.html,Advances in Neural Information Processing Systems,Shengjia Zhao;Michael Kim;Roshni Sahoo;Tengyu Ma;Stefano Ermon,2021,108,"@inproceedings{2-23212,
  title={Calibrating predictions to decisions: a novel approach to multi-class calibration},
  author={Zhao, Shengjia and Kim, Michael and Sahoo, Roshni and Ma, Tengyu and Ermon, Stefano},
  year={2021},
  booktitle={Advances in Neural Information Processing Systems}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery, Education / Teaching / Research",Operational,Forecasting,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-2323,acm,Marco: Supporting Business Document Workflows via Collection-Centric Information Foraging with Large Language Models,"Knowledge workers often need to extract and analyze information from a collection of documents to solve complex information tasks in the workplace, e.g., hiring managers reviewing resumes or analysts assessing risk in contracts. However, foraging for relevant information can become tedious and repetitive over many documents and criteria of interest. We introduce Marco, a mixed-initiative workspace supporting sensemaking over diverse business document collections. Through collection-centric assistance, Marco reduces the cognitive costs of extracting and structuring information, allowing users to prioritize comparative synthesis and decision making processes. Users interactively communicate their information needs to an AI assistant using natural language and compose schemas that provide an overview of a document collection. Findings from a usability study (n=16) demonstrate that when using Marco, users complete sensemaking tasks 16% more quickly, with less effort, and without diminishing accuracy. A design probe with seven domain experts identifies how Marco can benefit various real-world workflows.",10.1145/3613904.3641969,https://doi.org/10.1145/3613904.3641969,CHI Conference on Human Factors in Computing Systems,"Fok, Raymond; Lipka, Nedim; Sun, Tong; Siu, Alexa F",2024,2,"@inproceedings{2-2323,
  title = {Marco: Supporting Business Document Workflows via Collection-Centric Information Foraging with Large Language Models},
  author = {Fok, Raymond and Lipka, Nedim and Sun, Tong and Siu, Alexa F},
  year = {2024},
  doi = {10.1145/3613904.3641969},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",System/Artifact contributions,Finance / Business / Economy,Operational,"Analyzing, Advising","Decision-maker, Knowledge provider","Alter decision outcomes, Change trust, Change cognitive demands, Change affective-perceptual, Restrict human agency",Change AI responses,"highlighting and extraction, recommendations",domain knowledge,Textual,Yes,Yes
2-23235,neurips,Chessgpt: bridging policy learning and language modeling,"When solving decision-making tasks, humans typically depend on information from two key sources: (1) Historical policy data, which provides interaction replay from the environment, and (2) Analytical insights in natural language form, exposing the invaluable thought process or strategic considerations. Despite this, the majority of preceding research focuses on only one source: they either use historical replay exclusively to directly learn policy or value functions, or engaged in language model training utilizing mere language corpus. In this paper, we argue that a powerful autonomous agent should cover both sources. Thus, we propose ChessGPT, a GPT model bridging policy learning and language modeling by integrating data from these two sources in Chess games. Specifically, we build a large-scale game and language dataset related to chess. Leveraging the dataset, we showcase two model examples ChessCLIP and ChessGPT, integrating policy learning and language modeling. Finally, we propose a full evaluation framework for evaluating language model's chess ability. Experimental results validate our model and dataset's effectiveness. We open source our code, model, and dataset at https://github.com/waterhorse1/ChessGPT.",NA,https://proceedings.neurips.cc/paper_files/paper/2023/hash/16b14e3f288f076e0ca73bdad6405f77-Abstract-Datasets_and_Benchmarks.html,Advances in Neural Information Processing Systems,Xidong Feng;Yicheng Luo;Ziyan Wang;Hongrui Tang;Mengyue Yang;Kun Shao;David Mguni;Yali Du;Jun Wang,2023,89,"@inproceedings{2-23235,
  title     = {Chessgpt: Bridging Policy Learning and Language Modeling},
  author    = {Feng, Xidong and Luo, Yicheng and Wang, Ziyan and Tang, Hongrui and Yang, Mengyue and Shao, Kun and Mguni, David and Du, Yali and Wang, Jun},
  year      = {2023},
  booktitle = {Advances in Neural Information Processing Systems}
}",System/Artifact contributions,Media / Communication / Entertainment,Individual,"Executing, Collaborating","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-23293,neurips,Counterbalancing learning and strategic incentives in allocation markets,"Motivated by the high discard rate of donated organs in the United States, we study an allocation problem in the presence of learning and strategic incentives. We consider a setting where a benevolent social planner decides whether and how to allocate a single indivisible object to a queue of strategic agents. The object has a common true quality, good or bad, which is ex-ante unknown to everyone. Each agent holds an informative, yet noisy, private signal about the quality. To make a correct allocation decision the planner attempts to learn the object quality by truthfully eliciting agents' signals. Under the commonly applied sequential offering mechanism, we show that learning is hampered by the presence of strategic incentives as herding may emerge. This can result in incorrect allocation and welfare loss. To overcome these issues, we propose a novel class of incentive-compatible mechanisms. Our mechanism involves a batch-by-batch, dynamic voting process using a majority rule. We prove that the proposed voting mechanisms improve the probability of correct allocation whenever agents are sufficiently well informed. Particularly, we show that such an improvement can be achieved via a simple greedy algorithm. We quantify the improvement using simulations.",NA,https://proceedings.neurips.cc/paper_files/paper/2021/hash/5cc3749a6e56ef6d656735dff9176074-Abstract.html,Advances in Neural Information Processing Systems,Jamie Kang;Faidra Monachou;Moran Koren;Itai Ashlagi,2021,3,"@inproceedings{2-23293,
  title={Counterbalancing learning and strategic incentives in allocation markets},
  author={Kang, Jamie and Monachou, Faidra and Koren, Moran and Ashlagi, Itai},
  year={2021},
  booktitle={Advances in Neural Information Processing Systems}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,Analyzing,"Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-2330,acm,Towards Provably Moral AI Agents in Bottom-up Learning Frameworks,"We examine moral machine decision making as inspired by a central question posed by Rossi with respect to moral preferences: can AI systems based on statistical machine learning (which do not provide a natural way to explain or justify their decisions) be used for embedding morality into a machine in a way that allows us to prove that nothing morally wrong will happen? We argue for an evaluation which is held to the same standards as a human agent, removing the demand that ethical behaviour is always achieved. We introduce four key meta-qualities desired for our moral standards, and then proceed to clarify how we can prove that an agent will correctly learn to perform moral actions given a set of samples within certain error bounds. Our group-dynamic approach enables us to demonstrate that the learned models converge to a common function to achieve stability. We further explain a valuable intrinsic consistency check made possible through the derivation of logical statements from the machine learning model. In all, this work proposes an approach for building ethical AI systems, coming from the perspective of artificial intelligence research, and sheds important light on understanding how much learning is required in order for an intelligent agent to behave morally with negligible error.",10.1145/3278721.3278728,https://doi.org/10.1145/3278721.3278728,"AAAI/ACM Conference on AI, Ethics, and Society","Shaw, Nolan P.; Stöckel, Andreas; Orr, Ryan W.; Lidbetter, Thomas F.; Cohen, Robin",2018,35,"@inproceedings{2-2330,
  title={Towards Provably Moral {AI} Agents in Bottom-up Learning Frameworks},
  author={Shaw, Nolan P. and St{\""o}ckel, Andreas and Orr, Ryan W. and Lidbetter, Thomas F. and Cohen, Robin},
  year={2018},
  doi={10.1145/3278721.3278728},
  booktitle={AAAI/ACM Conference on AI, Ethics, and Society}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,no such info,Executing,Guardian,NA,NA,NA,NA,NA,Yes,No
2-23303,neurips,Curriculum design for teaching via demonstrations: theory and applications,"We consider the problem of teaching via demonstrations in sequential decision-making settings. In particular, we study how to design a personalized curriculum over demonstrations to speed up the learner's convergence. We provide a unified curriculum strategy for two popular learner models: Maximum Causal Entropy Inverse Reinforcement Learning (MaxEnt-IRL) and Cross-Entropy Behavioral Cloning (CrossEnt-BC). Our unified strategy induces a ranking over demonstrations based on a notion of difficulty scores computed w.r.t. the teacher's optimal policy and the learner's current policy. Compared to the state of the art, our strategy doesn't require access to the learner's internal dynamics and still enjoys similar convergence guarantees under mild technical conditions. Furthermore, we adapt our curriculum strategy to the setting where no teacher agent is present using task-specific difficulty scores. Experiments on a synthetic car driving environment and navigation-based environments demonstrate the effectiveness of our curriculum strategy.",NA,https://proceedings.neurips.cc/paper_files/paper/2021/hash/56c51a39a7c77d8084838cc920585bd0-Abstract.html,Advances in Neural Information Processing Systems,Gaurav Yengera;Rati Devidze;Parameswaran Kamalaruban;Adish Singla,2021,13,"@inproceedings{2-23303,
  title = {Curriculum design for teaching via demonstrations: theory and applications},
  author = {Gaurav Yengera and Rati Devidze and Parameswaran Kamalaruban and Adish Singla},
  year = {2021},
  booktitle = {Advances in Neural Information Processing Systems}
}",Algorithmic contributions,Education / Teaching / Research,Operational,"Advising, Executing","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-2332,acm,Explainability Is Not a Game,"When the decisions of ML models impact people, one should expect explanations to offer the strongest guarantees of rigor. However, the most popular XAI approaches offer none.",10.1145/3635301,https://doi.org/10.1145/3635301,Communications of the ACM,"Marques-Silva, Joao; Huang, Xuanxiang",2024,55,"@article{2-2332,
  title={Explainability Is Not a Game},
  author={Marques-Silva, Joao and Huang, Xuanxiang},
  year={2024},
  journal={Communications of the ACM},
  doi={10.1145/3635301}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,no such info,"Explaining, Executing","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-2333,acm,Valuing an Engagement Surface using a Large Scale Dynamic Causal Model,"With recent rapid growth in online shopping, AI-powered Engagement Surfaces (ES) have become ubiquitous across retail services. These engagement surfaces perform an increasing range of functions, including recommending new products for purchase, reminding customers of their orders and providing delivery notifications. Understanding the causal effect of engagement surfaces on value driven for customers and businesses remains an open scientific question. In this paper, we develop a dynamic causal model at scale to disentangle value attributable to an ES, and to assess its effectiveness. We demonstrate the application of this model to inform business decision-making by understanding returns on investment in the ES, and identifying product lines and features where the ES adds the most value.",10.1145/3637528.3671604,https://doi.org/10.1145/3637528.3671604,ACM SIGKDD Conference on Knowledge Discovery and Data Mining,"Mukerji, Abhimanyu; More, Sushant; Kannan, Ashwin Viswanathan; Ravi, Lakshmi; Chen, Hua; Kohli, Naman; Khawand, Chris; Mandalapu, Dinesh",2024,0,"@inproceedings{2-2333,
  title = {Valuing an Engagement Surface using a Large Scale Dynamic Causal Model},
  author = {Mukerji, Abhimanyu and More, Sushant and Kannan, Ashwin Viswanathan and Ravi, Lakshmi and Chen, Hua and Kohli, Naman and Khawand, Chris and Mandalapu, Dinesh},
  year = {2024},
  doi = {10.1145/3637528.3671604},
  booktitle = {Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining}
}",Algorithmic contributions,Finance / Business / Economy,Organizational,"Advising, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-23333,neurips,Decision-making behavior evaluation framework for llms under uncertain context,"When making decisions under uncertainty, individuals often deviate from rational behavior, which can be evaluated across three dimensions: risk preference, probability weighting, and loss aversion. Given the widespread use of large language models (LLMs) in supporting decision-making processes, it is crucial to assess whether their behavior aligns with human norms and ethical expectations or exhibits potential biases. Although several empirical studies have investigated the rationality and social behavior performance of LLMs, their internal decision-making tendencies and capabilities remain inadequately understood. This paper proposes a framework, grounded in behavioral economics theories, to evaluate the decision-making behaviors of LLMs. With a multiple-choice-list experiment, we initially estimate the degree of risk preference, probability weighting, and loss aversion in a context-free setting for three commercial LLMs: ChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro. Our results reveal that LLMs generally exhibit patterns similar to humans, such as risk aversion and loss aversion, with a tendency to overweight small probabilities, but there are significant variations in the degree to which these behaviors are expressed across different LLMs. Further, we explore their behavior when embedded with socio-demographic features of human beings, uncovering significant disparities across various demographic characteristics.",NA,https://openreview.net/forum?id=re0ly2Ylcu,Advances in Neural Information Processing Systems,Jingru Jia;Zehua Yuan;Junhao Pan;Paul E McNamara;Deming Chen,2024,0,"@inproceedings{2-23333,
  title={Decision-making behavior evaluation framework for llms under uncertain context},
  author={Jia, Jingru and Yuan, Zehua and Pan, Junhao and McNamara, Paul E and Chen, Deming},
  year={2024},
  booktitle={Advances in Neural Information Processing Systems}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,no such info,"Advising, Explaining","Knowledge provider, Decision-maker, Guardian",no such info,no such info,"model risk, safety/creativity bias",NA,"Textual, Conversational/Natural Language",Yes,Yes
2-2336,acm,Privacy-preserving Crowd-guided AI Decision-making in Ethical Dilemmas,"With the rapid development of artificial intelligence (AI), ethical issues surrounding AI have attracted increasing attention. In particular, autonomous vehicles may face moral dilemmas in accident scenarios, such as staying the course resulting in hurting pedestrians or swerving leading to hurting passengers. To investigate such ethical dilemmas, recent studies have adopted preference aggregation, in which each voter expresses her/his preferences over decisions for the possible ethical dilemma scenarios, and a centralized system aggregates these preferences to obtain the winning decision. Although a useful methodology for building ethical AI systems, such an approach can potentially violate the privacy of voters since moral preferences are sensitive information and their disclosure can be exploited by malicious parties resulting in negative consequences. In this paper, we report a first-of-its-kind privacy-preserving crowd-guided AI decision-making approach in ethical dilemmas. We adopt the formal and popular notion of differential privacy to quantify privacy, and consider four granularities of privacy protection by taking voter-/record-level privacy protection and centralized/distributed perturbation into account, resulting in four approaches VLCP, RLCP, VLDP, and RLDP, respectively. Moreover, we propose different algorithms to achieve these privacy protection granularities, while retaining the accuracy of the learned moral preference model. Specifically, VLCP and RLCP are implemented with the data aggregator setting a universal privacy parameter and perturbing the averaged moral preference to protect the privacy of voters' data. VLDP and RLDP are implemented in such a way that each voter perturbs her/his local moral preference with a personalized privacy parameter. Extensive experiments based on both synthetic data and real-world data of voters' moral decisions demonstrate that the proposed approaches achieve high accuracy of preference aggregation while protecting individual voter's privacy.",10.1145/3357384.3357954,https://doi.org/10.1145/3357384.3357954,ACM International Conference on Information and Knowledge Management (CIKM),"Wang, Teng; Zhao, Jun; Yu, Han; Liu, Jinyan; Yang, Xinyu; Ren, Xuebin; Shi, Shuyu",2019,19,"@inproceedings{2-2336,
  title = {Privacy-preserving Crowd-guided AI Decision-making in Ethical Dilemmas},
  author = {Wang, Teng and Zhao, Jun and Yu, Han and Liu, Jinyan and Yang, Xinyu and Ren, Xuebin and Shi, Shuyu},
  year = {2019},
  doi = {10.1145/3357384.3357954},
  booktitle = {Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM)}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Institutional,Executing,Decision-maker,no such info,no such info,"delegation, prediction of alternative, recommendations","privacy concern, personalized settings",Conversational/Natural Language,Yes,Yes
2-23364,neurips,Direct preference-based evolutionary multi-objective optimization with dueling bandits,"The ultimate goal of multi-objective optimization (MO) is to assist human decision-makers (DMs) in identifying solutions of interest (SOI) that optimally reconcile multiple objectives according to their preferences. Preference-based evolutionary MO (PBEMO) has emerged as a promising framework that progressively approximates SOI by involving human in the optimization-cum-decision-making process. Yet, current PBEMO approaches are prone to be inefficient and misaligned with the DM’s true aspirations, especially when inadvertently exploiting mis-calibrated reward models. This is further exacerbated when considering the stochastic nature of human feedback. This paper proposes a novel framework that navigates MO to SOI by directly leveraging human feedback without being restricted by a predefined reward model nor cumbersome model selection. Specifically, we developed a clustering-based stochastic dueling bandits algorithm that strategically scales well to high-dimensional dueling bandits, and achieves a regret of $\\mathcal{O}(K^2\\log T)$, where $K$ is the number of clusters and $T$ is the number of rounds. The learned preferences are then transformed into a unified probabilistic format that can be readily adapted to prevalent EMO algorithms. This also leads to a principled termination criterion that strategically manages human cognitive loads and computational budget. Experiments on $48$ benchmark test problems, including synthetic problems, RNA inverse design and protein structure prediction, fully demonstrate the effectiveness of our proposed approach.",NA,https://openreview.net/forum?id=owHj0G15cd,Advances in Neural Information Processing Systems,Tian Huang;Shengbo Wang;Ke Li,2024,5,"@inproceedings{2-23364,
  title = {Direct Preference-Based Evolutionary Multi-Objective Optimization with Dueling Bandits},
  author = {Tian Huang and Shengbo Wang and Ke Li},
  year = {2024},
  booktitle = {Advances in Neural Information Processing Systems}
}",Algorithmic contributions,"Education / Teaching / Research, Generic / Abstract / Domain-agnostic",Operational,"Collaborating, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-23367,neurips,Discrimination in online markets: effects of social bias on learning from reviews and policy design,"The increasing popularity of online two-sided markets such as ride-sharing, accommodation and freelance labor platforms, goes hand in hand with new socioeconomic challenges. One major issue remains the existence of bias and discrimination against certain social groups. We study this problem using a two-sided large market model with employers and workers mediated by a platform. Employers who seek to hire workers face uncertainty about a candidate worker's skill level. Therefore, they base their hiring decision on learning from past reviews about an individual worker as well as on their (possibly misspecified) prior beliefs about the ability level of the social group the worker belongs to. Drawing upon the social learning literature with bounded rationality and limited information, uncertainty combined with social bias leads to unequal hiring opportunities between workers of different social groups. Although the effect of social bias decreases as the number of reviews increases (consistent with empirical findings), minority workers still receive lower expected payoffs. Finally, we consider a simple directed matching policy (DM), which combines learning and matching to make better matching decisions for minority workers. Under this policy, there exists a steady-state equilibrium, in which DM reduces the discrimination gap.",NA,https://proceedings.neurips.cc/paper_files/paper/2019/hash/e00406144c1e7e35240afed70f34166a-Abstract.html,Advances in Neural Information Processing Systems,Faidra Georgia Monachou;Itai Ashlagi,2019,24,"@inproceedings{2-23367,
  title = {Discrimination in online markets: effects of social bias on learning from reviews and policy design},
  author = {Faidra Georgia Monachou and Itai Ashlagi},
  year = {2019},
  booktitle = {Advances in Neural Information Processing Systems}
}",Theoretical contributions,Finance / Business / Economy,Operational,"Executing, Advising, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-23404,neurips,Eai: emotional decision-making of llms in strategic games and ethical dilemmas,"One of the urgent tasks of artificial intelligence is to assess the safety and alignment of large language models (LLMs) with human behavior. Conventional verification only in pure natural language processing benchmarks can be insufficient. Since emotions often influence human decisions, this paper examines LLM alignment in complex strategic and ethical environments, providing an in-depth analysis of the drawbacks of our psychology and the emotional impact on decision-making in humans and LLMs. We introduce the novel EAI framework for integrating emotion modeling into LLMs to examine the emotional impact on ethics and LLM-based decision-making in various strategic games, including bargaining and repeated games. Our experimental study with various LLMs demonstrated that emotions can significantly alter the ethical decision-making landscape of LLMs, highlighting the need for robust mechanisms to ensure consistent ethical standards. Our game-theoretic analysis revealed that LLMs are susceptible to emotional biases influenced by model size, alignment strategies, and primary pretraining language. Notably, these biases often diverge from typical human emotional responses, occasionally leading to unexpected drops in cooperation rates, even under positive emotional influence. Such behavior complicates the alignment of multiagent systems, emphasizing the need for benchmarks that can rigorously evaluate the degree of emotional alignment. Our framework provides a foundational basis for developing such benchmarks.",NA,https://openreview.net/forum?id=8aAaYEwNR4,Advances in Neural Information Processing Systems,Mikhail Mozikov;Nikita Severin;Valeria Bodishtianu;Maria Glushanina;Ivan Nasonov;Daniil Orekhov;Vladislav Pekhotin;Ivan Makovetskiy;Mikhail Baklashkin;Vasily Lavrentyev;Akim Tsvigun;Denis Turdakov;Tatiana Shavrina;Andrey Savchenko;Ilya Makarov,2024,15,"@inproceedings{2-23404,
  title     = {EAI: Emotional Decision-Making of LLMs in Strategic Games and Ethical Dilemmas},
  author    = {Mikhail Mozikov and Nikita Severin and Valeria Bodishtianu and Maria Glushanina and Ivan Nasonov and Daniil Orekhov and Vladislav Pekhotin and Ivan Makovetskiy and Mikhail Baklashkin and Vasily Lavrentyev and Akim Tsvigun and Denis Turdakov and Tatiana Shavrina and Andrey Savchenko and Ilya Makarov},
  year      = {2024},
  booktitle = {Advances in Neural Information Processing Systems}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Organizational,Executing,Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-2341,acm,How Should AI Systems Talk to Users when Collecting their Personal Information? Effects of Role Framing and Self-Referencing on Human-AI Interaction,"AI systems collect our personal information in order to provide personalized services, raising privacy concerns and making users leery. As a result, systems have begun emphasizing overt over covert collection of information by directly asking users. This poses an important question for ethical interaction design, which is dedicated to improving user experience while promoting informed decision-making: Should the interface tout the benefits of information disclosure and frame itself as a help-provider? Or, should it appear as a help-seeker? We decided to find out by creating a mockup of a news recommendation system called Mindz and conducting an online user study (N=293) with the following four variations: AI system as help seeker vs. help provider vs. both vs. neither. Data showed that even though all participants received the same recommendations, power users tended to trust a help-seeking Mindz more whereas non-power users favored one that is both help-seeker and help-provider.",10.1145/3411764.3445415,https://doi.org/10.1145/3411764.3445415,CHI Conference on Human Factors in Computing Systems,"Liao, Mengqi; Sundar, S. Shyam",2021,0,"@inproceedings{2-2341,
  title = {How Should AI Systems Talk to Users when Collecting their Personal Information? Effects of Role Framing and Self-Referencing on Human-AI Interaction},
  author = {Liao, Mengqi and Sundar, S. Shyam},
  year = {2021},
  doi = {10.1145/3411764.3445415},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Advising, Analyzing, Collaborating","Decision-maker, Decision-subject, Stakeholder","Change trust, Change cognitive demands, Shape ethical norms, Alter decision outcomes",no such info,"recommendations, seeking help, providing help",NA,"Textual, Conversational/Natural Language",Yes,Yes
2-23456,neurips,Expert load matters: operating networks at high accuracy and low manual effort,"In human-AI collaboration systems for critical applications, in order to ensure minimal error, users should set an operating point based on model confidence to determine when the decision should be delegated to human experts. Samples for which model confidence is lower than the operating point would be manually analysed by experts to avoid mistakes.Such systems can become truly useful only if they consider two aspects: models should be confident only for samples for which they are accurate, and the number of samples delegated to experts should be minimized.The latter aspect is especially crucial for applications where available expert time is limited and expensive, such as healthcare. The trade-off between the model accuracy and the number of samples delegated to experts can be represented by a curve that is similar to an ROC curve, which we refer to as confidence operating characteristic (COC) curve. In this paper, we argue that deep neural networks should be trained by taking into account both accuracy and expert load and, to that end, propose a new complementary loss function for classification that maximizes the area under this COC curve.This promotes simultaneously the increase in network accuracy and the reduction in number of samples delegated to humans.We perform experiments on multiple computer vision and medical image datasets for classification.Our results demonstrate that the proposed loss improves classification accuracy and delegates less number of decisions to experts, achieves better out-of-distribution samples detection and on par calibration performance compared to existing loss functions.",NA,https://proceedings.neurips.cc/paper_files/paper/2023/hash/348346383eb58ed19def02e233c408d6-Abstract-Conference.html,Advances in Neural Information Processing Systems,Sara Sangalli;Ertunc Erdil;Ender Konukoglu,2023,6,"@inproceedings{2-23456,
  title={Expert load matters: operating networks at high accuracy and low manual effort},
  author={Sangalli, Sara and Erdil, Ertunc and Konukoglu, Ender},
  year={2023},
  booktitle={Advances in Neural Information Processing Systems}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Executing","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-23459,neurips,Explainable and efficient randomized voting rules,"With a rapid growth in the deployment of AI tools for making critical decisions (or aiding humans in doing so), there is a growing demand to be able to explain to the stakeholders how these tools arrive at a decision. Consequently, voting is frequently used to make such decisions due to its inherent explainability. Recent work suggests that using randomized (as opposed to deterministic) voting rules can lead to significant efficiency gains measured via the distortion framework. However, rules that use intricate randomization can often become too complex to explain to the stakeholders; losing explainability can eliminate the key advantage of voting over black-box AI tools, which may outweigh the efficiency gains.We study the efficiency gains which can be unlocked by using voting rules that add a simple randomization step to a deterministic rule, thereby retaining explainability. We focus on two such families of rules, randomized positional scoring rules and random committee member rules, and show, theoretically and empirically, that they indeed achieve explainability and efficiency simultaneously to some extent.",NA,https://proceedings.neurips.cc/paper_files/paper/2023/hash/47eb2874a790d5b1f554b9bb93b3de9d-Abstract-Conference.html,Advances in Neural Information Processing Systems,Soroush Ebadian;Aris Filos-Ratsikas;Mohamad Latifian;Nisarg Shah,2023,10,"@inproceedings{2-23459,
  title={Explainable and efficient randomized voting rules},
  author={Ebadian, Soroush and Filos-Ratsikas, Aris and Latifian, Mohamad and Shah, Nisarg},
  year={2023},
  booktitle={Advances in Neural Information Processing Systems}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,no such info,"Advising, Explaining","Stakeholder, Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-2351,acm,Certification Labels for Trustworthy AI: Insights From an Empirical Mixed-Method Study,"Auditing plays a pivotal role in the development of trustworthy AI. However, current research primarily focuses on creating auditable AI documentation, which is intended for regulators and experts rather than end-users affected by AI decisions. How to communicate to members of the public that an AI has been audited and considered trustworthy remains an open challenge. This study empirically investigated certification labels as a promising solution. Through interviews (N = 12) and a census-representative survey (N = 302), we investigated end-users’ attitudes toward certification labels and their effectiveness in communicating trustworthiness in low- and high-stakes AI scenarios. Based on the survey results, we demonstrate that labels can significantly increase end-users’ trust and willingness to use AI in both low- and high-stakes scenarios. However, end-users’ preferences for certification labels and their effect on trust and willingness to use AI were more pronounced in high-stake scenarios. Qualitative content analysis of the interviews revealed opportunities and limitations of certification labels, as well as facilitators and inhibitors for the effective use of labels in the context of AI. For example, while certification labels can mitigate data-related concerns expressed by end-users (e.g., privacy and data protection), other concerns (e.g., model performance) are more challenging to address. Our study provides valuable insights and recommendations for designing and implementing certification labels as a promising constituent within the trustworthy AI ecosystem.",10.1145/3593013.3593994,https://doi.org/10.1145/3593013.3593994,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Scharowski, Nicolas; Benk, Michaela; Kühne, Swen J.; Wettstein, Léane; Brühlmann, Florian",2023,49,"@inproceedings{2-2351,
  title     = {Certification Labels for Trustworthy {AI}: Insights From an Empirical Mixed-Method Study},
  author    = {Scharowski, Nicolas and Benk, Michaela and K{\""u}hne, Swen J. and Wettstein, L{\'e}ane and Br{\""u}hlmann, Florian},
  year      = {2023},
  doi       = {10.1145/3593013.3593994},
  booktitle = {ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Institutional,"Advising, Auditing, Analyzing","Decision-maker, Knowledge provider","Change trust, Alter decision outcomes, Change cognitive demands, Shift responsibility, Shape ethical norms",no such info,predicted labels,certification labels,Autonomous System,Yes,Yes
2-2352,acm,Cross-Scenario Maneuver Decision with Adaptive Perception for Autonomous Driving,"Autonomous driving is a rapidly advancing field that promises to revolutionize the transportation industry through an intelligent perception-and-decision paradigm. Despite decades of research, existing methods are limited in adapting to complex scenarios or expanding to unseen situations, which pose significant challenges to the development of autonomous driving. Inspired by the process of human learning to drive, autonomous vehicles can prioritize developing driving capabilities in basic scenarios and then extending the atomic abilities to more complex scenarios. To this end, we proposed a perception-and-decision framework, called ATEND, which consists of an adaptive perception module and a maneuver decision module. Specifically, the perception module based on Variational Autoencoder is proposed to map perceptual data of complex scenarios into basic scenarios. Then the reinforcement learning-based decision module can make high-level decisions in transformed scenarios. Once ATEND learns to drive in basic scenarios, it can achieve safe and efficient driving in real scenarios without additional training. Extensive experiments in different traffic scenarios evidence that the proposed framework advances the state of the art in terms of both macroscopic and microscopic effectiveness.",10.1145/3583780.3614831,https://doi.org/10.1145/3583780.3614831,ACM International Conference on Information and Knowledge Management (CIKM),"Fu, Yuan; Liu, Shuncheng; Xia, Yuyang; Guo, Fangda; Zheng, Kai",2023,2,"@inproceedings{2-2352,
  title={Cross-Scenario Maneuver Decision with Adaptive Perception for Autonomous Driving},
  author={Fu, Yuan and Liu, Shuncheng and Xia, Yuyang and Guo, Fangda and Zheng, Kai},
  year={2023},
  doi={10.1145/3583780.3614831},
  booktitle={Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM)}
}",System/Artifact contributions,Transportation / Mobility / Planning,Individual,Executing,Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-2355,acm,"Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare","Bias in applications of machine learning (ML) to healthcare is usually attributed to unrepresentative or incomplete data, or to underlying health disparities. This article identifies a more pervasive source of bias that affects the clinical utility of ML-enabled prediction tools: target specification bias. Target specification bias arises when the operationalization of the target variable does not match its definition by decision makers. The mismatch is often subtle, and stems from the fact that decision makers are typically interested in predicting the outcomes of counterfactual, rather than actual, healthcare scenarios. Target specification bias persists independently of data limitations and health disparities. When left uncorrected, it gives rise to an overestimation of predictive accuracy, to inefficient utilization of medical resources, and to suboptimal decisions that can harm patients. Recent work in metrology – the science of measurement – suggests ways of counteracting target specification bias and avoiding its harmful consequences.",10.1145/3600211.3604678,https://doi.org/10.1145/3600211.3604678,"AAAI/ACM Conference on AI, Ethics, and Society","Tal, Eran",2023,16,"@inproceedings{2-2355,
  title={Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare},
  author={Tal, Eran},
  year={2023},
  doi={10.1145/3600211.3604678},
  booktitle={AAAI/ACM Conference on AI, Ethics, and Society}
}",Theoretical contributions,Healthcare / Medicine / Surgery,Institutional,Forecasting,"Decision-subject, Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-2357,acm,Why or Why Not? The Effect of Justification Styles on Chatbot Recommendations,"Chatbots or conversational recommenders have gained increasing popularity as a new paradigm for Recommender Systems (RS). Prior work on RS showed that providing explanations can improve transparency and trust, which are critical for the adoption of RS. Their interactive and engaging nature makes conversational recommenders a natural platform to not only provide recommendations but also justify the recommendations through explanations. The recent surge of interest inexplainable AI enables diverse styles of justification, and also invites questions on how styles of justification impact user perception. In this article, we explore the effect of “why” justifications and “why not” justifications on users’ perceptions of explainability and trust. We developed and tested a movie-recommendation chatbot that provides users with different types of justifications for the recommended items. Our online experiment (n = 310) demonstrates that the “why” justifications (but not the “why not” justifications) have a significant impact on users’ perception of the conversational recommender. Particularly, “why” justifications increase users’ perception of system transparency, which impacts perceived control, trusting beliefs and in turn influences users’ willingness to depend on the system’s advice. Finally, we discuss the design implications for decision-assisting chatbots.",10.1145/3441715,https://doi.org/10.1145/3441715,ACM Transactions on Information Systems,"Wilkinson, Daricia; Alkan, Öznur; Liao, Q. Vera; Mattetti, Massimiliano; Vejsbjerg, Inge; Knijnenburg, Bart P.; Daly, Elizabeth",2021,67,"@article{2-2357,
  title = {Why or Why Not? The Effect of Justification Styles on Chatbot Recommendations},
  author = {Wilkinson, Daricia and Alkan, Öznur and Liao, Q. Vera and Mattetti, Massimiliano and Vejsbjerg, Inge and Knijnenburg, Bart P. and Daly, Elizabeth},
  year = {2021},
  doi = {10.1145/3441715},
  journal = {ACM Transactions on Information Systems}
}",Empirical contributions,Media / Communication / Entertainment,Individual,"Advising, Explaining",Decision-maker,"Change trust, Change affective-perceptual",no such info,"“why” justifications, “why not” justifications",NA,Interactive interface,Yes,Yes
2-2363,acm,To Err is AI: Imperfect Interventions and Repair in a Conversational Agent Facilitating Group Chat Discussions,"Conversational agents (CAs) can analyze online conversations using natural language techniques and effectively facilitate group discussions by sending supervisory messages. However, if a CA makes imperfect interventions, users may stop trusting the CA and discontinue using it. In this study, we demonstrate how inaccurate interventions of a CA and a conversational repair strategy can influence user acceptance of the CA, members' participation in the discussion, perceived discussion experience between the members, and group performance. We built a CA that encourages the participation of members with low contributions in an online chat discussion in which a small group (3-6 members) performs a decision-making task. Two types of errors can occur when detecting under-contributing members: 1) false-positive (FP) errors happen when the CA falsely identifies a member as under-contributing and 2) false-negative (FN) errors occur when the CA misses detecting an under-contributing member. We designed a conversational repair strategy that gives users a chance to contest the detection results and the agent sends a correctional message if an error is detected. Through an online study with 175 participants, we found that participants who received FN error messages reported higher acceptance of the CA and better discussion experience, but participated less compared to those who received FP error messages. The conversational repair strategy moderated the effect of errors such as improving the perceived discussion experience of participants who received FP error messages. Based on our findings, we offer design implications for which model should be selected by practitioners between high precision (i.e., fewer FP errors) and high recall (i.e., fewer FN errors) models depending on the desired effects. When frequent FP errors are expected, we suggest using the conversational repair strategy to improve the perceived discussion experience.",10.1145/3579532,https://doi.org/10.1145/3579532,Proceedings of the ACM on Human-Computer Interaction,"Do, Hyo Jin; Kong, Ha-Kyung; Tetali, Pooja; Lee, Jaewook; Bailey, Brian P.",2023,0,"@article{2-2363,
  title = {To Err is AI: Imperfect Interventions and Repair in a Conversational Agent Facilitating Group Chat Discussions},
  author = {Do, Hyo Jin and Kong, Ha-Kyung and Tetali, Pooja and Lee, Jaewook and Bailey, Brian P.},
  year = {2023},
  doi = {10.1145/3579532},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,Collaborating,"Decision-maker, Decision-subject","Alter decision outcomes, Change trust, Change affective-perceptual","Change AI responses, Update AI competence","false-positive (FP) errors, false-negative (FN) errors",corrective feedback,Conversational/Natural Language,Yes,Yes
2-2364,acm,The Expertise Involved in Deciding which HITs are Worth Doing on Amazon Mechanical Turk,"Crowdworkers depend on Amazon Mechanical Turk (AMT) as an important source of income and it is left to workers to determine which tasks on AMT are fair and worth completing. While there are existing tools that assist workers in making these decisions, workers still spend significant amounts of time finding fair labor. Difficulties in this process may be a contributing factor in the imbalance between the median hourly earnings (2.00/hour) and what the average requester pays (11.00/hour). In this paper, we study how novices and experts select what tasks are worth doing. We argue that differences between the two populations likely lead to the wage imbalances. For this purpose, we first look at workers' comments in TurkOpticon (a tool where workers share their experience with requesters on AMT). We use this study to start to unravel what fair labor means for workers. In particular, we identify the characteristics of labor that workers consider is of ""good quality” and labor that is of ""poor quality” (e.g., work that pays too little.) Armed with this knowledge, we then conduct an experiment to study how experts and novices rate tasks that are of both good and poor quality. Through our research we uncover that experts and novices both treat good quality labor in the same way. However, there are significant differences in how experts and novices rate poor quality labor, and whether they believe the poor quality labor is worth doing. This points to several future directions, including machine learning models that support workers in detecting poor quality labor, and paths for educating novice workers on how to make better labor decisions on AMT.",10.1145/3449202,https://doi.org/10.1145/3449202,Proceedings of the ACM on Human-Computer Interaction,"Hanrahan, Benjamin V.; Chen, Anita; Ma, JiaHua; Ma, Ning F.; Squicciarini, Anna; Savage, Saiph",2021,21,"@article{2-2364,
  title = {The Expertise Involved in Deciding which HITs are Worth Doing on Amazon Mechanical Turk},
  author = {Hanrahan, Benjamin V. and Chen, Anita and Ma, JiaHua and Ma, Ning F. and Squicciarini, Anna and Savage, Saiph},
  year = {2021},
  doi = {10.1145/3449202},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Everyday / Employment / Public Service,Individual,"Analyzing, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-23650,neurips,Large language models play starcraft ii:benchmarks and a chain of summarization approach,"With the continued advancement of Large Language Models (LLMs) Agents in reasoning, planning, and decision-making, benchmarks have become crucial in evaluating these skills. However, there is a notable gap in benchmarks for real-time strategic decision-making. StarCraft II (SC2), with its complex and dynamic nature, serves as an ideal setting for such evaluations. To this end, we have developed TextStarCraft II, a specialized environment for assessing LLMs in real-time strategic scenarios within SC2. Addressing the limitations of traditional Chain of Thought (CoT) methods, we introduce the Chain of Summarization (CoS) method, enhancing LLMs' capabilities in rapid and effective decision-making. Our key experiments included: 1. LLM Evaluation: Tested 10 LLMs in TextStarCraft II, most of them defeating LV5 build-in AI, showcasing effective strategy skills. 2. Commercial Model Knowledge: Evaluated four commercial models on SC2 knowledge; GPT-4 ranked highest by Grandmaster-level experts. 3. Human-AI Matches: Experimental results showed that fine-tuned LLMs performed on par with Gold-level players in real-time matches, demonstrating comparable strategic abilities. All code and data from this study have been made pulicly available at https://github.com/histmeisah/Large-Language-Models-play-StarCraftII",NA,https://openreview.net/forum?id=kEPpD7yETM,Advances in Neural Information Processing Systems,Weiyu Ma;Qirui Mi;Yongcheng Zeng;Xue Yan;Runji Lin;Yuqiao Wu;Jun Wang;Haifeng Zhang,2024,106,"@inproceedings{2-23650,
  title = {Large Language Models Play StarCraft II: Benchmarks and a Chain of Summarization Approach},
  author = {Weiyu Ma and Qirui Mi and Yongcheng Zeng and Xue Yan and Runji Lin and Yuqiao Wu and Jun Wang and Haifeng Zhang},
  year = {2024},
  booktitle = {Advances in Neural Information Processing Systems},
}",Methodological contributions,Media / Communication / Entertainment,no such info,"Executing, Explaining",Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-2368,acm,Continuous and Proactive Software Architecture Evaluation: An IoT Case,"Design-time evaluation is essential to build the initial software architecture to be deployed. However, experts’ assumptions made at design-time are unlikely to remain true indefinitely in systems that are characterized by scale, hyperconnectivity, dynamism, and uncertainty in operations (e.g. IoT). Therefore, experts’ design-time decisions can be challenged at run-time. A continuous architecture evaluation that systematically assesses and intertwines design-time and run-time decisions is thus necessary. This paper proposes the first proactive approach to continuous architecture evaluation of the system leveraging the support of simulation. The approach evaluates software architectures by not only tracking their performance over time, but also forecasting their likely future performance through machine learning of simulated instances of the architecture. This enables architects to make cost-effective informed decisions on potential changes to the architecture. We perform an IoT case study to show how machine learning on simulated instances of architecture can fundamentally guide the continuous evaluation process and influence the outcome of architecture decisions. A series of experiments is conducted to demonstrate the applicability and effectiveness of the approach. We also provide the architect with recommendations on how to best benefit from the approach through choice of learners and input parameters, grounded on experimentation and evidence.",10.1145/3492762,https://doi.org/10.1145/3492762,ACM Transactions on Software Engineering and Methodology,"Sobhy, Dalia; Minku, Leandro; Bahsoon, Rami; Kazman, Rick",2022,8,"@article{2-2368,
  title={Continuous and Proactive Software Architecture Evaluation: An IoT Case},
  author={Sobhy, Dalia and Minku, Leandro and Bahsoon, Rami and Kazman, Rick},
  year={2022},
  doi={10.1145/3492762},
  journal={ACM Transactions on Software Engineering and Methodology}
}",Methodological contributions,Software / Systems / Security,Organizational,"Forecasting, Advising, Executing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-2370,acm,Epistemic Therapy for Bias in Automated Decision-Making,"Despite recent interest in both the critical and machine learning literature on ""bias"" in artificial intelligence (AI) systems, the nature of specific biases stemming from the interaction of machines, humans, and data remains ambiguous. Influenced by Gendler's work on human cognitive biases, we introduce the concept of alief-discordant belief, the tension between the intuitive moral dispositions of designers and the explicit representations generated by algorithms. Our discussion of alief-discordant belief diagnoses the ethical concerns that arise when designing AI systems atop human biases. We furthermore codify the relationship between data, algorithms, and engineers as components of this cognitive discordance, comprising a novel epistemic framework for ethics in AI.",10.1145/3306618.3314294,https://doi.org/10.1145/3306618.3314294,"AAAI/ACM Conference on AI, Ethics, and Society","Gilbert, Thomas Krendl; Mintz, Yonatan",2019,70,"@inproceedings{2-2370,
  title = {Epistemic Therapy for Bias in Automated Decision-Making},
  author = {Gilbert, Thomas Krendl and Mintz, Yonatan},
  year = {2019},
  doi = {10.1145/3306618.3314294},
  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Operational,"Executing, Advising","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-23749,neurips,Making the cut: a bandit-based approach to tiered interviewing,"Given a huge set of applicants, how should a firm allocate sequential resume screenings, phone interviews, and in-person site visits? In a tiered interview process, later stages (e.g., in-person visits) are more informative, but also more expensive than earlier stages (e.g., resume screenings). Using accepted hiring models and the concept of structured interviews, a best practice in human resources, we cast tiered hiring as a combinatorial pure exploration (CPE) problem in the stochastic multi-armed bandit setting. The goal is to select a subset of arms (in our case, applicants) with some combinatorial structure. We present new algorithms in both the probably approximately correct (PAC) and fixed-budget settings that select a near-optimal cohort with provable guarantees. We show via simulations on real data from one of the largest US-based computer science graduate programs that our algorithms make better hiring decisions or use less budget than the status quo.",NA,https://proceedings.neurips.cc/paper_files/paper/2019/hash/d3fad7d3634dbfb61018813546edbccb-Abstract.html,Advances in Neural Information Processing Systems,Candice Schumann;Zhi Lang;Jeffrey Foster;John Dickerson,2019,13,"@inproceedings{2-23749,
  title={Making the cut: a bandit-based approach to tiered interviewing},
  author={Schumann, Candice and Lang, Zhi and Foster, Jeffrey and Dickerson, John},
  year={2019},
  booktitle={Advances in Neural Information Processing Systems}
}",Algorithmic contributions,Everyday / Employment / Public Service,Operational,Executing,"Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-2375,acm,EvidenceQuest: An Interactive Evidence Discovery System for Explainable Artificial Intelligence,"Explainable Artificial Intelligence (XAI) aims to make artificial intelligence (AI) systems transparent and understandable to humans, providing clear explanations for the decisions made by AI models. This paper presents a novel pipeline and a digital dashboard that provides a user-friendly platform for interpreting the results of machine learning algorithms using XAI technology. The dashboard utilizes evidence-based design principles to deliver information clearly and concisely, enabling users to better understand the decisions made by their algorithms. We integrate XAI services into the dashboard to explain the algorithm's predictions, allowing users to understand how their models function and make informed decisions. We demonstrate a motivating scenario in banking and present how the proposed system enhances transparency and accountability and improves trust in the technology.",10.1145/3616855.3635697,https://doi.org/10.1145/3616855.3635697,ACM International Conference on Web Search and Data Mining (WSDM),"Hanif, Ambreen; Beheshti, Amin; Zhang, Xuyun; Wood, Steven; Benatallah, Boualem; Foo, Eu Jin",2024,33,"@inproceedings{2-2375,
  title = {EvidenceQuest: An Interactive Evidence Discovery System for Explainable Artificial Intelligence},
  author = {Hanif, Ambreen and Beheshti, Amin and Zhang, Xuyun and Wood, Steven and Benatallah, Boualem and Foo, Eu Jin},
  year = {2024},
  doi = {10.1145/3616855.3635697},
  booktitle = {Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM)}
}",System/Artifact contributions,Finance / Business / Economy,Operational,"Advising, Explaining, Forecasting",Decision-maker,Change cognitive demands,"Update AI competence, Change AI responses","prediction of alternative, visual analysis, evidence-supported explanations",NA,"Interactive interface, Visual",Yes,Yes
2-23765,neurips,Mediq: question-asking llms and a benchmark for reliable interactive clinical reasoning,"Users typically engage with LLMs interactively, yet most existing benchmarks evaluate them in a static, single-turn format, posing reliability concerns in interactive scenarios. We identify a key obstacle towards reliability: LLMs are trained to answer any question, even with incomplete context or insufficient knowledge. In this paper, we propose to change the static paradigm to an interactive one, develop systems that proactively ask questions to gather more information and respond reliably, and introduce an benchmark—MEDIQ—to evaluate question-asking ability in LLMs. MEDIQ simulates clinical interactions consisting of a Patient System and an adaptive Expert System; with potentially incomplete initial information, the Expert refrains from making diagnostic decisions when unconfident, and instead elicits missing details via follow-up questions. We provide a pipeline to convert single-turn medical benchmarks into an interactive format. Our results show that directly prompting state-of-the-art LLMs to ask questions degrades performance, indicating that adapting LLMs to proactive information-seeking settings is nontrivial. We experiment with abstention strategies to better estimate model confidence and decide when to ask questions, improving diagnostic accuracy by 22.3%; however, performance still lags compared to an (unrealistic in practice) upper bound with complete information upfront. Further analyses show improved interactive performance with filtering irrelevant contexts and reformatting conversations. Overall, we introduce a novel problem towards LLM reliability, an interactive MEDIQ benchmark and a novel question-asking system, and highlight directions to extend LLMs’ information-seeking abilities in critical domains.",NA,https://openreview.net/forum?id=W4pIBQ7bAI,Advances in Neural Information Processing Systems,Shuyue Stella Li;Vidhisha Balachandran;Shangbin Feng;Jonathan S. Ilgen;Emma Pierson;Pang Wei Koh;Yulia Tsvetkov,2024,0,"@inproceedings{2-23765,
  title={Mediq: question-asking llms and a benchmark for reliable interactive clinical reasoning},
  author={Shuyue Stella Li and Vidhisha Balachandran and Shangbin Feng and Jonathan S. Ilgen and Emma Pierson and Pang Wei Koh and Yulia Tsvetkov},
  year={2024},
  booktitle={Advances in Neural Information Processing Systems}
}",Dataset/Benchmark contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Collaborating, Advising","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-2377,acm,CourIRL: Predicting Couriers' Behavior in Last-Mile Delivery Using Crossed-Attention Inverse Reinforcement Learning,"Human behavior prediction is an essential AI-based task, which has inspired many real-world applications. In last-mile logistics, predicting couriers' behavior can benefit the couriers' preference learning and workflow optimization. In this paper, we devote to the behavioral prediction of courier workload and quantify their workload by the working time spent at each area of interest (AOI). Considering the behavior interpretability of inverse reinforcement learning (IRL), existing studies have applied IRL to some real-world transportation prediction scenarios. However, in last-mile logistics, the platform assigns multiple orders to each courier, and couriers also receive new tasks in real-time, which additionally influence the couriers' subsequent decisions. The uncertainty in decision spaces and dynamic the workflow distribution make it more challenging to predict the couriers' working time. In this paper, we propose CourIRL, a practical IRL-based framework leveraging cross-attention to integrate Couriers' historical and spatio-temporal features to predict their future working time. CourIRL formulates the couriers' pick-up and delivery tour as a sequential decision-making process and designs a model-free IRL to learn decision-making preference vectors. A multi-head cross-attention mechanism-based deep regression model is proposed for fine-grained working-time prediction. The results of extensive experiments on two real-world datasets demonstrate that the proposed CourIRL surpasses the state-of-the-art baselines by an average of 6.11% across settings, showing the efficacy and potential contributions of CourIRL in last-mile logistics.",10.1145/3627673.3680046,https://doi.org/10.1145/3627673.3680046,ACM International Conference on Information and Knowledge Management (CIKM),"Wang, Shuai; Kong, Tongtong; Guo, Baoshen; Lin, Li; Wang, Haotian",2024,0,"@inproceedings{2-2377,
  title     = {CourIRL: Predicting Couriers' Behavior in Last-Mile Delivery Using Crossed-Attention Inverse Reinforcement Learning},
  author    = {Wang, Shuai and Kong, Tongtong and Guo, Baoshen and Lin, Li and Wang, Haotian},
  year      = {2024},
  doi       = {10.1145/3627673.3680046},
  booktitle = {Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM)}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,"Advising, Forecasting, Executing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-2385,acm,MockSniffer: characterizing and recommending mocking decisions for unit tests,"In unit testing, mocking is popularly used to ease test effort, reduce test flakiness, and increase test coverage by replacing the actual dependencies with simple implementations. However, there are no clear criteria to determine which dependencies in a unit test should be mocked. Inappropriate mocking can have undesirable consequences: under-mocking could result in the inability to isolate the class under test (CUT) from its dependencies while over-mocking increases the developers' burden on maintaining the mocked objects and may lead to spurious test failures. According to existing work, various factors can determine whether a dependency should be mocked. As a result, mocking decisions are often difficult to make in practice. Studies on the evolution of mocked objects also showed that developers tend to change their mocking decisions: 17% of the studied mocked objects were introduced sometime after the test scripts were created and another 13% of the originally mocked objects eventually became unmocked. In this work, we are motivated to develop an automated technique to make mocking recommendations to facilitate unit testing. We studied 10,846 test scripts in four actively maintained open-source projects that use mocked objects, aiming to characterize the dependencies that are mocked in unit testing. Based on our observations on mocking practices, we designed and implemented a tool, MockSniffer, to identify and recommend mocks for unit tests. The tool is fully automated and requires only the CUT and its dependencies as input. It leverages machine learning techniques to make mocking recommendations by holistically considering multiple factors that can affect developers' mocking decisions. Our evaluation of MockSniffer on ten open-source projects showed that it outperformed three baseline approaches, and achieved good performance in two potential application scenarios.",10.1145/3324884.3416539,https://doi.org/10.1145/3324884.3416539,IEEE/ACM International Conference on Automated Software Engineering (ASE),"Zhu, Hengcheng; Wei, Lili; Wen, Ming; Liu, Yepang; Cheung, Shing-Chi; Sheng, Qin; Zhou, Cui",2021,1,"@inproceedings{2-2385,
  title={MockSniffer: characterizing and recommending mocking decisions for unit tests},
  author={Zhu, Hengcheng and Wei, Lili and Wen, Ming and Liu, Yepang and Cheung, Shing-Chi and Sheng, Qin and Zhou, Cui},
  year={2021},
  doi={10.1145/3324884.3416539},
  booktitle={Proceedings of the IEEE/ACM International Conference on Automated Software Engineering (ASE)}
}",System/Artifact contributions,Software / Systems / Security,Operational,Advising,"Developer, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-23851,neurips,Neural pseudo-label optimism for the bank loan problem,"We study a class of classification problems best exemplified by the \\emph{bank loan} problem, where a lender decides whether or not to issue a loan. The lender only observes whether a customer will repay a loan if the loan is issued to begin with, and thus modeled decisions affect what data is available to the lender for future decisions. As a result, it is possible for the lender's algorithm to get stuck'' with a self-fulfilling model. This model never corrects its false negatives, since it never sees the true label for rejected data, thus accumulating infinite regret. In the case of linear models, this issue can be addressed by adding optimism directly into the model predictions. However, there are few methods that extend to the function approximation case using Deep Neural Networks. We present Pseudo-Label Optimism (PLOT), a conceptually and computationally simple method for this setting applicable to DNNs. \\PLOT{} adds an optimistic label to the subset of decision points the current model is deciding on, trains the model on all data so far (including these points along with their optimistic labels), and finally uses the resulting \\emph{optimistic} model for decision making. \\PLOT{} achieves competitive performance on a set of three challenging benchmark problems, requiring minimal hyperparameter tuning. We also show that \\PLOT{} satisfies a logarithmic regret guarantee, under a Lipschitz and logistic mean label model, and under a separability condition on the data.",NA,https://proceedings.neurips.cc/paper_files/paper/2021/hash/33d6548e48d4318ceb0e3916a79afc84-Abstract.html,Advances in Neural Information Processing Systems,Aldo Pacchiano;Shaun Singh;Edward Chou;Alex Berg;Jakob Foerster,2021,9,"@inproceedings{2-23851,
  title={Neural pseudo-label optimism for the bank loan problem},
  author={Pacchiano, Aldo and Singh, Shaun and Chou, Edward and Berg, Alex and Foerster, Jakob},
  year={2021},
  booktitle={Advances in Neural Information Processing Systems}
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Executing, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-2387,acm,AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy,"Large language models (LLMs) match and sometimes exceed human performance in many domains. This study explores the potential of LLMs to augment human judgment in a forecasting task. We evaluate the effect on human forecasters of two LLM assistants: one designed to provide high-quality (‘superforecasting’) advice, and the other designed to be overconfident and base-rate neglecting, thus providing noisy forecasting advice. We compare participants using these assistants to a control group that received a less advanced model that did not provide numerical predictions or engage in explicit discussion of predictions. Participants (N = 991) answered a set of six forecasting questions and had the option to consult their assigned LLM assistant throughout. Our preregistered analyses show that interacting with each of our frontier LLM assistants significantly enhances prediction accuracy by between 24% and 28% compared to the control group. Exploratory analyses showed a pronounced outlier effect in one forecasting item, without which we find that the superforecasting assistant increased accuracy by 41%, compared with 29% for the noisy assistant. We further examine whether LLM forecasting augmentation disproportionately benefits less skilled forecasters, degrades the wisdom-of-the-crowd by reducing prediction diversity, or varies in effectiveness with question difficulty. Our data do not consistently support these hypotheses. Our results suggest that access to a frontier LLM assistant, even a noisy one, can be a helpful decision aid in cognitively demanding tasks compared to a less powerful model that does not provide specific forecasting advice. However, the effects of outliers suggest that further research into the robustness of this pattern is needed.",10.1145/3707649,https://doi.org/10.1145/3707649,ACM Transactions on Interactive Intelligent Systems,"Schoenegger, Philipp; Park, Peter S.; Karger, Ezra; Trott, Sean; Tetlock, Philip E.",2024,33,"@article{2-2387,
  title = {AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy},
  author = {Schoenegger, Philipp and Park, Peter S. and Karger, Ezra and Trott, Sean and Tetlock, Philip E.},
  year = {2024},
  doi = {10.1145/3707649},
  journal = {ACM Transactions on Interactive Intelligent Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Advising, Forecasting","Decision-maker, Guardian, Decision-subject",Alter decision outcomes,no such info,"high-quality advice, noisy forecasting advice",NA,Interactive interface,Yes,Yes
2-2392,acm,What can AI do for me? evaluating machine learning interpretations in cooperative play,"Machine learning is an important tool for decision making, but its ethical and responsible application requires rigorous vetting of its interpretability and utility: an understudied problem, particularly for natural language processing models. We propose an evaluation of interpretation on a real task with real human users, where the effectiveness of interpretation is measured by how much it improves human performance. We design a grounded, realistic human-computer cooperative setting using a question answering task, Quizbowl. We recruit both trivia experts and novices to play this game with computer as their teammate, who communicates its prediction via three different interpretations. We also provide design guidance for natural language processing human-in-the-loop settings.",10.1145/3301275.3302265,https://doi.org/10.1145/3301275.3302265,ACM International Conference on Intelligent User Interfaces (IUI),"Feng, Shi; Boyd-Graber, Jordan",2019,27,"@inproceedings{2-2392,
  title     = {What can {AI} do for me? Evaluating Machine Learning Interpretations in Cooperative Play},
  author    = {Feng, Shi and Boyd-Graber, Jordan},
  year      = {2019},
  booktitle = {Proceedings of the ACM International Conference on Intelligent User Interfaces (IUI)},
  doi       = {10.1145/3301275.3302265}
}","System/Artifact contributions, Empirical contributions",Generic / Abstract / Domain-agnostic,Individual,"Explaining, Advising, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-2407,acm,Extraction and Management of Rationale,"Software developers often have to make many design decisions. The underlying logic behind these decisions, also called design rationale, represents beneficial and valuable information. In the past, researchers have tried to automatically extract and exploit this information, however, prior techniques are only applicable to specific contexts and there is insufficient progress on an automated end-to-end rationale extraction and management system. In this research project, we propose to use Natural Language Processing (NLP) and Machine Learning (ML) techniques to create a system for the automated extraction, structuring and management of design rationale. This system would support and ensure the consistency and the coherence of the development process.",10.1145/3551349.3559568,https://doi.org/10.1145/3551349.3559568,IEEE/ACM International Conference on Automated Software Engineering (ASE),"Dhaouadi, Mouna",2023,3,"@inproceedings{2-2407,
  title = {Extraction and Management of Rationale},
  author = {Dhaouadi, Mouna},
  year = {2023},
  booktitle = {IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  doi = {10.1145/3551349.3559568}
}",System/Artifact contributions,Software / Systems / Security,Operational,"Explaining, Analyzing, Executing","Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-2412,acm,ViCE: visual counterfactual explanations for machine learning models,"The continued improvements in the predictive accuracy of machine learning models have allowed for their widespread practical application. Yet, many decisions made with seemingly accurate models still require verification by domain experts. In addition, end-users of a model also want to understand the reasons behind specific decisions. Thus, the need for interpretability is increasingly paramount. In this paper we present an interactive visual analytics tool, ViCE, that generates counterfactual explanations to contextualize and evaluate model decisions. Each sample is assessed to identify the minimal set of changes needed to flip the model's output. These explanations aim to provide end-users with personalized actionable insights with which to understand, and possibly contest or improve, automated decisions. The results are effectively displayed in a visual interface where counterfactual explanations are highlighted and interactive methods are provided for users to explore the data and model. The functionality of the tool is demonstrated by its application to a home equity line of credit dataset.",10.1145/3377325.3377536,https://doi.org/10.1145/3377325.3377536,International Conference on Intelligent User Interfaces (IUI),"Gomez, Oscar; Holter, Steffen; Yuan, Jun; Bertini, Enrico",2020,0,"@inproceedings{2-2412,
  title = {ViCE: Visual Counterfactual Explanations for Machine Learning Models},
  author = {Gomez, Oscar and Holter, Steffen and Yuan, Jun and Bertini, Enrico},
  year = {2020},
  doi = {10.1145/3377325.3377536},
  booktitle = {International Conference on Intelligent User Interfaces (IUI)}
}",System/Artifact contributions,"Finance / Business / Economy, Generic / Abstract / Domain-agnostic",Operational,"Explaining, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-2417,acm,Learning Personalized Risk Preferences for Recommendation,"The rapid growth of e-commerce has made people accustomed to shopping online. Before making purchases on e-commerce websites, most consumers tend to rely on rating scores and review information to make purchase decisions. With this information, they can infer the quality of products to reduce the risk of purchase. Specifically, items with high rating scores and good reviews tend to be less risky, while items with low rating scores and bad reviews might be risky to purchase. On the other hand, the purchase behaviors will also be influenced by consumers' tolerance of risks, known as the risk attitudes. Economists have studied risk attitudes for decades. These studies reveal that people are not always rational enough when making decisions, and their risk attitudes may vary in different circumstances.Most existing works over recommendation systems do not consider users' risk attitudes in modeling, which may lead to inappropriate recommendations to users. For example, suggesting a risky item to a risk-averse person or a conservative item to a risk-seeking person may result in the reduction of user experience. In this paper, we propose a novel risk-aware recommendation framework that integrates machine learning and behavioral economics to uncover the risk mechanism behind users' purchasing behaviors. Concretely, we first develop statistical methods to estimate the risk distribution of each item and then draw the Nobel-award winning Prospect Theory into our model to learn how users choose from probabilistic alternatives that involve risks, where the probabilities of the outcomes are uncertain. Experiments on several e-commerce datasets demonstrate that by taking user risk preferences into consideration, our approach can achieve better performance than many classical recommendation approaches, and further analyses also verify the advantages of risk-aware recommendation beyond accuracy.",10.1145/3397271.3401056,https://doi.org/10.1145/3397271.3401056,International ACM SIGIR Conference on Research and Development in Information Retrieval,"Ge, Yingqiang; Xu, Shuyuan; Liu, Shuchang; Fu, Zuohui; Sun, Fei; Zhang, Yongfeng",2020,29,"@inproceedings{2-2417,
  title = {Learning Personalized Risk Preferences for Recommendation},
  author = {Ge, Yingqiang and Xu, Shuyuan and Liu, Shuchang and Fu, Zuohui and Sun, Fei and Zhang, Yongfeng},
  year = {2020},
  doi = {10.1145/3397271.3401056},
  booktitle = {International ACM SIGIR Conference on Research and Development in Information Retrieval}
}",Algorithmic contributions,Finance / Business / Economy,Individual,"Advising, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-2426,acm,TrolleyMod v1.0: An Open-Source Simulation and Data-Collection Platform for Ethical Decision Making in Autonomous Vehicles,"This paper presents TrolleyMod v1.0, an open-source platform based on the CARLA simulator for the collection of ethical decision-making data for autonomous vehicles. This platform is designed to facilitate experiments aiming to observe and record human decisions and actions in high-fidelity simulations of ethical dilemmas that occur in the context of driving. Targeting experiments in the class of trolley problems, TrolleyMod provides a seamless approach to creating new experimental settings and environments with the realistic physics-engine and the high-quality graphical capabilities of CARLA and the Unreal Engine. Also, TrolleyMod provides a straightforward interface between the CARLA environment and Python to enable the implementation of custom controllers, such as deep reinforcement learning agents. The results of such experiments can be used for sociological analyses, as well as the training and tuning of value-aligned autonomous vehicles based on social values that are inferred from observations.",10.1145/3306618.3314239,https://doi.org/10.1145/3306618.3314239,"AAAI/ACM Conference on AI, Ethics, and Society","Behzadan, Vahid; Minton, James; Munir, Arslan",2019,7,"@inproceedings{2-2426,
  title = {TrolleyMod v1.0: An Open-Source Simulation and Data-Collection Platform for Ethical Decision Making in Autonomous Vehicles},
  author = {Behzadan, Vahid and Minton, James and Munir, Arslan},
  year = {2019},
  doi = {10.1145/3306618.3314239},
  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society}
}",System/Artifact contributions,Transportation / Mobility / Planning,Individual,"Executing, Collaborating","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-24397,neurips,"What i cannot predict, i do not understand: a human-centered evaluation framework for explainability methods","A multitude of explainability methods has been described to try to help users better understand how modern AI systems make decisions. However, most performance metrics developed to evaluate these methods have remained largely theoretical -- without much consideration for the human end-user. In particular, it is not yet clear (1) how useful current explainability methods are in real-world scenarios; and (2) whether current performance metrics accurately reflect the usefulness of explanation methods for the end user. To fill this gap, we conducted psychophysics experiments at scale (n=1,150n=1,150n=1,150) to evaluate the usefulness of representative attribution methods in three real-world scenarios. Our results demonstrate that the degree to which individual attribution methods help human participants better understand an AI system varies widely across these scenarios. This suggests the need to move beyond quantitative improvements of current attribution methods, towards the development of complementary approaches that provide qualitatively different sources of information to human end-users.",NA,https://proceedings.neurips.cc/paper_files/paper/2022/hash/13113e938f2957891c0c5e8df811dd01-Abstract-Conference.html,Advances in Neural Information Processing Systems,Julien Colin;Thomas FEL;Remi Cadene;Thomas Serre,2022,118,"@inproceedings{2-24397,
  title={What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods},
  author={Julien Colin and Thomas FEL and Remi Cadene and Thomas Serre},
  year={2022},
  booktitle={Advances in Neural Information Processing Systems}
}","Empirical contributions, Methodological contributions","Education / Teaching / Research, Generic / Abstract / Domain-agnostic",Individual,"Explaining, Forecasting",Decision-maker,Alter decision outcomes,no such info,"visual explanations, textual explanations",NA,"Textual, Visual, Interactive interface",Yes,Yes
2-2441,acm,Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions,"As the use of algorithmic systems in high-stakes decision-making increases, the ability to contest algorithmic decisions is being recognised as an important safeguard for individuals. Yet, there is little guidance on what 'contestability'–the ability to contest decisions–in relation to algorithmic decision-making requires. Recent research presents different conceptualisations of contestability in algorithmic decision-making. We contribute to this growing body of work by describing and analysing the perspectives of people and organisations who made submissions in response to Australia's proposed 'AI Ethics Framework', the first framework of its kind to include 'contestability' as a core ethical principle. Our findings reveal that while the nature of contestability is disputed, it is seen as a way to protect individuals, and it resembles contestability in relation to human decision-making. We reflect on and discuss the implications of these findings.",10.1145/3449180,https://doi.org/10.1145/3449180,Proceedings of the ACM on Human-Computer Interaction,"Lyons, Henrietta; Velloso, Eduardo; Miller, Tim",2021,3,"@article{2-2441,
  title = {Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions},
  author = {Lyons, Henrietta and Velloso, Eduardo and Miller, Tim},
  year = {2021},
  doi = {10.1145/3449180},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Operational,"Executing, Advising","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-24426,neurips,Avis: autonomous visual information seeking with large language model agent,"In this paper, we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs via tree search, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Responding to visual questions that necessitate external knowledge, such as \\What event is commemorated by the building depicted in this image?\\, is a complex task. This task presents a combinatorial search space that demands a sequence of actions, including invoking APIs, analyzing their responses, and making informed decisions. We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next, an LLM-powered reasoner that analyzes and extracts key information from the tool outputs, and a working memory component that retains the acquired information throughout the process. The collected user behavior serves as a guide for our system in two key ways. First, we create a transition graph by analyzing the sequence of decisions made by users. This graph delineates distinct states and confines the set of actions available at each state. Second, we use examples of user decision-making to provide our LLM-powered planner and reasoner with relevant contextual instances, enhancing their capacity to make informed decisions. We show that AVIS achieves state-of-the-art results on knowledge-based visual question answering benchmarks such as Infoseek and OK-VQA.",NA,https://proceedings.neurips.cc/paper_files/paper/2023/hash/029df12a9363313c3e41047844ecad94-Abstract-Conference.html,Advances in Neural Information Processing Systems,Ziniu Hu;Ahmet Iscen;Chen Sun;Kai-Wei Chang;Yizhou Sun;David Ross;Cordelia Schmid;Alireza Fathi,2023,23,"@inproceedings{2-24426,
  title={Avis: Autonomous Visual Information Seeking with Large Language Model Agent},
  author={Hu, Ziniu and Iscen, Ahmet and Sun, Chen and Chang, Kai-Wei and Sun, Yizhou and Ross, David and Schmid, Cordelia and Fathi, Alireza},
  year={2023},
  booktitle={Advances in Neural Information Processing Systems}
}",System/Artifact contributions,Generic / Abstract / Domain-agnostic,Individual,Analyzing,"Knowledge provider, Decision-maker",Alter decision outcomes,"Update AI competence, Change AI responses","visual answers, captions, external textual knowledge",NA,"Visual, Autonomous System",Yes,Yes
2-24435,neurips,Interventionally consistent surrogates for complex simulation models,"Large-scale simulation models of complex socio-technical systems provide decision-makers with high-fidelity testbeds in which policy interventions can be evaluated and _what-if_ scenarios explored. Unfortunately, the high computational cost of such models inhibits their widespread use in policy-making settings. Surrogate models can address these computational limitations, but to do so they must behave consistently with the simulator under interventions of interest. In this paper, we build upon recent developments in causal abstractions to develop a framework for learning interventionally consistent surrogate models for large-scale, complex simulation models. We provide theoretical results showing that our proposed approach induces surrogates to behave consistently with high probability with respect to the simulator across interventions of interest, facilitating rapid experimentation with policy interventions in complex systems. We further demonstrate with empirical studies that conventionally trained surrogates can misjudge the effect of interventions and misguide decision-makers towards suboptimal interventions, while surrogates trained for _interventional_ consistency with our method closely mimic the behaviour of the original simulator under interventions of interest.",NA,https://openreview.net/forum?id=UtTjgMDTFO,Advances in Neural Information Processing Systems,Joel Dyer;Nicholas George Bishop;Yorgos Felekis;Fabio Massimo Zennaro;Ani Calinescu;Theodoros Damoulas;Michael J. Wooldridge,2024,7,"@inproceedings{2-24435,
  title={Interventionally consistent surrogates for complex simulation models},
  author={Dyer, Joel and Bishop, Nicholas George and Felekis, Yorgos and Zennaro, Fabio Massimo and Calinescu, Ani and Damoulas, Theodoros and Wooldridge, Michael J.},
  year={2024},
  booktitle={Advances in Neural Information Processing Systems}
}",Algorithmic contributions,Law / Policy / Governance,no such info,"Forecasting, Analyzing","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-24438,neurips,Online algorithms for multi-shop ski rental with machine learned advice,"We study the problem of augmenting online algorithms with machine learned (ML) advice. In particular, we consider the \\emph{multi-shop ski rental} (MSSR) problem, which is a generalization of the classical ski rental problem. In MSSR, each shop has different prices for buying and renting a pair of skis, and a skier has to make decisions on when and where to buy. We obtain both deterministic and randomized online algorithms with provably improved performance when either a single or multiple ML predictions are used to make decisions. These online algorithms have no knowledge about the quality or the prediction error type of the ML prediction. The performance of these online algorithms are robust to the poor performance of the predictors, but improve with better predictions. Extensive experiments using both synthetic and real world data traces verify our theoretical observations and show better performance against algorithms that purely rely on online decision making.",NA,https://proceedings.neurips.cc/paper_files/paper/2020/hash/5cc4bb753030a3d804351b2dfec0d8b5-Abstract.html,Advances in Neural Information Processing Systems,Shufan Wang;Jian Li;Shiqiang Wang,2020,10,"@inproceedings{2-24438,
  title={Online algorithms for multi-shop ski rental with machine learned advice},
  author={Wang, Shufan and Li, Jian and Wang, Shiqiang},
  year={2020},
  booktitle={Advances in Neural Information Processing Systems}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service",Individual,"Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-24439,neurips,Opensrh: optimizing brain tumor surgery using intraoperative stimulated raman histology,"Accurate intraoperative diagnosis is essential for providing safe and effective care during brain tumor surgery. Our standard-of-care diagnostic methods are time, resource, and labor intensive, which restricts access to optimal surgical treatments. To address these limitations, we propose an alternative workflow that combines stimulated Raman histology (SRH), a rapid optical imaging method, with deep learning-based automated interpretation of SRH images for intraoperative brain tumor diagnosis and real-time surgical decision support. Here, we present OpenSRH, the first public dataset of clinical SRH images from 300+ brain tumors patients and 1300+ unique whole slide optical images. OpenSRH contains data from the most common brain tumors diagnoses, full pathologic annotations, whole slide tumor segmentations, raw and processed optical imaging data for end-to-end model development and validation. We provide a framework for patch-based whole slide SRH classification and inference using weak (i.e. patient-level) diagnostic labels. Finally, we benchmark two computer vision tasks: multi-class histologic brain tumor classification and patch-based contrastive representation learning. We hope OpenSRH will facilitate the clinical translation of rapid optical imaging and real-time ML-based surgical decision support in order to improve the access, safety, and efficacy of cancer surgery in the era of precision medicine.",NA,https://proceedings.neurips.cc/paper_files/paper/2022/hash/b6b5f50a2001ad1cbccca96e693c4ab4-Abstract-Datasets_and_Benchmarks.html,Advances in Neural Information Processing Systems,Cheng Jiang;Asadur Chowdury;Xinhai Hou;Akhil Kondepudi;Christian Freudiger;Kyle Conway;Sandra Camelo-Piragua;Daniel Orringer;Honglak Lee;Todd Hollon,2022,20,"@inproceedings{2-24439,
  title={Opensrh: optimizing brain tumor surgery using intraoperative stimulated Raman histology},
  author={Jiang, Cheng and Chowdury, Asadur and Hou, Xinhai and Kondepudi, Akhil and Freudiger, Christian and Conway, Kyle and Camelo-Piragua, Sandra and Orringer, Daniel and Lee, Honglak and Hollon, Todd},
  year={2022},
  booktitle={Advances in Neural Information Processing Systems},
}",Dataset/Benchmark contributions,Healthcare / Medicine / Surgery,Operational,Forecasting,"Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-2444,acm,Explainability's Gain is Optimality's Loss? How Explanations Bias Decision-making,"Decisions in organizations are about evaluating alternatives and choosing the one that would best serve organizational goals. To the extent that the evaluation of alternatives could be formulated as a predictive task with appropriate metrics, machine learning algorithms are increasingly being used to improve the efficiency of the process. Explanations help to facilitate communication between the algorithm and the human decision-maker, making it easier for the latter to interpret and make decisions on the basis of predictions by the former. Feature-based explanations' semantics of causal models, however, induce leakage from the decision-maker's prior beliefs. Our findings from a field experiment demonstrate empirically how this leads to confirmation bias and disparate impact on the decision-maker's confidence in the predictions. Such differences can lead to sub-optimal and biased decision outcomes.",10.1145/3514094.3534156,https://doi.org/10.1145/3514094.3534156,"AAAI/ACM Conference on AI, Ethics, and Society","Wan, Charles; Belo, Rodrigo; Zejnilovic, Leid",2022,0,"@inproceedings{2-2444,
  title = {Explainability's Gain is Optimality's Loss? How Explanations Bias Decision-making},
  author = {Wan, Charles and Belo, Rodrigo and Zejnilovic, Leid},
  year = {2022},
  doi = {10.1145/3514094.3534156},
  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service",Operational,"Forecasting, Explaining, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-2445,acm,How to Explain and Justify Almost Any Decision: Potential Pitfalls for Accountability in AI Decision-Making,"Discussion of the “right to an explanation” has been increasingly relevant because of its potential utility for auditing automated decision systems, as well as for making objections to such decisions. However, most existing work on explanations focuses on collaborative environments, where designers are motivated to implement good-faith explanations that reveal potential weaknesses of a decision system. This motivation may not hold in an auditing environment. Thus, we ask: how much could explanations be used maliciously to defend a decision system? In this paper, we demonstrate how a black-box explanation system developed to defend a black-box decision system could manipulate decision recipients or auditors into accepting an intentionally discriminatory decision model. In a case-by-case scenario where decision recipients are unable to share their cases and explanations, we find that most individual decision recipients could receive a verifiable justification, even if the decision system is intentionally discriminatory. In a system-wide scenario where every decision is shared, we find that while justifications frequently contradict each other, there is no intuitive threshold to determine if these contradictions are because of malicious justifications or because of simplicity requirements of these justifications conflicting with model behavior. We end with discussion of how system-wide metrics may be more useful than explanation systems for evaluating overall decision fairness, while explanations could be useful outside of fairness auditing.",10.1145/3593013.3593972,https://doi.org/10.1145/3593013.3593972,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Zhou, Joyce; Joachims, Thorsten",2023,0,"@inproceedings{2-2445,
  title   = {How to Explain and Justify Almost Any Decision: Potential Pitfalls for Accountability in AI Decision-Making},
  author  = {Zhou, Joyce and Joachims, Thorsten},
  year    = {2023},
  doi     = {10.1145/3593013.3593972},
  booktitle = {ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Explaining, Auditing","Decision-maker, Stakeholder","Alter decision outcomes, Shape ethical norms",no such info,"classification, feature-based explanations",NA,"Textual, Conversational/Natural Language",Yes,Yes
2-24458,pmlr,{A}uto{p}rognosis: automated clinical prognostic modeling via {b}ayesian optimization with structured kernel learning,"Clinical prognostic models derived from largescale healthcare data can inform critical diagnostic and therapeutic decisions. To enable off-theshelf usage of machine learning (ML) in prognostic research, we developed AUTOPROGNOSIS: a system for automating the design of predictive modeling pipelines tailored for clinical prognosis. AUTOPROGNOSIS optimizes ensembles of pipeline configurations efficiently using a novel batched Bayesian optimization (BO) algorithm that learns a low-dimensional decomposition of the pipelines’ high-dimensional hyperparameter space in concurrence with the BO procedure. This is achieved by modeling the pipelines’ performances as a black-box function with a Gaussian process prior, and modeling the “similarities” between the pipelines’ baseline algorithms via a sparse additive kernel with a Dirichlet prior. Meta-learning is used to warmstart BO with external data from “similar” patient cohorts by calibrating the priors using an algorithm that mimics the empirical Bayes method. The system automatically explains its predictions by presenting the clinicians with logical association rules that link patients’ features to predicted risk strata. We demonstrate the utility of AUTOPROGNOSIS using 10 major patient cohorts representing various aspects of cardiovascular patient care.",NA,https://proceedings.mlr.press/v80/alaa18b.html,International Conference on Machine Learning (ICML),"Alaa, Ahmed;van der Schaar, Mihaela",2018,0,"@inproceedings{2-24458,
  title = {AutoPrognosis: Automated Clinical Prognostic Modeling via Bayesian Optimization with Structured Kernel Learning},
  author = {Alaa, Ahmed and van der Schaar, Mihaela},
  year = {2018},
  booktitle = {International Conference on Machine Learning (ICML)}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Explaining","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-2446,acm,What People Think AI Should Infer From Faces,"Faces play an indispensable role in human social life. At present, computer vision artificial intelligence (AI) captures and interprets human faces for a variety of digital applications and services. The ambiguity of facial information has recently led to a debate among scholars in different fields about the types of inferences AI should make about people based on their facial looks. AI research often justifies facial AI inference-making by referring to how people form impressions in first-encounter scenarios. Critics raise concerns about bias and discrimination and warn that facial analysis AI resembles an automated version of physiognomy. What has been missing from this debate, however, is an understanding of how “non-experts” in AI ethically evaluate facial AI inference-making. In a two-scenario vignette study with 24 treatment groups, we show that non-experts (N = 3745) reject facial AI inferences such as trustworthiness and likability from portrait images in a low-stake advertising and a high-stake hiring context. In contrast, non-experts agree with facial AI inferences such as skin color or gender in the advertising but not the hiring decision context. For each AI inference, we ask non-experts to justify their evaluation in a written response. Analyzing 29,760 written justifications, we find that non-experts are either “evidentialists” or “pragmatists”: they assess the ethical status of a facial AI inference based on whether they think faces warrant sufficient or insufficient evidence for an inference (evidentialist justification) or whether making the inference results in beneficial or detrimental outcomes (pragmatist justification). Non-experts’ justifications underscore the normative complexity behind facial AI inference-making. AI inferences with insufficient evidence can be rationalized by considerations of relevance while irrelevant inferences can be justified by reference to sufficient evidence. We argue that participatory approaches contribute valuable insights for the development of ethical AI in an increasingly visual data culture.",10.1145/3531146.3533080,https://doi.org/10.1145/3531146.3533080,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Engelmann, Severin; Ullstein, Chiara; Papakyriakopoulos, Orestis; Grossklags, Jens",2022,2,"@inproceedings{2-2446,
  title = {What People Think AI Should Infer From Faces},
  author = {Engelmann, Severin and Ullstein, Chiara and Papakyriakopoulos, Orestis and Grossklags, Jens},
  year = {2022},
  doi = {10.1145/3531146.3533080},
  booktitle = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}",Empirical contributions,"Healthcare / Medicine / Surgery, Media / Communication / Entertainment, Generic / Abstract / Domain-agnostic",Institutional,"Analyzing, Executing","Decision-subject, Guardian, Knowledge provider, Stakeholder",Shape ethical norms,no such info,visual analysis,NA,Visual,Yes,Yes
2-24475,pmlr,{Eddi}: efficient dynamic discovery of high-value information with partial {vae},"Many real-life decision making situations allow further relevant information to be acquired at a specific cost, for example, in assessing the health status of a patient we may decide to take additional measurements such as diagnostic tests or imaging scans before making a final assessment. Acquiring more relevant information enables better decision making, but may be costly. How can we trade off the desire to make good decisions by acquiring further information with the cost of performing that acquisition? To this end, we propose a principled framework, named <em>EDDI</em> (Efficient Dynamic Discovery of high-value Information), based on the theory of Bayesian experimental design. In EDDI, we propose a novel <em>partial variational autoencoder</em> (Partial VAE) to predict missing data entries problematically given any subset of the observed ones, and combine it with an acquisition function that maximizes expected information gain on a set of target variables. We show cost reduction at the same decision quality and improved decision quality at the same cost in multiple machine learning benchmarks and two real-world health-care applications.",NA,https://proceedings.mlr.press/v97/ma19c.html,International Conference on Machine Learning (ICML),"Ma, Chao;Tschiatschek, Sebastian;Palla, Konstantina;Hernandez-Lobato, Jose Miguel;Nowozin, Sebastian;Zhang, Cheng",2019,32,"@inproceedings{2-24475,
  title = {EDDI: Efficient Dynamic Discovery of High-Value Information with Partial VAE},
  author = {Ma, Chao and Tschiatschek, Sebastian and Palla, Konstantina and Hernandez-Lobato, Jose Miguel and Nowozin, Sebastian and Zhang, Cheng},
  year = {2019},
  booktitle = {International Conference on Machine Learning (ICML)}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Executing, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-24477,pmlr,{F}air{rr}: pre-processing for group fairness through randomized response,"The increasing usage of machine learning models in consequential decision-making processes has spurred research into the fairness of these systems. While significant work has been done to study group fairness in the in-processing and post-processing setting, there has been little that theoretically connects these results to the pre-processing domain. This paper extends recent fair statistical learning results and proposes that achieving group fairness in downstream models can be formulated as finding the optimal design matrix in which to modify a response variable in a Randomized Response framework. We show that measures of group fairness can be directly controlled for with optimal model utility, proposing a pre-processing algorithm called FairRR that yields excellent downstream model utility and fairness.",NA,https://proceedings.mlr.press/v238/john-ward24a.html,The International Conference on Artificial Intelligence and Statistics (AISTATS),"John Ward, Joshua;Zeng, Xianli;Cheng, Guang",2024,3,"@inproceedings{2-24477,
  title = {FairRR: Pre-processing for Group Fairness through Randomized Response},
  author = {Ward, Joshua John and Zeng, Xianli and Cheng, Guang},
  year = {2024},
  booktitle = {The International Conference on Artificial Intelligence and Statistics (AISTATS)}
}",Algorithmic contributions,"Law / Policy / Governance, Everyday / Employment / Public Service, Generic / Abstract / Domain-agnostic",Institutional,"Forecasting, Executing",Decision-subject,NA,NA,NA,NA,NA,Yes,No
2-24479,pmlr,{Fairer}: fairness as decision rationale alignment,"Deep neural networks (DNNs) have made significant progress, but often suffer from fairness issues, as deep models typically show distinct accuracy differences among certain subgroups (e.g., males and females). Existing research addresses this critical issue by employing fairness-aware loss functions to constrain the last-layer outputs and directly regularize DNNs. Although the fairness of DNNs is improved, it is unclear how the trained network makes a fair prediction, which limits future fairness improvements. In this paper, we investigate fairness from the perspective of decision rationale and define the parameter parity score to characterize the fair decision process of networks by analyzing neuron influence in various subgroups. Extensive empirical studies show that the unfair issue could arise from the unaligned decision rationales of subgroups. Existing fairness regularization terms fail to achieve decision rationale alignment because they only constrain last-layer outputs while ignoring intermediate neuron alignment. To address the issue, we formulate the fairness as a new task, i.e., decision rationale alignment that requires DNNs’ neurons to have consistent responses on subgroups at both intermediate processes and the final prediction. To make this idea practical during optimization, we relax the naive objective function and propose gradient-guided parity alignment, which encourages gradient-weighted consistency of neurons across subgroups. Extensive experiments on a variety of datasets show that our method can significantly enhance fairness while sustaining a high level of accuracy and outperforming other approaches by a wide margin.",NA,https://proceedings.mlr.press/v202/li23h.html,International Conference on Machine Learning (ICML),"Li, Tianlin;Guo, Qing;Liu, Aishan;Du, Mengnan;Li, Zhiming;Liu, Yang",2023,825,"@inproceedings{2-24479,
  title = {Fairer: Fairness as decision rationale alignment},
  author = {Li, Tianlin and Guo, Qing and Liu, Aishan and Du, Mengnan and Li, Zhiming and Liu, Yang},
  year = {2023},
  booktitle = {International Conference on Machine Learning (ICML)}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Institutional,"Forecasting, Analyzing, Executing","Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-24480,pmlr,Fairness constraints: mechanisms for fair classification,"Algorithmic decision making systems are ubiquitous across a wide variety of online as well as offline services. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead, even in the absence of intent, to a lack of fairness, i.e., their outcomes can disproportionately hurt (or, benefit) particular groups of people sharing one or more sensitive attributes (e.g., race, sex). In this paper, we introduce a flexible mechanism to design fair classifiers by leveraging a novel intuitive measure of decision boundary (un)fairness. We instantiate this mechanism with two well-known classifiers, logistic regression and support vector machines, and show on real-world data that our mechanism allows for a fine-grained control on the degree of fairness, often at a small cost in terms of accuracy.",NA,https://proceedings.mlr.press/v54/zafar17a.html,International Conference on Artificial Intelligence and Statistics,"Zafar, Muhammad Bilal;Valera, Isabel;Rogriguez, Manuel Gomez;Gummadi, Krishna P.",2017,2,"@inproceedings{2-24480,
  title = {Fairness Constraints: Mechanisms for Fair Classification},
  author = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel Gomez and Gummadi, Krishna P.},
  year = {2017},
  booktitle = {International Conference on Artificial Intelligence and Statistics}
}",Algorithmic contributions,"Finance / Business / Economy, Everyday / Employment / Public Service, Generic / Abstract / Domain-agnostic",Institutional,"Forecasting, Executing",Decision-subject,NA,NA,NA,NA,NA,Yes,No
2-24495,pmlr,{Learning cost-effective and interpretable treatment regimes},"Decision makers, such as doctors and judges, make crucial decisions such as recommending treatments to patients, and granting bails to defendants on a daily basis. Such decisions typically involve weighting the potential benefits of taking an action against the costs involved. In this work, we aim to automate this task of learning cost-effective, interpretable and actionable treatment regimes. We formulate this as a problem of learning a decision list – a sequence of if-then-else rules – which maps characteristics of subjects (eg., diagnostic test results of patients) to treatments. We propose a novel objective to construct a decision list which maximizes outcomes for the population, and minimizes overall costs. Since we do not observe the outcomes corresponding to counterfactual scenarios, we use techniques from causal inference literature to infer them. We model the problem of learning the decision list as a Markov Decision Process (MDP) and employ a variant of the Upper Confidence Bound for Trees (UCT) strategy which leverages customized checks for pruning the search space effectively. Experimental results on real world observational data capturing judicial bail decisions and treatment recommendations for asthma patients demonstrate the effectiveness of our approach.",NA,https://proceedings.mlr.press/v54/lakkaraju17a.html,International Conference on Artificial Intelligence and Statistics,"Lakkaraju, Himabindu;Rudin, Cynthia",2017,135,"@inproceedings{2-24495,
  title = {Learning Cost-Effective and Interpretable Treatment Regimes},
  author = {Lakkaraju, Himabindu and Rudin, Cynthia},
  year = {2017},
  booktitle = {International Conference on Artificial Intelligence and Statistics}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Executing, Analyzing, Advising, Explaining","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-2450,acm,Why Am I Not Seeing It? Understanding Users’ Needs for Counterfactual Explanations in Everyday Recommendations,"Intelligent everyday applications typically rely on automated Recommender Systems (RS) to generate recommendations that help users make decisions among a large number of options. Due to the increasing complexity of RS and the lack of transparency in its algorithmic decision-making, researchers have recognized the need to support users with explanations. While many traditional Explainable AI methods fall short in disclosing the internal intricacy of recommender systems, counterfactual explanations provide many desirable explainable features by offering human-like explanations that contrast an existing recommendation with alternatives. However, there is a lack of empirical research in understanding users’ needs of counterfactual explanations in their usage of everyday intelligent applications. In this paper, we investigate whether and when to provide counterfactual explanations to support people’s decision-making with everyday recommendations through a question-driven approach. We conducted a preliminary survey study and an interview study to understand how existing explanations might be insufficient to support users and elicit the triggers that prompt them to ask why not questions and seek additional explanations. The findings reveal that the utility of decision is a primary factor that may affect their counterfactual information needs. We then conducted an online scenario-based survey to quantify the correlation between utility and explanation needs and found significant correlations between the measured variables.",10.1145/3531146.3533189,https://doi.org/10.1145/3531146.3533189,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Shang, Ruoxi; Feng, K. J. Kevin; Shah, Chirag",2022,44,"@inproceedings{2-2450,
  title = {Why Am I Not Seeing It? Understanding Users’ Needs for Counterfactual Explanations in Everyday Recommendations},
  author = {Shang, Ruoxi and Feng, K. J. Kevin and Shah, Chirag},
  year = {2022},
  doi = {10.1145/3531146.3533189},
  booktitle = {ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}",Empirical contributions,"Everyday / Employment / Public Service, Generic / Abstract / Domain-agnostic",Individual,"Advising, Explaining",Decision-maker,"Change cognitive demands, Change trust, Change affective-perceptual, Alter decision outcomes",no such info,"counterfactual explanations, recommendations",NA,Textual,Yes,Yes
2-24527,pmlr,A bandit model for human-machine decision making with private information and opacity,"Applications of machine learning inform human decision makers in a broad range of tasks. The resulting problem is usually formulated in terms of a single decision maker. We argue that it should rather be described as a two-player learning problem where one player is the machine and the other the human. While both players try to optimize the final decision, the setup is often characterized by (1) the presence of private information and (2) opacity, that is imperfect understanding between the decision makers. We prove that both properties can complicate decision making considerably. A lower bound quantifies the worst-case hardness of optimally advising a decision maker who is opaque or has access to private information. An upper bound shows that a simple coordination strategy is nearly minimax optimal. More efficient learning is possible under certain assumptions on the problem, for example that both players learn to take actions independently. Such assumptions are implicit in existing literature, for example in medical applications of machine learning, but have not been described or justified theoretically.",NA,https://proceedings.mlr.press/v151/bordt22a.html,The International Conference on Artificial Intelligence and Statistics (AISTATS),"Bordt, Sebastian;Von Luxburg, Ulrike",2022,11,"@inproceedings{2-24527,
  title={A bandit model for human-machine decision making with private information and opacity},
  author={Bordt, Sebastian and Von Luxburg, Ulrike},
  year={2022},
  booktitle={The International Conference on Artificial Intelligence and Statistics (AISTATS)}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,"Advising, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-24530,pmlr,A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions,"Every year there are more than 3.6 million referrals made to child protection agencies across the US. The practice of screening calls is left to each jurisdiction to follow local practices and policies, potentially leading to large variation in the way in which referrals are treated across the country. Whilst increasing access to linked administrative data is available, it is difficult for welfare workers to make systematic use of historical information about all the children and adults on a single referral call. Risk prediction models that use routinely collected administrative data can help call workers to better identify cases that are likely to result in adverse outcomes. However, the use of predictive analytics in the area of child welfare is contentious. There is a possibility that some communities—such as those in poverty or from particular racial and ethnic groups—will be disadvantaged by the reliance on government administrative data. On the other hand, these analytics tools can augment or replace human judgments, which themselves are biased and imperfect. In this paper we describe our work on developing, validating, fairness auditing, and deploying a risk prediction model in Allegheny County, Pennsylvania, USA. We discuss the results of our analysis to-date, and also highlight key problems and data bias issues that present challenges for model evaluation and deployment.",NA,https://proceedings.mlr.press/v81/chouldechova18a.html,"Conference on Fairness, Accountability, and Transparency (FAccT)","Chouldechova, Alexandra;Benavides-Prado, Diana;Fialko, Oleksandr;Vaithianathan, Rhema",2018,523,"@inproceedings{2-24530,
  author    = {Chouldechova, Alexandra and Benavides-Prado, Diana and Fialko, Oleksandr and Vaithianathan, Rhema},
  title     = {A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions},
  year      = {2018},
  booktitle = {Conference on Fairness, Accountability, and Transparency (FAccT)}
}",Empirical contributions,Everyday / Employment / Public Service,Organizational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-2454,acm,Human-Algorithm Collaboration: Achieving Complementarity and Avoiding Unfairness,"Much of machine learning research focuses on predictive accuracy: given a task, create a machine learning model (or algorithm) that maximizes accuracy. In many settings, however, the final prediction or decision of a system is under the control of a human, who uses an algorithm’s output along with their own personal expertise in order to produce a combined prediction. One ultimate goal of such collaborative systems is complementarity: that is, to produce lower loss (equivalently, greater payoff or utility) than either the human or algorithm alone. However, experimental results have shown that even in carefully-designed systems, complementary performance can be elusive. Our work provides three key contributions. First, we provide a theoretical framework for modeling simple human-algorithm systems and demonstrate that multiple prior analyses can be expressed within it. Next, we use this model to prove conditions where complementarity is impossible, and give constructive examples of where complementarity is achievable. Finally, we discuss the implications of our findings, especially with respect to the fairness of a classifier. In sum, these results deepen our understanding of key factors influencing the combined performance of human-algorithm systems, giving insight into how algorithmic tools can best be designed for collaborative environments.",10.1145/3531146.3533221,https://doi.org/10.1145/3531146.3533221,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Donahue, Kate; Chouldechova, Alexandra; Kenthapadi, Krishnaram",2022,74,"@inproceedings{2-2454,
  title     = {Human-Algorithm Collaboration: Achieving Complementarity and Avoiding Unfairness},
  author    = {Donahue, Kate and Chouldechova, Alexandra and Kenthapadi, Krishnaram},
  year      = {2022},
  doi       = {10.1145/3531146.3533221},
  booktitle = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,no such info,"Forecasting, Advising, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-24559,pmlr,A psychological theory of explainability,"The goal of explainable Artificial Intelligence (XAI) is to generate human-interpretable explanations, but there are no computationally precise theories of how humans interpret AI generated explanations. The lack of theory means that validation of XAI must be done empirically, on a case-by-case basis, which prevents systematic theory-building in XAI. We propose a psychological theory of how humans draw conclusions from saliency maps, the most common form of XAI explanation, which for the first time allows for precise prediction of explainee inference conditioned on explanation. Our theory posits that absent explanation humans expect the AI to make similar decisions to themselves, and that they interpret an explanation by comparison to the explanations they themselves would give. Comparison is formalized via Shepard’s universal law of generalization in a similarity space, a classic theory from cognitive science. A pre-registered user study on AI image classifications with saliency map explanations demonstrate that our theory quantitatively matches participants’ predictions of the AI.",NA,https://proceedings.mlr.press/v162/yang22c.html,International Conference on Machine Learning (ICML),"Yang, Scott Cheng-Hsin;Folke, Nils Erik Tomas;Shafto, Patrick",2022,0,"@inproceedings{2-24559,
  title     = {A psychological theory of explainability},
  author    = {Yang, Scott Cheng-Hsin and Folke, Nils Erik Tomas and Shafto, Patrick},
  year      = {2022},
  booktitle = {International Conference on Machine Learning (ICML)}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Individual,"Explaining, Forecasting","Decision-maker, Knowledge provider",Alter decision outcomes,no such info,"visual explanations, prediction of alternative",NA,Visual,Yes,Yes
2-2456,acm,"""What is Your Envisioned Future?"": Toward Human-AI Enrichment in Data Work of Asthma Care","Patient-generated health data (PGHD) is crucial for healthcare providers' decision making, as it complements clinical data by providing a more holistic view of patients' daily conditions. We interviewed 20 healthcare providers in asthma care to envision future technologies to support their PGHD use. We found that healthcare providers want future artificial intelligence (AI) systems to enhance their ability to treat patients by analyzing PGHD for profiling risk and predicting deterioration. Despite the potential benefits of AI, providers perceived various challenges of AI use with PGHD, including AI-driven data inequity, added burden, lack of trust toward AI, and fear of being replaced by AI. Clinicians wished for a future of co-dependent human-AI collaboration, where AI will help them to improve their clinical practice. In turn, healthcare providers can improve AI systems by making AI outputs more trustworthy and humane. Through the lens of data feminism, we discuss the importance of considering context and aligning the complex human infrastructure before designing or deploying PGHD-based AI systems in clinical settings. We highlight the opportunity to design for human-AI enrichment, where humans and AI not only partner with each other for improved performance, but also enrich each other to enhance each other's work overtime.",10.1145/3555157,https://doi.org/10.1145/3555157,Proceedings of the ACM on Human-Computer Interaction,"Su, Zhaoyuan; He, Lu; Jariwala, Sunit P; Zheng, Kai; Chen, Yunan",2022,30,"@article{2-2456,
  title = {{""What is Your Envisioned Future?"": Toward Human-AI Enrichment in Data Work of Asthma Care}},
  author = {Su, Zhaoyuan and He, Lu and Jariwala, Sunit P and Zheng, Kai and Chen, Yunan},
  year = {2022},
  doi = {10.1145/3555157},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Healthcare / Medicine / Surgery,Institutional,"Forecasting, Analyzing, Collaborating","Decision-maker, Knowledge provider, Guardian","Change trust, Change cognitive demands, Restrict human agency",no such info,"risk level, predicting and preventing deterioration",triaging pervasive AI suggestions,NA,Yes,Yes
2-2457,acm,You Complete Me: Human-AI Teams and Complementary Expertise,"People consider recommendations from AI systems in diverse domains ranging from recognizing tumors in medical images to deciding which shoes look cute with an outfit. Implicit in the decision process is the perceived expertise of the AI system. In this paper, we investigate how people trust and rely on an AI assistant that performs with different levels of expertise relative to the person, ranging from completely overlapping expertise to perfectly complementary expertise. Through a series of controlled online lab studies where participants identified objects with the help of an AI assistant, we demonstrate that participants were able to perceive when the assistant was an expert or non-expert within the same task and calibrate their reliance on the AI to improve team performance. We also demonstrate that communicating expertise through the linguistic properties of the explanation text was effective, where embracing language increased reliance and distancing language reduced reliance on AI.",10.1145/3491102.3517791,https://doi.org/10.1145/3491102.3517791,ACM CHI Conference on Human Factors in Computing Systems,"Zhang, Qiaoning; Lee, Matthew L; Carter, Scott",2022,137,"@inproceedings{2-2457,
  title = {You Complete Me: Human-AI Teams and Complementary Expertise},
  author = {Zhang, Qiaoning and Lee, Matthew L and Carter, Scott},
  year = {2022},
  doi = {10.1145/3491102.3517791},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Advising, Explaining, Collaborating",Decision-maker,"Change trust, Change cognitive demands",no such info,"textual explanations, recommendations, illustrative figure","domain knowledge, trust","Textual, Interactive interface, Visual",Yes,Yes
2-24598,pmlr,Active learning for decision-making from imbalanced observational data,"Machine learning can help personalized decision support by learning models to predict individual treatment effects (ITE). This work studies the reliability of prediction-based decision-making in a task of deciding which action $a$ to take for a target unit after observing its covariates $\\tilde{x}$ and predicted outcomes $\\hat{p}(\\tilde{y} \\mid \\tilde{x}, a)$. An example case is personalized medicine and the decision of which treatment to give to a patient. A common problem when learning these models from observational data is imbalance, that is, difference in treated/control covariate distributions, which is known to increase the upper bound of the expected ITE estimation error. We propose to assess the decision-making reliability by estimating the ITE model’s Type S error rate, which is the probability of the model inferring the sign of the treatment effect wrong. Furthermore, we use the estimated reliability as a criterion for active learning, in order to collect new (possibly expensive) observations, instead of making a forced choice based on unreliable predictions. We demonstrate the effectiveness of this decision-making aware active learning in two decision-making tasks: in simulated data with binary outcomes and in a medical dataset with synthetic and continuous treatment outcomes.",NA,https://proceedings.mlr.press/v97/sundin19a.html,International Conference on Machine Learning (ICML),"Sundin, Iiris;Schulam, Peter;Siivola, Eero;Vehtari, Aki;Saria, Suchi;Kaski, Samuel",2019,40,"@inproceedings{2-24598,
  title     = {Active learning for decision-making from imbalanced observational data},
  author    = {Sundin, Iiris and Schulam, Peter and Siivola, Eero and Vehtari, Aki and Saria, Suchi and Kaski, Samuel},
  year      = {2019},
  booktitle = {International Conference on Machine Learning (ICML)}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-2460,acm,"""It's like a glimpse into the future"": Exploring the Role of Blood Glucose Prediction Technologies for Type 1 Diabetes Self-Management","Self-management of type 1 diabetes (T1D) involves multiple factors, frequent anticipation of changes in blood glucose, and complex decision-making. ML-based blood glucose predictions (BGP) may be valuable in supporting T1D management. However, it may be difficult for people with T1D to integrate BGP into their decision-making due to prediction uncertainty and interpretation. In this study, we investigate the lived experience of people with T1D focusing on their needs and expectations in using apps that provide BGP. We designed MOON-T1D, an app that shows simulated BGP and conducted a five-day study using the Experience Sampling Method coupled with semi-structured interviews with 15 individuals with T1D who used MOON-T1D. A reflexive thematic analysis of our data revealed implications for the design and use of BGP, including the complex role of emotions and trust surrounding predictions, and ways in which BGP may ease or complicate T1D management.",10.1145/3613904.3642234,https://doi.org/10.1145/3613904.3642234,CHI Conference on Human Factors in Computing Systems,"Barth, Clara-Maria; Bernard, Jürgen; Huang, Elaine M.",2024,14,"@inproceedings{2-2460,
  title = {``It's like a glimpse into the future'': Exploring the Role of Blood Glucose Prediction Technologies for Type 1 Diabetes Self-Management},
  author = {Barth, Clara-Maria and Bernard, J{\""u}rgen and Huang, Elaine M.},
  year = {2024},
  doi = {10.1145/3613904.3642234},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}","Empirical contributions, System/Artifact contributions",Healthcare / Medicine / Surgery,Individual,"Advising, Forecasting",Decision-maker,"Alter decision outcomes, Change trust, Change cognitive demands, Change affective-perceptual, Restrict human agency",no such info,set valued predictions,additional information,Interactive interface,Yes,Yes
2-2462,acm,"What's fair is… fair? Presenting JustEFAB, an ethical framework for operationalizing medical ethics and social justice in the integration of clinical machine learning: JustEFAB","The problem of algorithmic bias represents an ethical threat to the fair treatment of patients when their care involves machine learning (ML) models informing clinical decision-making. The design, development, testing, and integration of ML models therefore require a lifecycle approach to bias identification and mitigation efforts. Presently, most work focuses on the ML tool alone, neglecting the larger sociotechnical context in which these models operate. Moreover, the narrow focus on technical definitions of fairness must be integrated within the larger context of medical ethics in order to facilitate equitable care with ML. Drawing from principles of medical ethics, research ethics, feminist philosophy of science, and justice-based theories, we describe the Justice, Equity, Fairness, and Anti-Bias (JustEFAB) guideline intended to support the design, testing, validation, and clinical evaluation of ML models with respect to algorithmic fairness. This paper describes JustEFAB's development and vetting through multiple advisory groups and the lifecycle approach to addressing fairness in clinical ML tools. We present an ethical decision-making framework to support design and development, adjudication between ethical values as design choices, silent trial evaluation, and prospective clinical evaluation guided by medical ethics and social justice principles. We provide some preliminary considerations for oversight and safety to support ongoing attention to fairness issues. We envision this guideline as useful to many stakeholders, including ML developers, healthcare decision-makers, research ethics committees, regulators, and other parties who have interest in the fair and judicious use of clinical ML tools.",10.1145/3593013.3594096,https://doi.org/10.1145/3593013.3594096,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Mccradden, Melissa; Odusi, Oluwadara; Joshi, Shalmali; Akrout, Ismail; Ndlovu, Kagiso; Glocker, Ben; Maicas, Gabriel; Liu, Xiaoxuan; Mazwi, Mjaye; Garnett, Tee; Oakden-Rayner, Lauren; Alfred, Myrtede; Sihlahla, Irvine; Shafei, Oswa; Goldenberg, Anna",2023,32,"@inproceedings{2-2462,
  title = {What's fair is... fair? Presenting JustEFAB, an ethical framework for operationalizing medical ethics and social justice in the integration of clinical machine learning: JustEFAB},
  author = {Mccradden, Melissa and Odusi, Oluwadara and Joshi, Shalmali and Akrout, Ismail and Ndlovu, Kagiso and Glocker, Ben and Maicas, Gabriel and Liu, Xiaoxuan and Mazwi, Mjaye and Garnett, Tee and Oakden-Rayner, Lauren and Alfred, Myrtede and Sihlahla, Irvine and Shafei, Oswa and Goldenberg, Anna},
  year = {2023},
  doi = {10.1145/3593013.3594096},
  booktitle = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}",Theoretical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Monitoring, Forecasting","Decision-maker, Developer, Guardian",NA,NA,NA,NA,NA,Yes,No
2-24628,pmlr,Algorithms for fairness in sequential decision making,"It has recently been shown that if feedback effects of decisions are ignored, then imposing fairness constraints such as demographic parity or equality of opportunity can actually exacerbate unfairness. We propose to address this challenge by modeling feedback effects as Markov decision processes (MDPs). First, we propose analogs of fairness properties for the MDP setting. Second, we propose algorithms for learning fair decision-making policies for MDPs. Finally, we demonstrate the need to account for dynamical effects using simulations on a loan applicant MDP.",NA,https://proceedings.mlr.press/v130/wen21a.html,The International Conference on Artificial Intelligence and Statistics (AISTATS),"Wen, Min;Bastani, Osbert;Topcu, Ufuk",2021,95,"@inproceedings{2-24628,
  title={Algorithms for fairness in sequential decision making},
  author={Wen, Min and Bastani, Osbert and Topcu, Ufuk},
  year={2021},
  booktitle={The International Conference on Artificial Intelligence and Statistics (AISTATS)}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Institutional,Executing,"Decision-subject, Developer",NA,NA,NA,NA,NA,Yes,No
2-24629,pmlr,All of the fairness for edge prediction with optimal transport,"Machine learning and data mining algorithms have been increasingly used recently to support decision-making systems in many areas of high societal importance such as healthcare, education, or security. While being very efficient in their predictive abilities, the deployed algorithms sometimes tend to learn an inductive model with a discriminative bias due to the presence of this latter in the learning sample. This problem gave rise to a new field of algorithmic fairness where the goal is to correct the discriminative bias introduced by a certain attribute in order to decorrelate it from the model’s output. In this paper, we study the problem of fairness for the task of edge prediction in graphs, a largely underinvestigated scenario compared to a more popular setting of fair classification. To this end, we formulate the problem of fair edge prediction, analyze it theoretically, and propose an embedding-agnostic repairing procedure for the adjacency matrix of an arbitrary graph with a trade-off between the group and individual fairness. We experimentally show the versatility of our approach and its capacity to provide explicit control over different notions of fairness and prediction accuracy.",NA,https://proceedings.mlr.press/v130/laclau21a.html,The International Conference on Artificial Intelligence and Statistics (AISTATS),"Laclau, Charlotte;Redko, Ievgen;Choudhary, Manvi;Largeron, Christine",2021,63,"@inproceedings{2-24629,
  title = {All of the fairness for edge prediction with optimal transport},
  author = {Laclau, Charlotte and Redko, Ievgen and Choudhary, Manvi and Largeron, Christine},
  year = {2021},
  booktitle = {The International Conference on Artificial Intelligence and Statistics (AISTATS)}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery, Education / Teaching / Research, Software / Systems / Security",Institutional,Forecasting,"Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-2464,acm,AI Knowledge: Improving AI Delegation through Human Enablement,"When collaborating with artificial intelligence (AI), humans can often delegate tasks to leverage complementary AI competencies. However, humans often delegate inefficiently. Enabling humans with knowledge about AI can potentially improve inefficient AI delegation. We conducted a between-subjects experiment (two groups, n = 111) to examine how enabling humans with AI knowledge can improve AI delegation in human-AI collaboration. We find that AI knowledge-enabled humans align their delegation decisions more closely with their assessment of how suitable a task is for humans or AI (i.e., task appraisal). We show that delegation decisions closely aligned with task appraisal increase task performance. However, we also find that AI knowledge lowers future intentions to use AI, suggesting that AI knowledge is not strictly positive for human-AI collaboration. Our study contributes to HCI design guidelines with a new perspective on AI features, educating humans regarding general AI functioning and their own (human) performance and biases.",10.1145/3544548.3580794,https://doi.org/10.1145/3544548.3580794,ACM CHI Conference on Human Factors in Computing Systems,"Pinski, Marc; Adam, Martin; Benlian, Alexander",2023,1,"@inproceedings{2-2464,
  title     = {AI Knowledge: Improving AI Delegation through Human Enablement},
  author    = {Pinski, Marc and Adam, Martin and Benlian, Alexander},
  year      = {2023},
  doi       = {10.1145/3544548.3580794},
  booktitle = {Proceedings of the ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Executing, Advising",Decision-maker,"Restrict human agency, Alter decision outcomes",no such info,AI knowledge,NA,"Textual, Visual, Conversational/Natural Language",Yes,Yes
2-24658,pmlr,Auditing fairness under unobserved confounding,"A fundamental problem in decision-making systems is the presence of inequity along demographic lines. However, inequity can be difficult to quantify, particularly if our notion of equity relies on hard-to-measure notions like risk (e.g., equal access to treatment for those who would die without it). Auditing such inequity requires accurate measurements of individual risk, which is difficult to estimate in the realistic setting of unobserved confounding. In the case that these unobservables “explain” an apparent disparity, we may understate or overstate inequity. In this paper, we show that one can still give informative bounds on allocation rates among high-risk individuals, even while relaxing or (surprisingly) even when eliminating the assumption that all relevant risk factors are observed. We utilize the fact that in many real-world settings (e.g., the introduction of a novel treatment) we have data from a period prior to any allocation, to derive unbiased estimates of risk. We apply our framework to a real-world setting of Paxlovid allocation to COVID-19 patients, finding that observed racial inequity cannot be explained by unobserved confounders of the same strength as important observed covariates.",NA,https://proceedings.mlr.press/v238/byun24a.html,The International Conference on Artificial Intelligence and Statistics (AISTATS),"Byun, Yewon;Sam, Dylan;Oberst, Michael;Lipton, Zachary;Wilder, Bryan",2024,10,"@inproceedings{2-24658,
  title={Auditing fairness under unobserved confounding},
  author={Byun, Yewon and Sam, Dylan and Oberst, Michael and Lipton, Zachary and Wilder, Bryan},
  year={2024},
  booktitle={The International Conference on Artificial Intelligence and Statistics (AISTATS)}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Organizational,Auditing,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-2466,acm,The Soul of Work: Evaluation of Job Meaningfulness and Accountability in Human-AI Collaboration,"Work is an important part of our lives - not only as a way to earn a living but as a crucial source for experiencing meaningfulness. The introduction of autonomous systems (or in the widest sense ""artificial intelligence"", AI) will fundamentally impact work practices. However, while most existing models of human-AI collaboration focus on performance goals, less is known about their potential influence on job satisfaction. In this paper, we present an online experiment in which we compared the perception of job meaningfulness and accountability in a human-AI collaboration across three interaction paradigms: Supervisory, Advisory, and Interactive. Our results showed that, unlike the common notion of supervisory control, people find their job more satisfying when they directly interact with the AI and are involved and remain accountable for action and decision-making. Introducing AI as a teammate in the interactive paradigm was associated with the highest job meaningfulness.",10.1145/3637407,https://doi.org/10.1145/3637407,Proceedings of the ACM on Human-Computer Interaction,"Sadeghian, Shadan; Uhde, Alarith; Hassenzahl, Marc",2024,0,"@article{2-2466,
  title = {The Soul of Work: Evaluation of Job Meaningfulness and Accountability in Human-AI Collaboration},
  author = {Sadeghian, Shadan and Uhde, Alarith and Hassenzahl, Marc},
  year = {2024},
  doi = {10.1145/3637407},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Everyday / Employment / Public Service,Operational,"Advising, Collaborating","Decision-maker, Stakeholder","Change affective-perceptual, Restrict human agency, Shift responsibility",Change AI responses,recommendations,corrective feedback,"Textual, Conversational/Natural Language",Yes,Yes
2-24664,pmlr,Balancing competing objectives with noisy data: score-based classifiers for welfare-aware machine learning,"While real-world decisions involve many competing objectives, algorithmic decisions are often evaluated with a single objective function. In this paper, we study algorithmic policies which explicitly trade off between a private objective (such as profit) and a public objective (such as social welfare). We analyze a natural class of policies which trace an empirical Pareto frontier based on learned scores, and focus on how such decisions can be made in noisy or data-limited regimes. Our theoretical results characterize the optimal strategies in this class, bound the Pareto errors due to inaccuracies in the scores, and show an equivalence between optimal strategies and a rich class of fairness-constrained profit-maximizing policies. We then present empirical results in two different contexts — online content recommendation and sustainable abalone fisheries — to underscore the generality of our approach to a wide range of practical decisions. Taken together, these results shed light on inherent trade-offs in using machine learning for decisions that impact social welfare.",NA,https://proceedings.mlr.press/v119/rolf20a.html,International Conference on Machine Learning (ICML),"Rolf, Esther;Simchowitz, Max;Dean, Sarah;Liu, Lydia T.;Bjorkegren, Daniel;Hardt, Moritz;Blumenstock, Joshua",2020,0,"@inproceedings{2-24664,
  title={Balancing competing objectives with noisy data: score-based classifiers for welfare-aware machine learning},
  author={Rolf, Esther and Simchowitz, Max and Dean, Sarah and Liu, Lydia T. and Bjorkegren, Daniel and Hardt, Moritz and Blumenstock, Joshua},
  year={2020},
  booktitle={International Conference on Machine Learning (ICML)}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Environment / Resources / Energy, Media / Communication / Entertainment",Institutional,"Forecasting, Advising","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-2470,acm,Towards Tangible Algorithms: Exploring the Experiences of Tangible Interactions with Movie Recommender Algorithms,"Artificial Intelligence (AI) supports many of our everyday activities and decisions. However, personalized algorithmic recommendations often produce adverse experiences due to a lack of awareness, control, or transparency. While research has directed solutions on graphical user interfaces (GUIs), there are no explorations of Tangible User Interfaces (TUIs) to improve the experience with such systems, despite the valid existing academic arguments in favor of this exploration. Therefore, centering on transparency and control, we analyzed how 18 users of movie recommender systems perceived four different TUIs using individual co-design sessions and post-interview questionnaires. Through thematic analysis, we identified seven design considerations while designing TUIs to interact with algorithmic movie recommender systems: (1) Distinctions between TUIs and GUIs; (2) TUIs replacing predominant interfaces; (3) Preference for single-device TUIs; (4) The relevance of granular control for TUIs; (5) Apparent transparency limitations of TUIs; (6) TUIs and algorithmic social computing; and (7) Overview of specific design choices, including advantages and disadvantages of soft, hard, rounded, cubic, and humanoid interfaces. These findings inspired Recffy: the first functional TUI designed to enhance awareness and control in personalized movie recommendations. Based on this study, we propose the concept of Tangible Algorithms: TUIs dedicated to enhancing the interaction of algorithmic systems and their profiling processes or decisions in a specific context. Furthermore, we describe the relevance of tangible algorithms and design guidelines to promote them in diverse AI contexts. Finally, we invite the HCI and CSCW community to continue exploring tangible algorithms to address the interaction with algorithmic systems, including the collaborative and social computing dynamics they can promote in diverse AI contexts.",10.1145/3555757,https://doi.org/10.1145/3555757,Proceedings of the ACM on Human-Computer Interaction,"Alvarado, Oscar; Vanden Abeele, Vero; Geerts, David; Verbert, Katrien",2022,0,"@article{2-2470,
  title = {Towards Tangible Algorithms: Exploring the Experiences of Tangible Interactions with Movie Recommender Algorithms},
  author = {Alvarado, Oscar and Vanden Abeele, Vero and Geerts, David and Verbert, Katrien},
  year = {2022},
  doi = {10.1145/3555757},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",System/Artifact contributions,Media / Communication / Entertainment,Individual,Advising,Decision-maker,"Change trust, Change cognitive demands, Change affective-perceptual","Update AI competence, Change AI responses","personalized movie recommendations, random suggestions",NA,"Physical / Embodiment, Textual, Visual",Yes,Yes
2-2471,acm,Assessing Post-hoc Explainability of the BKT Algorithm,"As machine intelligence is increasingly incorporated into educational technologies, it becomes imperative for instructors and students to understand the potential flaws of the algorithms on which their systems rely. This paper describes the design and implementation of an interactive post-hoc explanation of the Bayesian Knowledge Tracing algorithm which is implemented in learning analytics systems used across the United States. After a user-centered design process to smooth out interaction design difficulties, we ran a controlled experiment to evaluate whether the interactive or static version of the explainable led to increased learning. Our results reveal that learning about an algorithm through an explainable depends on users' educational background. For other contexts, designers of post-hoc explainables must consider their users' educational background to best determine how to empower more informed decision-making with AI-enhanced systems.",10.1145/3375627.3375856,https://doi.org/10.1145/3375627.3375856,"AAAI/ACM Conference on AI, Ethics, and Society","Zhou, Tongyu; Sheng, Haoyu; Howley, Iris",2020,22,"@inproceedings{2-2471,
  title = {Assessing Post-hoc Explainability of the BKT Algorithm},
  author = {Zhou, Tongyu and Sheng, Haoyu and Howley, Iris},
  year = {2020},
  doi = {10.1145/3375627.3375856},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society}
}",Empirical contributions,Education / Teaching / Research,Operational,"Explaining, Analyzing","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-24720,pmlr,Causal conceptions of fairness and their consequences,"Recent work highlights the role of causality in designing equitable decision-making algorithms. It is not immediately clear, however, how existing causal conceptions of fairness relate to one another, or what the consequences are of using these definitions as design principles. Here, we first assemble and categorize popular causal definitions of algorithmic fairness into two broad families: (1) those that constrain the effects of decisions on counterfactual disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions <em>almost always</em>—in a measure theoretic sense—result in strongly Pareto dominated decision policies, meaning there is an alternative, unconstrained policy favored by every stakeholder with preferences drawn from a large, natural class. For example, in the case of college admissions decisions, policies constrained to satisfy causal fairness definitions would be disfavored by every stakeholder with neutral or positive preferences for both academic preparedness and diversity. Indeed, under a prominent definition of causal fairness, we prove the resulting policies require admitting all students with the same probability, regardless of academic qualifications or group membership. Our results highlight formal limitations and potential adverse consequences of common mathematical notions of causal fairness.",NA,https://proceedings.mlr.press/v162/nilforoshan22a.html,International Conference on Machine Learning (ICML),"Nilforoshan, Hamed;Gaebler, Johann D;Shroff, Ravi;Goel, Sharad",2022,76,"@inproceedings{2-24720,
  title={Causal conceptions of fairness and their consequences},
  author={Nilforoshan, Hamed and Gaebler, Johann D and Shroff, Ravi and Goel, Sharad},
  year={2022},
  booktitle={International Conference on Machine Learning (ICML)}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service",Institutional,Advising,"Stakeholder, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-24729,pmlr,Characterizing fairness over the set of good models under selective labels,"Algorithmic risk assessments are used to inform decisions in a wide variety of high-stakes settings. Often multiple predictive models deliver similar overall performance but differ markedly in their predictions for individual cases, an empirical phenomenon known as the “Rashomon Effect.” These models may have different properties over various groups, and therefore have different predictive fairness properties. We develop a framework for characterizing predictive fairness properties over the set of models that deliver similar overall performance, or “the set of good models.” Our framework addresses the empirically relevant challenge of selectively labelled data in the setting where the selection decision and outcome are unconfounded given the observed data features. Our framework can be used to 1) audit for predictive bias; or 2) replace an existing model with one that has better fairness properties. We illustrate these use cases on a recidivism prediction task and a real-world credit-scoring task.",NA,https://proceedings.mlr.press/v139/coston21a.html,International Conference on Machine Learning (ICML),"Coston, Amanda;Rambachan, Ashesh;Chouldechova, Alexandra",2021,49,"@inproceedings{2-24729,
  title={Characterizing fairness over the set of good models under selective labels},
  author={Coston, Amanda and Rambachan, Ashesh and Chouldechova, Alexandra},
  year={2021},
  booktitle={International Conference on Machine Learning (ICML)}
}",Methodological contributions,"Law / Policy / Governance, Finance / Business / Economy",Operational,"Auditing, Forecasting","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-24735,pmlr,Clinician-in-the-loop decision making: reinforcement learning with near-optimal set-valued policies,"Standard reinforcement learning (RL) aims to find an optimal policy that identifies the best action for each state. However, in healthcare settings, many actions may be near-equivalent with respect to the reward (e.g., survival). We consider an alternative objective – learning set-valued policies to capture near-equivalent actions that lead to similar cumulative rewards. We propose a model-free algorithm based on temporal difference learning and a near-greedy heuristic for action selection. We analyze the theoretical properties of the proposed algorithm, providing optimality guarantees and demonstrate our approach on simulated environments and a real clinical task. Empirically, the proposed algorithm exhibits good convergence properties and discovers meaningful near-equivalent actions. Our work provides theoretical, as well as practical, foundations for clinician/human-in-the-loop decision making, in which humans (e.g., clinicians, patients) can incorporate additional knowledge (e.g., side effects, patient preference) when selecting among near-equivalent actions.",NA,https://proceedings.mlr.press/v119/tang20c.html,International Conference on Machine Learning (ICML),"Tang, Shengpu;Modi, Aditya;Sjoding, Michael;Wiens, Jenna",2020,0,"@inproceedings{2-24735,
  title = {Clinician-in-the-loop decision making: reinforcement learning with near-optimal set-valued policies},
  author = {Tang, Shengpu and Modi, Aditya and Sjoding, Michael and Wiens, Jenna},
  year = {2020},
  booktitle = {International Conference on Machine Learning (ICML)}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,Executing,"Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-24762,pmlr,Conformal prediction sets improve human decision making,"In response to everyday queries, humans explicitly signal uncertainty and offer alternative answers when they are unsure. Machine learning models that output calibrated prediction sets through conformal prediction mimic this human behaviour; larger sets signal greater uncertainty while providing alternatives. In this work, we study the usefulness of conformal prediction sets as an aid for human decision making by conducting a pre-registered randomized controlled trial with conformal prediction sets provided to human subjects. With statistical significance, we find that when humans are given conformal prediction sets their accuracy on tasks improves compared to fixed-size prediction sets with the same coverage guarantee. The results show that quantifying model uncertainty with conformal prediction is helpful for human-in-the-loop decision making and human-AI teams.",NA,https://proceedings.mlr.press/v235/cresswell24a.html,International Conference on Machine Learning (ICML),"Cresswell, Jesse C.;Sui, Yi;Kumar, Bhargava;Vouitsis, No\\{e}l",2024,48,"@inproceedings{2-24762,
  title={Conformal prediction sets improve human decision making},
  author={Cresswell, Jesse C. and Sui, Yi and Kumar, Bhargava and Vouitsis, No{\'e}l},
  year={2024},
  booktitle={International Conference on Machine Learning (ICML)}
}",Empirical contributions,"Education / Teaching / Research, Everyday / Employment / Public Service",Individual,"Forecasting, Advising","Decision-maker, Decision-subject","Alter decision outcomes, Change cognitive demands",no such info,prediction of alternative,language proficiency,"Visual, Interactive interface, Textual",Yes,Yes
2-24775,pmlr,Contextualized policy recovery: modeling and interpreting medical decisions with adaptive imitation learning,"Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models force a tradeoff between accuracy and interpretability, limiting data-driven interpretations of human decision-making processes. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically under different contexts. Thus, we develop Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem, where each context poses a unique task and complex decision policies can be constructed piece-wise from many simple context-specific policies. CPR models each context-specific policy as a linear map, and generates new policy models <em>on-demand</em> as contexts are updated with new observations. We provide two flavors of the CPR framework: one focusing on exact local interpretability, and one retaining full global interpretability. We assess CPR through studies on simulated and real data, achieving state-of-the-art performance on predicting antibiotic prescription in intensive care units ($+22$% AUROC vs. previous SOTA) and predicting MRI prescription for Alzheimer’s patients ($+7.7$% AUROC vs. previous SOTA). With this improvement, CPR closes the accuracy gap between interpretable and black-box methods, allowing high-resolution exploration and analysis of context-specific decision models.",NA,https://proceedings.mlr.press/v235/deuschel24a.html,International Conference on Machine Learning (ICML),"Deuschel, Jannik;Ellington, Caleb;Luo, Yingtao;Lengerich, Ben;Friederich, Pascal;Xing, Eric P.",2024,0,"@inproceedings{2-24775,
  title={Contextualized policy recovery: modeling and interpreting medical decisions with adaptive imitation learning},
  author={Deuschel, Jannik and Ellington, Caleb and Luo, Yingtao and Lengerich, Ben and Friederich, Pascal and Xing, Eric P.},
  year={2024},
  booktitle={International Conference on Machine Learning (ICML)}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Explaining","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-24779,pmlr,Continuous-time decision transformer for healthcare applications,"Offline reinforcement learning (RL) is a promising approach for training intelligent medical agents to learn treatment policies and assist decision making in many healthcare applications, such as scheduling clinical visits and assigning dosages for patients with chronic conditions. In this paper, we investigate the potential usefulness of Decision Transformer (Chen et al., 2021)–a new offline RL paradign– in medical domains where decision making in continuous time is desired. As Decision Transformer only handles discrete-time (or turn-based) sequential decision making scenarios, we generalize it to Continuous-Time Decision Transformer that not only considers the past clinical measurements and treatments but also the timings of previous visits, and learns to suggest the timings of future visits as well as the treatment plan at each visit. Extensive experiments on synthetic datasets and simulators motivated by real-world medical applications demonstrate that Continuous-Time Decision Transformer is able to outperform competitors and has clinical utility in terms of improving patients’ health and prolonging their survival by learning high-performance policies from logged data generated using policies of different levels of quality.",NA,https://proceedings.mlr.press/v206/zhang23i.html,The International Conference on Artificial Intelligence and Statistics (AISTATS),"Zhang, Zhiyue;Mei, Hongyuan;Xu, Yanxun",2023,26,"@inproceedings{2-24779,
  title={Continuous-time decision transformer for healthcare applications},
  author={Zhang, Zhiyue and Mei, Hongyuan and Xu, Yanxun},
  year={2023},
  booktitle={The International Conference on Artificial Intelligence and Statistics (AISTATS)}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Executing","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-2478,acm,"CollabCoder: A Lower-barrier, Rigorous Workflow for Inductive Collaborative Qualitative Analysis with Large Language Models","Collaborative Qualitative Analysis (CQA) can enhance qualitative analysis rigor and depth by incorporating varied viewpoints. Nevertheless, ensuring a rigorous CQA procedure itself can be both complex and costly. To lower this bar, we take a theoretical perspective to design a one-stop, end-to-end workflow, CollabCoder, that integrates Large Language Models (LLMs) into key inductive CQA stages. In the independent open coding phase, CollabCoder offers AI-generated code suggestions and records decision-making data. During the iterative discussion phase, it promotes mutual understanding by sharing this data within the coding team and using quantitative metrics to identify coding (dis)agreements, aiding in consensus-building. In the codebook development phase, CollabCoder provides primary code group suggestions, lightening the workload of developing a codebook from scratch. A 16-user evaluation confirmed the effectiveness of CollabCoder, demonstrating its advantages over the existing CQA platform. All related materials of CollabCoder, including code and further extensions, will be included in: https://gaojie058.github.io/CollabCoder/.",10.1145/3613904.3642002,https://doi.org/10.1145/3613904.3642002,CHI Conference on Human Factors in Computing Systems,"Gao, Jie; Guo, Yuchen; Lim, Gionnieve; Zhang, Tianqin; Zhang, Zheng; Li, Toby Jia-Jun; Perrault, Simon Tangi",2024,9,"@inproceedings{2-2478,
  title = {CollabCoder: A Lower-barrier, Rigorous Workflow for Inductive Collaborative Qualitative Analysis with Large Language Models},
  author = {Gao, Jie and Guo, Yuchen and Lim, Gionnieve and Zhang, Tianqin and Zhang, Zheng and Li, Toby Jia-Jun and Perrault, Simon Tangi},
  year = {2024},
  doi = {10.1145/3613904.3642002},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",System/Artifact contributions,Education / Teaching / Research,Operational,"Analyzing, Advising, Collaborating","Decision-maker, Developer, Knowledge provider","Change cognitive demands, Alter decision outcomes, Change trust",Change AI responses,AI suggestions,NA,Interactive interface,Yes,Yes
2-24835,pmlr,Designing decision support systems using counterfactual prediction sets,"Decision support systems for classification tasks are predominantly designed to predict the value of the ground truth labels. However, since their predictions are not perfect, these systems also need to make human experts understand when and how to use these predictions to update their own predictions. Unfortunately, this has been proven challenging. In this context, it has been recently argued that an alternative type of decision support systems may circumvent this challenge. Rather than providing a single label prediction, these systems provide a set of label prediction values constructed using a conformal predictor, namely a prediction set, and forcefully ask experts to predict a label value from the prediction set. However, the design and evaluation of these systems have so far relied on stylized expert models, questioning their promise. In this paper, we revisit the design of this type of systems from the perspective of online learning and develop a methodology that does not require, nor assumes, an expert model. Our methodology leverages the nested structure of the prediction sets provided by any conformal predictor and a natural counterfactual monotonicity assumption to achieve an exponential improvement in regret in comparison to vanilla bandit algorithms. We conduct a large-scale human subject study ($n = 2{,}751$) to compare our methodology to several competitive baselines. The results show that, for decision support systems based on prediction sets, limiting experts’ level of agency leads to greater performance than allowing experts to always exercise their own agency.",NA,https://proceedings.mlr.press/v235/straitouri24a.html,International Conference on Machine Learning (ICML),"Straitouri, Eleni;Gomez Rodriguez, Manuel",2024,26,"@inproceedings{2-24835,
  title={Designing Decision Support Systems Using Counterfactual Prediction Sets},
  author={Straitouri, Eleni and Gomez Rodriguez, Manuel},
  year={2024},
  booktitle={International Conference on Machine Learning (ICML)}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Operational,"Forecasting, Advising, Executing","Decision-maker, Decision-subject, Knowledge provider","Alter decision outcomes, Change trust, Restrict human agency",Update AI competence,"prediction of alternative, counterfactual rewards",limited agency,"Textual, Visual, Interactive interface",Yes,Yes
2-24902,pmlr,Efficient resource allocation with fairness constraints in restless multi-armed bandits,"Restless Multi-Armed Bandits (RMAB) is an apt model to represent decision-making problems in public health interventions (e.g., tuberculosis, maternal, and child care), anti-poaching planning, sensor monitoring, personalized recommendations and many more. Existing research in RMAB has contributed mechanisms and theoretical results to a wide variety of settings, where the focus is on maximizing expected value. In this paper, we are interested in ensuring that RMAB decision making is also fair to different arms while maximizing expected value. In the context of public health settings, this would ensure that different people and/or communities are fairly represented while making public health intervention decisions. To achieve this goal, we formally define the fairness constraints in RMAB and provide planning and learning methods to solve RMAB in a fair manner. We demonstrate key theoretical properties of fair RMAB and experimentally demonstrate that our proposed methods handle fairness constraints without sacrificing significantly on solution quality.",NA,https://proceedings.mlr.press/v180/li22e.html,Conference on Uncertainty in Artificial Intelligence (UAI),"Li, Dexun.;Varakantham, Pradeep",2022,19,"@inproceedings{2-24902,
  title={Efficient resource allocation with fairness constraints in restless multi-armed bandits},
  author={Li, Dexun and Varakantham, Pradeep},
  year={2022},
  booktitle={Conference on Uncertainty in Artificial Intelligence (UAI)}
}",Theoretical contributions,Healthcare / Medicine / Surgery,Institutional,"Analyzing, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-24954,pmlr,Fair and accurate decision making through group-aware learning,"The integration of machine learning models in various real-world applications is becoming more prevalent to assist humans in their daily decision-making tasks as a result of recent advancements in this field. However, it has been discovered that there is a tradeoff between the accuracy and fairness of these decision-making tasks. In some cases, these AI systems can be unfair by exhibiting bias or discrimination against certain social groups, which can have severe consequences in real life. Inspired by one of the most well-known human learning skills called grouping, we address this issue by proposing a novel machine learning (ML) framework where the ML model learns to group a diverse set of problems into distinct subgroups to solve each subgroup using its specific sub-model. Our proposed framework involves three stages of learning, which are formulated as a three-level optimization problem: 1) grouping problems into subgroups, 2) learning group-specific sub-models for problem-solving, and 3) updating group assignments of training examples by minimizing validation loss. These three learning stages are performed end-to-end in a joint manner using gradient descent. To improve fairness and accuracy, we develop an efficient optimization algorithm to solve this three-level optimization problem. To further decrease the risk of overfitting in small datasets using our LBG method, we incorporate domain adaptation techniques in the second stage of training. We further apply our method to differentiable neural architecture search (NAS) methods.",NA,https://proceedings.mlr.press/v202/hosseini23a.html,International Conference on Machine Learning (ICML),"Hosseini, Ramtin;Zhang, Li;Garg, Bhanu;Xie, Pengtao",2023,6,"@inproceedings{2-24954,
  title={Fair and accurate decision making through group-aware learning},
  author={Hosseini, Ramtin and Zhang, Li and Garg, Bhanu and Xie, Pengtao},
  year={2023},
  booktitle={International Conference on Machine Learning (ICML)}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Institutional,"Forecasting, Executing","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-24955,pmlr,Fair contextual multi-armed bandits: theory and experiments,"When an AI system interacts with multiple users, it frequently needs to make allocation decisions. For instance, a virtual agent decides whom to pay attention to in a group, or a factory robot selects a worker to deliver a part.Demonstrating fairness in decision making is essential for such systems to be broadly accepted. We introduce a Multi-Armed Bandit algorithm with fairness constraints, where fairness is defined as a minimum rate at which a task or a resource is assigned to a user. The proposed algorithm uses contextual information about the users and the task and makes no assumptions on how the losses capturing the performance of different users are generated. We provide theoretical guarantees of performance and empirical results from simulation and an online user study. The results highlight the benefit of accounting for contexts in fair decision making, especially when users perform better at some contexts and worse at others.",NA,https://proceedings.mlr.press/v124/chen20a.html,Conference on Uncertainty in Artificial Intelligence (UAI),"Chen, Yifang;Cuellar, Alex;Luo, Haipeng;Modi, Jignesh;Nemlekar, Heramb;Nikolaidis, Stefanos",2020,84,"@inproceedings{2-24955,
  title={Fair contextual multi-armed bandits: theory and experiments},
  author={Chen, Yifang and Cuellar, Alex and Luo, Haipeng and Modi, Jignesh and Nemlekar, Heramb and Nikolaidis, Stefanos},
  year={2020},
  booktitle={Conference on Uncertainty in Artificial Intelligence (UAI)}
}",Algorithmic contributions,"Manufacturing / Industry / Automation, Generic / Abstract / Domain-agnostic",Operational,Executing,Decision-maker,"Alter decision outcomes, Change affective-perceptual, Restrict human agency, Change trust",no such info,"recommendations, delegation",NA,Textual,Yes,Yes
2-24958,pmlr,Fair off-policy learning from observational data,"Algorithmic decision-making in practice must be fair for legal, ethical, and societal reasons. To achieve this, prior research has contributed various approaches that ensure fairness in machine learning predictions, while comparatively little effort has focused on fairness in decision-making, specifically off-policy learning. In this paper, we propose a novel framework for fair off-policy learning: we learn decision rules from observational data under different notions of fairness, where we explicitly assume that observational data were collected under a different – potentially discriminatory – behavioral policy. Importantly, our framework applies to different fairness notions for off-policy learning, where fairness is formalized based on actions or policy values. As our main contribution, we propose a neural network-based framework to learn optimal policies under different fairness notions. We further provide theoretical guarantees in the form of generalization bounds for the finite-sample version of our framework. We demonstrate the effectiveness of our framework through extensive numerical experiments using both simulated and real-world data. Altogether, our work enables algorithmic decision-making in a wide array of practical applications where fairness must be ensured.",NA,https://proceedings.mlr.press/v235/frauen24a.html,International Conference on Machine Learning (ICML),"Frauen, Dennis;Melnychuk, Valentyn;Feuerriegel, Stefan",2024,9,"@inproceedings{2-24958,
  title={Fair off-policy learning from observational data},
  author={Frauen, Dennis and Melnychuk, Valentyn and Feuerriegel, Stefan},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2024}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Institutional,Executing,"Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-24961,pmlr,Fairness and bias in online selection,"There is growing awareness and concern about fairness in machine learning and algorithm design. This is particularly true in online selection problems where decisions are often biased, for example, when assessing credit risks or hiring staff. We address the issues of fairness and bias in online selection by introducing multi-color versions of the classic secretary and prophet problem. Interestingly, existing algorithms for these problems are either very unfair or very inefficient, so we develop optimal fair algorithms for these new problems and provide tight bounds on their competitiveness. We validate our theoretical findings on real-world data.",NA,https://proceedings.mlr.press/v139/correa21a.html,International Conference on Machine Learning (ICML),"Correa, Jose;Cristi, Andres;Duetting, Paul;Norouzi-Fard, Ashkan",2021,24,"@inproceedings{2-24961,
  title={Fairness and bias in online selection},
  author={Correa, Jose and Cristi, Andres and Duetting, Paul and Norouzi-Fard, Ashkan},
  year={2021},
  booktitle={International Conference on Machine Learning (ICML)}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy, Everyday / Employment / Public Service",Operational,"Analyzing, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-24965,pmlr,Fairness interventions as ({d}is){i}ncentives for strategic manipulation,"Although machine learning (ML) algorithms are widely used to make decisions about individuals in various domains, concerns have arisen that (1) these algorithms are vulnerable to strategic manipulation and gaming the algorithm; and (2) ML decisions may exhibit bias against certain social groups. Existing works have largely examined these as two separate issues, e.g., by focusing on building ML algorithms robust to strategic manipulation, or on training a fair ML algorithm. In this study, we set out to understand the impact they each have on the other, and examine how to characterize fair policies in the presence of strategic behavior. The strategic interaction between a decision maker and individuals (as decision takers) is modeled as a two-stage (Stackelberg) game; when designing an algorithm, the former anticipates the latter may manipulate their features in order to receive more favorable decisions. We analytically characterize the equilibrium strategies of both, and examine how the algorithms and their resulting fairness properties are affected when the decision maker is strategic (anticipates manipulation), as well as the impact of fairness interventions on equilibrium strategies. In particular, we identify conditions under which anticipation of strategic behavior may mitigate/exacerbate unfairness, and conditions under which fairness interventions can serve as (dis)incentives for strategic manipulation.",NA,https://proceedings.mlr.press/v162/zhang22l.html,International Conference on Machine Learning (ICML),"Zhang, Xueru;Khalili, Mohammad Mahdi;Jin, Kun;Naghizadeh, Parinaz;Liu, Mingyan",2022,29,"@inproceedings{2-24965,
  title = {Fairness Interventions as (Dis)incentives for Strategic Manipulation},
  author = {Zhang, Xueru and Khalili, Mohammad Mahdi and Jin, Kun and Naghizadeh, Parinaz and Liu, Mingyan},
  year = {2022},
  booktitle = {International Conference on Machine Learning (ICML)}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Institutional,"Executing, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-2497,acm,Piecemeal Knowledge Acquisition for Computational Normative Reasoning,"We present a hybrid approach to knowledge acquisition and representation for machine ethics—or more generally, computational normative reasoning. Building on recent research in artificial intelligence and law, our approach is modeled on the familiar practice of decision-making under precedential constraint in the common law. We first provide a formal characterization of this practice, showing how a body of normative information can be constructed in a way that is piecemeal, distributed, and responsive to particular circumstances. We then discuss two possible applications: first, a robot childminder, and second, moral judgment in a bioethical domain.",10.1145/3514094.3534182,https://doi.org/10.1145/3514094.3534182,"AAAI/ACM Conference on AI, Ethics, and Society","Canavotto, Ilaria; Horty, John",2022,14,"@inproceedings{2-2497,
  title     = {Piecemeal Knowledge Acquisition for Computational Normative Reasoning},
  author    = {Canavotto, Ilaria and Horty, John},
  year      = {2022},
  doi       = {10.1145/3514094.3534182},
  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery, Everyday / Employment / Public Service",Operational,"Executing, Explaining","Knowledge provider, Guardian",NA,NA,NA,NA,NA,Yes,No
2-24996,pmlr,Forecasticu: a prognostic decision support system for timely prediction of intensive care unit admission,"We develop ForecastICU: a prognostic decision support system that monitors hospitalized patients and prompts alarms for intensive care unit (ICU) admissions. ForecastICU is first trained in an offline stage by constructing a Bayesian belief system that corresponds to its belief about how trajectories of physiological data streams of the patient map to a clinical status. After that, ForecastICU monitors a new patient in real-time by observing her physiological data stream, updating its belief about her status over time, and prompting an alarm whenever its belief process hits a predefined threshold (confidence). Using a real-world dataset obtained from UCLA Ronald Reagan Medical Center, we show that ForecastICU can predict ICU admissions 9 hours before a physician’s decision (for a sensitivity of 40% and a precision of 50%). Also, ForecastICU performs consistently better than other state-of-the-art machine learning algorithms in terms of sensitivity, precision, and timeliness: it can predict ICU admissions 3 hours earlier, and offers a 7.8% gain in sensitivity and a 5.1% gain in precision compared to the best state-of-the-art algorithm. Moreover, ForecastICU offers an area under curve (AUC) gain of 22.3% compared to the Rothman index, which is the currently deployed technology in most hospital wards.",NA,https://proceedings.mlr.press/v48/yoon16.html,International Conference on Machine Learning,"Yoon, Jinsung;Alaa, Ahmed;Hu, Scott;Schaar, Mihaela",2016,61,"@inproceedings{2-24996,
  title={Forecasticu: a prognostic decision support system for timely prediction of intensive care unit admission},
  author={Yoon, Jinsung and Alaa, Ahmed and Hu, Scott and Schaar, Mihaela},
  year={2016},
  booktitle={International Conference on Machine Learning}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,"Institutional, Operational","Forecasting, Advising, Executing, Monitoring","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-25095,pmlr,Improving expert predictions with conformal prediction,"Automated decision support systems promise to help human experts solve multiclass classification tasks more efficiently and accurately. However, existing systems typically require experts to understand when to cede agency to the system or when to exercise their own agency. Otherwise, the experts may be better off solving the classification tasks on their own. In this work, we develop an automated decision support system that, by design, does not require experts to understand when to trust the system to improve performance. Rather than providing (single) label predictions and letting experts decide when to trust these predictions, our system provides sets of label predictions constructed using conformal prediction—prediction sets—and forcefully asks experts to predict labels from these sets. By using conformal prediction, our system can precisely trade-off the probability that the true label is not in the prediction set, which determines how frequently our system will mislead the experts, and the size of the prediction set, which determines the difficulty of the classification task the experts need to solve using our system. In addition, we develop an efficient and near-optimal search method to find the conformal predictor under which the experts benefit the most from using our system. Simulation experiments using synthetic and real expert predictions demonstrate that our system may help experts make more accurate predictions and is robust to the accuracy of the classifier the conformal predictor relies on.",NA,https://proceedings.mlr.press/v202/straitouri23a.html,International Conference on Machine Learning (ICML),"Straitouri, Eleni;Wang, Lequn;Okati, Nastaran;Gomez Rodriguez, Manuel",2023,21,"@inproceedings{2-25095,
  title={Improving expert predictions with conformal prediction},
  author={Straitouri, Eleni and Wang, Lequn and Okati, Nastaran and Gomez Rodriguez, Manuel},
  year={2023},
  booktitle={International Conference on Machine Learning (ICML)}
}",System/Artifact contributions,Generic / Abstract / Domain-agnostic,Operational,"Analyzing, Advising","Decision-maker, Knowledge provider","Alter decision outcomes, Change trust",no such info,"system accuracy, prediction of alternative",NA,"Visual, Textual, Interactive interface",Yes,Yes
2-25113,pmlr,Information discrepancy in strategic learning,"We initiate the study of the effects of non-transparency in decision rules on individuals’ ability to improve in strategic learning settings. Inspired by real-life settings, such as loan approvals and college admissions, we remove the assumption typically made in the strategic learning literature, that the decision rule is fully known to individuals, and focus instead on settings where it is inaccessible. In their lack of knowledge, individuals try to infer this rule by learning from their peers (e.g., friends and acquaintances who previously applied for a loan), naturally forming groups in the population, each with possibly different type and level of information regarding the decision rule. We show that, in equilibrium, the principal’s decision rule optimizing welfare across sub-populations may cause a strong negative externality: the true quality of some of the groups can actually deteriorate. On the positive side, we show that, in many natural cases, optimal improvement can be guaranteed simultaneously for all sub-populations. We further introduce a measure we term information overlap proxy, and demonstrate its usefulness in characterizing the disparity in improvements across sub-populations. Finally, we identify a natural condition under which improvement can be guaranteed for all sub-populations while maintaining high predictive accuracy. We complement our theoretical analysis with experiments on real-world datasets.",NA,https://proceedings.mlr.press/v162/bechavod22a.html,International Conference on Machine Learning (ICML),"Bechavod, Yahav;Podimata, Chara;Wu, Steven;Ziani, Juba",2022,10,"@inproceedings{2-25113,
  title={Information discrepancy in strategic learning},
  author={Bechavod, Yahav and Podimata, Chara and Wu, Steven and Ziani, Juba},
  year={2022},
  booktitle={International Conference on Machine Learning (ICML)}
}",Theoretical contributions,"Finance / Business / Economy, Generic / Abstract / Domain-agnostic",Operational,"Analyzing, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-25136,pmlr,Interventions over predictions: reframing the ethical debate for actuarial risk assessment,"Actuarial risk assessments are frequently touted as a neutral way to counteract implicit bias and increase the fairness of decisions made at almost every juncture of the criminal justice system, from pretrial release to sentencing, parole and probation. In recent times these assessments have come under increased scrutiny, as critics claim that the statistical techniques underlying them might reproduce existing patterns of discrimination and historical biases that are reflected in the data. Much of this debate is centered around competing notions of fairness and predictive accuracy, which seek to problematize the use of variables that act as “proxies” for protected classes, such as race and gender. However, these debates fail to address the core ethical issue at hand - that current risk assessments are ill-equipped to support ethical punishment and rehabilitation practices in the criminal justice system, because they offer only a limited insight into the underlying drivers of criminal behavior. In this paper, we examine the prevailing paradigms of fairness currently under debate and propose an alternative methodology for identifying the underlying social and structural factors that drive criminal behavior. We argue that the core ethical debate surrounding the use of regression in risk assessments is not one of bias or accuracy. Rather, it’s one of purpose. If machine learning is operationalized merely in the service of predicting future crime, then it becomes difficult to break cycles of criminalization that are driven by the iatrogenic effects of the criminal justice system itself. We posit that machine learning should not be used for prediction, rather it should be used to surface covariates that are fed into a causal model for understanding the social, structural and psychological drivers of crime. We propose an alternative application of machine learning and causal inference away from predicting risk scores to risk mitigation.",NA,https://proceedings.mlr.press/v81/barabas18a.html,"Conference on Fairness, Accountability, and Transparency (FAccT)","Barabas, Chelsea;Virza, Madars;Dinakar, Karthik;Ito, Joichi;Zittrain, Jonathan",2018,244,"@inproceedings{2-25136,
  title     = {Interventions over predictions: reframing the ethical debate for actuarial risk assessment},
  author    = {Barabas, Chelsea and Virza, Madars and Dinakar, Karthik and Ito, Joichi and Zittrain, Jonathan},
  year      = {2018},
  booktitle = {Conference on Fairness, Accountability, and Transparency (FAccT)}
}",Methodological contributions,Law / Policy / Governance,Operational,"Forecasting, Analyzing","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-2515,acm,AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems,"Recently, there has been an emergence of employing LLM-powered agents as believable human proxies, based on their remarkable decision-making capability. However, existing studies mainly focus on simulating human dialogue. Human non-verbal behaviors, such as item clicking in recommender systems, although implicitly exhibiting user preferences and could enhance the modeling of users, have not been deeply explored. The main reasons lie in the gap between language modeling and behavior modeling, as well as the incomprehension of LLMs about user-item relations.To address this issue, we propose AgentCF for simulating user-item interactions in recommender systems through agent-based collaborative filtering. We creatively consider not only users but also items as agents, and develop a collaborative learning approach that optimizes both kinds of agents together. Specifically, at each time step, we first prompt the user and item agents to interact autonomously. Then, based on the disparities between the agents' decisions and real-world interaction records, user and item agents are prompted to reflect on and adjust the misleading simulations collaboratively, thereby modeling their two-sided relations. The optimized agents can also propagate their preferences to other agents in subsequent interactions, implicitly capturing the collaborative filtering idea. Overall, the optimized agents exhibit diverse interaction behaviors within our framework, including user-item, user-user, item-item, and collective interactions. The results show that these agents can demonstrate personalized behaviors akin to those of real-world individuals, sparking the development of next-generation user behavior simulation.",10.1145/3589334.3645537,https://doi.org/10.1145/3589334.3645537,The ACM Web Conference (formerly known as WWW),"Zhang, Junjie; Hou, Yupeng; Xie, Ruobing; Sun, Wenqi; McAuley, Julian; Zhao, Wayne Xin; Lin, Leyu; Wen, Ji-Rong",2024,135,"@inproceedings{2-2515,
  title = {AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems},
  author = {Zhang, Junjie and Hou, Yupeng and Xie, Ruobing and Sun, Wenqi and McAuley, Julian and Zhao, Wayne Xin and Lin, Leyu and Wen, Ji-Rong},
  year = {2024},
  doi = {10.1145/3589334.3645537},
  booktitle = {The ACM Web Conference}
}",Algorithmic contributions,Media / Communication / Entertainment,Individual,"Analyzing, Collaborating","Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-25157,pmlr,Language agents with reinforcement learning for strategic play in the werewolf game,"Agents built with large language models (LLMs) have shown great potential across a wide range of domains. However, in complex decision-making tasks, pure LLM-based agents tend to exhibit intrinsic bias in their choice of actions, which is inherited from the model’s training data and results in suboptimal performance. To develop <em>strategic language agents</em>, i.e., agents that generate flexible language actions and possess strong decision-making abilities, we propose a novel framework that powers LLM-based agents with reinforcement learning (RL). We consider Werewolf, a popular social deduction game, as a challenging testbed that emphasizes versatile communication and strategic gameplay. To mitigate the intrinsic bias in language actions, our agents use an LLM to perform deductive reasoning and generate a diverse set of action candidates. Then an RL policy trained to optimize the decision-making ability chooses an action from the candidates to play in the game. Extensive experiments show that our agents overcome the intrinsic bias and outperform existing LLM-based agents in the Werewolf game. We also conduct human-agent experiments and find that our agents achieve human-level performance and demonstrate strong strategic play.",NA,https://proceedings.mlr.press/v235/xu24ad.html,International Conference on Machine Learning (ICML),"Xu, Zelai;Yu, Chao;Fang, Fei;Wang, Yu;Wu, Yi",2024,2,"@inproceedings{2-25157,
  title={Language agents with reinforcement learning for strategic play in the werewolf game},
  author={Xu, Zelai and Yu, Chao and Fang, Fei and Wang, Yu and Wu, Yi},
  year={2024},
  booktitle={International Conference on Machine Learning (ICML)}
}","Algorithmic contributions, Methodological contributions","Media / Communication / Entertainment, Education / Teaching / Research",Individual,"Executing, Explaining",Knowledge provider,"Shape ethical norms, Change cognitive demands, Restrict human agency, Alter decision outcomes",Update AI competence,"recommendations, filtering, reasoning","contextual knowledge, second opinion presentation","Conversational/Natural Language, Autonomous System",Yes,Yes
2-25166,pmlr,Leader: learning attention over driving behaviors for planning under uncertainty,"Uncertainty in human behaviors poses a significant challenge to autonomous driving in crowded urban environments. The partially observable Markov decision process (POMDP) offers a principled general framework for decision making under uncertainty and achieves real-time performance for complex tasks by leveraging Monte Carlo sampling. However, sampling may miss rare, but critical events, leading to potential safety concerns. To tackle this challenge, we propose a new algorithm, LEarning Attention over Driving bEhavioRs (LEADER), which learns to attend to critical human behaviors during planning. LEADER learns a neural network generator to provide attention over human behaviors; it integrates the attention into a belief-space planner through importance sampling, which biases planning towards critical events. To train the attention generator, we form a minimax game between the generator and the planner. By solving this minimax game, LEADER learns to perform risk-aware planning without explicit human effort on data labeling.",NA,https://proceedings.mlr.press/v205/danesh23a.html,Conference on Robot Learning (CoRL),"Danesh, Mohamad Hosein;Cai, Panpan;Hsu, David",2023,15,"@inproceedings{2-25166,
  title     = {Leader: Learning Attention over Driving Behaviors for Planning under Uncertainty},
  author    = {Danesh, Mohamad Hosein and Cai, Panpan and Hsu, David},
  year      = {2023},
  booktitle = {Conference on Robot Learning (CoRL)}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Analyzing, Executing","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-2518,acm,Peer Grading the Peer Reviews: A Dual-Role Approach for Lightening the Scholarly Paper Review Process,"Scientific peer review is pivotal to maintain quality standards for academic publication. The effectiveness of the reviewing process is currently being challenged by the rapid increase of paper submissions in various conferences. Those venues need to recruit a large number of reviewers of different levels of expertise and background. The submitted reviews often do not meet the conformity standards of the conferences. Such a situation poses an ever-bigger burden on the meta-reviewers when trying to reach a final decision. In this work, we propose a human-AI approach that estimates the conformity of reviews to the conference standards. Specifically, we ask peers to grade each other’s reviews anonymously with respect to important criteria of review conformity such as sufficient justification and objectivity. We introduce a Bayesian framework that learns the conformity of reviews from both the peer grading process, historical reviews and decisions of a conference, while taking into account grading reliability. Our approach helps meta-reviewers easily identify reviews that require clarification and detect submissions requiring discussions while not inducing additional overhead from reviewers. Through a large-scale crowdsourced study where crowd workers are recruited as graders, we show that the proposed approach outperforms machine learning or review grades alone and that it can be easily integrated into existing peer review systems.",10.1145/3442381.3450088,https://doi.org/10.1145/3442381.3450088,International World Wide Web Conference (The Web Conference),"Arous, Ines; Yang, Jie; Khayati, Mourad; Cudre-Mauroux, Philippe",2021,0,"@inproceedings{2-2518,
  title     = {Peer Grading the Peer Reviews: A Dual-Role Approach for Lightening the Scholarly Paper Review Process},
  author    = {Arous, Ines and Yang, Jie and Khayati, Mourad and Cudre-Mauroux, Philippe},
  year      = {2021},
  doi       = {10.1145/3442381.3450088},
  booktitle = {Proceedings of the International World Wide Web Conference (The Web Conference)}
}",System/Artifact contributions,Education / Teaching / Research,Operational,"Analyzing, Monitoring, Collaborating",Decision-maker,"Change cognitive demands, Alter decision outcomes","Update AI competence, Change AI responses","recommendations, confidence score",clarification,"Textual, Conversational/Natural Language",Yes,Yes
2-25185,pmlr,Learning fair scoring functions: bipartite ranking under roc-based fairness constraints,"Many applications of AI involve scoring individuals using a learned function of their attributes. These predictive risk scores are then used to take decisions based on whether the score exceeds a certain threshold, which may vary depending on the context. The level of delegation granted to such systems in critical applications like credit lending and medical diagnosis will heavily depend on how questions of fairness can be answered. In this paper, we study fairness for the problem of learning scoring functions from binary labeled data, a classic learning task known as bipartite ranking. We argue that the functional nature of the ROC curve, the gold standard measure of ranking accuracy in this context, leads to several ways of formulating fairness constraints. We introduce general families of fairness definitions based on the AUC and on ROC curves, and show that our ROC-based constraints can be instantiated such that classifiers obtained by thresholding the scoring function satisfy classification fairness for a desired range of thresholds. We establish generalization bounds for scoring functions learned under such constraints, design practical learning algorithms and show the relevance our approach with numerical experiments on real and synthetic data.",NA,https://proceedings.mlr.press/v130/vogel21a.html,The International Conference on Artificial Intelligence and Statistics (AISTATS),"Vogel, Robin;Bellet, Aur{\\'e}lien;Cl{\\'e}men{\\c{c}}on, Stephan",2021,39,"@inproceedings{2-25185,
  title={Learning fair scoring functions: bipartite ranking under ROC-based fairness constraints},
  author={Vogel, Robin and Bellet, Aur{\'e}lien and Cl{\'e}men{\c{c}}on, Stephan},
  year={2021},
  booktitle={The International Conference on Artificial Intelligence and Statistics (AISTATS)}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery, Finance / Business / Economy",Operational,Forecasting,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-25188,pmlr,Learning from clinical judgments: semi-{m}arkov-modulated marked {h}awkes processes for risk prognosis,"Critically ill patients in regular wards are vulnerable to unanticipated adverse events which require prompt transfer to the intensive care unit (ICU). To allow for accurate prognosis of deteriorating patients, we develop a novel continuous-time probabilistic model for a monitored patient’s temporal sequence of physiological data. Our model captures “informatively sampled” patient episodes: the clinicians’ decisions on when to observe a hospitalized patient’s vital signs and lab tests over time are represented by a marked Hawkes process, with intensity parameters that are modulated by the patient’s latent clinical states, and with observable physiological data (mark process) modeled as a switching multi-task Gaussian process. In addition, our model captures “informatively censored” patient episodes by representing the patient’s latent clinical states as an absorbing semi-Markov jump process. The model parameters are learned from offline patient episodes in the electronic health records via an EM-based algorithm. Experiments conducted on a cohort of patients admitted to a major medical center over a 3-year period show that risk prognosis based on our model significantly outperforms the currently deployed medical risk scores and other baseline machine learning algorithms.",NA,https://proceedings.mlr.press/v70/alaa17a.html,International Conference on Machine Learning (ICML),Ahmed M. Alaa;Scott Hu;Mihaela van der Schaar,2017,74,"@inproceedings{2-25188,
  title     = {Learning from Clinical Judgments: Semi-Markov-Modulated Marked Hawkes Processes for Risk Prognosis},
  author    = {Ahmed M. Alaa and Scott Hu and Mihaela van der Schaar},
  year      = {2017},
  booktitle = {International Conference on Machine Learning (ICML)}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Analyzing","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-25195,pmlr,Learning individually fair classifier with path-specific causal-effect constraint,"Machine learning is used to make decisions for individuals in various fields, which require us to achieve good prediction accuracy while ensuring fairness with respect to sensitive features (e.g., race and gender). This problem, however, remains difficult in complex real-world scenarios. To quantify unfairness under such situations, existing methods utilize path-specific causal effects. However, none of them can ensure fairness for each individual without making impractical functional assumptions about the data. In this paper, we propose a far more practical framework for learning an individually fair classifier. To avoid restrictive functional assumptions, we define the probability of individual unfairness (PIU) and solve an optimization problem where PIU’s upper bound, which can be estimated from data, is controlled to be close to zero. We elucidate why our method can guarantee fairness for each individual. Experimental results show that our method can learn an individually fair classifier at a slight cost of accuracy.",NA,https://proceedings.mlr.press/v130/chikahara21a.html,The International Conference on Artificial Intelligence and Statistics (AISTATS),"Chikahara, Yoichi;Sakaue, Shinsaku;Fujino, Akinori;Kashima, Hisashi",2021,22,"@inproceedings{2-25195,
  title={Learning individually fair classifier with path-specific causal-effect constraint},
  author={Chikahara, Yoichi and Sakaue, Shinsaku and Fujino, Akinori and Kashima, Hisashi},
  year={2021},
  booktitle={The International Conference on Artificial Intelligence and Statistics (AISTATS)}
}",Algorithmic contributions,"Finance / Business / Economy, Generic / Abstract / Domain-agnostic",Institutional,"Forecasting, Executing",Decision-subject,NA,NA,NA,NA,NA,Yes,No
2-25205,pmlr,Learning optimal fair policies,"Systematic discriminatory biases present in our society influence the way data is collected and stored, the way variables are defined, and the way scientific findings are put into practice as policy. Automated decision procedures and learning algorithms applied to such data may serve to perpetuate existing injustice or unfairness in our society. In this paper, we consider how to make optimal but fair decisions, which “break the cycle of injustice” by correcting for the unfair dependence of both decisions and outcomes on sensitive features (e.g., variables that correspond to gender, race, disability, or other protected attributes). We use methods from causal inference and constrained optimization to learn optimal policies in a way that addresses multiple potential biases which afflict data analysis in sensitive contexts, extending the approach of Nabi & Shpitser (2018). Our proposal comes equipped with the theoretical guarantee that the chosen fair policy will induce a joint distribution for new instances that satisfies given fairness constraints. We illustrate our approach with both synthetic data and real criminal justice data.",NA,https://proceedings.mlr.press/v97/nabi19a.html,International Conference on Machine Learning (ICML),"Nabi, Razieh;Malinsky, Daniel;Shpitser, Ilya",2019,143,"@inproceedings{2-25205,
  title={Learning optimal fair policies},
  author={Nabi, Razieh and Malinsky, Daniel and Shpitser, Ilya},
  year={2019},
  booktitle={International Conference on Machine Learning (ICML)}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance",Institutional,Executing,"Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-25209,pmlr,"Learning representations by humans, for humans","When machine predictors can achieve higher performance than the human decision-makers they support, improving the performance of human decision-makers is often conflated with improving machine accuracy. Here we propose a framework to directly support human decision-making, in which the role of machines is to reframe problems rather than to prescribe actions through prediction. Inspired by the success of representation learning in improving performance of machine predictors, our framework learns human-facing representations optimized for human performance. This “Mind Composed with Machine” framework incorporates a human decision-making model directly into the representation learning paradigm and is trained with a novel human-in-the-loop training procedure. We empirically demonstrate the successful application of the framework to various tasks and representational forms.",NA,https://proceedings.mlr.press/v139/hilgard21a.html,International Conference on Machine Learning (ICML),"Hilgard, Sophie;Rosenfeld, Nir;Banaji, Mahzarin R;Cao, Jack;Parkes, David",2021,1,"@inproceedings{2-25209,
  title={Learning representations by humans, for humans},
  author={Hilgard, Sophie and Rosenfeld, Nir and Banaji, Mahzarin R and Cao, Jack and Parkes, David},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2021}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy, Healthcare / Medicine / Surgery",no such info,"Advising, Analyzing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-25222,pmlr,Learning to defer to a population: a meta-learning approach,"The learning to defer (L2D) framework allows autonomous systems to be safe and robust by allocating difficult decisions to a human expert. All existing work on L2D assumes that each expert is well-identified, and if any expert were to change, the system should be re-trained. In this work, we alleviate this constraint, formulating an L2D system that can cope with never-before-seen experts at test-time. We accomplish this by using meta-learning, considering both optimization- and model-based variants. Given a small context set to characterize the currently available expert, our framework can quickly adapt its deferral policy. For the model-based approach, we employ an attention mechanism that is able to look for points in the context set that are similar to a given test point, leading to an even more precise assessment of the expert’s abilities. In the experiments, we validate our methods on image recognition, traffic sign detection, and skin lesion diagnosis benchmarks.",NA,https://proceedings.mlr.press/v238/tailor24a.html,The International Conference on Artificial Intelligence and Statistics (AISTATS),"Tailor, Dharmesh;Patra, Aditya;Verma, Rajeev;Manggala, Putra;Nalisnick, Eric",2024,23,"@inproceedings{2-25222,
  title     = {Learning to defer to a population: a meta-learning approach},
  author    = {Tailor, Dharmesh and Patra, Aditya and Verma, Rajeev and Manggala, Putra and Nalisnick, Eric},
  year      = {2024},
  booktitle = {The International Conference on Artificial Intelligence and Statistics (AISTATS)}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Transportation / Mobility / Planning, Healthcare / Medicine / Surgery, Education / Teaching / Research",Operational,"Analyzing, Collaborating, Forecasting","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-25255,pmlr,Making decisions that reduce discriminatory impacts,"As machine learning algorithms move into real-world settings, it is crucial to ensure they are aligned with societal values. There has been much work on one aspect of this, namely the discriminatory prediction problem: How can we reduce discrimination in the predictions themselves? While an important question, solutions to this problem only apply in a restricted setting, as we have full control over the predictions. Often we care about the non-discrimination of quantities we do not have full control over. Thus, we describe another key aspect of this challenge, the discriminatory impact problem: How can we reduce discrimination arising from the real-world impact of decisions? To address this, we describe causal methods that model the relevant parts of the real-world system in which the decisions are made. Unlike previous approaches, these models not only allow us to map the causal pathway of a single decision, but also to model the effect of interference–how the impact on an individual depends on decisions made about other people. Often, the goal of decision policies is to maximize a beneficial impact overall. To reduce the discrimination of these benefits, we devise a constraint inspired by recent work in counterfactual fairness, and give an efficient procedure to solve the constrained optimization problem. We demonstrate our approach with an example: how to increase students taking college entrance exams in New York City public schools.",NA,https://proceedings.mlr.press/v97/kusner19a.html,International Conference on Machine Learning (ICML),"Kusner, Matt;Russell, Chris;Loftus, Joshua;Silva, Ricardo",2019,22,"@inproceedings{2-25255,
  title={Making decisions that reduce discriminatory impacts},
  author={Kusner, Matt and Russell, Chris and Loftus, Joshua and Silva, Ricardo},
  year={2019},
  booktitle={International Conference on Machine Learning (ICML)}
}",Methodological contributions,"Law / Policy / Governance, Education / Teaching / Research",Institutional,"Executing, Advising","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-2529,acm,Off-Policy Learning-to-Bid with AuctionGym,"Online advertising opportunities are sold through auctions, billions of times every day across the web. Advertisers who participate in those auctions need to decide on a bidding strategy: how much they are willing to bid for a given impression opportunity. Deciding on such a strategy is not a straightforward task, because of the interactive and reactive nature of the repeated auction mechanism. Indeed, an advertiser does not observe counterfactual outcomes of bid amounts that were not submitted, and successful advertisers will adapt their own strategies based on bids placed by competitors. These characteristics complicate effective learning and evaluation of bidding strategies based on logged data alone.The interactive and reactive nature of the bidding problem lends itself to a bandit or reinforcement learning formulation, where a bidding strategy can be optimised to maximise cumulative rewards. Several design choices then need to be made regarding parameterisation, model-based or model-free approaches, and the formulation of the objective function. This work provides a unified framework for such ""learning to bid” methods, showing how many existing approaches fall under the value-based paradigm. We then introduce novel policy-based and doubly robust formulations of the bidding problem. To allow for reliable and reproducible offline validation of such methods without relying on sensitive proprietary data, we introduce AuctionGym: a simulation environment that enables the use of bandit learning for bidding strategies in online advertising auctions. We present results from a suite of experiments under varying environmental conditions, unveiling insights that can guide practitioners who need to decide on a model class. Empirical observations highlight the effectiveness of our newly proposed methods. AuctionGym is released under an open-source license, and we expect the research community to benefit from this tool.",10.1145/3580305.3599877,https://doi.org/10.1145/3580305.3599877,ACM SIGKDD Conference on Knowledge Discovery and Data Mining,"Jeunen, Olivier; Murphy, Sean; Allison, Ben",2023,17,"@inproceedings{2-2529,
  title     = {Off-Policy Learning-to-Bid with AuctionGym},
  author    = {Jeunen, Olivier and Murphy, Sean and Allison, Ben},
  year      = {2023},
  booktitle = {Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi       = {10.1145/3580305.3599877}
}",System/Artifact contributions,Finance / Business / Economy,Operational,"Advising, Analyzing, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-2533,acm,"""Why is 'Chicago' deceptive?"" Towards Building Model-Driven Tutorials for Humans","To support human decision making with machine learning models, we often need to elucidate patterns embedded in the models that are unsalient, unknown, or counterintuitive to humans. While existing approaches focus on explaining machine predictions with real-time assistance, we explore model-driven tutorials to help humans understand these patterns in a train- ing phase. We consider both tutorials with guidelines from scientific papers, analogous to current practices of science communication, and automatically selected examples from training data with explanations. We use deceptive review detection as a testbed and conduct large-scale, randomized human-subject experiments to examine the effectiveness of such tutorials. We find that tutorials indeed improve human performance, with and without real-time assistance. In particular, although deep learning provides superior predictive performance than simple models, tutorials and explanations from simple models are more useful to humans. Our work suggests future directions for human-centered tutorials and explanations towards a synergy between humans and AI.",10.1145/3313831.3376873,https://doi.org/10.1145/3313831.3376873,Conference on Human Factors in Computing Systems,"Lai, Vivian; Liu, Han; Tan, Chenhao",2020,4,"@inproceedings{2-2533,
  title = {Why is 'Chicago' Deceptive? Towards Building Model-Driven Tutorials for Humans},
  author = {Lai, Vivian and Liu, Han and Tan, Chenhao},
  year = {2020},
  booktitle = {Conference on Human Factors in Computing Systems},
  doi = {10.1145/3313831.3376873}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Operational,"Explaining, Advising","Decision-maker, Knowledge provider","Alter decision outcomes, Change cognitive demands, Change affective-perceptual",no such info,"tutorials with guidelines from scientific papers, automatically selected examples from training data with explanations, signed highlights, predicted labels, guidelines, accuracy statement, selected examples with spaced repetition tutorial",NA,Interactive interface,Yes,Yes
2-25376,pmlr,On the adversarial robustness of causal algorithmic recourse,"Algorithmic recourse seeks to provide actionable recommendations for individuals to overcome unfavorable classification outcomes from automated decision-making systems. Recourse recommendations should ideally be robust to reasonably small uncertainty in the features of the individual seeking recourse. In this work, we formulate the adversarially robust recourse problem and show that recourse methods that offer minimally costly recourse fail to be robust. We then present methods for generating adversarially robust recourse for linear and for differentiable classifiers. Finally, we show that regularizing the decision-making classifier to behave locally linearly and to rely more strongly on actionable features facilitates the existence of adversarially robust recourse.",NA,https://proceedings.mlr.press/v162/dominguez-olmedo22a.html,International Conference on Machine Learning (ICML),"Dominguez-Olmedo, Ricardo;Karimi, Amir H;Sch{\\o}lkopf, Bernhard",2022,93,"@inproceedings{2-25376,
  title = {On the adversarial robustness of causal algorithmic recourse},
  author = {Dominguez-Olmedo, Ricardo and Karimi, Amir H and Sch{\""o}lkopf, Bernhard},
  year = {2022},
  booktitle = {International Conference on Machine Learning (ICML)}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance, Finance / Business / Economy, Everyday / Employment / Public Service",Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-25398,pmlr,On validation and planning of an optimal decision rule with application in healthcare studies,"In the current era of personalized recommendation, one major interest is to develop an optimal individualized decision rule that assigns individuals with the best treatment option according to their covariates. Estimation of optimal decision rules (ODR) has been extensively investigated recently, however, at present, no testing procedure is proposed to verify whether these ODRs are significantly better than the naive decision rule that always assigning individuals to a fixed treatment option. In this paper, we propose a testing procedure for detecting the existence of an ODR that is better than the naive decision rule under the randomized trials. We construct the proposed test based on the difference of estimated value functions using the augmented inverse probability weighted method. The asymptotic distributions of the proposed test statistic under the null and local alternative hypotheses are established. Based on the established asymptotic distributions, we further develop a sample size calculation formula for testing the existence of an ODR in designing A/B tests. Extensive simulations and a real data application to a schizophrenia clinical trial data are conducted to demonstrate the empirical validity of the proposed methods.",NA,https://proceedings.mlr.press/v119/cai20b.html,International Conference on Machine Learning (ICML),"Cai, Hengrui;Lu, Wenbin;Song, Rui",2020,2,"@inproceedings{2-25398,
  title = {On validation and planning of an optimal decision rule with application in healthcare studies},
  author = {Cai, Hengrui and Lu, Wenbin and Song, Rui},
  year = {2020},
  booktitle = {International Conference on Machine Learning (ICML)}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,Executing,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-2541,acm,Preference Elicitation in Interactive and User-centered Algorithmic Recourse: an Initial Exploration,"Algorithmic Recourse aims to provide actionable explanations, or recourse plans, to overturn potentially unfavourable decisions taken by automated machine learning models. In this paper, we propose an interaction paradigm based on a guided interaction pattern aimed at both eliciting the users’ preferences and heading them toward effective recourse interventions. In a fictional task of money lending, we compare this approach with an exploratory interaction pattern based on a combination of alternative plans and the possibility of freely changing the configurations by the users themselves. Our results suggest that users may recognize that the guided interaction paradigm improves efficiency. However, they also feel less freedom to experiment with “what-if” scenarios. Nevertheless, the time spent on the purely exploratory interface tends to be perceived as a lack of efficiency, which reduces attractiveness, perspicuity, and dependability. Conversely, for the guided interface, more time on the interface seems to increase its attractiveness, perspicuity, and dependability while not impacting the perceived efficiency. That might suggest that this type of interfaces should combine these two approaches by trying to support exploratory behavior while gently pushing toward a guided effective solution.",10.1145/3627043.3659556,https://doi.org/10.1145/3627043.3659556,"ACM Conference on User Modeling, Adaptation and Personalization (UMAP)","Esfahani, Seyedehdelaram; De Toni, Giovanni; Lepri, Bruno; Passerini, Andrea; Tentori, Katya; Zancanaro, Massimo",2024,1,"@inproceedings{2-2541,
  title = {Preference Elicitation in Interactive and User-centered Algorithmic Recourse: an Initial Exploration},
  author = {Esfahani, Seyedehdelaram and De Toni, Giovanni and Lepri, Bruno and Passerini, Andrea and Tentori, Katya and Zancanaro, Massimo},
  year = {2024},
  doi = {10.1145/3627043.3659556},
  booktitle = {ACM Conference on User Modeling, Adaptation and Personalization (UMAP)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Individual,"Advising, Explaining",Decision-maker,"Alter decision outcomes, Restrict human agency, Change affective-perceptual, Change cognitive demands",Update AI competence,a combination of alternative plans,the possibility of freely changing the configurations,Interactive interface,Yes,Yes
2-2543,acm,(Beyond) Reasonable Doubt: Challenges that Public Defenders Face in Scrutinizing AI in Court,"Accountable use of AI systems in high-stakes settings relies on making systems contestable. In this paper we study efforts to contest AI systems in practice by studying how public defenders scrutinize AI in court. We present findings from interviews with 17 people in the U.S. public defense community to understand their perceptions of and experiences scrutinizing computational forensic software (CFS) — automated decision systems that the government uses to convict and incarcerate, such as facial recognition, gunshot detection, and probabilistic genotyping tools. We find that our participants faced challenges assessing and contesting CFS reliability due to difficulties (a) navigating how CFS is developed and used, (b) overcoming judges and jurors’ non-critical perceptions of CFS, and (c) gathering CFS expertise. To conclude, we provide recommendations that center the technical, social, and institutional context to better position interventions such as performance evaluations to support contestability in practice.",10.1145/3613904.3641902,https://doi.org/10.1145/3613904.3641902,CHI Conference on Human Factors in Computing Systems,"Jin, Angela; Salehi, Niloufar",2024,0,"@inproceedings{2-2543,
  author    = {Jin, Angela and Salehi, Niloufar},
  title     = {(Beyond) Reasonable Doubt: Challenges that Public Defenders Face in Scrutinizing AI in Court},
  year      = {2024},
  doi       = {10.1145/3613904.3641902},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Law / Policy / Governance,Organizational,"Forecasting, Analyzing, Advising","Decision-maker, Stakeholder, Guardian","Alter decision outcomes, Change trust",Shape AI for accountability,NA,"prior beliefs about technology, perceptions of other stakeholders, inability to understand important technical details",Autonomous System,Yes,Yes
2-2548,acm,Mediating Community-AI Interaction through Situated Explanation: The Case of AI-Led Moderation,"Artificial intelligence (AI) has become prevalent in our everyday technologies and impacts both individuals and communities. The explainable AI (XAI) scholarship has explored the philosophical nature of explanation and technical explanations, which are usually driven by experts in lab settings and can be challenging for laypersons to understand. In addition, existing XAI research tends to focus on the individual level. Little is known about how people understand and explain AI-led decisions in the community context. Drawing from XAI and activity theory, a foundational HCI theory, we theorize how explanation is situated in a community's shared values, norms, knowledge, and practices, and how situated explanation mediates community-AI interaction. We then present a case study of AI-led moderation, where community members collectively develop explanations of AI-led decisions, most of which are automated punishments. Lastly, we discuss the implications of this framework at the intersection of CSCW, HCI, and XAI.",10.1145/3415173,https://doi.org/10.1145/3415173,Proceedings of the ACM on Human-Computer Interaction,"Kou, Yubo; Gui, Xinning",2020,13,"@article{2-2548,
  title = {Mediating Community-AI Interaction through Situated Explanation: The Case of AI-Led Moderation},
  author = {Kou, Yubo and Gui, Xinning},
  year = {2020},
  doi = {10.1145/3415173},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Media / Communication / Entertainment",Operational,"Executing, Explaining","Knowledge provider, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-25492,pmlr,Popcorn: partially observed prediction constrained reinforcement learning,"Many medical decision-making tasks can be framed as partially observed Markov decision processes (POMDPs). However, prevailing two-stage approaches that first learn a POMDP and then solve it often fail because the model that best fits the data may not be well suited for planning. We introduce a new optimization objective that (a) produces both high-performing policies and high-quality generative models, even when some observations are irrelevant for planning, and (b) does so in batch off-policy settings that are typical in healthcare, when only retrospective data is available. We demonstrate our approach on synthetic examples and a challenging medical decision-making problem.",NA,https://proceedings.mlr.press/v108/futoma20a.html,International Conference on Artificial Intelligence and Statistics (AISTATS),"Futoma, Joseph;Hughes, Michael;Doshi-Velez, Finale",2020,0,"@inproceedings{2-25492,
  title     = {Popcorn: Partially Observed Prediction Constrained Reinforcement Learning},
  author    = {Futoma, Joseph and Hughes, Michael and Doshi-Velez, Finale},
  year      = {2020},
  booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,"Executing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-25498,pmlr,Position: social environment design should be further developed for {ai}-based policy-making,"Artificial Intelligence (AI) holds promise as a technology that can be used to improve government and economic policy-making. This paper proposes a new research agenda towards this end by introducing <b>Social Environment Design</b>, a general framework for the use of AI in automated policy-making that connects with the Reinforcement Learning, EconCS, and Computational Social Choice communities. The framework seeks to capture general economic environments, includes voting on policy objectives, and gives a direction for the systematic analysis of government and economic policy through AI simulation. We highlight key open problems for future research in AI-based policymaking. By solving these challenges, we hope to achieve various social welfare objectives, thereby promoting more ethical and responsible decision making.",NA,https://proceedings.mlr.press/v235/zhang24cl.html,International Conference on Machine Learning (ICML),"Zhang, Edwin;Zhao, Sadie;Wang, Tonghan;Hossain, Safwan;Gasztowtt, Henry;Zheng, Stephan;Parkes, David C.;Tambe, Milind;Chen, Yiling",2024,8,"@inproceedings{2-25498,
  title = {Position: Social Environment Design Should Be Further Developed for AI-Based Policy-Making},
  author = {Zhang, Edwin and Zhao, Sadie and Wang, Tonghan and Hossain, Safwan and Gasztowtt, Henry and Zheng, Stephan and Parkes, David C. and Tambe, Milind and Chen, Yiling},
  year = {2024},
  booktitle = {International Conference on Machine Learning (ICML)}
}",Theoretical contributions,Law / Policy / Governance,Institutional,"Analyzing, Advising","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-25510,pmlr,Predictive performance comparison of decision policies under confounding,"Predictive models are often introduced to decision-making tasks under the rationale that they improve performance over an existing decision-making policy. However, it is challenging to compare predictive performance against an existing decision-making policy that is generally under-specified and dependent on unobservable factors. These sources of uncertainty are often addressed in practice by making strong assumptions about the data-generating mechanism. In this work, we propose a method to compare the predictive performance of decision policies under a variety of modern identification approaches from the causal inference and off-policy evaluation literatures (e.g., instrumental variable, marginal sensitivity model, proximal variable). Key to our method is the insight that there are regions of uncertainty that we can safely ignore in the policy comparison. We develop a practical approach for finite-sample estimation of regret intervals under no assumptions on the parametric form of the status quo policy. We verify our framework theoretically and via synthetic data experiments. We conclude with a real-world application using our framework to support a pre-deployment evaluation of a proposed modification to a healthcare enrollment policy.",NA,https://proceedings.mlr.press/v235/guerdan24a.html,International Conference on Machine Learning (ICML),"Guerdan, Luke;Coston, Amanda Lee;Holstein, Ken;Wu, Steven",2024,2,"@inproceedings{2-25510,
  title     = {Predictive performance comparison of decision policies under confounding},
  author    = {Guerdan, Luke and Coston, Amanda Lee and Holstein, Ken and Wu, Steven},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2024}
}",Methodological contributions,"Healthcare / Medicine / Surgery, Generic / Abstract / Domain-agnostic",Operational,"Analyzing, Forecasting","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-2552,acm,Crossmod: A Cross-Community Learning-based System to Assist Reddit Moderators,"In this paper, we introduce a novel sociotechnical moderation system for Reddit called Crossmod. Through formative interviews with 11 active moderators from 10 different subreddits, we learned about the limitations of currently available automated tools, and how a new system could extend their capabilities. Developed out of these interviews, Crossmod makes its decisions based on cross-community learning—an approach that leverages a large corpus of previous moderator decisions via an ensemble of classifiers. Finally, we deployed Crossmod in a controlled environment, simulating real-time conversations from two large subreddits with over 10M subscribers each. To evaluate Crossmod's moderation recommendations, 4 moderators reviewed comments scored by Crossmod that had been drawn randomly from existing threads. Crossmod achieved an overall accuracy of 86% when detecting comments that would be removed by moderators, with high recall (over 87.5%). Additionally, moderators reported that they would have removed 95.3% of the comments flagged by Crossmod; however, 98.3% of these comments were still online at the time of this writing (i.e., not removed by the current moderation system). To the best of our knowledge, Crossmod is the first open source, AI-backed sociotechnical moderation system to be designed using participatory methods.",10.1145/3359276,https://doi.org/10.1145/3359276,Proceedings of the ACM on Human-Computer Interaction,"Chandrasekharan, Eshwar; Gandhi, Chaitrali; Mustelier, Matthew Wortley; Gilbert, Eric",2019,228,"@article{2-2552,
  title={Crossmod: A Cross-Community Learning-based System to Assist Reddit Moderators},
  author={Chandrasekharan, Eshwar and Gandhi, Chaitrali and Mustelier, Matthew Wortley and Gilbert, Eric},
  year={2019},
  doi={10.1145/3359276},
  journal={Proceedings of the ACM on Human-Computer Interaction}
}",System/Artifact contributions,Media / Communication / Entertainment,Operational,"Advising, Analyzing","Decision-maker, Guardian",Alter decision outcomes,"Shape AI for accountability, Change AI responses","cross-community learning, moderation recommendations",evaluation,Autonomous System,Yes,Yes
2-2553,acm,Bias Mitigation for Toxicity Detection via Sequential Decisions,"Increased social media use has contributed to the greater prevalence of abusive, rude, and offensive textual comments. Machine learning models have been developed to detect toxic comments online, yet these models tend to show biases against users with marginalized or minority identities (e.g., females and African Americans). Established research in debiasing toxicity classifiers often (1) takes a static or batch approach, assuming that all information is available and then making a one-time decision; and (2) uses a generic strategy to mitigate different biases (e.g., gender and racial biases) that assumes the biases are independent of one another. However, in real scenarios, the input typically arrives as a sequence of comments/words over time instead of all at once. Thus, decisions based on partial information must be made while additional input is arriving. Moreover, social bias is complex by nature. Each type of bias is defined within its unique context, which, consistent with intersectionality theory within the social sciences, might be correlated with the contexts of other forms of bias. In this work, we consider debiasing toxicity detection as a sequential decision-making process where different biases can be interdependent. In particular, we study debiasing toxicity detection with two aims: (1) to examine whether different biases tend to correlate with each other; and (2) to investigate how to jointly mitigate these correlated biases in an interactive manner to minimize the total amount of bias. At the core of our approach is a framework built upon theories of sequential Markov Decision Processes that seeks to maximize the prediction accuracy and minimize the bias measures tailored to individual biases. Evaluations on two benchmark datasets empirically validate the hypothesis that biases tend to be correlated and corroborate the effectiveness of the proposed sequential debiasing strategy.",10.1145/3477495.3531945,https://doi.org/10.1145/3477495.3531945,International ACM SIGIR Conference on Research and Development in Information Retrieval,"Cheng, Lu; Mosallanezhad, Ahmadreza; Silva, Yasin N.; Hall, Deborah L.; Liu, Huan",2022,46,"@inproceedings{2-2553,
  title     = {Bias Mitigation for Toxicity Detection via Sequential Decisions},
  author    = {Cheng, Lu and Mosallanezhad, Ahmadreza and Silva, Yasin N. and Hall, Deborah L. and Liu, Huan},
  year      = {2022},
  booktitle = {International ACM SIGIR Conference on Research and Development in Information Retrieval},
  doi       = {10.1145/3477495.3531945}
}",Methodological contributions,Media / Communication / Entertainment,Operational,"Advising, Auditing, Analyzing","Guardian, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-2554,acm,Using affordances to improve AI support of social media posting decisions,"Intelligent systems are limited in their ability to match the fluid social needs of people. We use affordances—people's perceptions of the utilities of a target system—as a means of creating models that provide intelligent systems with a better understanding of how people make decisions. We study affordance-based models in the context of social network site (SNS) usage, a domain where people have complex social needs often poorly supported by technology. Using data collected via a scenario-based survey (N=674), we build two affordance-based models about people's multi-SNS posting behavior. Our results highlight the feasibility of using affordances to help intelligent systems support people's decision-making behavior: both of our models are 15% more accurate than a majority-class baseline, and they are 33% and 48% more accurate than a random baseline for this task. We contrast our approach with other ways of modeling posting behavior and discuss the implications of using affordances for modeling human behavior for intelligent systems.",10.1145/3377325.3377504,https://doi.org/10.1145/3377325.3377504,International Conference on Intelligent User Interfaces (IUI),"Kaur, Harmanpreet; Lampe, Cliff; Lasecki, Walter S.",2020,6,"@inproceedings{2-2554,
  title={Using affordances to improve AI support of social media posting decisions},
  author={Kaur, Harmanpreet and Lampe, Cliff and Lasecki, Walter S.},
  year={2020},
  doi={10.1145/3377325.3377504},
  booktitle={International Conference on Intelligent User Interfaces (IUI)}
}",Algorithmic contributions,Media / Communication / Entertainment,Individual,"Analyzing, Advising",Decision-maker,"Alter decision outcomes, Change cognitive demands",no such info,recommendations,NA,Textual,Yes,Yes
2-2557,acm,Human-AI Learning Performance in Multi-Armed Bandits,"People frequently face challenging decision-making problems in which outcomes are uncertain or unknown. Artificial intelligence (AI) algorithms exist that can outperform humans at learning such tasks. Thus, there is an opportunity for AI agents to assist people in learning these tasks more effectively. In this work, we use a multi-armed bandit as a controlled setting in which to explore this direction. We pair humans with a selection of agents and observe how well each human-agent team performs. We find that team performance can beat both human and agent performance in isolation. Interestingly, we also find that an agent's performance in isolation does not necessarily correlate with the human-agent team's performance. A drop in agent performance can lead to a disproportionately large drop in team performance, or in some settings can even improve team performance. Pairing a human with an agent that performs slightly better than them can make them perform much better, while pairing them with an agent that performs the same can make them them perform much worse. Further, our results suggest that people have different exploration strategies and might perform better with agents that match their strategy. Overall, optimizing human-agent team performance requires going beyond optimizing agent performance, to understanding how the agent's suggestions will influence human decision-making.",10.1145/3306618.3314245,https://doi.org/10.1145/3306618.3314245,"AAAI/ACM Conference on AI, Ethics, and Society","Pandya, Ravi; Huang, Sandy H.; Hadfield-Menell, Dylan; Dragan, Anca D.",2019,80,"@inproceedings{2-2557,
  title = {Human-AI Learning Performance in Multi-Armed Bandits},
  author = {Pandya, Ravi and Huang, Sandy H. and Hadfield-Menell, Dylan and Dragan, Anca D.},
  year = {2019},
  doi = {10.1145/3306618.3314245},
  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society}
}","Empirical contributions, Algorithmic contributions",Generic / Abstract / Domain-agnostic,Individual,"Advising, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-2562,acm,"The Good, the Bad, and the Unflinchingly Selfish: Cooperative Decision-Making can be Predicted with high Accuracy when using only Three Behavioral Types","The human willingness to pay costs to benefit anonymous others is often explained by social preferences: rather than only valuing their own material payoff, people also care in some fashion about the outcomes of others. But how successful is this concept of outcome-based social preferences for actually predicting out-of-sample behavior? We investigate this question by having 1067 human subjects each make 20 cooperation decisions, and using machine learning to predict their last 5 choices based on their first 15. We find that decisions can be predicted with high accuracy by models that include outcome-based features and allow for heterogeneity across individuals in baseline cooperativeness and the weights placed on the outcome-based features (AUC=0.89). It is not necessary, however, to have a fully heterogeneous model – excellent predictive power (AUC=0.88) is achieved by a model that allows three different sets of baseline cooperativeness and feature weights (i.e. three behavioral types), defined based on the participant's cooperation frequency in the 15 training trials: those who cooperated at least half the time, those who cooperated less than half the time, and those who never cooperated. Finally, we provide evidence that this inclination to cooperate cannot be well proxied by other personality/morality survey measures or demographics, and thus is a natural kind (or ""cooperative phenotype"").",10.1145/2940716.2940761,https://doi.org/10.1145/2940716.2940761,ACM Conference on Economics and Computation (EC),"Epstein, Ziv; Peysakhovich, Alexander; Rand, David G.",2016,23,"@inproceedings{2-2562,
  title = {The Good, the Bad, and the Unflinchingly Selfish: Cooperative Decision-Making can be Predicted with High Accuracy when Using Only Three Behavioral Types},
  author = {Epstein, Ziv and Peysakhovich, Alexander and Rand, David G.},
  year = {2016},
  doi = {10.1145/2940716.2940761},
  booktitle = {ACM Conference on Economics and Computation (EC)}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Forecasting, Executing, Advising","Decision-maker, Decision-subject",no such info,Update AI competence,outcome-based features,individual heterogeneity,NA,Yes,Yes
2-25670,pmlr,Sample efficient learning of predictors that complement humans,"One of the goals of learning algorithms is to complement and reduce the burden on human decision makers. The expert deferral setting wherein an algorithm can either predict on its own or defer the decision to a downstream expert helps accomplish this goal. A fundamental aspect of this setting is the need to learn complementary predictors that improve on the human’s weaknesses rather than learning predictors optimized for average error. In this work, we provide the first theoretical analysis of the benefit of learning complementary predictors in expert deferral. To enable efficiently learning such predictors, we consider a family of consistent surrogate loss functions for expert deferral and analyze their theoretical properties. Finally, we design active learning schemes that require minimal amount of data of human expert predictions in order to learn accurate deferral systems.",NA,https://proceedings.mlr.press/v162/charusaie22a.html,International Conference on Machine Learning (ICML),"Charusaie, Mohammad-Amin;Mozannar, Hussein;Sontag, David;Samadi, Samira",2022,58,"@inproceedings{2-25670,
  title={Sample efficient learning of predictors that complement humans},
  author={Charusaie, Mohammad-Amin and Mozannar, Hussein and Sontag, David and Samadi, Samira},
  year={2022},
  booktitle={International Conference on Machine Learning (ICML)}
}","Algorithmic contributions, Theoretical contributions","Generic / Abstract / Domain-agnostic, Education / Teaching / Research",Individual,"Analyzing, Forecasting, Executing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-25741,pmlr,Stochastic methods for auc optimization subject to auc-based fairness constraints,"As machine learning being used increasingly in making high-stakes decisions, an arising challenge is to avoid unfair AI systems that lead to discriminatory decisions for protected population. A direct approach for obtaining a fair predictive model is to train the model through optimizing its prediction performance subject to fairness constraints. Among various fairness constraints, the ones based on the area under the ROC curve (AUC) are emerging recently because they are threshold-agnostic and effective for unbalanced data. In this work, we formulate the problem of training a fairness-aware predictive model as an AUC optimization problem subject to a class of AUC-based fairness constraints. This problem can be reformulated as a min-max optimization problem with min-max constraints, which we solve by stochastic first-order methods based on a new Bregman divergence designed for the special structure of the problem. We numerically demonstrate the effectiveness of our approach on real-world data under different fairness metrics.",NA,https://proceedings.mlr.press/v206/yao23b.html,The International Conference on Artificial Intelligence and Statistics (AISTATS),"Yao, Yao;Lin, Qihang;Yang, Tianbao",2023,12,"@inproceedings{2-25741,
  title     = {Stochastic Methods for AUC Optimization Subject to AUC-based Fairness Constraints},
  author    = {Yao, Yao and Lin, Qihang and Yang, Tianbao},
  year      = {2023},
  booktitle = {The International Conference on Artificial Intelligence and Statistics (AISTATS)}
}",Algorithmic contributions,"Finance / Business / Economy, Law / Policy / Governance, Generic / Abstract / Domain-agnostic",no such info,Forecasting,"Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-25746,pmlr,Strategic instrumental variable regression: recovering causal relationships from strategic responses,"In settings where Machine Learning (ML) algorithms automate or inform consequential decisions about people, individual decision subjects are often incentivized to strategically modify their observable attributes to receive more favorable predictions. As a result, the distribution the assessment rule is trained on may differ from the one it operates on in deployment. While such distribution shifts, in general, can hinder accurate predictions, our work identifies a unique opportunity associated with shifts due to strategic responses: We show that we can use strategic responses effectively to recover causal relationships between the observable features and outcomes we wish to predict, even under the presence of unobserved confounding variables. Specifically, our work establishes a novel connection between strategic responses to ML models and instrumental variable (IV) regression by observing that the sequence of deployed models can be viewed as an instrument that affects agents’ observable features but does not directly influence their outcomes. We show that our causal recovery method can be utilized to improve decision-making across several important criteria: individual fairness, agent outcomes, and predictive risk. In particular, we show that if decision subjects differ in their ability to modify non-causal attributes, any decision rule deviating from the causal coefficients can lead to (potentially unbounded) individual-level unfairness. .",NA,https://proceedings.mlr.press/v162/harris22a.html,International Conference on Machine Learning (ICML),"Harris, Keegan;Ngo, Dung Daniel T;Stapleton, Logan;Heidari, Hoda;Wu, Steven",2022,0,"@inproceedings{2-25746,
  title={Strategic instrumental variable regression: recovering causal relationships from strategic responses},
  author={Harris, Keegan and Ngo, Dung Daniel T and Stapleton, Logan and Heidari, Hoda and Wu, Steven},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2022}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Operational,Forecasting,"Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-25754,pmlr,Superhuman fairness,"The fairness of machine learning-based decisions has become an increasingly important focus in the design of supervised machine learning methods. Most fairness approaches optimize a specified trade-off between performance measure(s) (e.g., accuracy, log loss, or AUC) and fairness metric(s) (e.g., demographic parity, equalized odds). This begs the question: are the right performance-fairness trade-offs being specified? We instead re-cast fair machine learning as an imitation learning task by introducing superhuman fairness, which seeks to simultaneously outperform human decisions on multiple predictive performance and fairness measures. We demonstrate the benefits of this approach given suboptimal decisions.",NA,https://proceedings.mlr.press/v202/memarrast23a.html,International Conference on Machine Learning (ICML),"Memarrast, Omid;Vu, Linh;Ziebart, Brian D",2023,0,"@inproceedings{2-25754,
  title     = {Superhuman fairness},
  author    = {Memarrast, Omid and Vu, Linh and Ziebart, Brian D},
  year      = {2023},
  booktitle = {International Conference on Machine Learning (ICML)}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Institutional,Executing,"Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-25850,pmlr,Using {ai} uncertainty quantification to improve human decision-making,"AI Uncertainty Quantification (UQ) has the potential to improve human decision-making beyond AI predictions alone by providing additional probabilistic information to users. The majority of past research on AI and human decision-making has concentrated on model explainability and interpretability, with little focus on understanding the potential impact of UQ on human decision-making. We evaluated the impact on human decision-making for instance-level UQ, calibrated using a strict scoring rule, in two online behavioral experiments. In the first experiment, our results showed that UQ was beneficial for decision-making performance compared to only AI predictions. In the second experiment, we found UQ had generalizable benefits for decision-making across a variety of representations for probabilistic information. These results indicate that implementing high quality, instance-level UQ for AI may improve decision-making with real systems compared to AI predictions alone.",NA,https://proceedings.mlr.press/v235/marusich24a.html,International Conference on Machine Learning (ICML),"Marusich, Laura;Bakdash, Jonathan;Zhou, Yan;Kantarcioglu, Murat",2024,0,"@inproceedings{2-25850,
  title = {Using {AI} Uncertainty Quantification to Improve Human Decision-Making},
  author = {Marusich, Laura and Bakdash, Jonathan and Zhou, Yan and Kantarcioglu, Murat},
  year = {2024},
  booktitle = {International Conference on Machine Learning (ICML)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service, Finance / Business / Economy, Education / Teaching / Research",no such info,Forecasting,"Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-25897,sage,A machine learning approach to deal with ambiguity in the humanitarian decision‐making,"One of the major challenges for humanitarian organizations in response planning is dealing with the inherent ambiguity and uncertainty in disaster situations. The available information that comes from different sources in postdisaster settings may involve missing elements and inconsistencies, which can hamper effective humanitarian decision‐making. In this paper, we propose a new methodological framework based on graph clustering and stochastic optimization to support humanitarian decision‐makers in analyzing the implications of divergent estimates from multiple data sources on final decisions and efficiently integrating these estimates into decision‐making. To the best of our knowledge, the integration of ambiguous information into decision‐making by combining a cluster machine learning method with stochastic optimization has not been done before. We illustrate the proposed approach on a realistic case study that focuses on locating shelters to serve internally displaced people (IDP) in a conflict setting, specifically, the Syrian civil war. We use the needs assessment data from two different reliable sources to estimate the shelter needs in Idleb, a district of Syria. The analysis of data provided by two assessment sources has indicated a high degree of ambiguity due to inconsistent estimates. We apply the proposed methodology to integrate divergent estimates in making shelter location decisions. The results highlight that our methodology leads to higher satisfaction of demand for shelters than other approaches such as a classical stochastic programming model. Moreover, we show that our solution integrates information coming from both sources more efficiently thereby hedging against the ambiguity more effectively. With the newly proposed methodology, the decision‐maker is able to analyze the degree of ambiguity in the data and the degree of consensus between different data sources to ultimately make better decisions for delivering humanitarian aid.",10.1111/poms.14018,https://doi.org/10.1111/poms.14018,Production and Operations Management Society (POMS) Journal,Emilia Grass;Janosch Ortmann;Burcu Balcik;Walter Rei,2023,23,"@article{2-25897,
  title = {A Machine Learning Approach to Deal with Ambiguity in the Humanitarian Decision-Making},
  author = {Emilia Grass and Janosch Ortmann and Burcu Balcik and Walter Rei},
  year = {2023},
  doi = {10.1111/poms.14018},
  journal = {Production and Operations Management Society (POMS) Journal}
}",Methodological contributions,"Everyday / Employment / Public Service, Defense / Military / Emergency",Operational,"Analyzing, Advising","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-25901,sage,A sociotechnical systems framework for the application of artificial intelligence in health care delivery,"In the coming years, artificial intelligence (AI) will pervade almost every aspect of the health care delivery system. AI has the potential to improve patient safety (e.g., diagnostic accuracy) as well as reduce the burden on clinicians (e.g., documentation-related workload); however, these benefits are yet to be realized. AI is only one element of a larger sociotechnical system that needs to be considered for effective AI application. In this paper, we describe the current challenges of integrating AI into clinical care and propose a sociotechnical systems (STS) approach for AI design and implementation. We demonstrate the importance of an STS approach through a case study on the design and implementation of a clinical decision support (CDS). In order for AI to reach its potential, the entire work system as well as clinical workflow must be systematically considered throughout the design of AI technology.",10.1177/15553434221097357,https://doi.org/10.1177/15553434221097357,Journal of Cognitive Engineering and Decision Making,Megan E. Salwei;Pascale Carayon,2022,72,"@article{2-25901,
  title = {A sociotechnical systems framework for the application of artificial intelligence in health care delivery},
  author = {Megan E. Salwei and Pascale Carayon},
  year = {2022},
  doi = {10.1177/15553434221097357},
  journal = {Journal of Cognitive Engineering and Decision Making}
}","Empirical contributions, Theoretical contributions",Healthcare / Medicine / Surgery,Operational,"Advising, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-25906,sage,Algorithmic decision-making? The user interface and its role for human involvement in decisions supported by artificial intelligence,"Artificial intelligence can provide organizations with prescriptive options for decision-making. Based on the notions of algorithmic decision-making and user involvement, we assess the role of artificial intelligence in workplace decisions. Using a case study on the implementation and use of cognitive software in a telecommunications company, we address how actors can become distanced from or remain involved in decision-making. Our results show that humans are increasingly detached from decision-making spatially as well as temporally and in terms of rational distancing and cognitive displacement. At the same time, they remain attached to decision-making because of accidental and infrastructural proximity, imposed engagement, and affective adhesion. When human and algorithmic intelligence become unbalanced in regard to humans’ attachment to decision-making, three performative effects result: deferred decisions, workarounds, and (data) manipulations. We conceptualize the user interface that presents decisions to humans as a mediator between human detachment and attachment and, thus, between algorithmic and humans’ decisions. These findings contrast the traditional view of automated media as diminishing user involvement and have useful implications for research on artificial intelligence and algorithmic decision-making in organizations.",10.1177/1350508419855714,https://doi.org/10.1177/1350508419855714,Organization,Verena Bader;Stephan Kaiser,2019,11,"@article{2-25906,
  title={Algorithmic decision-making? The user interface and its role for human involvement in decisions supported by artificial intelligence},
  author={Bader, Verena and Kaiser, Stephan},
  year={2019},
  doi={10.1177/1350508419855714},
  journal={Organization}
}",Empirical contributions,"Finance / Business / Economy, Generic / Abstract / Domain-agnostic",Operational,"Advising, Analyzing",Decision-maker,"Change affective-perceptual, Alter decision outcomes",Update AI competence,"recommendations, prediction of alternative",domain knowledge,"Visual, Textual, Semi-Autonomous System",Yes,Yes
2-25911,sage,An approach for combining clinical judgment with machine learning to inform medical decision making: analysis of nonemergency surgery strategies for acute appendicitis in patients with multiple long-term conditions,"Background Machine learning (ML) methods can identify complex patterns of treatment effect heterogeneity. However, before ML can help to personalize decision making, transparent approaches must be developed that draw on clinical judgment. We develop an approach that combines clinical judgment with ML to generate appropriate comparative effectiveness evidence for informing decision making. Methods We motivate this approach in evaluating the effectiveness of nonemergency surgery (NES) strategies, such as antibiotic therapy, for people with acute appendicitis who have multiple long-term conditions (MLTCs) compared with emergency surgery (ES). Our 4-stage approach 1) draws on clinical judgment about which patient characteristics and morbidities modify the relative effectiveness of NES; 2) selects additional covariates from a high-dimensional covariate space (P &gt; 500) by applying an ML approach, least absolute shrinkage and selection operator (LASSO), to large-scale administrative data (N = 24,312); 3) generates estimates of comparative effectiveness for relevant subgroups; and 4) presents evidence in a suitable form for decision making. Results This approach provides useful evidence for clinically relevant subgroups. We found that overall NES strategies led to increases in the mean number of days alive and out-of-hospital compared with ES, but estimates differed across subgroups, ranging from 21.2 (95% confidence interval: 1.8 to 40.5) for patients with chronic heart failure and chronic kidney disease to −10.4 (−29.8 to 9.1) for patients with cancer and hypertension. Our interactive tool for visualizing ML output allows for findings to be customized according to the specific needs of the clinical decision maker. Conclusions This principled approach of combining clinical judgment with an ML approach can improve trust, relevance, and usefulness of the evidence generated for clinical decision making. Highlights Machine learning (ML) methods have many potential applications in medical decision making, but the lack of model interpretability and usability constitutes an important barrier for the wider adoption of ML evidence in practice. We develop a 4-stage approach for integrating clinical judgment into the way an ML approach is used to estimate and report comparative effectiveness. We illustrate the approach in undertaking an evaluation of nonemergency surgery (NES) strategies for acute appendicitis in patients with multiple long-term conditions and find that NES strategies lead to better outcomes compared with emergency surgery and that the effects differ across subgroups. We develop an interactive tool for visualizing the results of this study that allows findings to be customized according to the user’s preferences.",10.1177/0272989X241289336,https://doi.org/10.1177/0272989X241289336,Medical Decision Making: An International Journal of the Society for Medical Decision Making,S. Moler-Zapata;A. Hutchings;R. Grieve;R. Hinchliffe;N. Smart;S. R. Moonesinghe;G. Bellingan;R. Vohra;S. Moug;S. O’Neill,2024,13,"@article{2-25911,
  title={An approach for combining clinical judgment with machine learning to inform medical decision making: analysis of nonemergency surgery strategies for acute appendicitis in patients with multiple long-term conditions},
  author={Moler-Zapata, S. and Hutchings, A. and Grieve, R. and Hinchliffe, R. and Smart, N. and Moonesinghe, S. R. and Bellingan, G. and Vohra, R. and Moug, S. and O’Neill, S.},
  year={2024},
  doi={10.1177/0272989X241289336},
  journal={Medical Decision Making: An International Journal of the Society for Medical Decision Making}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-25915,sage,Antecedents of trust and adoption intention toward artificially intelligent recommendation systems in travel planning: a heuristic–systematic model,"Drawing on the dual process theory, this study investigates the impacts of systematic and heuristic cues on travelers’ cognitive trust, emotional trust, and adoption intention toward artificial intelligence (AI)–based recommendation systems in travel planning. The moderating effect of perceived risk is also examined. Two studies with both scenario-based surveys and lab experiment approaches are conducted. Findings suggest that while travelers utilize both systematic and heuristic cues, effects of systematic cues on adoption as a decision aid is stronger than the effects of heuristic cues. Emotional trust has a stronger impact on intention to adopt as a delegated agent than cognitive trust. Perceived risk moderates the relationships between systematic and heuristic cues, trust, and adoption intentions. When travelers perceive high risk, they rely more on systematic cues through building cognitive trust. However, when the level of perceived risk is low, travelers depend more on heuristic cues through establishing emotional trust.",10.1177/0047287520966395,https://doi.org/10.1177/0047287520966395,Journal of Travel Research,Si Shi;Yuhuang Gong;Dogan Gursoy,2021,303,"@article{2-25915,
  title = {Antecedents of trust and adoption intention toward artificially intelligent recommendation systems in travel planning: a heuristic–systematic model},
  author = {Si Shi and Yuhuang Gong and Dogan Gursoy},
  year = {2021},
  doi = {10.1177/0047287520966395},
  journal = {Journal of Travel Research}
}",Empirical contributions,Transportation / Mobility / Planning,Individual,Advising,Decision-maker,"Change trust, Alter decision outcomes",no such info,"systematic cues, heuristic cues, recommendations",NA,"Autonomous System, Interactive interface",Yes,Yes
2-25927,sage,Artificial intelligence in utilitarian vs. Hedonic contexts: the “word-of-machine” effect,"Rapid development and adoption of AI, machine learning, and natural language processing applications challenge managers and policy makers to harness these transformative technologies. In this context, the authors provide evidence of a novel “word-of-machine” effect, the phenomenon by which utilitarian/hedonic attribute trade-offs determine preference for, or resistance to, AI-based recommendations compared with traditional word of mouth, or human-based recommendations. The word-of-machine effect stems from a lay belief that AI recommenders are more competent than human recommenders in the utilitarian realm and less competent than human recommenders in the hedonic realm. As a consequence, importance or salience of utilitarian attributes determine preference for AI recommenders over human ones, and importance or salience of hedonic attributes determine resistance to AI recommenders over human ones (Studies 1–4). The word-of machine effect is robust to attribute complexity, number of options considered, and transaction costs. The word-of-machine effect reverses for utilitarian goals if a recommendation needs matching to a person’s unique preferences (Study 5) and is eliminated in the case of human–AI hybrid decision making (i.e., augmented rather than artificial intelligence; Study 6). An intervention based on the consider-the-opposite protocol attenuates the word-of-machine effect (Studies 7a–b).",10.1177/0022242920957347,https://doi.org/10.1177/0022242920957347,Journal of Marketing,Chiara Longoni;Luca Cian,2022,811,"@article{2-25927,
  title = {Artificial intelligence in utilitarian vs. Hedonic contexts: the ""word-of-machine"" effect},
  author = {Chiara Longoni and Luca Cian},
  year = {2022},
  doi = {10.1177/0022242920957347},
  journal = {Journal of Marketing}
}",Empirical contributions,"Finance / Business / Economy, Everyday / Employment / Public Service",Individual,Advising,"Decision-maker, Guardian",Alter decision outcomes,Shape AI for accountability,recommendations,recommendations,Interactive interface,Yes,Yes
2-25938,sage,Building human-like artificial agents: a general cognitive algorithm for emulating human decision-making in dynamic environments,"One of the early goals of artificial intelligence (AI) was to create algorithms that exhibited behavior indistinguishable from human behavior (i.e., human-like behavior). Today, AI has diverged, often aiming to excel in tasks inspired by human capabilities and outperform humans, rather than replicating human cogntion and action. In this paper, I explore the overarching question of whether computational algorithms have achieved this initial goal of AI. I focus on dynamic decision-making, approaching the question from the perspective of computational cognitive science. I present a general cognitive algorithm that intends to emulate human decision-making in dynamic environments, as defined in instance-based learning theory (IBLT). I use the cognitive steps proposed in IBLT to organize and discuss current evidence that supports some of the human-likeness of the decision-making mechanisms. I also highlight the significant gaps in research that are required to improve current models and to create higher fidelity in computational algorithms to represent human decision processes. I conclude with concrete steps toward advancing the construction of algorithms that exhibit human-like behavior with the ultimate goal of supporting human dynamic decision-making.",10.1177/17456916231196766,https://doi.org/10.1177/17456916231196766,Perspectives on Psychological Science,Cleotilde Gonzalez,2024,0,"@article{2-25938,
  title={Building human-like artificial agents: a general cognitive algorithm for emulating human decision-making in dynamic environments},
  author={Gonzalez, Cleotilde},
  year={2024},
  journal={Perspectives on Psychological Science},
  doi={10.1177/17456916231196766}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Operational,"Analyzing, Executing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-25948,sage,Contested delegation: understanding critical public responses to algorithmic decision-making in the uk and australia,"In public and private sectors alike, decision-making is increasingly carried out through the employment of ‘algorithmic actors’ and artificial intelligence. The apparent efficiency of these means in the eyes of politicians and the public has made recourse to them possible. Along with this belief in their efficiency, however, fears emerge that nonhuman actors have displaced judicious human decision-making. This article examines this belief and its contestation, drawing on overlapping notions of ‘delegation’ in the political sociologies of Bruno Latour and Pierre Bourdieu. We undertake two case studies of attempts to delegate decision-making to algorithms: the 2020 UK ‘A-level’ grade determination and the Australian ‘robodebt’ welfare funds recovery scheme. In both cases, the decision-making delegated to algorithms was publicly discredited as critics invoked a different form of fairness than the one used by those deploying the technology. In the ‘A-level’ case, complainants drew on a grammar of individual merit, while complainants in the ‘robodebt’ case made a technical critique of the algorithm’s efficiency. Using a theory of delegation, we contribute to understanding how publics articulate resistance to automated decision-making.",10.1177/00380261221105380,https://doi.org/10.1177/00380261221105380,The Sociological Review,Geoffrey Mead;Barbara Barbosa Neves,2023,20,"@article{2-25948,
  title={Contested delegation: understanding critical public responses to algorithmic decision-making in the UK and Australia},
  author={Mead, Geoffrey and Neves, Barbara Barbosa},
  year={2023},
  doi={10.1177/00380261221105380},
  journal={The Sociological Review}
}",Empirical contributions,Law / Policy / Governance,Institutional,Executing,Decision-subject,"Restrict human agency, Shift responsibility, Alter decision outcomes, Change trust","Update AI competence, Shape AI for accountability",algorithmic biases,"delegation, reasserting human oversight",Autonomous System,Yes,Yes
2-25979,sage,For me or against me? Reactions to ai (vs. Human) decisions that are favorable or unfavorable to the self and the role of fairness perception,"Public reactions to algorithmic decisions often diverge. While high-profile media coverage suggests that the use of AI in organizational decision-making is viewed as unfair and received negatively, recent survey results suggest that such use of AI is perceived as fair and received positively. Drawing on fairness heuristic theory, the current research reconciles this apparent contradiction by examining the roles of decision outcome and fairness perception on individuals’ attitudinal (Studies 1–3, 5) and behavioral (Study 4) reactions to algorithmic (vs. human) decisions. Results from six experiments (N = 2,794) showed that when the decision was unfavorable, AI was perceived as fairer than human, leading to a less negative reaction. This heightened fairness perception toward AI is shaped by its perceived unemotionality. Furthermore, reminders about the potential biases of AI in decision-making attenuate the differential fairness perception between AI and human. Theoretical and practical implications of the findings are discussed.",10.1177/01461672241288338,https://doi.org/10.1177/01461672241288338,Personality and Social Psychology Bulletin,Jungmin Choi;Melody M. Chao,2024,0,"@article{2-25979,
  title = {For me or against me? Reactions to AI (vs. Human) decisions that are favorable or unfavorable to the self and the role of fairness perception},
  author = {Jungmin Choi and Melody M. Chao},
  year = {2024},
  doi = {10.1177/01461672241288338},
  journal = {Personality and Social Psychology Bulletin}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Executing, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-25983,sage,From making automated decision making visible to mapping the unknowable human: counter-mapping automated decision making in social services in australia,"Automated decision-making (ADM) technologies, like artificial intelligence and machine learning, are increasingly being used by governments. Researchers have attempted to map the deployment of these technologies. However, mapping is an inherently political act, reinforcing dominant discourses and imaginings of technological futures. In this article, I engage with critical cartography to outline the potential of counter-mapping for researching automation in decision making, with the purpose of mapping, to quote from Hodgson and Schroeder in 2002, “against dominant power structures, to further seemingly progressive goals.” Drawing on the case of ADM in Australian social services, I reflexively account for how counter-mapping can provide a method for moving beyond dominant discourses of efficiency, cost cutting, and industriousness, to allow the alternative voices of service users’ experiences of ADM to be heard. I argue that future ADM mapping needs to focus on making visible those who are subject to the decisions of automated systems, but are usually made unknowable by the over-confident calculability of dominant ADM discourses.",10.1177/10778004221096853,https://doi.org/10.1177/10778004221096853,Qualitative Inquiry,Lyndal Naomi Sleep,2022,9,"@article{2-25983,
  title={From making automated decision making visible to mapping the unknowable human: Counter-mapping automated decision making in social services in Australia},
  author={Sleep, Lyndal Naomi},
  year={2022},
  journal={Qualitative Inquiry},
  doi={10.1177/10778004221096853}
}",Methodological contributions,Law / Policy / Governance,Operational,Executing,"Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-25990,sage,How much reliability is enough? A context-specific view on human interaction with (artificial) agents from different perspectives,"Tasks classically performed by human–human teams in today’s workplaces are increasingly given to human–technology teams instead. The role of technology is not only played by classic decision support systems (DSSs) but more and more by artificial intelligence (AI). Reliability is a key factor influencing trust in technology. Therefore, we investigated the reliability participants require in order to perceive the support agents (human, AI, and DSS) as “highly reliable.” We then examined how trust differed between these highly reliable agents. Whilst there is a range of research identifying trust as an important determinant in human–DSS interaction, the question is whether these findings are also applicable to the interaction between humans and AI. To study these issues, we conducted an experiment (N = 300) with two different tasks, usually performed by dyadic teams (loan assignment and x-ray screening), from two different perspectives (i.e., working together or being evaluated by the agent). In contrast to our hypotheses, the required reliability if working together was equal regardless of the agent. Nevertheless, participants trusted the human more than an AI or DSS. They also required that AI be more reliable than a human when used to evaluate themselves, thus illustrating the importance of changing perspective.",10.1177/15553434221104615,https://doi.org/10.1177/15553434221104615,Journal of Cognitive Engineering and Decision Making,Ksenia Appelganc;Tobias Rieger;Eileen Roesler;Dietrich Manzey,2022,0,"@article{2-25990,
  title = {How much reliability is enough? A context-specific view on human interaction with (artificial) agents from different perspectives},
  author = {Appelganc, Ksenia and Rieger, Tobias and Roesler, Eileen and Manzey, Dietrich},
  year = {2022},
  doi = {10.1177/15553434221104615},
  journal = {Journal of Cognitive Engineering and Decision Making}
}",Empirical contributions,"Finance / Business / Economy, Healthcare / Medicine / Surgery",Operational,"Advising, Collaborating, Executing",Decision-maker,Change trust,no such info,"(continuous) support, system accuracy",reliability,"Textual, Visual",Yes,Yes
2-25998,sage,Improving fairness in criminal justice algorithmic risk assessments using optimal transport and conformal prediction sets,"In the United States and elsewhere, risk assessment algorithms are being used to help inform criminal justice decision-makers. A common intent is to forecast an offender’s “future dangerousness.” Such algorithms have been correctly criticized for potential unfairness, and there is an active cottage industry trying to make repairs. In this paper, we use counterfactual reasoning to consider the prospects for improved fairness when members of a disadvantaged class are treated by a risk algorithm as if they are members of an advantaged class. We combine a machine learning classifier trained in a novel manner with an optimal transport adjustment for the relevant joint probability distributions, which together provide a constructive response to claims of bias-in-bias-out. A key distinction is made between fairness claims that are empirically testable and fairness claims that are not. We then use confusion tables and conformal prediction sets to evaluate achieved fairness for estimated risk. Our data are a random sample of 300,000 offenders at their arraignments for a large metropolitan area in the United States during which decisions to release or detain are made. We show that substantial improvement in fairness can be achieved consistently with a Pareto improvement for legally protected classes.",10.1177/00491241231155883,https://doi.org/10.1177/00491241231155883,Sociological Methods & Research,Richard A. Berk;Arun Kumar Kuchibhotla;Eric Tchetgen Tchetgen,2024,6,"@article{2-25998,
  title={Improving fairness in criminal justice algorithmic risk assessments using optimal transport and conformal prediction sets},
  author={Berk, Richard A. and Kuchibhotla, Arun Kumar and Tchetgen Tchetgen, Eric},
  year={2024},
  doi={10.1177/00491241231155883},
  journal={Sociological Methods \& Research}
}",Methodological contributions,Law / Policy / Governance,Organizational,"Forecasting, Auditing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-26007,sage,"It’s a peoples game, isn’t it?! A comparison between the investment returns of business angels and machine learning algorithms","Investors increasingly use machine learning (ML) algorithms to support their early stage investment decisions. However, it remains unclear if algorithms can make better investment decisions and if so, why. Building on behavioral decision theory, our study compares the investment returns of an algorithm with those of 255 business angels (BAs) investing via an angel investment platform. We explore the influence of human biases and experience on BAs’ returns and find that investors only outperformed the algorithm when they had extensive investment experience and managed to suppress their cognitive biases. These results offer novel insights into the role of cognitive limitations, experience, and the use of algorithms in early stage investing.",10.1177/1042258720945206,https://doi.org/10.1177/1042258720945206,Entrepreneurship Theory and Practice,Ivo Blohm;Torben Antretter;Charlotta Sirén;Dietmar Grichnik;Joakim Wincent,2022,110,"@article{2-26007,
  title = {It's a peoples game, isn't it?! A comparison between the investment returns of business angels and machine learning algorithms},
  author = {Blohm, Ivo and Antretter, Torben and Sirén, Charlotta and Grichnik, Dietmar and Wincent, Joakim},
  year = {2022},
  doi = {10.1177/1042258720945206},
  journal = {Entrepreneurship Theory and Practice}
}",Empirical contributions,Finance / Business / Economy,Operational,"Advising, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-26010,sage,Justitia ex machina: the impact of an ai system on legal decision-making and discretionary authority,"Governments increasingly use algorithms to inform or supplant decision-making. Artificial Intelligence systems in particular are considered objective, consistent and efficient decision-makers, but have also been shown to be fallible. Furthermore, the adoption of artificial intelligence (AI) in government is fraught with challenges which are only partly understood and rarely studied in practice. In this paper, we draw on science and technology studies and human computer interaction and report on a critical case study of the development and use of an AI system for processing traffic violation appeal at a Dutch court. Although much empirical work on algorithms in practice is primarily observational in nature, we employ a canonical action research approach and actively participate in the development of the AI system. We draw on data collected in the form of interviews, observations, documents and a user-experiment. Based on this material we provide: 1. An in-depth empirical account of the tensions between street-level bureaucrats, screen-level bureaucrats and street-level algorithms; 2. An analysis of the differences between decisions made by, with and without the AI system and find that use of the AI systems impacts decisions made by legal experts; 3. A confirmation of earlier work that finds AI systems can best be applied in support of legal-decision making and demonstrate how the decision-making process of the traffic violation cases may mitigate some of the risks of algorithmic decision-making.",10.1177/20539517241255101,https://doi.org/10.1177/20539517241255101,Big Data & Society,Daan Kolkman;Floris Bex;Nitin Narayan;Manuella van der Put,2024,0,"@article{2-26010,
  title = {Justitia ex machina: the impact of an AI system on legal decision-making and discretionary authority},
  author = {Daan Kolkman and Floris Bex and Nitin Narayan and Manuella van der Put},
  year = {2024},
  doi = {10.1177/20539517241255101},
  journal = {Big Data \& Society}
}",Empirical contributions,Law / Policy / Governance,Operational,"Executing, Advising","Decision-maker, Stakeholder, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-26011,sage,Keeping humans in the loop: pooling knowledge through artificial swarm intelligence to improve business decision making,"This article explores how a collaboration technology called Artificial Swarm Intelligence (ASI) addresses the limitations associated with group decision making, amplifies the intelligence of human groups, and facilitates better business decisions. It demonstrates of how ASI has been used by businesses to harness the diverse perspectives that individual participants bring to groups and to facilitate convergence upon decisions. It advances the understanding of how artificial intelligence (AI) can be used to enhance, rather than replace, teams as they collaborate to make business decisions.",10.1177/0008125619862256,https://doi.org/10.1177/0008125619862256,California Management Review,Lynn Metcalf;David A. Askay;Louis B. Rosenberg,2019,0,"@article{2-26011,
  title     = {Keeping humans in the loop: pooling knowledge through artificial swarm intelligence to improve business decision making},
  author    = {Metcalf, Lynn and Askay, David A. and Rosenberg, Louis B.},
  year      = {2019},
  doi       = {10.1177/0008125619862256},
  journal   = {California Management Review}
}",Empirical contributions,Finance / Business / Economy,Organizational,"Collaborating, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-26013,sage,Knowing when to pass: the effect of ai reliability in risky decision contexts,"Objective This study manipulates the presence and reliability of AI recommendations for risky decisions to measure the effect on task performance, behavioral consequences of trust, and deviation from a probability matching collaborative decision-making model. Background Although AI decision support improves performance, people tend to underutilize AI recommendations, particularly when outcomes are uncertain. As AI reliability increases, task performance improves, largely due to higher rates of compliance (following action recommendations) and reliance (following no-action recommendations). Methods In a between-subject design, participants were assigned to a high reliability AI, low reliability AI, or a control condition. Participants decided whether to bet that their team would win in a series of basketball games tying compensation to performance. We evaluated task performance (in accuracy and signal detection terms) and the behavioral consequences of trust (via compliance and reliance). Results AI recommendations improved task performance, had limited impact on risk-taking behavior, and were under-valued by participants. Accuracy, sensitivity (d’), and reliance increased in the high reliability AI condition, but there was no effect on response bias (c) or compliance. Participant behavior was only consistent with a probability matching model for compliance in the low reliability condition. Conclusion In a pay-off structure that incentivized risk-taking, the primary value of the AI recommendations was in determining when to perform no action (i.e., pass on bets). Application In risky contexts, designers need to consider whether action or no-action recommendations will be more influential to design appropriate interventions.",10.1177/00187208221100691,https://doi.org/10.1177/00187208221100691,Human Factors: The Journal of the Human Factors and Ergonomics Society,Hannah Elder;Casey Canfield;Daniel B. Shank;Tobias Rieger;Casey Hines,2024,35,"@article{2-26013,
  title = {Knowing when to pass: the effect of AI reliability in risky decision contexts},
  author = {Hannah Elder and Casey Canfield and Daniel B. Shank and Tobias Rieger and Casey Hines},
  year = {2024},
  doi = {10.1177/00187208221100691},
  journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Media / Communication / Entertainment",Individual,Advising,Decision-maker,"Alter decision outcomes, Change trust",no such info,"recommendations, uncertainty",domain knowledge,"Textual, Visual, Interactive interface",Yes,Yes
2-26014,sage,Knowledge-based decision intelligence in street lighting management,"As the availability of computational power and communication technologies increases, Humans and systems are able to tackle increasingly challenging decision problems. Taking decisions over incomplete visions of a situation is particularly challenging and calls for a set of intertwined skills that must be put into place under a clear rationale. This work addresses how to deliver autonomous decisions for the management of a public street lighting network, to optimize energy consumption without compromising light quality patterns. Our approach is grounded in an holistic methodology, combining semantic and Artificial Intelligence principles to define methods and artefacts for supporting decisions to be taken in the context of an incomplete domain. That is, a domain with absence of data and of explicit domain assertions.",10.3233/ICA-210671,https://doi.org/10.3233/ICA-210671,Integrated Computer-Aided Engineering,Cristóvão Sousa;Daniel Teixeira;Davide Carneiro;Diogo Nunes;Paulo Novais,2022,5,"@article{2-26014,
  title     = {Knowledge-based decision intelligence in street lighting management},
  author    = {Sousa, Crist{\'o}v{\~a}o and Teixeira, Daniel and Carneiro, Davide and Nunes, Diogo and Novais, Paulo},
  year      = {2022},
  doi       = {10.3233/ICA-210671},
  journal   = {Integrated Computer-Aided Engineering}
}",Methodological contributions,Transportation / Mobility / Planning,Operational,"Executing, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-26026,sage,"Making sense of decision support systems: rationales, translations and potentials for critical reflections on the reality of child protection","Decision support systems, which incorporate artificial intelligence and big data, are receiving significant attention in the public sector. Decision support systems are sociocultural artefacts that are subject to a mix of technical and political choices, and critical investigation of these choices and the rationales they reflect are paramount since they are inscribed into and may cause harm, violate fundamental rights and reproduce negative social patterns. Applying and merging the concepts of sense-making and translation, this article investigates the rationales, translations and critical reflections that shape the development of a decision support system to support social workers assessing referrals concerning child neglect. It presents findings from a qualitative case study conducted in 2019–2020 at the Citizen Centre Children and Young People, Copenhagen Municipality, Denmark. The analysis shows how key actors through processes of translation construct, negotiate and readjust problem definitions, roles, interests, responsibilities and ideas of ambiguity and accountability. Although technological solutionism is present in these processes, it is not the only rationale invested. Rather, technological and data-driven rationales are adjusted to and merged with rationales of efficiency, return on investment and child welfare. Through continuous renegotiation of roles, responsibilities and problems according to these rationales, the key actors attempt to orchestrate ways of managing the complexity facing child welfare services by projecting images of future potentials of the decision support system that are yet to be realised.",10.1177/20539517221125163,https://doi.org/10.1177/20539517221125163,Big Data & Society,Andreas Møller Jørgensen;Maria Appel Nissen,2022,19,"@article{2-26026,
  title = {Making sense of decision support systems: rationales, translations and potentials for critical reflections on the reality of child protection},
  author = {Andreas Møller Jørgensen and Maria Appel Nissen},
  year = {2022},
  doi = {10.1177/20539517221125163},
  journal = {Big Data & Society}
}",Empirical contributions,"Healthcare / Medicine / Surgery, Everyday / Employment / Public Service",Operational,Advising,"Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-26031,sage,Minority social influence and moral decision-making in human–ai interaction: the effects of identity and specialization cues,"In group decision-making, the behavior of each member is sensitive to the social influence of other majority members. Research on majority influence has shown that multiple non-human agents with anthropomorphic cues can exert normative pressure on a lone human decision-maker. However, how individuals perceive and respond to minority influence exerted by a lone machine is rarely discussed. Hence, a between-subjects experiment was conducted to examine how different minority identity (human vs artificial intelligence [AI]) and specialization (specialist vs generalist) cues influence individuals’ perceptions and behavior in response to moral dilemmas in a joint human–AI group. The results confirmed the significant role of specialization cues in predicting in-group identification, source credibility, and conversion behavior. In addition, the participants perceived the human minority as more credible than the AI minority, which prompted conversion behavior when the minority was labeled as a specialist rather than as a generalist.",10.1177/14614448221138072,https://doi.org/10.1177/14614448221138072,New Media & Society,Yuheng Wu;Ki Joon Kim;Yi Mou,2024,11,"@article{2-26031,
  title={Minority social influence and moral decision-making in human--ai interaction: the effects of identity and specialization cues},
  author={Wu, Yuheng and Kim, Ki Joon and Mou, Yi},
  year={2024},
  doi={10.1177/14614448221138072},
  journal={New Media \& Society}
}",Empirical contributions,"Everyday / Employment / Public Service, Generic / Abstract / Domain-agnostic",no such info,"Collaborating, Advising",Decision-maker,"Alter decision outcomes, Change trust",no such info,identity cues,NA,Textual,Yes,Yes
2-26032,sage,Multi-objective optimal control for proactive decision making with temporal logic models,"The operation of today’s robots entails interactions with humans, e.g., in autonomous driving amidst human-driven vehicles. To effectively do so, robots must proactively decode the intent of humans and concurrently leverage this knowledge for safe, cooperative task satisfaction: a problem we refer to as proactive decision making. However, simultaneous intent decoding and robotic control requires reasoning over several possible human behavioral models, resulting in high-dimensional state trajectories. In this paper, we address the proactive decision-making problem using a novel combination of formal methods, control, and data mining techniques. First, we distill high-dimensional state trajectories of human–robot interaction into concise, symbolic behavioral summaries that can be learned from data. Second, we leverage formal methods to model high-level agent goals, safe interaction, and information-seeking behavior with temporal logic formulas. Finally, we design a novel decision-making scheme that maintains a belief distribution over models of human behavior, and proactively plans informative actions. After showing several desirable theoretical properties, we apply our framework to a dataset of humans driving in crowded merging scenarios. For it, temporal logic models are generated and used to synthesize control strategies using tree-based value iteration and deep reinforcement learning. In addition, we illustrate how data-driven models of human responses to informative robot probes, such as from generative models such as conditional variational autoencoders, can be clustered with formal specifications. Results from simulated self-driving car scenarios demonstrate that data-driven strategies enable safe interaction, correct model identification, and significant dimensionality reduction.",10.1177/0278364919868290,https://doi.org/10.1177/0278364919868290,The International Journal of Robotics Research,Sandeep P. Chinchali;Scott C. Livingston;Mo Chen;Marco Pavone,2019,19,"@article{2-26032,
  title={Multi-objective optimal control for proactive decision making with temporal logic models},
  author={Chinchali, Sandeep P. and Livingston, Scott C. and Chen, Mo and Pavone, Marco},
  year={2019},
  journal={The International Journal of Robotics Research},
  doi={10.1177/0278364919868290}
}",Methodological contributions,"Manufacturing / Industry / Automation, Transportation / Mobility / Planning",Operational,Executing,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-26046,sage,Organizational decision-making structures in the age of artificial intelligence,"How does organizational decision-making change with the advent of artificial intelligence (AI)-based decision-making algorithms? This article identifies the idiosyncrasies of human and AI-based decision making along five key contingency factors: specificity of the decision search space, interpretability of the decision-making process and outcome, size of the alternative set, decision-making speed, and replicability. Based on a comparison of human and AI-based decision making along these dimensions, the article builds a novel framework outlining how both modes of decision making may be combined to optimally benefit the quality of organizational decision making. The framework presents three structural categories in which decisions of organizational members can be combined with AI-based decisions: full human to AI delegation; hybrid—human-to-AI and AI-to-human—sequential decision making; and aggregated human–AI decision making.",10.1177/0008125619862257,https://doi.org/10.1177/0008125619862257,California Management Review,Yash Raj Shrestha;Shiko M. Ben-Menahem;Georg von Krogh,2019,5,"@article{2-26046,
  title={Organizational decision-making structures in the age of artificial intelligence},
  author={Shrestha, Yash Raj and Ben-Menahem, Shiko M. and von Krogh, Georg},
  year={2019},
  journal={California Management Review},
  doi={10.1177/0008125619862257}
}",Methodological contributions,Finance / Business / Economy,Organizational,"Executing, Advising",Decision-maker,"Alter decision outcomes, Shape ethical norms",no such info,delegation,NA,"Semi-Autonomous System, Autonomous System",Yes,Yes
2-26047,sage,Organizationally intractable decision problems and the intellectual virtues of heuristics,"There is no theory in strategic management and other related fields for identifying decision problems that cannot be solved by organizations using rational analytical technologies of the type typically taught in MBA programs. Furthermore, some and perhaps many scholars in strategic management believe that the alternative of heuristics or “rules of thumb” is little more than crude guesses for decision making when compared to rational analytical technologies. This is reflected in a paucity of research in strategic management on heuristics. I propose a theory of “organizational intractability” based roughly on the metaphor provided by “computational intractability” in computer science. I demonstrate organizational intractability for a common model of the joint strategic planning and resource allocation decision problem. This raises the possibility that heuristics are necessary for deciding many important decisions that are intractable for organizations. This possibility parallels the extensive use of heuristics in artificial intelligence for computationally intractable problems, where heuristics are often the most powerful approach possible. Some important managerial heuristics are documented from both the finance and strategic management literatures. Based on all of this, I discuss some directions for theory of and research on organizational intractability and heuristics in strategic management.",10.1177/0149206316679253,https://doi.org/10.1177/0149206316679253,Journal of Management,Richard A. Bettis,2017,18,"@article{2-26047,
  title={Organizationally intractable decision problems and the intellectual virtues of heuristics},
  author={Bettis, Richard A.},
  year={2017},
  journal={Journal of Management},
  doi={10.1177/0149206316679253}
}",Theoretical contributions,Finance / Business / Economy,Institutional,"Advising, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-26049,sage,Pacar: covid-19 pandemic control decision making via large-scale agent-based modeling and deep reinforcement learning,"Background Policy makers are facing more complicated challenges to balance saving lives and economic development in the post-vaccination era during a pandemic. Epidemic simulation models and pandemic control methods are designed to tackle this problem. However, most of the existing approaches cannot be applied to real-world cases due to the lack of adaptability to new scenarios and micro representational ability (especially for system dynamics models), the huge computation demand, and the inefficient use of historical information. Methods We propose a novel Pandemic Control decision making framework via large-scale Agent-based modeling and deep Reinforcement learning (PaCAR) to search optimal control policies that can simultaneously minimize the spread of infection and the government restrictions. In the framework, we develop a new large-scale agent-based simulator with vaccine settings implemented to be calibrated and serve as a realistic environment for a city or a state. We also design a novel reinforcement learning architecture applicable to the pandemic control problem, with a reward carefully designed by the net monetary benefit framework and a sequence learning network to extract information from the sequential epidemiological observations, such as number of cases, vaccination, and so forth. Results Our approach outperforms the baselines designed by experts or adopted by real-world governments and is flexible in dealing with different variants, such as Alpha and Delta in COVID-19. PaCAR succeeds in controlling the pandemic with the lowest economic costs and relatively short epidemic duration and few cases. We further conduct extensive experiments to analyze the reasoning behind the resulting policy sequence and try to conclude this as an informative reference for policy makers in the post-vaccination era of COVID-19 and beyond. Limitations The modeling of economic costs, which are directly estimated by the level of government restrictions, is rather simple. This article mainly focuses on several specific control methods and single-wave pandemic control. Conclusions The proposed framework PaCAR can offer adaptive pandemic control recommendations on different variants and population sizes. Intelligent pandemic control empowered by artificial intelligence may help us make it through the current COVID-19 and other possible pandemics in the future with less cost both of lives and economy. Highlights We introduce a new efficient, large-scale agent-based epidemic simulator in our framework PaCAR, which can be applied to train reinforcement learning networks in a real-world scenario with a population of more than 10,000,000. We develop a novel learning mechanism in PaCAR, which augments reinforcement learning with sequence learning, to learn the tradeoff policy decision of saving lives and economic development in the post-vaccination era. We demonstrate that the policy learned by PaCAR outperforms different benchmark policies under various reality conditions during COVID-19. We analyze the resulting policy given by PaCAR, and the lessons may shed light on better pandemic preparedness plans in the future.",10.1177/0272989X221107902,https://doi.org/10.1177/0272989X221107902,Medical Decision Making: An International Journal of the Society for Medical Decision Making,Xudong Guo;Peiyu Chen;Shihao Liang;Zengtao Jiao;Linfeng Li;Jun Yan;Yadong Huang;Yi Liu;Wenhui Fan,2022,0,"@article{2-26049,
  title={Pacar: COVID-19 pandemic control decision making via large-scale agent-based modeling and deep reinforcement learning},
  author={Guo, Xudong and Chen, Peiyu and Liang, Shihao and Jiao, Zengtao and Li, Linfeng and Yan, Jun and Huang, Yadong and Liu, Yi and Fan, Wenhui},
  year={2022},
  doi={10.1177/0272989X221107902},
  journal={Medical Decision Making: An International Journal of the Society for Medical Decision Making}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Organizational,Advising,"Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-26050,sage,Personalization of medical treatment decisions: simplifying complex models while maintaining patient health outcomes,"Background Personalizing medical treatments based on patient-specific risks and preferences can improve patient health. However, models to support personalized treatment decisions are often complex and difficult to interpret, limiting their clinical application. Methods We present a new method, using machine learning to create meta-models, for simplifying complex models for personalizing medical treatment decisions. We consider simple interpretable models, interpretable ensemble models, and noninterpretable ensemble models. We use variable selection with a penalty for patient-specific risks and/or preferences that are difficult, risky, or costly to obtain. We interpret the meta-models to the extent permitted by their model architectures. We illustrate our method by applying it to simplify a previously developed model for personalized selection of antipsychotic drugs for patients with schizophrenia. Results The best simplified interpretable, interpretable ensemble, and noninterpretable ensemble models contained at most half the number of patient-specific risks and preferences compared with the original model. The simplified models achieved 60.5% (95% credible interval [crI]: 55.2–65.4), 60.8% (95% crI: 55.5–65.7), and 83.8% (95% crI: 80.8–86.6), respectively, of the net health benefit of the original model (quality-adjusted life-years gained). Important variables in all models were similar and made intuitive sense. Computation time for the meta-models was orders of magnitude less than for the original model. Limitations The simplified models share the limitations of the original model (e.g., potential biases). Conclusions Our meta-modeling method is disease- and model- agnostic and can be used to simplify complex models for personalization, allowing for variable selection in addition to improved model interpretability and computational performance. Simplified models may be more likely to be adopted in clinical settings and can help improve equity in patient outcomes.",10.1177/0272989X211037921,https://doi.org/10.1177/0272989X211037921,Medical Decision Making: An International Journal of the Society for Medical Decision Making,Christopher Weyant;Margaret L. Brandeau,2022,13,"@article{2-26050,
  title={Personalization of medical treatment decisions: simplifying complex models while maintaining patient health outcomes},
  author={Weyant, Christopher and Brandeau, Margaret L.},
  year={2022},
  journal={Medical Decision Making: An International Journal of the Society for Medical Decision Making},
  volume={42},
  number={1},
  pages={102-118},
  doi={10.1177/0272989X211037921}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-26063,sage,Revealing icu cognitive work through naturalistic decision-making methods,"The fragile health of patients who are admitted to a burn intensive care unit (ICU) requires clinicians and clinical teams to perform complex cognitive work that includes time-pressured diagnostic and therapeutic decisions that are based on emergent and interrelated patient information. Barriers to clinician efforts delay patient care and increase care cost, length of stay, and the potential for misadventures. The Cooperative Communication System is a real-time information technology system in its final year of development that is designed to support individual and team cognitive work and communication in the burn ICU. The project has used cognitive systems engineering methods to reveal genotypes: the traits that mold this naturalistic decision-making work setting. Requirements derived from findings guided development of seven core features, configurable displays, and machine learning features that enable clinicians to obtain and use the most important information on individual patients and among and across patients. Recent evaluation data demonstrate the system’s usability and value to the clinical staff. More efficient, reliable collaboration among members of the ICU staff who use the Cooperative Communication System is expected to improve patient safety and improve patient outcomes.",10.1177/1555343416664845,https://doi.org/10.1177/1555343416664845,Journal of Cognitive Engineering and Decision Making,Christopher Nemeth;Josh Blomberg;Christopher Argenta;Maria L. Serio-Melvin;Jose Salinas;Jeremy Pamplin,2016,3,"@article{2-26063,
  title={Revealing icu cognitive work through naturalistic decision-making methods},
  author={Nemeth, Christopher and Blomberg, Josh and Argenta, Christopher and Serio-Melvin, Maria L. and Salinas, Jose and Pamplin, Jeremy},
  year={2016},
  doi={10.1177/1555343416664845},
  journal={Journal of Cognitive Engineering and Decision Making}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-26105,sage,"Trust in autonomous cars: exploring the role of shared moral values, reasoning, and emotion in safety-critical decisions","Objective Autonomous cars (ACs) controlled by artificial intelligence are expected to play a significant role in transportation in the near future. This study investigated determinants of trust in ACs. Background Trust in ACs influences different variables, including the intention to adopt AC technology. Several studies on risk perception have verified that shared value determines trust in risk managers. Previous research has confirmed the effect of value similarity on trust in artificial intelligence. We focused on moral beliefs, specifically utilitarianism (belief in promoting a greater good) and deontology (belief in condemning deliberate harm), and tested the effects of shared moral beliefs on trust in ACs. Method We conducted three experiments (N = 128, 71, and 196, for each), adopting a thought experiment similar to the well-known trolley problem. We manipulated shared moral beliefs (shared vs. unshared) and driver (AC vs. human), providing participants with different moral dilemma scenarios. Trust in ACs was measured through a questionnaire. Results The results of Experiment 1 showed that shared utilitarian belief strongly influenced trust in ACs. In Experiment 2 and Experiment 3, however, we did not find statistical evidence that shared deontological belief had an effect on trust in ACs. Conclusion The results of the three experiments suggest that the effect of shared moral beliefs on trust varies depending on the values that ACs share with humans. Application To promote AC implementation, policymakers and developers need to understand which values are shared between ACs and humans to enhance trust in ACs.",10.1177/0018720820933041,https://doi.org/10.1177/0018720820933041,Human Factors: The Journal of the Human Factors and Ergonomics Society,Ryosuke Yokoi;Kazuya Nakayachi,2021,45,"@article{2-26105,
  title={Trust in autonomous cars: exploring the role of shared moral values, reasoning, and emotion in safety-critical decisions},
  author={Yokoi, Ryosuke and Nakayachi, Kazuya},
  year={2021},
  journal={Human Factors: The Journal of the Human Factors and Ergonomics Society},
  volume={63},
  number={6},
  pages={1004--1019},
  doi={10.1177/0018720820933041}
}",Empirical contributions,Transportation / Mobility / Planning,Institutional,Executing,"Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-26120,sage,When chatgpt gives incorrect answers: the impact of inaccurate information by generative ai on tourism decision-making,"This study investigates how inaccurate information provided by ChatGPT impacts travelers’ acceptance of recommendations. Six experiments were conducted based on the accessibility-diagnosticity framework. These examined the moderating role of the prominence and type of incorrect information and their effects on decision-making. The results show that participants perceived more accuracy and trustworthiness, leading to stronger intentions to visit when incorrect information was absent. However, there was a decline in their intentions to visit when incorrect information was present and more prominent or in the same domain. This effect diminished when multiple domains were involved or when participants were focused on the initial task. The research highlights that both the prominence and type of incorrect information are boundary conditions and provides insights into AI applications in tourism. Furthermore, it offers practical implications for online travel agencies in terms of user interface and user experience design planning.",10.1177/00472875231212996,https://doi.org/10.1177/00472875231212996,Journal of Travel Research,Jeong Hyun Kim;Jungkeun Kim;Jooyoung Park;Changju Kim;Jihoon Jhang;Brian King,2023,140,"@article{2-26120,
  title={When chatgpt gives incorrect answers: the impact of inaccurate information by generative ai on tourism decision-making},
  author={Kim, Jeong Hyun and Kim, Jungkeun and Park, Jooyoung and Kim, Changju and Jhang, Jihoon and King, Brian},
  year={2023},
  doi={10.1177/00472875231212996},
  journal={Journal of Travel Research}
}",Empirical contributions,Media / Communication / Entertainment,Individual,Advising,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-26121,sage,When do humans heed ai agents’ advice? When should they?,"Objective We manipulate the presence, skill, and display of artificial intelligence (AI) recommendations in a strategy game to measure their effect on users’ performance. Background Many applications of AI require humans and AI agents to make decisions collaboratively. Success depends on how appropriately humans rely on the AI agent. We demonstrate an evaluation method for a platform that uses neural network agents of varying skill levels for the simple strategic game of Connect Four. Methods We report results from a 2 × 3 between-subjects factorial experiment that varies the format of AI recommendations (categorical or probabilistic) and the AI agent’s amount of training (low, medium, or high). On each round of 10 games, participants proposed a move, saw the AI agent’s recommendations, and then moved. Results Participants’ performance improved with a highly skilled agent, but quickly plateaued, as they relied uncritically on the agent. Participants relied too little on lower skilled agents. The display format had no effect on users’ skill or choices. Conclusions The value of these AI agents depended on their skill level and users’ ability to extract lessons from their advice. Application Organizations employing AI decision support systems must consider behavioral aspects of the human-agent team. We demonstrate an approach to evaluating competing designs and assessing their performance.",10.1177/00187208231190459,https://doi.org/10.1177/00187208231190459,Human Factors: The Journal of the Human Factors and Ergonomics Society,Richard E. Dunning;Baruch Fischhoff;Alex L. Davis,2024,8,"@article{2-26121,
  title = {When do humans heed AI agents’ advice? When should they?},
  author = {Richard E. Dunning and Baruch Fischhoff and Alex L. Davis},
  year = {2024},
  doi = {10.1177/00187208231190459},
  journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society}
}",Empirical contributions,Media / Communication / Entertainment,Individual,"Advising, Collaborating",Decision-maker,"Alter decision outcomes, Change trust",no such info,recommendations,NA,Interactive interface,Yes,Yes
2-26122,sage,When is mass prophylaxis cost-effective for epidemic control? A comparison of decision approaches,"Background For certain communicable disease outbreaks, mass prophylaxis of uninfected individuals can curtail new infections. When an outbreak emerges, decision makers could benefit from methods to quickly determine whether mass prophylaxis is cost-effective. We consider 2 approaches: a simple decision model and machine learning meta-models. The motivating example is plague in Madagascar. Methods We use a susceptible-exposed-infectious-removed (SEIR) epidemic model to derive a decision rule based on the fraction of the population infected, effective reproduction ratio, infection fatality rate, quality-adjusted life-year loss associated with death, prophylaxis effectiveness and cost, time horizon, and willingness-to-pay threshold. We also develop machine learning meta-models of a detailed model of plague in Madagascar using logistic regression, random forest, and neural network models. In numerical experiments, we compare results using the decision rule and the meta-models to results obtained using the simulation model. We vary the initial fraction of the population infected, the effective reproduction ratio, the intervention start date and duration, and the cost of prophylaxis. Limitations We assume homogeneous mixing and no negative side effects due to antibiotic prophylaxis. Results The simple decision rule matched the SEIR model outcome in 85.4% of scenarios. Using data for a 2017 plague outbreak in Madagascar, the decision rule correctly indicated that mass prophylaxis was not cost-effective. The meta-models were significantly more accurate, with an accuracy of 92.8% for logistic regression, 95.8% for the neural network model, and 96.9% for the random forest model. Conclusions A simple decision rule using minimal information about an outbreak can accurately evaluate the cost-effectiveness of mass prophylaxis for outbreak mitigation. Meta-models of a complex disease simulation can achieve higher accuracy but with greater computational and data requirements and less interpretability. Highlights We use a susceptible-exposed-infectious-removed model and net monetary benefit to derive a simple decision rule to evaluate the cost-effectiveness of mass prophylaxis. We use the example of plague in Madagascar to compare the performance of the analytically derived decision rule to that of machine learning meta-models trained on a stochastic dynamic transmission model. We assess the accuracy of each approach for different combinations of disease dynamics and intervention scenarios. The machine learning meta-models are more accurate predictors of mass prophylaxis cost-effectiveness. However, the simple decision rule is also accurate and may be a preferred substitute in low-resource settings.",10.1177/0272989X221098409,https://doi.org/10.1177/0272989X221098409,Medical Decision Making: An International Journal of the Society for Medical Decision Making,Giovanni S. P. Malloy;Margaret L. Brandeau,2022,3,"@article{2-26122,
  title = {When is mass prophylaxis cost-effective for epidemic control? A comparison of decision approaches},
  author = {Giovanni S. P. Malloy and Margaret L. Brandeau},
  year = {2022},
  doi = {10.1177/0272989X221098409},
  journal = {Medical Decision Making: An International Journal of the Society for Medical Decision Making}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Monitoring",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-26123,sage,When post hoc explanation knocks: consumer responses to explainable ai recommendations,"Artificial intelligence (AI) recommendations are becoming increasingly prevalent, but consumers are often reluctant to trust them, in part due to the “black-box” nature of algorithm-facilitated recommendation agents. Despite the acknowledgment of the vital role of interpretability in consumer trust in AI recommendations, it remains unclear how to effectively increase interpretability perceptions and consequently enhance positive consumer responses. The current research addresses this issue by investigating the effects of the presence and type of post hoc explanations in boosting positive consumer responses to AI recommendations in different decision-making domains. Across four studies, the authors demonstrate that the presence of post hoc explanations increases interpretability perceptions, which in turn fosters positive consumer responses (e.g., trust, purchase intention, and click-through) to AI recommendations. Moreover, they show that the facilitating effect of post hoc explanations is stronger in the utilitarian (vs. hedonic) decision-making domain. Further, explanation type modulates the effectiveness of post hoc explanations such that attribute-based explanations are more effective in enhancing trust in the utilitarian decision-making domain, whereas user-based explanations are more effective in the hedonic decision-making domain.",10.1177/10949968231200221,https://doi.org/10.1177/10949968231200221,Journal of Interactive Marketing,Changdong Chen;Allen Ding Tian;Ruochen Jiang,2024,35,"@article{2-26123,
  title={When post hoc explanation knocks: consumer responses to explainable AI recommendations},
  author={Chen, Changdong and Tian, Allen Ding and Jiang, Ruochen},
  year={2024},
  doi={10.1177/10949968231200221},
  journal={Journal of Interactive Marketing}
}",Empirical contributions,Finance / Business / Economy,Individual,"Advising, Explaining",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-26128,science,"The accuracy, fairness, and limits of predicting recidivism","Algorithms for predicting recidivism are commonly used to assess a criminal defendant’s likelihood of committing a crime. These predictions are used in pretrial, parole, and sentencing decisions. Proponents of these systems argue that big data and advanced machine learning make these analyses more accurate and less biased than humans. We show, however, that the widely used commercial risk assessment software COMPAS is no more accurate or fair than predictions made by people with little or no criminal justice expertise. In addition, despite COMPAS’s collection of 137 features, the same accuracy can be achieved with a simple linear classifier with only two features.",NA,https://www.science.org/doi/10.1126/sciadv.aao5580,Science Advances,Julia Dressel;Hany Farid,2018,91,"@article{2-26128,
  title={The accuracy, fairness, and limits of predicting recidivism},
  author={Dressel, Julia and Farid, Hany},
  year={2018},
  journal={Science Advances}
}",Empirical contributions,Law / Policy / Governance,Operational,Forecasting,"Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-26145,science,Presyndromic surveillance for improved detection of emerging public health threats,"Existing public health surveillance systems that rely on predefined symptom categories, or syndromes, are effective at monitoring known illnesses, but there is a critical need for innovation in “presyndromic” surveillance that detects biothreats with rare or previously unseen symptomology. We introduce a data-driven, automated machine learning approach for presyndromic surveillance that learns newly emerging syndromes from free-text emergency department chief complaints, identifies localized case clusters among subpopulations, and incorporates practitioner feedback to automatically distinguish between relevant and irrelevant clusters, thus providing personalized, actionable decision support. Blinded evaluations by New York City’s Department of Health and Mental Hygiene demonstrate that our approach identifies more events of public health interest and achieves a lower false-positive rate compared to a state-of-the-art baseline.",NA,https://www.science.org/doi/10.1126/sciadv.abm4920,Science Advances,Mallory Nobles;Ramona Lall;Robert W. Mathes;[...];Daniel B. Neill;+0 authors,2022,12,"@article{2-26145,
  title={Presyndromic surveillance for improved detection of emerging public health threats},
  author={Nobles, Mallory and Lall, Ramona and Mathes, Robert W. and Neill, Daniel B.},
  year={2022},
  journal={Science Advances}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Guardian, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-26149,science,From motor control to team play in simulated humanoid football,"Learning to combine control at the level of joint torques with longer-term goal-directed behavior is a long-standing challenge for physically embodied artificial agents. Intelligent behavior in the physical world unfolds across multiple spatial and temporal scales: Although movements are ultimately executed at the level of instantaneous muscle tensions or joint torques, they must be selected to serve goals that are defined on much longer time scales and that often involve complex interactions with the environment and other agents. Recent research has demonstrated the potential of learning-based approaches applied to the respective problems of complex movement, long-term planning, and multiagent coordination. However, their integration traditionally required the design and optimization of independent subsystems and remains challenging. In this work, we tackled the integration of motor control and long-horizon decision-making in the context of simulated humanoid football, which requires agile motor control and multiagent coordination. We optimized teams of agents to play simulated football via reinforcement learning, constraining the solution space to that of plausible movements learned using human motion capture data. They were trained to maximize several environment rewards and to imitate pretrained football-specific skills if doing so led to improved performance. The result is a team of coordinated humanoid football players that exhibit complex behavior at different scales, quantified by a range of analysis and statistics, including those used in real-world sport analytics. Our work constitutes a complete demonstration of learned integrated decision-making at multiple scales in a multiagent setting.",NA,https://www.science.org/doi/10.1126/scirobotics.abo0235,Science Robotics,Siqi Liu;Guy Lever;Zhe Wang;Josh Merel;S. M. Ali Eslami;Daniel Hennes;Wojciech M. Czarnecki;Yuval Tassa;Shayegan Omidshafiei;Abbas Abdolmaleki;Noah Y. Siegel;Leonard Hasenclever;Luke Marris;Saran Tunyasuvunakool;H. Francis Song;Markus Wulfmeier;Paul Muller;Tuomas Haarnoja;Brendan Tracey;Karl Tuyls;Thore Graepel;[...];Nicolas Heess;+16 authors,2022,4,"@article{2-26149,
  title={From motor control to team play in simulated humanoid football},
  author={Liu, Siqi and Lever, Guy and Wang, Zhe and Merel, Josh and Eslami, S. M. Ali and Hennes, Daniel and Czarnecki, Wojciech M. and Tassa, Yuval and Omidshafiei, Shayegan and Abdolmaleki, Abbas and Siegel, Noah Y. and Hasenclever, Leonard and Marris, Luke and Tunyasuvunakool, Saran and Song, H. Francis and Wulfmeier, Markus and Muller, Paul and Haarnoja, Tuomas and Tracey, Brendan and Tuyls, Karl and Graepel, Thore and Heess, Nicolas},
  year={2022},
  journal={Science Robotics}
}",Algorithmic contributions,Media / Communication / Entertainment,Operational,"Executing, Collaborating",Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-26161,science,All-printed soft human-machine interface for robotic physicochemical sensing,"Ultrasensitive multimodal physicochemical sensing for autonomous robotic decision-making has numerous applications in agriculture, security, environmental protection, and public health. Previously reported robotic sensing technologies have primarily focused on monitoring physical parameters such as pressure and temperature. Integrating chemical sensors for autonomous dry-phase analyte detection on a robotic platform is rather extremely challenging and substantially underdeveloped. Here, we introduce an artificial intelligence–powered multimodal robotic sensing system (M-Bot) with an all-printed mass-producible soft electronic skin–based human-machine interface. A scalable inkjet printing technology with custom-developed nanomaterial inks was used to manufacture flexible physicochemical sensor arrays for electrophysiology recording, tactile perception, and robotic sensing of a wide range of hazardous materials including nitroaromatic explosives, pesticides, nerve agents, and infectious pathogens such as SARS-CoV-2. The M-Bot decodes the surface electromyography signals collected from the human body through machine learning algorithms for remote robotic control and can perform in situ threat compound detection in extreme or contaminated environments with user-interactive tactile and threat alarm feedback. The printed electronic skin–based robotic sensing technology can be further generalized and applied to other remote sensing platforms. Such diversity was validated on an intelligent multimodal robotic boat platform that can efficiently track the source of trace amounts of hazardous compounds through autonomous and intelligent decision-making algorithms. This fully printed human-machine interactive multimodal sensing technology could play a crucial role in designing future intelligent robotic systems and can be easily reconfigured toward numerous practical wearable and robotic applications.",NA,https://www.science.org/doi/10.1126/scirobotics.abn0495,Science Robotics,You Yu;Jiahong Li;Samuel A. Solomon;Jihong Min;Jiaobing Tu;Wei Guo;Changhao Xu;Yu Song;[...];Wei Gao;+3 authors,2022,306,"@article{2-26161,
  title={All-printed soft human-machine interface for robotic physicochemical sensing},
  author={Yu, You and Li, Jiahong and Solomon, Samuel A. and Min, Jihong and Tu, Jiaobing and Guo, Wei and Xu, Changhao and Song, Yu and Gao, Wei},
  year={2022},
  journal={Science Robotics}
}",System/Artifact contributions,"Manufacturing / Industry / Automation, Defense / Military / Emergency",Operational,"Analyzing, Executing",Developer,NA,NA,NA,NA,NA,Yes,No
2-26187,springernature,Interpretable and Fair Mechanisms for Abstaining Classifiers,"Abstaining classifiers have the option to refrain from providing a prediction for instances that are difficult to classify. The abstention mechanism is designed to trade off the classifier’s performance on the accepted data while ensuring a minimum number of predictions. In this setting, often fairness concerns arise when the abstention mechanism solely reduces errors for the majority groups of the data, resulting in increased performance differences across demographic groups. While there exist a bunch of methods that aim to reduce discrimination when abstaining, there is no mechanism that can do so in an explainable way. In this paper, we fill this gap by introducing Interpretable and Fair Abstaining Classifier( IFAC) , an algorithm that can reject predictions both based on their uncertainty and their unfairness. By rejecting possibly unfair predictions, our method reduces error and positive decision rate differences across demographic groups of the non-rejected data. Since the unfairness-based rejections are based on an interpretable-by-design method, i. e. , rule-based fairness checks and situation testing, we create a transparent process that can empower human decision-makers to review the unfair predictions and make more just decisions for them. This explainable aspect is especially important in light of recent AI regulations, mandating that any high-risk decision task should be overseen by human experts to reduce discrimination risks. ( Code and Appendix for this work is available on: https://github.com/calathea21/IFAC).",10.1007/978-3-031-70368-3_25,http://dx.doi.org/10.1007/978-3-031-70368-3_25,European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD),"Lenders, Daphne;Pugnana, Andrea;Pellungrini, Roberto;Calders, Toon;Pedreschi, Dino;Giannotti, Fosca",2024,4,"@inproceedings{2-26187,
  title={Interpretable and Fair Mechanisms for Abstaining Classifiers},
  author={Lenders, Daphne and Pugnana, Andrea and Pellungrini, Roberto and Calders, Toon and Pedreschi, Dino and Giannotti, Fosca},
  year={2024},
  booktitle={European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)},
  doi={10.1007/978-3-031-70368-3_25}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Institutional,"Advising, Explaining","Decision-maker, Stakeholder, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-26228,springernature,Reporting guideline for the early-stage clinical evaluation of decision support systems driven by artificial intelligence: DECIDE-AI,"A growing number of artificial intelligence( AI) -based clinical decision support systems are showing promising performance in preclinical, in silico evaluation, but few have yet demonstrated real benefit to patient care. Early-stage clinical evaluation is important to assess an AI system’s actual clinical performance at small scale, ensure its safety, evaluate the human factors surrounding its use and pave the way to further large-scale trials. However, the reporting of these early studies remains inadequate. The present statement provides a multi-stakeholder, consensus-based reporting guideline for the Developmental and Exploratory Clinical Investigations of DEcision support systems driven by Artificial Intelligence( DECIDE-AI). We conducted a two-round, modified Delphi process to collect and analyze expert opinion on the reporting of early clinical evaluation of AI systems. Experts were recruited from 20 pre-defined stakeholder categories. The final composition and wording of the guideline was determined at a virtual consensus meeting. The checklist and the Explanation & Elaboration( E&E) sections were refined based on feedback from a qualitative evaluation process. In total, 123 experts participated in the first round of Delphi, 138 in the second round, 16 in the consensus meeting and 16 in the qualitative evaluation. The DECIDE-AI reporting guideline comprises 17 AI-specific reporting items( made of 28 subitems) and ten generic reporting items, with an E&E paragraph provided for each. Through consultation and consensus with a range of stakeholders, we developed a guideline comprising key items that should be reported in early-stage clinical studies of AI-based decision support systems in healthcare. By providing an actionable checklist of minimal reporting items, the DECIDE-AI guideline will facilitate the appraisal of these studies and replicability of their findings. The DECIDE-AI checklist, resulting from a multi-stakeholder group of experts in a Delphi process and following the EQUATOR Network’s recommendations, includes key items that should be reported in early-stage clinical studies of AI-based decision support systems, to ensure a responsible and transparent deployment of AI systems in healthcare.",10.1038/s41591-022-01772-9,http://dx.doi.org/10.1038/s41591-022-01772-9,Nature Medicine,"Vasey, Baptiste;Nagendran, Myura;Campbell, Bruce;Clifton, David A.;Collins, Gary S.;Denaxas, Spiros;Denniston, Alastair K.;Faes, Livia;Geerts, Bart;Ibrahim, Mudathir;Liu, Xiaoxuan;Mateen, Bilal A.;Mathur, Piyush;McCradden, Melissa D.;Morgan, Lauren;Ordish, Johan;Rogers, Campbell;Saria, Suchi;Ting, Daniel S. W.;Watkinson, Peter;Weber, Wim;Wheatstone, Peter;McCulloch, Peter;the DECIDE-AI expert group",2022,7,"@article{2-26228,
  title = {Reporting guideline for the early-stage clinical evaluation of decision support systems driven by artificial intelligence: DECIDE-AI},
  author = {Vasey, Baptiste and Nagendran, Myura and Campbell, Bruce and Clifton, David A. and Collins, Gary S. and Denaxas, Spiros and Denniston, Alastair K. and Faes, Livia and Geerts, Bart and Ibrahim, Mudathir and Liu, Xiaoxuan and Mateen, Bilal A. and Mathur, Piyush and McCradden, Melissa D. and Morgan, Lauren and Ordish, Johan and Rogers, Campbell and Saria, Suchi and Ting, Daniel S. W. and Watkinson, Peter and Weber, Wim and Wheatstone, Peter and McCulloch, Peter and the DECIDE-AI expert group},
  year = {2022},
  doi = {10.1038/s41591-022-01772-9},
  journal = {Nature Medicine}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Guardian, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-26277,springernature,Can We Estimate Shortand Intermediate-term Survival in Patients Undergoing Surgery for Metastatic Bone Disease?,"Background Objective means of estimating survival can be used to guide surgical decision-making and to risk-stratify patients for clinical trials. Although a free, online tool( www. pathfx. org) can estimate 3and 12-month survival, recent work, including a survey of the Musculoskeletal Tumor Society, indicated that estimates at 1 and 6 months after surgery also would be helpful. Longer estimates help justify the need for more durable and expensive reconstructive options, and very short estimates could help identify those who will not survive 1 month and should not undergo surgery. Thereby, an important use of this tool would be to help avoid unsuccessful and expensive surgery during the last month of life. Questions/Purposes We seek to provide a reliable, objective means of estimating survival in patients with metastatic bone disease. After generating models to derive 1and 6-month survival estimates, we determined suitability for clinical use by applying receiver operator characteristic( ROC) ( area under the curve [AUC] > 0. 7) and decision curve analysis( DCA) , which determines whether using PATHFx can improve outcomes, but also discerns in which kinds of patients PATHFx should not be used. Methods We used two, existing, skeletal metastasis registries chosen for their quality and availability. Data from Memorial Sloan-Kettering Cancer Center( training set, n = 189) was used to develop two Bayesian Belief Networks trained to estimate the likelihood of survival at 1 and 6 months after surgery. Next, data from eight major referral centers across Scandinavia( n = 815) served as the external validation set—that is, as a means to test model performance in a different patient population. The diversity of the data between the training set from Memorial Sloan-Kettering Cancer Center and the Scandinavian external validation set is important to help ensure the models are applicable to patients in various settings with differing demographics and treatment philosophies. We considered disease-specific, laboratory, and demographic information, and the surgeon’s estimate of survival. For each model, we calculated the area under the ROC curve( AUC) as a metric of discriminatory ability and the Net Benefit using DCA to determine whether the models were suitable for clinical use. Results On external validation, the AUC for the 1and 6-month models were 0. 76( 95% CI, 0. 72–0. 80) and 0. 76( 95% CI, 0. 73–0. 79) , respectively. The models conferred a positive net benefit on DCA, indicating each could be used rather than assume all patients or no patients would survive greater than 1 or 6 months, respectively. Conclusions Decision analysis confirms that the 1and 6-month Bayesian models are suitable for clinical use. Clinical Relevance These data support upgrading www. pathfx. org with the algorithms described above, which is designed to guide surgical decision-making, and function as a risk stratification method in support of clinical trials. This updating has been done, so now surgeons may use any web browser to generate survival estimates at 1, 3, 6, and 12 months after surgery, at no cost. Just as short estimates of survival help justify palliative therapy or less-invasive approaches to stabilization, more favorable survival estimates at 6 or 12 months are used to justify more durable, complicated, and expensive reconstructive options.",10.1007/s11999-016-5187-3,http://dx.doi.org/10.1007/s11999-016-5187-3,Clinical Orthopaedics and Related Research,"Forsberg, Jonathan A.;Wedin, Rikard;Boland, Patrick J.;Healey, John H.",2017,0,"@article{2-26277,
  title = {Can We Estimate Shortand Intermediate-term Survival in Patients Undergoing Surgery for Metastatic Bone Disease?},
  author = {Forsberg, Jonathan A. and Wedin, Rikard and Boland, Patrick J. and Healey, John H.},
  year = {2017},
  doi = {10.1007/s11999-016-5187-3},
  journal = {Clinical Orthopaedics and Related Research}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-26473,springernature,Improving the pull requests review process using learning-to-rank algorithms,"Collaborative software development platforms( such as GitHub and GitLab) have become increasingly popular as they have attracted thousands of external contributors to contribute to open source projects. The external contributors may submit their contributions via pull requests, which must be reviewed before being integrated into the central repository. During the review process, reviewers provide feedback to contributors, conduct tests and request further modifications before finally accepting or rejecting the contributions. The role of reviewers is key to maintain the effective review process of the project. However, the number of decisions that reviewers can make is far superseded by the increasing number of pull requests submissions. To help reviewers to perform more decisions on pull requests within their limited working time, we propose a learning-to-rank( LtR) approach to recommend pull requests that can be quickly reviewed by reviewers. Different from a binary model for predicting the decisions of pull requests, our ranking approach complements the existing list of pull requests based on their likelihood of being quickly merged or rejected. We use 18 metrics to build LtR models and we use six different LtR algorithms, such as ListNet, RankNet, MART and random forest. We conduct empirical studies on 74 Java projects to compare the performances of the six LtR algorithms. We compare the best performing algorithm against two baselines obtained from previous research regarding pull requests prioritization: the first-in-and-first-out( FIFO) baseline and the small-size-first baseline. We then conduct a survey with GitHub reviewers to understand the perception of code reviewers regarding the usefulness of our approach. We observe that:( 1) The random forest LtR algorithm outperforms other five well adapted LtR algorithms to rank quickly merged pull requests. ( 2) The random forest LtR algorithm performs better than both the FIFO and the small-size-first baselines, which means our LtR approach can help reviewers make more decisions and improve their productivity. ( 3) The contributor’s social connections and contributor’s experience are the most influential metrics to rank pull requests that can be quickly merged. ( 4) The GitHub reviewers that participated in our survey acknowledge that our approach complements existing prioritization baselines to help them to prioritize and to review more pull requests.",10.1007/s10664-019-09696-8,http://dx.doi.org/10.1007/s10664-019-09696-8,Empirical Software Engineering,"Zhao, Guoliang;Costa, Daniel Alencar;Zou, Ying",2019,59,"@article{2-26473,
  title={Improving the pull requests review process using learning-to-rank algorithms},
  author={Zhao, Guoliang and Costa, Daniel Alencar and Zou, Ying},
  year={2019},
  journal={Empirical Software Engineering},
  volume={24},
  number={6},
  pages={3207--3245},
  doi={10.1007/s10664-019-09696-8}
}",Algorithmic contributions,Software / Systems / Security,Operational,"Advising, Analyzing","Developer, Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-26550,springernature,Expert gaze as a usability indicator of medical AI decision support systems: a preliminary study,"Given the current state of medical artificial intelligence( AI) and perceptions towards it, collaborative systems are becoming the preferred choice for clinical workflows. This work aims to address expert interaction with medical AI support systems to gain insight towards how these systems can be better designed with the user in mind. As eye tracking metrics have been shown to be robust indicators of usability, we employ them for evaluating the usability and user interaction with medical AI support systems. We use expert gaze to assess experts’ interaction with an AI software for caries detection in bitewing x-ray images. We compared standard viewing of bitewing images without AI support versus viewing where AI support could be freely toggled on and off. We found that experts turned the AI on for roughly 25% of the total inspection task, and generally turned it on halfway through the course of the inspection. Gaze behavior showed that when supported by AI, more attention was dedicated to user interface elements related to the AI support, with more frequent transitions from the image itself to these elements. When considering that expert visual strategy is already optimized for fast and effective image inspection, such interruptions in attention can lead to increased time needed for the overall assessment. Gaze analysis provided valuable insights into an AI’s usability for medical image inspection. Further analyses of these tools and how to delineate metrical measures of usability should be developed.",10.1038/s41746-024-01192-8,http://dx.doi.org/10.1038/s41746-024-01192-8,Nature Partner Journals Digital Medicine,"Castner, Nora;Arsiwala-Scheppach, Lubaina;Mertens, Sarah;Krois, Joachim;Thaqi, Enkeleda;Kasneci, Enkelejda;Wahl, Siegfried;Schwendicke, Falk",2024,12,"@article{2-26550,
  title = {Expert gaze as a usability indicator of medical AI decision support systems: a preliminary study},
  author = {Castner, Nora and Arsiwala-Scheppach, Lubaina and Mertens, Sarah and Krois, Joachim and Thaqi, Enkeleda and Kasneci, Enkelejda and Wahl, Siegfried and Schwendicke, Falk},
  year = {2024},
  doi = {10.1038/s41746-024-01192-8},
  journal = {Nature Partner Journals Digital Medicine}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Auditing, Advising, Analyzing","Knowledge provider, Decision-maker",Change cognitive demands,Change AI responses,"(continuous) support, procedural instructions",NA,Autonomous System,Yes,Yes
2-26559,springernature,Machine learning to guide the use of adjuvant therapies for breast cancer,"Accurate prediction of the individualized survival benefit of adjuvant therapy is key to making informed therapeutic decisions for patients with early invasive breast cancer. Machine learning technologies can enable accurate prognostication of patient outcomes under different treatment options by modelling complex interactions between risk factors in a data-driven fashion. Here, we use an automated and interpretable machine learning algorithm to develop a breast cancer prognostication and treatment benefit prediction model—Adjutorium—using data from large-scale cohorts of nearly one million women captured in the national cancer registries of the United Kingdom and the United States. We trained and internally validated the Adjutorium model on 395, 862 patients from the UK National Cancer Registration and Analysis Service( NCRAS) , and then externally validated the model among 571, 635 patients from the US Surveillance, Epidemiology, and End Results( SEER) programme. Adjutorium exhibited significantly improved accuracy compared to the major prognostic tool in current clinical use( PREDICT v2. 1) in both internal and external validation. Importantly, our model substantially improved accuracy in specific subgroups known to be under-served by existing models. Adjutorium is currently implemented as a web-based decision support tool( https://vanderschaar-lab. com/adjutorium/) to aid decisions on adjuvant therapy in women with early breast cancer, and can be publicly accessed by patients and clinicians worldwide. Methods are available to support clinical decisions regarding adjuvant therapies in breast cancer, but they have limitations in accuracy, generalizability and interpretability. Alaa et al. present an automated machine learning model of breast cancer that predicts patient survival and adjuvant treatment benefit to guide personalized therapeutic decisions.",10.1038/s42256-021-00353-8,http://dx.doi.org/10.1038/s42256-021-00353-8,Nature Machine Intelligence,"Alaa, Ahmed M.;Gurdasani, Deepti;Harris, Adrian L.;Rashbass, Jem;van der Schaar, Mihaela",2021,66,"@article{2-26559,
  title={Machine learning to guide the use of adjuvant therapies for breast cancer},
  author={Alaa, Ahmed M. and Gurdasani, Deepti and Harris, Adrian L. and Rashbass, Jem and van der Schaar, Mihaela},
  year={2021},
  doi={10.1038/s42256-021-00353-8},
  journal={Nature Machine Intelligence}
}","Algorithmic contributions, Empirical contributions",Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-26564,springernature,Large language model( ChatGPT) as a support tool for breast tumor board,"Large language models( LLM) such as ChatGPT have gained public and scientific attention. The aim of this study is to evaluate ChatGPT as a support tool for breast tumor board decisions making. We inserted into ChatGPT-3. 5 clinical information of ten consecutive patients presented in a breast tumor board in our institution. We asked the chatbot to recommend management. The results generated by ChatGPT were compared to the final recommendations of the tumor board. They were also graded independently by two senior radiologists. Grading scores were between 1–5( 1 = completely disagree, 5 = completely agree) , and in three different categories: summarization, recommendation, and explanation. The mean age was 49. 4, 8/10( 80%) of patients had invasive ductal carcinoma, one patient( 1/10, 10%) had a ductal carcinoma in-situ and one patient( 1/10, 10%) had a phyllodes tumor with atypia. In seven out of ten cases( 70%) , ChatGPT’s recommendations were similar to the tumor board’s decisions. Mean scores while grading the chatbot’s summarization, recommendation and explanation by the first reviewer were 3. 7, 4. 3, and 4. 6 respectively. Mean values for the second reviewer were 4. 3, 4. 0, and 4. 3, respectively. In this proof-of-concept study, we present initial results on the use of an LLM as a decision support tool in a breast tumor board. Given the significant advancements, it is warranted for clinicians to be familiar with the potential benefits and harms of the technology.",10.1038/s41523-023-00557-8,http://dx.doi.org/10.1038/s41523-023-00557-8,Nature Partner Journal Breast Cancer,"Sorin, Vera;Klang, Eyal;Sklair-Levy, Miri;Cohen, Israel;Zippel, Douglas B.;Balint Lahat, Nora;Konen, Eli;Barash, Yiftach",2023,32,"@article{2-26564,
  title={Large language model (ChatGPT) as a support tool for breast tumor board},
  author={Sorin, Vera and Klang, Eyal and Sklair-Levy, Miri and Cohen, Israel and Zippel, Douglas B. and Balint Lahat, Nora and Konen, Eli and Barash, Yiftach},
  year={2023},
  doi={10.1038/s41523-023-00557-8},
  journal={Nature Partner Journal Breast Cancer}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Collaborating, Explaining, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-26611,springernature,A novel group recommender system for domain-independent decision support customizing a grouping genetic algorithm,"Group formation is a complex task requiring computational support to succeed. In the literature, there has been considerable effort in the development of algorithms for composing groups as well as their evaluation. The most widely used approach is the Genetic Algorithm, as, it can handle numerous variables, generating optimal solutions according to the problem requirements. In this study, a novel genetic algorithm was developed for forming groups using innovative genetic operators, such as a modification of 1-point and 2-point crossover, the gene and the group crossover, to improve its performance and accuracy. Moreover, the proposed algorithm can be characterized as domain-independent, as it allows any input regardless of the domain problem; i. e. , whether the groups concern objects, items or people, or whether the field of application is industry, education, healthcare, etc. The grouping genetic algorithm has been evaluated using a dataset from the literature in terms of its settings, showing that the tournament selection is better to be chosen when a quick solution is required, while the introduced gene and group crossover operators are superior to the classic ones. Furthermore, the combination of up to three crossover operators is ideal solution concerning algorithm’s accuracy and execution time. The effectiveness of the algorithm was tested in two grouping cases based on its acceptability. Both the students participated in forming collaborative groups and the professors participated in evaluating the groups of courses created were highly satisfied with the results. The contribution of this research is that it can help the stakeholders achieve an effective grouping using the presented genetic algorithm. In essence, they have the flexibility to execute the genetic algorithm in different contexts as many times as they want until to succeed the preferred output by choosing the number of operators for either greater accuracy or reduced execution time.",10.1007/s11257-023-09360-3,http://dx.doi.org/10.1007/s11257-023-09360-3,User Modeling and User-Adapted Interaction,"Krouska, Akrivi;Troussas, Christos;Sgouropoulou, Cleo",2023,29,"@article{2-26611,
  title     = {A novel group recommender system for domain-independent decision support customizing a grouping genetic algorithm},
  author    = {Krouska, Akrivi and Troussas, Christos and Sgouropoulou, Cleo},
  year      = {2023},
  doi       = {10.1007/s11257-023-09360-3},
  journal   = {User Modeling and User-Adapted Interaction}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Organizational,"Advising, Executing","Stakeholder, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-26646,springernature,"Out with AI, in with the psychiatrist: a preference for human-derived clinical decision support in depression care","Advancements in artificial intelligence( AI) are enabling the development of clinical support tools( CSTs) in psychiatry to facilitate the review of patient data and inform clinical care. To promote their successful integration and prevent over-reliance, it is important to understand how psychiatrists will respond to information provided by AI-based CSTs, particularly if it is incorrect. We conducted an experiment to examine psychiatrists’ perceptions of AI-based CSTs for treating major depressive disorder( MDD) and to determine whether perceptions interacted with the quality of CST information. Eighty-three psychiatrists read clinical notes about a hypothetical patient with MDD and reviewed two CSTs embedded within a single dashboard: the note’s summary and a treatment recommendation. Psychiatrists were randomised to believe the source of CSTs was either AI or another psychiatrist, and across four notes, CSTs provided either correct or incorrect information. Psychiatrists rated the CSTs on various attributes. Ratings for note summaries were less favourable when psychiatrists believed the notes were generated with AI as compared to another psychiatrist, regardless of whether the notes provided correct or incorrect information. A smaller preference for psychiatrist-generated information emerged in ratings of attributes that reflected the summary’s accuracy or its inclusion of important information from the full clinical note. Ratings for treatment recommendations were also less favourable when their perceived source was AI, but only when recommendations were correct. There was little evidence that clinical expertise or familiarity with AI impacted results. These findings suggest that psychiatrists prefer human-derived CSTs. This preference was less pronounced for ratings that may have prompted a deeper review of CST information( i. e. a comparison with the full clinical note to evaluate the summary’s accuracy or completeness, assessing an incorrect treatment recommendation) , suggesting a role of heuristics. Future work should explore other contributing factors and downstream implications for integrating AI into psychiatric care.",10.1038/s41398-023-02509-z,http://dx.doi.org/10.1038/s41398-023-02509-z,Translational Psychiatry,"Maslej, Marta M.;Kloiber, Stefan;Ghassemi, Marzyeh;Yu, Joanna;Hill, Sean L.",2023,8,"@article{2-26646,
  title = {Out with AI, in with the psychiatrist: a preference for human-derived clinical decision support in depression care},
  author = {Maslej, Marta M. and Kloiber, Stefan and Ghassemi, Marzyeh and Yu, Joanna and Hill, Sean L.},
  year = {2023},
  doi = {10.1038/s41398-023-02509-z},
  journal = {Translational Psychiatry}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-26693,springernature,Explainable recommendation: when design meets trust calibration,"Human-AI collaborative decision-making tools are being increasingly applied in critical domains such as healthcare. However, these tools are often seen as closed and intransparent for human decision-makers. An essential requirement for their success is the ability to provide explanations about themselves that are understandable and meaningful to the users. While explanations generally have positive connotations, studies showed that the assumption behind users interacting and engaging with these explanations could introduce trust calibration errors such as facilitating irrational or less thoughtful agreement or disagreement with the AI recommendation. In this paper, we explore how to help trust calibration through explanation interaction design. Our research method included two main phases. We first conducted a think-aloud study with 16 participants aiming to reveal main trust calibration errors concerning explainability in AI-Human collaborative decision-making tools. Then, we conducted two co-design sessions with eight participants to identify design principles and techniques for explanations that help trust calibration. As a conclusion of our research, we provide five design principles: Design for engagement, challenging habitual actions, attention guidance, friction and support training and learning. Our findings are meant to pave the way towards a more integrated framework for designing explanations with trust calibration as a primary goal.",10.1007/s11280-021-00916-0,http://dx.doi.org/10.1007/s11280-021-00916-0,World Wide Web Journal,"Naiseh, Mohammad;Al-Thani, Dena;Jiang, Nan;Ali, Raian",2021,70,"@article{2-26693,
  title={Explainable recommendation: when design meets trust calibration},
  author={Naiseh, Mohammad and Al-Thani, Dena and Jiang, Nan and Ali, Raian},
  year={2021},
  journal={World Wide Web Journal},
  doi={10.1007/s11280-021-00916-0}
}",Methodological contributions,Healthcare / Medicine / Surgery,Institutional,"Explaining, Advising, Collaborating","Decision-maker, Knowledge provider","Alter decision outcomes, Change trust, Change cognitive demands, Change affective-perceptual, Restrict human agency","Change AI responses, Update AI competence","confidence score, counterfactual explanations, example-based explanations, textual explanations, systematic cues","domain knowledge, attention guidance","Textual, Interactive interface",Yes,Yes
2-26694,springernature,Enhancing the reliability and accuracy of AI-enabled diagnosis via complementarity-driven deferral to clinicians,"Predictive artificial intelligence( AI) systems based on deep learning have been shown to achieve expert-level identification of diseases in multiple medical imaging settings, but can make errors in cases accurately diagnosed by clinicians and vice versa. We developed Complementarity-Driven Deferral to Clinical Workflow( CoDoC) , a system that can learn to decide between the opinion of a predictive AI model and a clinical workflow. CoDoC enhances accuracy relative to clinician-only or AI-only baselines in clinical workflows that screen for breast cancer or tuberculosis( TB). For breast cancer screening, compared to double reading with arbitration in a screening program in the UK, CoDoC reduced false positives by 25% at the same false-negative rate, while achieving a 66% reduction in clinician workload. For TB triaging, compared to standalone AI and clinical workflows, CoDoC achieved a 5–15% reduction in false positives at the same false-negative rate for three of five commercially available predictive AI systems. To facilitate the deployment of CoDoC in novel futuristic clinical settings, we present results showing that CoDoC’s performance gains are sustained across several axes of variation( imaging modality, clinical setting and predictive AI system) and discuss the limitations of our evaluation and where further validation would be needed. We provide an open-source implementation to encourage further research and application. A collaboration system helps to integrate decisions between human experts and AI to optimize screening and triaging and to reduce clinicians’ workload.",10.1038/s41591-023-02437-x,http://dx.doi.org/10.1038/s41591-023-02437-x,Nature Medicine,"Dvijotham, Krishnamurthy (Dj);Winkens, Jim;Barsbey, Melih;Ghaisas, Sumedh;Stanforth, Robert;Pawlowski, Nick;Strachan, Patricia;Ahmed, Zahra;Azizi, Shekoofeh;Bachrach, Yoram;Culp, Laura;Daswani, Mayank;Freyberg, Jan;Kelly, Christopher;Kiraly, Atilla;Kohlberger, Timo;McKinney, Scott;Mustafa, Basil;Natarajan, Vivek;Geras, Krzysztof;Witowski, Jan;Qin, Zhi Zhen;Creswell, Jacob;Shetty, Shravya;Sieniek, Marcin;Spitz, Terry;Corrado, Greg;Kohli, Pushmeet;Cemgil, Taylan;Karthikesalingam, Alan",2023,115,"@article{2-26694,
        author = {Dvijotham, Krishnamurthy (Dj) and Winkens, Jim and Barsbey, Melih and Ghaisas, Sumedh and Stanforth, Robert and Pawlowski, Nick and Strachan, Patricia and Ahmed, Zahra and Azizi, Shekoofeh and Bachrach, Yoram and Culp, Laura and Daswani, Mayank and Freyberg, Jan and Kelly, Christopher and Kiraly, Atilla and Kohlberger, Timo and McKinney, Scott and Mustafa, Basil and Natarajan, Vivek and Geras, Krzysztof and Witowski, Jan and Qin, Zhi Zhen and Creswell, Jacob and Shetty, Shravya and Sieniek, Marcin and Spitz, Terry and Corrado, Greg and Kohli, Pushmeet and Cemgil, Taylan and Karthikesalingam, Alan},
        journal = {Nature Medicine},
        number = {7},
        pages = {1814--1820},
        title = {Enhancing the reliability and accuracy of AI-enabled diagnosis via complementarity-driven deferral to clinicians},
        volume = {29},
        doi={10.1038/s41591-023-02437-x},
        year = {2023}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Executing, Forecasting, Collaborating","Decision-subject, Knowledge provider, Decision-maker","Change cognitive demands, Alter decision outcomes",Update AI competence,"prediction of alternative, deferral",NA,Visual,Yes,Yes
2-26700,springernature,Explaining AI-Based Decision Support Systems Using Concept Localization Maps,"Human-centric explainability of AI-based Decision Support Systems( DSS) using visual input modalities is directly related to reliability and practicality of such algorithms. An otherwise accurate and robust DSS might not enjoy trust of domain experts in critical application areas if it is not able to provide reasonable justifications for its predictions. This paper introduces Concept Localization Maps( CLMs) , which is a novel approach towards explainable image classifiers employed as DSS. CLMs extend Concept Activation Vectors( CAVs) by locating significant regions corresponding to a learned concept in the latent space of a trained image classifier. They provide qualitative and quantitative assurance of a classifier’s ability to learn and focus on similar concepts important for human experts during image recognition. To better understand the effectiveness of the proposed method, we generated a new synthetic dataset called Simple Concept DataBase( SCDB) that includes annotations for 10 distinguishable concepts, and made it publicly available. We evaluated our proposed method on SCDB as well as a real-world dataset called CelebA. We achieved localization recall of above 80% for most relevant concepts and average recall above 60% for all concepts using SE-ResNeXt-50 on SCDB. Our results on both datasets show great promise of CLMs for easing acceptance of DSS in clinical practice.",10.1007/978-3-030-63820-7_21,http://dx.doi.org/10.1007/978-3-030-63820-7_21,Neural Information Processing,"Lucieri, Adriano;Bajwa, Muhammad Naseer;Dengel, Andreas;Ahmed, Sheraz",2020,101,"@inproceedings{2-26700,
  title = {Explaining AI-Based Decision Support Systems Using Concept Localization Maps},
  author = {Lucieri, Adriano and Bajwa, Muhammad Naseer and Dengel, Andreas and Ahmed, Sheraz},
  year = {2020},
  booktitle = {Neural Information Processing},
  doi = {10.1007/978-3-030-63820-7_21}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Operational,"Explaining, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-26701,springernature,AI-Assisted Decision-making: a Cognitive Modeling Approach to Infer Latent Reliance Strategies,"AI assistance is readily available to humans in a variety of decision-making applications. In order to fully understand the efficacy of such joint decision-making, it is important to first understand the human’s reliance on AI. However, there is a disconnect between how joint decision-making is studied and how it is practiced in the real world. More often than not, researchers ask humans to provide independent decisions before they are shown AI assistance. This is done to make explicit the influence of AI assistance on the human’s decision. We develop a cognitive model that allows us to infer the latent reliance strategy of humans on AI assistance without asking the human to make an independent decision. We validate the model’s predictions through two behavioral experiments. The first experiment follows a concurrent paradigm where humans are shown AI assistance alongside the decision problem. The second experiment follows a sequential paradigm where humans provide an independent judgment on a decision problem before AI assistance is made available. The model’s predicted reliance strategies closely track the strategies employed by humans in the two experimental paradigms. Our model provides a principled way to infer reliance on AI-assistance and may be used to expand the scope of investigation on human-AI collaboration.",10.1007/s42113-022-00157-y,http://dx.doi.org/10.1007/s42113-022-00157-y,Computational Brain & Behavior,"Tejeda, Heliodoro;Kumar, Aakriti;Smyth, Padhraic;Steyvers, Mark",2022,67,"@article{2-26701,
  title={AI-Assisted Decision-making: a Cognitive Modeling Approach to Infer Latent Reliance Strategies},
  author={Tejeda, Heliodoro and Kumar, Aakriti and Smyth, Padhraic and Steyvers, Mark},
  year={2022},
  doi={10.1007/s42113-022-00157-y},
  journal={Computational Brain \& Behavior}
}","Methodological contributions, Theoretical contributions",Generic / Abstract / Domain-agnostic,Individual,"Advising, Executing",Decision-maker,"Alter decision outcomes, Change affective-perceptual, Change trust",no such info,"system accuracy, prediction of alternative, confidence score",NA,Visual,Yes,Yes
2-26729,springernature,Natural statistics support a rational account of confidence biases,"Previous work has sought to understand decision confidence as a prediction of the probability that a decision will be correct, leading to debate over whether these predictions are optimal, and whether they rely on the same decision variable as decisions themselves. This work has generally relied on idealized, low-dimensional models, necessitating strong assumptions about the representations over which confidence is computed. To address this, we used deep neural networks to develop a model of decision confidence that operates directly over high-dimensional, naturalistic stimuli. The model accounts for a number of puzzling dissociations between decisions and confidence, reveals a rational explanation of these dissociations in terms of optimization for the statistics of sensory inputs, and makes the surprising prediction that, despite these dissociations, decisions and confidence depend on a common decision variable. Human decision confidence displays a number of biases and has been shown to dissociate from decision accuracy. Here, by using neural network and Bayesian models, the authors show that these effects can be explained by the statistics of sensory inputs.",10.1038/s41467-023-39737-2,http://dx.doi.org/10.1038/s41467-023-39737-2,Nature Communications,"Webb, Taylor W.;Miyoshi, Kiyofumi;So, Tsz Yan;Rajananda, Sivananda;Lau, Hakwan",2023,28,"@article{2-26729,
  title={Natural statistics support a rational account of confidence biases},
  author={Webb, Taylor W. and Miyoshi, Kiyofumi and So, Tsz Yan and Rajananda, Sivananda and Lau, Hakwan},
  year={2023},
  journal={Nature Communications},
  doi={10.1038/s41467-023-39737-2}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Individual,"Explaining, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-26774,springernature,Cognitive prostheses for goal achievement,"Procrastination takes a considerable toll on people’s lives, the economy and society at large. Procrastination is often a consequence of people’s propensity to prioritize their immediate experiences over the long-term consequences of their actions. This suggests that aligning immediate rewards with long-term values could be a promising way to help people make more future-minded decisions and overcome procrastination. Here we develop an approach to decision support that leverages artificial intelligence and game elements to restructure challenging sequential decision problems in such a way that it becomes easier for people to take the right course of action. A series of four increasingly realistic experiments suggests that this approach can enable people to make better decisions faster, procrastinate less, complete their work on time and waste less time on unimportant tasks. These findings suggest that our method is a promising step towards developing cognitive prostheses that help people achieve their goals. Lieder et al. leverage artificial intelligence to redesign our to-do lists into games that make us more productive. Four experiments suggest that their approach can help people make better decisions, overcome procrastination and prioritize better.",10.1038/s41562-019-0672-9,http://dx.doi.org/10.1038/s41562-019-0672-9,Nature Human Behaviour,"Lieder, Falk;Chen, Owen X.;Krueger, Paul M.;Griffiths, Thomas L.",2019,4,"@article{2-26774,
  title={Cognitive prostheses for goal achievement},
  author={Lieder, Falk and Chen, Owen X. and Krueger, Paul M. and Griffiths, Thomas L.},
  year={2019},
  doi={10.1038/s41562-019-0672-9},
  journal={Nature Human Behaviour}
}","Empirical contributions, Theoretical contributions",Generic / Abstract / Domain-agnostic,Individual,Advising,Decision-maker,"Alter decision outcomes, Change cognitive demands, Change affective-perceptual",no such info,NA,NA,"Interactive interface, Visual",Yes,Yes
2-26780,springernature,A hierarchical expert-guided machine learning framework for clinical decision support systems: an application to traumatic brain injury prognostication,"Prognosis of the long-term functional outcome of traumatic brain injury is essential for personalized management of that injury. Nonetheless, accurate prediction remains unavailable. Although machine learning has shown promise in many fields, including medical diagnosis and prognosis, such models are rarely deployed in real-world settings due to a lack of transparency and trustworthiness. To address these drawbacks, we propose a machine learning-based framework that is explainable and aligns with clinical domain knowledge. To build such a framework, additional layers of statistical inference and human expert validation are added to the model, which ensures the predicted risk score’s trustworthiness. Using 831 patients with moderate or severe traumatic brain injury to build a model using the proposed framework, an area under the receiver operating characteristic curve( AUC) and accuracy of 0. 8085 and 0. 7488 were achieved, respectively, in determining which patients will experience poor functional outcomes. The performance of the machine learning classifier is not adversely affected by the imposition of statistical and domain knowledge “checks and balances”. Finally, through a case study, we demonstrate how the decision made by a model might be biased if it is not audited carefully.",10.1038/s41746-021-00445-0,http://dx.doi.org/10.1038/s41746-021-00445-0,Nature Partner Journals Digital Medicine,"Farzaneh, Negar;Williamson, Craig A.;Gryak, Jonathan;Najarian, Kayvan",2021,55,"@article{2-26780,
  title = {A hierarchical expert-guided machine learning framework for clinical decision support systems: an application to traumatic brain injury prognostication},
  author = {Farzaneh, Negar and Williamson, Craig A. and Gryak, Jonathan and Najarian, Kayvan},
  year = {2021},
  doi = {10.1038/s41746-021-00445-0},
  journal = {Nature Partner Journals Digital Medicine}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Forecasting","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-26784,springernature,Deep learning-aided decision support for diagnosis of skin disease across skin tones,"Although advances in deep learning systems for image-based medical diagnosis demonstrate their potential to augment clinical decision-making, the effectiveness of physician–machine partnerships remains an open question, in part because physicians and algorithms are both susceptible to systematic errors, especially for diagnosis of underrepresented populations. Here we present results from a large-scale digital experiment involving board-certified dermatologists( n = 389) and primary-care physicians( n = 459) from 39 countries to evaluate the accuracy of diagnoses submitted by physicians in a store-and-forward teledermatology simulation. In this experiment, physicians were presented with 364 images spanning 46 skin diseases and asked to submit up to four differential diagnoses. Specialists and generalists achieved diagnostic accuracies of 38% and 19%, respectively, but both specialists and generalists were four percentage points less accurate for the diagnosis of images of dark skin as compared to light skin. Fair deep learning system decision support improved the diagnostic accuracy of both specialists and generalists by more than 33%, but exacerbated the gap in the diagnostic accuracy of generalists across skin tones. These results demonstrate that well-designed physician–machine partnerships can enhance the diagnostic accuracy of physicians, illustrating that success in improving overall diagnostic accuracy does not necessarily address bias. In a large-scale study involving 389 board-certified dermatologists and 459 primary-care physicians from 39 countries, the impact of a deep learning-aided decision support system on physicians’ diagnostic accuracy was tested across 46 skin diseases and for both light and dark skin tones.",10.1038/s41591-023-02728-3,http://dx.doi.org/10.1038/s41591-023-02728-3,Nature Medicine,"Groh, Matthew;Badri, Omar;Daneshjou, Roxana;Koochek, Arash;Harris, Caleb;Soenksen, Luis R.;Doraiswamy, P. Murali;Picard, Rosalind",2024,0,"@article{2-26784,
  title={Deep learning-aided decision support for diagnosis of skin disease across skin tones},
  author={Groh, Matthew and Badri, Omar and Daneshjou, Roxana and Koochek, Arash and Harris, Caleb and Soenksen, Luis R. and Doraiswamy, P. Murali and Picard, Rosalind},
  year={2024},
  journal={Nature Medicine},
  doi={10.1038/s41591-023-02728-3}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Decision-maker, Decision-subject","Change trust, Alter decision outcomes, Shape ethical norms",no such info,"preliminary diagnoses, visual explanations, confidence score","corrective feedback, domain knowledge","Textual, Visual, Interactive interface",Yes,Yes
2-26826,springernature,A reinforcement learning diffusion decision model for value-based decisions,"Psychological models of value-based decision-making describe how subjective values are formed and mapped to single choices. Recently, additional efforts have been made to describe the temporal dynamics of these processes by adopting sequential sampling models from the perceptual decision-making tradition, such as the diffusion decision model( DDM). These models, when applied to value-based decision-making, allow mapping of subjective values not only to choices but also to response times. However, very few attempts have been made to adapt these models to situations in which decisions are followed by rewards, thereby producing learning effects. In this study, we propose a new combined reinforcement learning diffusion decision model( RLDDM) and test it on a learning task in which pairs of options differ with respect to both value difference and overall value. We found that participants became more accurate and faster with learning, responded faster and more accurately when options had more dissimilar values, and decided faster when confronted with more attractive( i. e. , overall more valuable) pairs of options. We demonstrate that the suggested RLDDM can accommodate these effects and does so better than previously proposed models. To gain a better understanding of the model dynamics, we also compare it to standard DDMs and reinforcement learning models. Our work is a step forward towards bridging the gap between two traditions of decision-making research.",10.3758/s13423-018-1554-2,http://dx.doi.org/10.3758/s13423-018-1554-2,Psychonomic Bulletin & Review,"Fontanesi, Laura;Gluth, Sebastian;Spektor, Mikhail S.;Rieskamp, Jörg",2019,252,"@article{2-26826,
  title={A reinforcement learning diffusion decision model for value-based decisions},
  author={Fontanesi, Laura and Gluth, Sebastian and Spektor, Mikhail S. and Rieskamp, J{\""o}rg},
  year={2019},
  journal={Psychonomic Bulletin & Review},
  doi={10.3758/s13423-018-1554-2}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Individual,Analyzing,"Decision-subject, Knowledge provider, Decision-maker",Alter decision outcomes,Change AI responses,"prediction of alternative, confidence score",adaptability,"Textual, Visual, Conversational/Natural Language",Yes,Yes
2-26862,springernature,The impact of inconsistent human annotations on AI driven clinical decision making,"In supervised learning model development, domain experts are often used to provide the class labels( annotations). Annotation inconsistencies commonly occur when even highly experienced clinical experts annotate the same phenomenon( e. g. , medical image, diagnostics, or prognostic status) , due to inherent expert bias, judgments, and slips, among other factors. While their existence is relatively well-known, the implications of such inconsistencies are largely understudied in real-world settings, when supervised learning is applied on such ‘noisy’ labelled data. To shed light on these issues, we conducted extensive experiments and analyses on three real-world Intensive Care Unit( ICU) datasets. Specifically, individual models were built from a common dataset, annotated independently by 11 Glasgow Queen Elizabeth University Hospital ICU consultants, and model performance estimates were compared through internal validation( Fleiss’ κ = 0. 383 i. e. , fair agreement). Further, broad external validation( on both static and time series datasets) of these 11 classifiers was carried out on a HiRID external dataset, where the models’ classifications were found to have low pairwise agreements( average Cohen’s κ = 0. 255 i. e. , minimal agreement). Moreover, they tend to disagree more on making discharge decisions( Fleiss’ κ = 0. 174) than predicting mortality( Fleiss’ κ = 0. 267). Given these inconsistencies, further analyses were conducted to evaluate the current best practices in obtaining gold-standard models and determining consensus. The results suggest that:( a) there may not always be a “super expert” in acute clinical settings( using internal and external validation model performances as a proxy) ; and( b) standard consensus seeking( such as majority vote) consistently leads to suboptimal models. Further analysis, however, suggests that assessing annotation learnability and using only ‘learnable’ annotated datasets for determining consensus achieves optimal models in most cases.",10.1038/s41746-023-00773-3,http://dx.doi.org/10.1038/s41746-023-00773-3,Nature Partner Journals Digital Medicine,"Sylolypavan, Aneeta;Sleeman, Derek;Wu, Honghan;Sim, Malcolm",2023,0,"@article{2-26862,
  title = {The impact of inconsistent human annotations on AI driven clinical decision making},
  author = {Sylolypavan, Aneeta and Sleeman, Derek and Wu, Honghan and Sim, Malcolm},
  year = {2023},
  doi = {10.1038/s41746-023-00773-3},
  journal = {Nature Partner Journals Digital Medicine}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Knowledge provider, Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-26912,springernature,Machine learning-derived clinical decision algorithm for the diagnosis of hyperfunctioning parathyroid glands in patients with primary hyperparathyroidism,"Purpose To train and validate machine learning-derived clinical decision algorithm( ML CDA) for the diagnosis of hyperfunctioning parathyroid glands using preoperative variables to facilitate surgical planning. Methods This retrospective study included 458 consecutive primary hyperparathyroidism( PHPT) patients who underwent combined 4D-CT and sestamibi SPECT/CT( MIBI) with subsequent parathyroidectomy from February 2013 to September 2016. The study cohort was divided into training( first 400 patients) and validation sets( remaining 58 patients). Sixteen clinical, laboratory, and imaging variables were evaluated. A random forest algorithm selected the best predictor variables and generated a clinical decision algorithm with the highest performance( ML CDA). The ML CDA was trained to predict the probability of a hyperfunctioning vs normal gland for each of the four parathyroid glands in a patient. The reference standard was a four-quadrant location on operative reports and pathology. The accuracy of ML CDA was prospectively validated. Results Of 16 variables, the algorithm selected 3 variables for optimal prediction: combined 4D-CT and MIBI using( 1) sensitive reading, ( 2) specific reading, and( 3) cross-product of serum calcium and parathyroid hormone levels and outputted an ML CDA using five probability categories for hyperfunctioning glands. The ML CDA demonstrated excellent accuracy for correct classification in the training( 4D-CT + MIBI: 0. 91 [95% CI: 0. 89–0. 92]) and validation sets( 4D-CT + MIBI: 0. 90 [95% CI: 0. 86–0. 94]. Conclusion Machine learning generated a clinical decision algorithm that accurately diagnosed hyperfunctioning parathyroid glands through classification into probability categories, which can be implemented for improved preoperative planning and convey diagnostic certainty. Key Points Question Can an ML CDA use preoperative variables for the diagnosis of hyperfunctioning parathyroid glands to facilitate surgical planning ? Findings The developed ML CDA demonstrated excellent accuracy for correct classification in the training( 0. 91 [95% CI: 0. 89–0. 92]) and validation sets( 0. 90 [95% CI: 0. 86–0. 94]). Clinical relevance Using standard preoperative variables, an ML CDA for diagnosing hyperfunctioning parathyroid glands can be implemented to improve preoperative parathyroid localization and included in radiology reports for surgical planning. Graphical Abstract",10.1007/s00330-024-11159-8,http://dx.doi.org/10.1007/s00330-024-11159-8,European Radiology,"Yeh, Randy;Kuo, Jennifer H.;Huang, Bernice;Shobeiri, Parnian;Lee, James A.;Tay, Yu-Kwang Donovan;Tabacco, Gaia;Bilezikian, John P.;Dercle, Laurent",2024,158,"@article{2-26912,
  title = {Machine learning-derived clinical decision algorithm for the diagnosis of hyperfunctioning parathyroid glands in patients with primary hyperparathyroidism},
  author = {Yeh, Randy and Kuo, Jennifer H. and Huang, Bernice and Shobeiri, Parnian and Lee, James A. and Tay, Yu-Kwang Donovan and Tabacco, Gaia and Bilezikian, John P. and Dercle, Laurent},
  year = {2024},
  doi = {10.1007/s00330-024-11159-8},
  journal = {European Radiology}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-26921,springernature,Resource-Based Adaptive Robotic Process Automation,"Robotic process automation is evolving from robots mimicking human workers in automating information acquisition tasks, to robots performing human decision tasks using machine learning algorithms. In either of these situations, robots or automation agents can have distinct characteristics in their performance, much like human agents. Hence, the execution of an automated task may require adaptations with human participants executing the task when robots fail, to taking a supervisory role or having no involvement. In this paper, we consider different levels of automation, and the corresponding coordination required by resources that include human participants and robots. We capture resource characteristics and define business process constraints that support process adaptations with human-automation coordination. We then use a real-world business process and incorporate automation agents, compute resource characteristics, and use resource-aware constraints to illustrate resource-based process adaptations for its automation.",10.1007/978-3-030-49435-3_28,http://dx.doi.org/10.1007/978-3-030-49435-3_28,Advanced Information Systems Engineering (CAiSE),"Sindhgatta, Renuka;Hofstede, Arthur H. M.;Ghose, Aditya",2020,13,"@inproceedings{2-26921,
  title     = {Resource-Based Adaptive Robotic Process Automation},
  author    = {Sindhgatta, Renuka and Hofstede, Arthur H. M. and Ghose, Aditya},
  year      = {2020},
  booktitle = {Advanced Information Systems Engineering (CAiSE)},
  doi       = {10.1007/978-3-030-49435-3_28}
}",Methodological contributions,Manufacturing / Industry / Automation,Operational,"Executing, Collaborating","Developer, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-26946,springernature,Lane Change Decision-Making of Autonomous Driving Based on Interpretable Soft Actor-Critic Algorithm with Safety Awareness,"Safe and efficient lane change behavior is indispensable and significant for autonomous driving. A new lane change decision-making scheme is proposed for autonomous driving based on Soft Actor-Critic( SAC) algorithm. Combined the kinematics information with visual image information of ego vehicle, a multi-data fusion convolutional neural network structure is constructed to improve the perception ability of the algorithm with continuous action space. Moreover, the interpretability of the algorithm is enhanced through feature refinement of surrounding vehicles and key road environment information with attention mechanism. In order to ensure driving safety and driving efficiency, the reward function is constructed comprehensively considering the driving risk assessment and competitive consciousness. In static scenario and stochastic dynamically one, the experiments are conducted in the CARLA simulator and the results demonstrate the scheme proposed is effective and feasible.",10.1007/978-3-031-20503-3_40,http://dx.doi.org/10.1007/978-3-031-20503-3_40,Artificial Intelligence,"Yu, Di;Tian, Kang;Liu, Yuhui;Xu, Manchen",2022,3,"@inproceedings{2-26946,
  title={Lane Change Decision-Making of Autonomous Driving Based on Interpretable Soft Actor-Critic Algorithm with Safety Awareness},
  author={Yu, Di and Tian, Kang and Liu, Yuhui and Xu, Manchen},
  year={2022},
  doi={10.1007/978-3-031-20503-3_40},
  booktitle={Artificial Intelligence}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Analyzing","Decision-subject, Knowledge provider, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-26985,springernature,Personalising intravenous to oral antibiotic switch decision making through fair interpretable machine learning,"Antimicrobial resistance( AMR) and healthcare associated infections pose a significant threat globally. One key prevention strategy is to follow antimicrobial stewardship practices, in particular, to maximise targeted oral therapy and reduce the use of indwelling vascular devices for intravenous( IV) administration. Appreciating when an individual patient can switch from IV to oral antibiotic treatment is often non-trivial and not standardised. To tackle this problem we created a machine learning model to predict when a patient could switch based on routinely collected clinical parameters. 10, 362 unique intensive care unit stays were extracted and two informative feature sets identified. Our best model achieved a mean AUROC of 0. 80( SD 0. 01) on the hold-out set while not being biased to individuals protected characteristics. Interpretability methodologies were employed to create clinically useful visual explanations. In summary, our model provides individualised, fair, and interpretable predictions for when a patient could switch from IV-to-oral antibiotic treatment. Prospectively evaluation of safety and efficacy is needed before such technology can be applied clinically. The decision to switch patients from intravenous to oral antibiotic therapy is important for the individual and wider society. Here, authors show a machine learning model using routine clinical data can predict when a patient could switch.",10.1038/s41467-024-44740-2,http://dx.doi.org/10.1038/s41467-024-44740-2,Nature Communications,"Bolton, William J.;Wilson, Richard;Gilchrist, Mark;Georgiou, Pantelis;Holmes, Alison;Rawson, Timothy M.",2024,1,"@article{2-26985,
  title={Personalising intravenous to oral antibiotic switch decision making through fair interpretable machine learning},
  author={Bolton, William J. and Wilson, Richard and Gilchrist, Mark and Georgiou, Pantelis and Holmes, Alison and Rawson, Timothy M.},
  year={2024},
  journal={Nature Communications},
  doi={10.1038/s41467-024-44740-2}
}","Empirical contributions, Algorithmic contributions",Healthcare / Medicine / Surgery,Operational,"Forecasting, Explaining, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-27018,springernature,An Algorithmic Assessment of Parole Decisions,"Objectives Parole is an important mechanism for alleviating the extraordinary social and financial costs of mass incarceration. Yet parole boards can also present a major obstacle, denying parole to low-risk inmates who could safely be released from prison. We evaluate a major parole institution, the New York State Parole Board, quantifying the costs of non-risk-based decision-making. Methods Using ensemble machine Learning, we predict any arrest and any violent felony arrest within three years to generate criminal risk predictions for individuals released on parole in New York from 2012–2015. We quantify the social welfare loss of the Board’s non-risked-base decisions by rank ordering inmates by their predicted risk and estimating the crime rates that could have been achieved with counterfactual, risk-based release decisions. We also estimate the release rates that could have been achieved holding arrest rates constant. We attend to the “selective labels” problem in several ways, including by testing the validity of the algorithm for individuals who were denied parole but later released after the expiration of their sentence. Results We conservatively estimate that the Board could have more than doubled the release rate without increasing the total or violent felony arrest rate, and that they could have achieved these gains while simultaneously eliminating racial disparities in release rates. Conclusions This study demonstrates the utility of algorithms for evaluating criminal justice decision-making. Our analyses suggest that many individuals are being denied parole and incarcerated past their minimum sentence despite being a low risk to public safety.",10.1007/s10940-022-09563-8,http://dx.doi.org/10.1007/s10940-022-09563-8,Journal of Quantitative Criminology,"Laqueur, Hannah S.;Copus, Ryan W.",2024,18,"@article{2-27018,
  title={An Algorithmic Assessment of Parole Decisions},
  author={Laqueur, Hannah S. and Copus, Ryan W.},
  year={2024},
  journal={Journal of Quantitative Criminology},
  doi={10.1007/s10940-022-09563-8}
}",Empirical contributions,Law / Policy / Governance,Operational,"Forecasting, Monitoring","Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-27027,springernature,Complementarities between algorithmic and human decision-making: The case of antibiotic prescribing,"Artificial Intelligence has the potential to improve human decisions in complex environments, but its effectiveness can remain limited if humans hold context-specific private information. Using the empirical example of antibiotic prescribing for urinary tract infections, we show that full automation of prescribing fails to improve on physician decisions. Instead, optimally delegating a share of decisions to physicians, where they possess private diagnostic information, effectively utilizes the complementarity between algorithmic and human decisions. Combining physician and algorithmic decisions can achieve a reduction in inefficient overprescribing of antibiotics by 20. 3 percent.",10.1007/s11129-024-09284-1,http://dx.doi.org/10.1007/s11129-024-09284-1,Quantitative Marketing and Economics,"Ribers, Michael Allan;Ullrich, Hannes",2024,3,"@article{2-27027,
  title = {Complementarities between algorithmic and human decision-making: The case of antibiotic prescribing},
  author = {Ribers, Michael Allan and Ullrich, Hannes},
  year = {2024},
  doi = {10.1007/s11129-024-09284-1},
  journal = {Quantitative Marketing and Economics}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Collaborating, Advising","Decision-maker, Decision-subject, Knowledge provider","Alter decision outcomes, Change cognitive demands","Update AI competence, Change AI responses","preliminary diagnoses, prediction of alternative, delegation, confidence score","personalized settings, domain knowledge","Textual, Interactive interface",Yes,Yes
2-27034,springernature,The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care,"Sepsis is the third leading cause of death worldwide and the main cause of mortality in hospitals 1 – 3, but the best treatment strategy remains uncertain. In particular, evidence suggests that current practices in the administration of intravenous fluids and vasopressors are suboptimal and likely induce harm in a proportion of patients 1, 4 – 6. To tackle this sequential decision-making problem, we developed a reinforcement learning agent, the Artificial Intelligence( AI) Clinician, which extracted implicit knowledge from an amount of patient data that exceeds by many-fold the life-time experience of human clinicians and learned optimal treatment by analyzing a myriad of( mostly suboptimal) treatment decisions. We demonstrate that the value of the AI Clinician’s selected treatment is on average reliably higher than human clinicians. In a large validation cohort independent of the training data, mortality was lowest in patients for whom clinicians’ actual doses matched the AI decisions. Our model provides individualized and clinically interpretable treatment decisions for sepsis that could improve patient outcomes. A reinforcement learning agent, the AI Clinician, can assist physicians by providing individualized and clinically interpretable treatment decisions to improve patient outcomes.",10.1038/s41591-018-0213-5,http://dx.doi.org/10.1038/s41591-018-0213-5,Nature Medicine,"Komorowski, Matthieu;Celi, Leo A.;Badawi, Omar;Gordon, Anthony C.;Faisal, A. Aldo",2018,4,"@article{2-27034,
  title = {The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care},
  author = {Komorowski, Matthieu and Celi, Leo A. and Badawi, Omar and Gordon, Anthony C. and Faisal, A. Aldo},
  year = {2018},
  doi = {10.1038/s41591-018-0213-5},
  journal = {Nature Medicine}
}","System/Artifact contributions, Algorithmic contributions",Healthcare / Medicine / Surgery,Operational,"Executing, Advising","Decision-subject, Decision-maker",Alter decision outcomes,Update AI competence,recommendations,NA,"Textual, Interactive interface",Yes,Yes
2-27038,springernature,Impact of a deep learning assistant on the histopathologic classification of liver cancer,"Artificial intelligence( AI) algorithms continue to rival human performance on a variety of clinical tasks, while their actual impact on human diagnosticians, when incorporated into clinical workflows, remains relatively unexplored. In this study, we developed a deep learning-based assistant to help pathologists differentiate between two subtypes of primary liver cancer, hepatocellular carcinoma and cholangiocarcinoma, on hematoxylin and eosin-stained whole-slide images( WSI) , and evaluated its effect on the diagnostic performance of 11 pathologists with varying levels of expertise. Our model achieved accuracies of 0. 885 on a validation set of 26 WSI, and 0. 842 on an independent test set of 80 WSI. Although use of the assistant did not change the mean accuracy of the 11 pathologists( p = 0. 184, OR = 1. 281) , it significantly improved the accuracy( p = 0. 045, OR = 1. 499) of a subset of nine pathologists who fell within well-defined experience levels( GI subspecialists, non-GI subspecialists, and trainees). In the assisted state, model accuracy significantly impacted the diagnostic decisions of all 11 pathologists. As expected, when the model’s prediction was correct, assistance significantly improved accuracy( p = 0. 000, OR = 4. 289) , whereas when the model’s prediction was incorrect, assistance significantly decreased accuracy( p = 0. 000, OR = 0. 253) , with both effects holding across all pathologist experience levels and case difficulty levels. Our results highlight the challenges of translating AI models into the clinical setting, and emphasize the importance of taking into account potential unintended negative consequences of model assistance when designing and testing medical AI-assistance tools.",10.1038/s41746-020-0232-8,http://dx.doi.org/10.1038/s41746-020-0232-8,Nature Partner Journals Digital Medicine,"Kiani, Amirhossein;Uyumazturk, Bora;Rajpurkar, Pranav;Wang, Alex;Gao, Rebecca;Jones, Erik;Yu, Yifan;Langlotz, Curtis P.;Ball, Robyn L.;Montine, Thomas J.;Martin, Brock A.;Berry, Gerald J.;Ozawa, Michael G.;Hazard, Florette K.;Brown, Ryanne A.;Chen, Simon B.;Wood, Mona;Allard, Libby S.;Ylagan, Lourdes;Ng, Andrew Y.;Shen, Jeanne",2020,308,"@article{2-27038,
  title={Impact of a deep learning assistant on the histopathologic classification of liver cancer},
  author={Kiani, Amirhossein and Uyumazturk, Bora and Rajpurkar, Pranav and Wang, Alex and Gao, Rebecca and Jones, Erik and Yu, Yifan and Langlotz, Curtis P. and Ball, Robyn L. and Montine, Thomas J. and Martin, Brock A. and Berry, Gerald J. and Ozawa, Michael G. and Hazard, Florette K. and Brown, Ryanne A. and Chen, Simon B. and Wood, Mona and Allard, Libby S. and Ylagan, Lourdes and Ng, Andrew Y. and Shen, Jeanne},
  year={2020},
  doi={10.1038/s41746-020-0232-8},
  journal={Nature Partner Journals Digital Medicine}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-subject, Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-27045,springernature,"Interpretability, personalization and reliability of a machine learning based clinical decision support system","Artificial intelligence( AI) has achieved notable performances in many fields and its research impact in healthcare has been unquestionable. Nevertheless, the deployment of such computational models in clinical practice is still limited. Some of the major issues recognized as barriers to a successful real-world machine learning applications include lack of: transparency; reliability and personalization. Actually, these aspects are decisive not only for patient safety, but also to assure the confidence of professionals. Explainable AI aims at to achieve solutions for artificial intelligence transparency and reliability concerns, with the capacity to better understand and trust a model, providing the ability to justify its outcomes, thus effectively assisting clinicians in rationalizing the model prediction. This work proposes an innovative machine learning based approach, implementing a hybrid scheme, able to combine in a systematic way knowledge-driven and data-driven techniques. In a first step a global set of interpretable rules is generated, founded on clinical evidence. Then, in a second phase, a machine learning model is trained to select, from the global set of rules, the subset that is more appropriate for a given patient, according to his particular characteristics. This approach addresses simultaneously three of the central requirements of explainable AI—interpretability, personalization, and reliability—without impairing the accuracy of the model’s prediction. The scheme was validated with a real dataset provided by two Portuguese Hospitals, the Santa Cruz Hospital, Lisbon, and the Santo André Hospital, Leiria, comprising a total of N = 1111 patients that suffered an acute coronary syndrome event, where the 30 days mortality was assessed. When compared with standard black-box structures( e. g. feedforward neural network) the proposed scheme achieves similar performances, while ensures simultaneously clinical interpretability and personalization of the model, as well as provides a level of reliability to the estimated mortality risk.",10.1007/s10618-022-00821-8,http://dx.doi.org/10.1007/s10618-022-00821-8,Data Mining and Knowledge Discovery,"Valente, F.;Paredes, S.;Henriques, J.;Rocha, T.;Carvalho, P.;Morais, J.",2022,0,"@article{2-27045,
  title={Interpretability, personalization and reliability of a machine learning based clinical decision support system},
  author={Valente, F. and Paredes, S. and Henriques, J. and Rocha, T. and Carvalho, P. and Morais, J.},
  year={2022},
  journal={Data Mining and Knowledge Discovery},
  doi={10.1007/s10618-022-00821-8}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting, Explaining","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-27049,springernature,Do as AI say: susceptibility in deployment of clinical decision-aids,"Artificial intelligence( AI) models for decision support have been developed for clinical settings such as radiology, but little work evaluates the potential impact of such systems. In this study, physicians received chest X-rays and diagnostic advice, some of which was inaccurate, and were asked to evaluate advice quality and make diagnoses. All advice was generated by human experts, but some was labeled as coming from an AI system. As a group, radiologists rated advice as lower quality when it appeared to come from an AI system; physicians with less task-expertise did not. Diagnostic accuracy was significantly worse when participants received inaccurate advice, regardless of the purported source. This work raises important considerations for how advice, AI and non-AI, should be deployed in clinical environments.",10.1038/s41746-021-00385-9,http://dx.doi.org/10.1038/s41746-021-00385-9,Nature Partner Journals Digital Medicine,"Gaube, Susanne;Suresh, Harini;Raue, Martina;Merritt, Alexander;Berkowitz, Seth J.;Lermer, Eva;Coughlin, Joseph F.;Guttag, John V.;Colak, Errol;Ghassemi, Marzyeh",2021,447,"@article{2-27049,
  title = {Do as AI say: susceptibility in deployment of clinical decision-aids},
  author = {Gaube, Susanne and Suresh, Harini and Raue, Martina and Merritt, Alexander and Berkowitz, Seth J. and Lermer, Eva and Coughlin, Joseph F. and Guttag, John V. and Colak, Errol and Ghassemi, Marzyeh},
  year = {2021},
  doi = {10.1038/s41746-021-00385-9},
  journal = {Nature Partner Journals Digital Medicine}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Guardian, Decision-subject, Knowledge provider","Change cognitive demands, Alter decision outcomes, Change trust, Change affective-perceptual",no such info,preliminary diagnoses,domain knowledge,"Textual, Visual, Conversational/Natural Language",Yes,Yes
2-27055,springernature,Dermatologist-like explainable AI enhances trust and confidence in diagnosing melanoma,"Artificial intelligence( AI) systems have been shown to help dermatologists diagnose melanoma more accurately, however they lack transparency, hindering user acceptance. Explainable AI( XAI) methods can help to increase transparency, yet often lack precise, domain-specific explanations. Moreover, the impact of XAI methods on dermatologists’ decisions has not yet been evaluated. Building upon previous research, we introduce an XAI system that provides precise and domain-specific explanations alongside its differential diagnoses of melanomas and nevi. Through a three-phase study, we assess its impact on dermatologists’ diagnostic accuracy, diagnostic confidence, and trust in the XAI-support. Our results show strong alignment between XAI and dermatologist explanations. We also show that dermatologists’ confidence in their diagnoses, and their trust in the support system significantly increase with XAI compared to conventional AI. This study highlights dermatologists’ willingness to adopt such XAI systems, promoting future use in the clinic. Artificial intelligence has become popular as a cancer classification tool, but there is distrust of such systems due to their lack of transparency. Here, the authors develop an explainable AI system which produces textand region-based explanations alongside its classifications which was assessed using clinicians’ diagnostic accuracy, diagnostic confidence, and their trust in the system.",10.1038/s41467-023-43095-4,http://dx.doi.org/10.1038/s41467-023-43095-4,Nature Communications,"Chanda, Tirtha;Hauser, Katja;Hobelsberger, Sarah;Bucher, Tabea-Clara;Garcia, Carina Nogueira;Wies, Christoph;Kittler, Harald;Tschandl, Philipp;Navarrete-Dechent, Cristian;Podlipnik, Sebastian;Chousakos, Emmanouil;Crnaric, Iva;Majstorovic, Jovana;Alhajwan, Linda;Foreman, Tanya;Peternel, Sandra;Sarap, Sergei;Özdemir, İrem;Barnhill, Raymond L.;Llamas-Velasco, Mar;Poch, Gabriela;Korsing, Sören;Sondermann, Wiebke;Gellrich, Frank Friedrich;Heppt, Markus V.;Erdmann, Michael;Haferkamp, Sebastian;Drexler, Konstantin;Goebeler, Matthias;Schilling, Bastian;Utikal, Jochen S.;Ghoreschi, Kamran;Fröhling, Stefan;Krieghoff-Henning, Eva;Brinker, Titus J.;Reader Study Consortium",2024,125,"@article{2-27055,
  title = {Dermatologist-like explainable AI enhances trust and confidence in diagnosing melanoma},
  author = {Chanda, Tirtha and Hauser, Katja and Hobelsberger, Sarah and Bucher, Tabea-Clara and Garcia, Carina Nogueira and Wies, Christoph and Kittler, Harald and Tschandl, Philipp and Navarrete-Dechent, Cristian and Podlipnik, Sebastian and Chousakos, Emmanouil and Crnaric, Iva and Majstorovic, Jovana and Alhajwan, Linda and Foreman, Tanya and Peternel, Sandra and Sarap, Sergei and \""{O}zdemir, \.{I}rem and Barnhill, Raymond L. and Llamas-Velasco, Mar and Poch, Gabriela and Korsing, S\""{o}ren and Sondermann, Wiebke and Gellrich, Frank Friedrich and Heppt, Markus V. and Erdmann, Michael and Haferkamp, Sebastian and Drexler, Konstantin and Goebeler, Matthias and Schilling, Bastian and Utikal, Jochen S. and Ghoreschi, Kamran and Fr\""{o}hling, Stefan and Krieghoff-Henning, Eva and Brinker, Titus J. and Reader Study Consortium},
  year = {2024},
  doi = {10.1038/s41467-023-43095-4},
  journal = {Nature Communications}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Explaining","Decision-maker, Guardian","Change trust, Alter decision outcomes, Change affective-perceptual","Update AI competence, Change AI responses","confidence score, prediction of alternative, textual explanations, visual explanations",NA,"Textual, Visual",Yes,Yes
2-27068,springernature,HIVE: Evaluating the Human Interpretability of Visual Explanations,"As AI technology is increasingly applied to high-impact, high-risk domains, there have been a number of new methods aimed at making AI models more human interpretable. Despite the recent growth of interpretability work, there is a lack of systematic evaluation of proposed techniques. In this work, we introduce HIVE( Human Interpretability of Visual Explanations) , a novel human evaluation framework that assesses the utility of explanations to human users in AI-assisted decision making scenarios, and enables falsifiable hypothesis testing, cross-method comparison, and human-centered evaluation of visual interpretability methods. To the best of our knowledge, this is the first work of its kind. Using HIVE, we conduct IRB-approved human studies with nearly 1000 participants and evaluate four methods that represent the diversity of computer vision interpretability works: GradCAM, BagNet, ProtoPNet, and ProtoTree. Our results suggest that explanations engender human trust, even for incorrect predictions, yet are not distinct enough for users to distinguish between correct and incorrect predictions. We open-source HIVE to enable future studies and encourage more human-centered approaches to interpretability research. HIVE can be found at https://princetonvisualai. github. io/HIVE.",10.1007/978-3-031-19775-8_17,http://dx.doi.org/10.1007/978-3-031-19775-8_17,European Conference on Computer Vision,"Kim, Sunnie S. Y.;Meister, Nicole;Ramaswamy, Vikram V.;Fong, Ruth;Russakovsky, Olga",2022,160,"@inproceedings{2-27068,
  title={HIVE: Evaluating the Human Interpretability of Visual Explanations},
  author={Kim, Sunnie S. Y. and Meister, Nicole and Ramaswamy, Vikram V. and Fong, Ruth and Russakovsky, Olga},
  year={2022},
  booktitle={European Conference on Computer Vision},
  doi={10.1007/978-3-031-19775-8_17}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,no such info,"Explaining, Advising","Decision-maker, Guardian","Alter decision outcomes, Change trust, Change cognitive demands, Change affective-perceptual",no such info,"visual explanations, ground-truth class labels",NA,"Visual, Textual, Interactive interface",Yes,Yes
2-27074,springernature,A drug mix and dose decision algorithm for individualized type 2 diabetes management,"Pharmacotherapy guidelines for type 2 diabetes( T2D) emphasize patient-centered care, but applying this approach effectively in outpatient practice remains challenging. Data-driven treatment optimization approaches could enhance individualized T2D management, but current approaches cannot account for drug-specific and dose-dependent variations in safety and efficacy. We developed and evaluated an AI Drug mix and dose Advisor( AIDA) for glycemic management, using electronic medical records from 107, 854 T2D patients in the SingHealth Diabetes Registry. Given a patient’s medical profile, AIDA leverages a predict-then-optimize approach to identify the minimal drug mix and dose changes required to optimize glycemic control, subject to clinical knowledge-based guidelines. On unseen data from large internal, external, and temporal validation sets, AIDA recommendations were estimated to improve post-visit glycated hemoglobin( HbA 1c) by an average of 0. 40–0. 68% over standard of care( P < 0. 0001). In qualitative evaluations on 60 diverse cases by a panel of three endocrinologists, AIDA recommendations were mostly rated as reasonable and precise. Finally, AIDA’s ability to account for drug-dose specifics offered several advantages over competing methods, including greater consistency with practice preferences and clinical guidelines for practical but effective options, indication-based treatments, and renal dosing. As AIDA provides drug-dose recommendations to improve outcomes for individual T2D patients, it could be used for clinical decision support at point-of-care, especially in resource-limited settings.",10.1038/s41746-024-01230-5,http://dx.doi.org/10.1038/s41746-024-01230-5,Nature Partner Journals Digital Medicine,"Nambiar, Mila;Bee, Yong Mong;Chan, Yu En;Ho Mien, Ivan;Guretno, Feri;Carmody, David;Lee, Phong Ching;Chia, Sing Yi;Salim, Nur Nasyitah Mohamed;Krishnaswamy, Pavitra",2024,1,"@article{2-27074,
  title = {A drug mix and dose decision algorithm for individualized type 2 diabetes management},
  author = {Nambiar, Mila and Bee, Yong Mong and Chan, Yu En and Ho Mien, Ivan and Guretno, Feri and Carmody, David and Lee, Phong Ching and Chia, Sing Yi and Salim, Nur Nasyitah Mohamed and Krishnaswamy, Pavitra},
  year = {2024},
  doi = {10.1038/s41746-024-01230-5},
  journal = {Nature Partner Journals Digital Medicine}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Individual,"Advising, Analyzing","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-27084,springernature,Public attitudes value interpretability but prioritize accuracy in Artificial Intelligence,"As Artificial Intelligence( AI) proliferates across important social institutions, many of the most powerful AI systems available are difficult to interpret for end-users and engineers alike. Here, we sought to characterize public attitudes towards AI interpretability. Across seven studies( N = 2475) , we demonstrate robust and positive attitudes towards interpretable AI among non-experts that generalize across a variety of real-world applications and follow predictable patterns. Participants value interpretability positively across different levels of AI autonomy and accuracy, and rate interpretability as more important for AI decisions involving high stakes and scarce resources. Crucially, when AI interpretability trades off against AI accuracy, participants prioritize accuracy over interpretability under the same conditions driving positive attitudes towards interpretability in the first place: amidst high stakes and scarce resources. These attitudes could drive a proliferation of AI systems making high-impact ethical decisions that are difficult to explain and understand. For many AI systems, it is hard to interpret how they make decisions. Here, the authors show that non-experts value interpretability in AI, especially for decisions involving high stakes and scarce resources, but they sacrifice AI interpretability when it trades off against AI accuracy.",10.1038/s41467-022-33417-3,http://dx.doi.org/10.1038/s41467-022-33417-3,Nature Communications,"Nussberger, Anne-Marie;Luo, Lan;Celis, L. Elisa;Crockett, M. J.",2022,106,"@article{2-27084,
  title = {Public attitudes value interpretability but prioritize accuracy in Artificial Intelligence},
  author = {Nussberger, Anne-Marie and Luo, Lan and Celis, L. Elisa and Crockett, M. J.},
  year = {2022},
  doi = {10.1038/s41467-022-33417-3},
  journal = {Nature Communications}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Institutional,"Executing, Analyzing","Decision-subject, Guardian, Stakeholder","Change affective-perceptual, Change trust",Update AI competence,"recommendations, textual explanations, system accuracy",NA,Textual,Yes,Yes
2-27150,springernature,Improving Human Decision-making by Discovering Efficient Strategies for Hierarchical Planning,"To make good decisions in the real world, people need efficient planning strategies because their computational resources are limited. Knowing which planning strategies would work best for people in different situations would be very useful for understanding and improving human decision-making. Our ability to compute those strategies used to be limited to very small and very simple planning tasks. Here, we introduce a cognitively inspired reinforcement learning method that can overcome this limitation by exploiting the hierarchical structure of human behavior. We leverage it to understand and improve human planning in large and complex sequential decision problems. Our method decomposes sequential decision problems into two sub-problems: setting a goal and planning how to achieve it. Our method can discover optimal human planning strategies for larger and more complex tasks than was previously possible. The discovered strategies achieve a better tradeoff between decision quality and computational cost than both human planning and existing planning algorithms. We demonstrate that teaching people to use those strategies significantly increases their level of resource-rationality in tasks that require planning up to eight steps ahead. By contrast, none of the previous approaches was able to improve human performance on these problems. These findings suggest that our cognitively informed approach makes it possible to leverage reinforcement learning to improve human decision-making in complex sequential decision problems. Future work can leverage our method to develop decision support systems that improve human decision-making in the real world.",10.1007/s42113-022-00128-3,http://dx.doi.org/10.1007/s42113-022-00128-3,Computational Brain & Behavior,"Consul, Saksham;Heindrich, Lovis;Stojcheski, Jugoslav;Lieder, Falk",2022,1,"@article{2-27150,
  title={Improving Human Decision-making by Discovering Efficient Strategies for Hierarchical Planning},
  author={Consul, Saksham and Heindrich, Lovis and Stojcheski, Jugoslav and Lieder, Falk},
  year={2022},
  doi={10.1007/s42113-022-00128-3},
  journal={Computational Brain \& Behavior}
}","Methodological contributions, Algorithmic contributions","Education / Teaching / Research, Generic / Abstract / Domain-agnostic",Individual,"Analyzing, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-27187,springernature,Applying XAI to an AI-based system for candidate management to mitigate bias and discrimination in hiring,"Assuming that potential biases of Artificial Intelligence( AI) -based systems can be identified and controlled for( e. g. , by providing high quality training data) , employing such systems to augment human resource( HR) -decision makers in candidate selection provides an opportunity to make selection processes more objective. However, as the final hiring decision is likely to remain with humans, prevalent human biases could still cause discrimination. This work investigates the impact of an AI-based system’s candidate recommendations on humans’ hiring decisions and how this relation could be moderated by an Explainable AI( XAI) approach. We used a self-developed platform and conducted an online experiment with 194 participants. Our quantitative and qualitative findings suggest that the recommendations of an AI-based system can reduce discrimination against older and female candidates but appear to cause fewer selections of foreign-race candidates. Contrary to our expectations, the same XAI approach moderated these effects differently depending on the context.",10.1007/s12525-022-00600-9,http://dx.doi.org/10.1007/s12525-022-00600-9,Electronic Markets,"Hofeditz, Lennart;Clausen, Sünje;Rieß, Alexander;Mirbabaie, Milad;Stieglitz, Stefan",2022,98,"@article{2-27187,
  title = {Applying XAI to an AI-based system for candidate management to mitigate bias and discrimination in hiring},
  author = {Hofeditz, Lennart and Clausen, S{\""u}nje and Rie{\ss}, Alexander and Mirbabaie, Milad and Stieglitz, Stefan},
  year = {2022},
  doi = {10.1007/s12525-022-00600-9},
  journal = {Electronic Markets}
}",Theoretical contributions,Everyday / Employment / Public Service,Operational,"Advising, Explaining","Decision-maker, Decision-subject, Knowledge provider, Stakeholder","Alter decision outcomes, Shape ethical norms",Shape AI for accountability,"recommendations, visual explanations, textual explanations",fairness constraints,"Textual, Visual",Yes,Yes
2-27242,springernature,Automatic Liver and Lesion Segmentation in CT Using Cascaded Fully Convolutional Neural Networks and 3D Conditional Random Fields,"Automatic segmentation of the liver and its lesion is an important step towards deriving quantitative biomarkers for accurate clinical diagnosis and computer-aided decision support systems. This paper presents a method to automatically segment liver and lesions in CT abdomen images using cascaded fully convolutional neural networks( CFCNs) and dense 3D conditional random fields( CRFs). We train and cascade two FCNs for a combined segmentation of the liver and its lesions. In the first step, we train a FCN to segment the liver as ROI input for a second FCN. The second FCN solely segments lesions from the predicted liver ROIs of step 1. We refine the segmentations of the CFCN using a dense 3D CRF that accounts for both spatial coherence and appearance. CFCN models were trained in a 2-fold cross-validation on the abdominal CT dataset 3DIRCAD comprising 15 hepatic tumor volumes. Our results show that CFCN-based semantic liver and lesion segmentation achieves Dice scores over $$94\\, \\%$$ for liver with computation times below 100 s per volume. We experimentally demonstrate the robustness of the proposed method as a decision support system with a high accuracy and speed for usage in daily clinical routine.",10.1007/978-3-319-46723-8_48,http://dx.doi.org/10.1007/978-3-319-46723-8_48,Medical Image Computing and Computer-Assisted Intervention (MICCAI),"Christ, Patrick Ferdinand;Elshaer, Mohamed Ezzeldin A.;Ettlinger, Florian;Tatavarty, Sunil;Bickel, Marc;Bilic, Patrick;Rempfler, Markus;Armbruster, Marco;Hofmann, Felix;D’Anastasi, Melvin;Sommer, Wieland H.;Ahmadi, Seyed-Ahmad;Menze, Bjoern H.",2016,914,"@inproceedings{2-27242,
  title = {Automatic Liver and Lesion Segmentation in CT Using Cascaded Fully Convolutional Neural Networks and 3D Conditional Random Fields},
  author = {Christ, Patrick Ferdinand and Elshaer, Mohamed Ezzeldin A. and Ettlinger, Florian and Tatavarty, Sunil and Bickel, Marc and Bilic, Patrick and Rempfler, Markus and Armbruster, Marco and Hofmann, Felix and D’Anastasi, Melvin and Sommer, Wieland H. and Ahmadi, Seyed-Ahmad and Menze, Bjoern H.},
  year = {2016},
  doi = {10.1007/978-3-319-46723-8_48},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention (MICCAI)}
}","Algorithmic contributions, Empirical contributions",Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-27365,springernature,Boosting the diagnostic power of amyloid-β PET using a data-driven spatially informed classifier for decision support,"Background Amyloid-β( Aβ) PET has emerged as clinically useful for more accurate diagnosis of patients with cognitive decline. Aβ deposition is a necessary cause or response to the cellular pathology of Alzheimer’s disease( AD). Usual clinical and research interpretation of amyloid PET does not fully utilise all information regarding the spatial distribution of signal. We present a data-driven, spatially informed classifier to boost the diagnostic power of amyloid PET in AD. Methods Voxel-wise k -means clustering of amyloid-positive voxels was performed; clusters were mapped to brain anatomy and tested for their associations by diagnostic category and disease severity with 758 amyloid PET scans from volunteers in the AD continuum from the Alzheimer’s Disease Neuroimaging Initiative( ADNI). A machine learning approach based on this spatially constrained model using an optimised quadratic support vector machine was developed for automatic classification of scans for AD vs non-AD pathology. Results This classifier boosted the accuracy of classification of AD scans to 81% using the amyloid PET alone with an area under the curve( AUC) of 0. 91 compared to other spatial methods. This increased sensitivity to detect AD by 15% and the AUC by 9% compared to the use of a composite region of interest SUVr. Conclusions The diagnostic classification accuracy of amyloid PET was improved using an automated data-driven spatial classifier. Our classifier highlights the importance of considering the spatial variation in Aβ PET signal for optimal interpretation of scans. The algorithm now is available to be evaluated prospectively as a tool for automated clinical decision support in research settings.",10.1186/s13195-021-00910-8,http://dx.doi.org/10.1186/s13195-021-00910-8,Alzheimer's Research & Therapy,"Venkataraman, Ashwin V.;Bai, Wenjia;Whittington, Alex;Myers, James F.;Rabiner, Eugenii A.;Lingford-Hughes, Anne;Matthews, Paul M.;for the Alzheimer’s Disease Neuroimaging Initiative",2021,3,"@article{2-27365,
  title = {Boosting the diagnostic power of amyloid-β PET using a data-driven spatially informed classifier for decision support},
  author = {Venkataraman, Ashwin V. and Bai, Wenjia and Whittington, Alex and Myers, James F. and Rabiner, Eugenii A. and Lingford-Hughes, Anne and Matthews, Paul M. and for the Alzheimer’s Disease Neuroimaging Initiative},
  year = {2021},
  doi = {10.1186/s13195-021-00910-8},
  journal = {Alzheimer's Research & Therapy}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-27430,springernature,Ethical machine decisions and the input-selection problem,"This article is about the role of factual uncertainty for moral decision-making as it concerns the ethics of machine decision-making( i. e. , decisions by AI systems, such as autonomous vehicles, autonomous robots, or decision support systems). The view that is defended here is that factual uncertainties require a normative evaluation and that ethics of machine decision faces a triple-edged problem, which concerns what a machine ought to do, given its technical constraints, what decisional uncertainty is acceptable, and what trade-offs are acceptable to decrease the decisional uncertainty.",10.1007/s11229-021-03296-0,http://dx.doi.org/10.1007/s11229-021-03296-0,Synthese,"Lundgren, Björn",2021,12,"@article{2-27430,
  title={Ethical machine decisions and the input-selection problem},
  author={Lundgren, Bj{\""o}rn},
  year={2021},
  doi={10.1007/s11229-021-03296-0},
  journal={Synthese}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,no such info,Executing,"Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-27471,springernature,Machine-learning algorithm to predict multidisciplinary team treatment recommendations in the management of basal cell carcinoma,"Background Basal cell carcinoma( BCC) is the most common human cancer. Facial BCCs most commonly occur on the nose and the management of these lesions is particularly complex, given the functional and complex implications of treatment. Multidisciplinary team( MDT) meetings are routinely held to integrate expertise from dermatologists, surgeons, oncologists, radiologists, pathologists and allied health professionals. The aim of this research was to develop a supervised machine-learning algorithm to predict MDT recommendations for nasal BCC to potentially reduce MDT caseload, provide automatic decision support and permit data audit in a health service context. Methods The study population included all consecutive patients who were discussed at skin cancer-specialised MDT( SSMDT) with a diagnosis of nasal BCC between January 1, 2015 and December 31, 2015. We conducted analyses for gender, age, anatomical location, histological subtype, tumour size, tumour recurrence, anticoagulation, pacemaker, immunosuppressants and therapeutic modalities( Mohs surgery, conventional excision or radiotherapy). We used S -statistic computing language to develop a supervised machine-learning algorithm. Results We found that 37. 5% of patients could be reliably predicted to be triaged to Mohs micrographic surgery( MMS) , based on tumour location and age. Similarly, the choice of conventional treatment( surgical excision or radiotherapy) by the MDT could be reliably predicted based on the patient’s age, tumour phenotype and lesion size. Accordingly, the algorithm reliably predicted the MDT decision outcome of 45. 1% of nasal BCCs. Conclusions Our study suggests that the machine-learning approach is a potentially useful tool for predicting MDT decisions for MMS vs conventional surgery or radiotherapy for a significant group of patients. We suggest that utilising this algorithm gives the MDT more time to consider more complex patients, where multiple factors, including recurrence, financial costs and cosmetic outcome, contribute to the final decision, but cannot be reliably predicted to determine that outcome. This approach has the potential to reduce the burden and improve the efficiency of the specialist skin MDT and, in turn, improve patient care, reduce waiting times and reduce the financial burden. Such an algorithm would need to be updated regularly to take into account any changes in patient referral patterns, treatment options or local clinical expertise. Clinical Trial Registration lPLAS_20-21_A08.",10.1038/s41416-021-01506-7,http://dx.doi.org/10.1038/s41416-021-01506-7,British Journal of Cancer,"Andrew, Tom W.;Hamnett, Nathan;Roy, Iain;Garioch, Jennifer;Nobes, Jenny;Moncrieff, Marc D.",2022,31,"@article{2-27471,
  title={Machine-learning algorithm to predict multidisciplinary team treatment recommendations in the management of basal cell carcinoma},
  author={Andrew, Tom W. and Hamnett, Nathan and Roy, Iain and Garioch, Jennifer and Nobes, Jenny and Moncrieff, Marc D.},
  year={2022},
  journal={British Journal of Cancer},
  doi={10.1038/s41416-021-01506-7}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-27560,springernature,Artificial intelligence enables comprehensive genome interpretation and nomination of candidate diagnoses for rare genetic diseases,"Background Clinical interpretation of genetic variants in the context of the patient’s phenotype is becoming the largest component of cost and time expenditure for genome-based diagnosis of rare genetic diseases. Artificial intelligence( AI) holds promise to greatly simplify and speed genome interpretation by integrating predictive methods with the growing knowledge of genetic disease. Here we assess the diagnostic performance of Fabric GEM, a new, AI-based, clinical decision support tool for expediting genome interpretation. Methods We benchmarked GEM in a retrospective cohort of 119 probands, mostly NICU infants, diagnosed with rare genetic diseases, who received whole-genome or whole-exome sequencing( WGS, WES). We replicated our analyses in a separate cohort of 60 cases collected from five academic medical centers. For comparison, we also analyzed these cases with current state-of-the-art variant prioritization tools. Included in the comparisons were trio, duo, and singleton cases. Variants underpinning diagnoses spanned diverse modes of inheritance and types, including structural variants( SVs). Patient phenotypes were extracted from clinical notes by two means: manually and using an automated clinical natural language processing( CNLP) tool. Finally, 14 previously unsolved cases were reanalyzed. Results GEM ranked over 90% of the causal genes among the top or second candidate and prioritized for review a median of 3 candidate genes per case, using either manually curated or CNLP-derived phenotype descriptions. Ranking of trios and duos was unchanged when analyzed as singletons. In 17 of 20 cases with diagnostic SVs, GEM identified the causal SVs as the top candidate and in 19/20 within the top five, irrespective of whether SV calls were provided or inferred ab initio by GEM using its own internal SV detection algorithm. GEM showed similar performance in absence of parental genotypes. Analysis of 14 previously unsolved cases resulted in a novel finding for one case, candidates ultimately not advanced upon manual review for 3 cases, and no new findings for 10 cases. Conclusions GEM enabled diagnostic interpretation inclusive of all variant types through automated nomination of a very short list of candidate genes and disorders for final review and reporting. In combination with deep phenotyping by CNLP, GEM enables substantial automation of genetic disease diagnosis, potentially decreasing cost and expediting case review.",10.1186/s13073-021-00965-0,http://dx.doi.org/10.1186/s13073-021-00965-0,Genome Medicine,"De La Vega, Francisco M.;Chowdhury, Shimul;Moore, Barry;Frise, Erwin;McCarthy, Jeanette;Hernandez, Edgar Javier;Wong, Terence;James, Kiely;Guidugli, Lucia;Agrawal, Pankaj B.;Genetti, Casie A.;Brownstein, Catherine A.;Beggs, Alan H.;Löscher, Britt-Sabina;Franke, Andre;Boone, Braden;Levy, Shawn E.;Õunap, Katrin;Pajusalu, Sander;Huentelman, Matt;Ramsey, Keri;Naymik, Marcus;Narayanan, Vinodh;Veeraraghavan, Narayanan;Billings, Paul;Reese, Martin G.;Yandell, Mark;Kingsmore, Stephen F.",2021,163,"@article{2-27560,
  title={Artificial intelligence enables comprehensive genome interpretation and nomination of candidate diagnoses for rare genetic diseases},
  author={De La Vega, Francisco M. and Chowdhury, Shimul and Moore, Barry and Frise, Erwin and McCarthy, Jeanette and Hernandez, Edgar Javier and Wong, Terence and James, Kiely and Guidugli, Lucia and Agrawal, Pankaj B. and Genetti, Casie A. and Brownstein, Catherine A. and Beggs, Alan H. and L{\""o}scher, Britt-Sabina and Franke, Andre and Boone, Braden and Levy, Shawn E. and {\""O}unap, Katrin and Pajusalu, Sander and Huentelman, Matt and Ramsey, Keri and Naymik, Marcus and Narayanan, Vinodh and Veeraraghavan, Narayanan and Billings, Paul and Reese, Martin G. and Yandell, Mark and Kingsmore, Stephen F.},
  year={2021},
  doi={10.1186/s13073-021-00965-0},
  journal={Genome Medicine}
}","Algorithmic contributions, System/Artifact contributions",Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising, Analyzing","Decision-maker, Decision-subject, Guardian, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-27567,springernature,Evaluating multimodal AI in medical diagnostics,"This study evaluates multimodal AI models’ accuracy and responsiveness in answering NEJM Image Challenge questions, juxtaposed with human collective intelligence, underscoring AI’s potential and current limitations in clinical diagnostics. Anthropic’s Claude 3 family demonstrated the highest accuracy among the evaluated AI models, surpassing the average human accuracy, while collective human decision-making outperformed all AI models. GPT-4 Vision Preview exhibited selectivity, responding more to easier questions with smaller images and longer questions.",10.1038/s41746-024-01208-3,http://dx.doi.org/10.1038/s41746-024-01208-3,Nature Partner Journals Digital Medicine,"Kaczmarczyk, Robert;Wilhelm, Theresa Isabelle;Martin, Ron;Roos, Jonas",2024,38,"@article{2-27567,
  title={Evaluating multimodal {AI} in medical diagnostics},
  author={Kaczmarczyk, Robert and Wilhelm, Theresa Isabelle and Martin, Ron and Roos, Jonas},
  year={2024},
  journal={Nature Partner Journals Digital Medicine},
  doi={10.1038/s41746-024-01208-3}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-27699,springernature,The influence of explainable vs non-explainable clinical decision support systems on rapid triage decisions: a mixed methods study,"Background During the COVID-19 pandemic, a variety of clinical decision support systems( CDSS) were developed to aid patient triage. However, research focusing on the interaction between decision support systems and human experts is lacking. Methods Thirty-two physicians were recruited to rate the survival probability of 59 critically ill patients by means of chart review. Subsequently, one of two artificial intelligence systems advised the physician of a computed survival probability. However, only one of these systems explained the reasons behind its decision-making. In the third step, physicians reviewed the chart once again to determine the final survival probability rating. We hypothesized that an explaining system would exhibit a higher impact on the physicians’ second rating( i. e. , higher weight-on-advice). Results The survival probability rating given by the physician after receiving advice from the clinical decision support system was a median of 4 percentage points closer to the advice than the initial rating. Weight-on-advice was not significantly different( p = 0. 115) between the two systems( with vs without explanation for its decision). Additionally, weight-on-advice showed no difference according to time of day or between board-qualified and not yet board-qualified physicians. Self-reported post-experiment overall trust was awarded a median of 4 out of 10 points. When asked after the conclusion of the experiment, overall trust was 5. 5/10( non-explaining median 4( IQR 3. 5–5. 5) , explaining median 7( IQR 5. 5–7. 5) , p = 0. 007). Conclusions Although overall trust in the models was low, the median( IQR) weight-on-advice was high( 0. 33( 0. 0–0. 56) ) and in line with published literature on expert advice. In contrast to the hypothesis, weight-on-advice was comparable between the explaining and non-explaining systems. In 30% of cases, weight-on-advice was 0, meaning the physician did not change their rating. The median of the remaining weight-on-advice values was 50%, suggesting that physicians either dismissed the recommendation or employed a “meeting halfway” approach. Newer technologies, such as clinical reasoning systems, may be able to augment the decision process rather than simply presenting unexplained bias.",10.1186/s12916-023-03068-2,http://dx.doi.org/10.1186/s12916-023-03068-2,BMC Medicine,"Laxar, Daniel;Eitenberger, Magdalena;Maleczek, Mathias;Kaider, Alexandra;Hammerle, Fabian Peter;Kimberger, Oliver",2023,19,"@article{2-27699,
  title = {The influence of explainable vs non-explainable clinical decision support systems on rapid triage decisions: a mixed methods study},
  author = {Laxar, Daniel and Eitenberger, Magdalena and Maleczek, Mathias and Kaider, Alexandra and Hammerle, Fabian Peter and Kimberger, Oliver},
  year = {2023},
  journal = {BMC Medicine},
  doi = {10.1186/s12916-023-03068-2}
}",Empirical contributions,Healthcare / Medicine / Surgery,"Institutional, Operational","Advising, Explaining","Decision-maker, Knowledge provider, Decision-subject","Change trust, Alter decision outcomes",no such info,"recommendations, visual explanations, textual explanations",domain knowledge,"Textual, Visual",Yes,Yes
2-277,aaai,Combining Machine Learning and Queueing Theory for Data-Driven Incarceration-Diversion Program Management,"Incarceration-diversion programs have proven effective in reducing recidivism. Accurate prediction of the number of individuals with different characteristics in the program and their program outcomes based on given eligibility criteria is crucial for successful implementation, because this prediction serves as the foundation for determining the appropriate program size and the consequent staffing requirements. However, this task poses challenges due to the complexities arising from varied outcomes and lengths-of-stay for the diverse individuals in incarceration-diversion programs. In collaboration with an Illinois government agency, we develop a framework to address these issues. Our framework combines ML and queueing model simulation, providing accurate predictions for the program census and interpretable insights into program dynamics and the impact of different decisions in counterfactual scenarios. Additionally, we deploy a user-friendly web app beta-version that allows program managers to visualize census data by counties and race groups. We showcase two decision support use cases: Changing program admission criteria and launching similar programs in new counties.",10.1609/aaai.v38i21.30330,https://ojs.aaai.org/index.php/AAAI/article/view/30330,AAAI Conference on Artificial Intelligence,Bingxuan Li;Antonio Castellanos;Pengyi Shi;Amy Ward,2024,7,"@inproceedings{2-277,
  title = {Combining Machine Learning and Queueing Theory for Data-Driven Incarceration-Diversion Program Management},
  author = {Bingxuan Li and Antonio Castellanos and Pengyi Shi and Amy Ward},
  year = {2024},
  doi = {10.1609/aaai.v38i21.30330},
  booktitle = {AAAI Conference on Artificial Intelligence}
}",System/Artifact contributions,Law / Policy / Governance,Organizational,"Advising, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-27755,springernature,Can a Bayesian Belief Network Be Used to Estimate 1-year Survival in Patients With Bone Sarcomas?,"Background Extremity sarcoma has a preponderance to present late with advanced stage at diagnosis. It is important to know why these patients die early from sarcoma and to predict those at high risk. Currently we have midto long-term outcome data on which to counsel patients and support treatment decisions, but in contrast to other cancer groups, very little on short-term mortality. Bayesian belief network modeling has been used to develop decision-support tools in various oncologic diagnoses, but to our knowledge, this approach has not been applied to patients with extremity sarcoma. Questions/purposes We sought to( 1) determine whether a Bayesian belief network could be used to estimate the likelihood of 1-year mortality using receiver operator characteristic analysis;( 2) describe the hierarchal relationships between prognostic and outcome variables; and( 3) determine whether the model was suitable for clinical use using decision curve analysis. Methods We considered all patients treated for primary bone sarcoma between 1970 and 2012, and excluded secondary metastasis, presentation with local recurrence, and benign tumors. The institution’s database yielded 3499 patients, of which six( 0. 2%) were excluded. Data extracted for analysis focused on patient demographics( age, sex) , tumor characteristics at diagnosis( size, metastasis, pathologic fracture) , survival, and cause of death. A Bayesian belief network generated conditional probabilities of variables and survival outcome at 1 year. A lift analysis determined the hierarchal relationship of variables. Internal validation of 699 test patients( 20% dataset) determined model accuracy. Decision curve analysis was performed comparing net benefit( capped at 85. 5%) for all threshold probabilities( survival output from model). Results We successfully generated a Bayesian belief network with five first-degree associates and describe their conditional relationship with survival after the diagnosis of primary bone sarcoma. On internal validation, the resultant model showed good predictive accuracy( area under the curve [AUC] = 0. 767; 95% CI, 0. 72–0. 83). The factors that predict the outcome of interest, 1-year mortality, in order of relative importance are synchronous metastasis( 6. 4) , patient’s age( 3) , tumor size( 2. 1) , histologic grade( 1. 8) , and presentation with a pathologic fracture( 1). Patient’s sex, tumor location, and inadvertent excision were second-degree associates and not directly related to the outcome of interest. Decision curve analysis shows that clinicians can accurately base treatment decisions on the 1-year model rather than assuming all patients, or no patients, will survive greater than 1 year. For threshold probabilities less than approximately 0. 5, the model is no better or no worse than assuming all patients will survive. Conclusions We showed that a Bayesian belief network can be used to predict 1-year mortality in patients presenting with a primary malignancy of bone and quantified the primary factors responsible for an increased risk of death. Synchronous metastasis, patient’s age, and the size of the tumor had the largest prognostic effect. We believe models such as these can be useful as clinical decision-support tools and, when properly externally validated, provide clinicians and patients with information germane to the treatment of bone sarcomas. Clinical Relevance Bone sarcomas are difficult to treat requiring multidisciplinary input to strategize management. An evidence-based survival prediction can be a powerful adjunctive to clinicians in this scenario. We believe the short-term predictions can be used to evaluate services, with 1-year mortality already being a quality indicator. Mortality predictors also can be incorporated in clinical trials, for example, to identify patients who are least likely to experience the side effects of experimental toxic chemotherapeutic agents.",10.1007/s11999-017-5346-1,http://dx.doi.org/10.1007/s11999-017-5346-1,Clinical Orthopaedics and Related Research,"Nandra, Rajpal;Parry, Michael;Forsberg, Jonathan;Grimer, Robert",2017,24,"@article{2-27755,
  title = {Can a Bayesian Belief Network Be Used to Estimate 1-year Survival in Patients With Bone Sarcomas?},
  author = {Nandra, Rajpal and Parry, Michael and Forsberg, Jonathan and Grimer, Robert},
  year = {2017},
  journal = {Clinical Orthopaedics and Related Research},
  doi = {10.1007/s11999-017-5346-1}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-28227,springernature,A real-time phenotyping framework using machine learning for plant stress severity rating in soybean,"Background Phenotyping is a critical component of plant research. Accurate and precise trait collection, when integrated with genetic tools, can greatly accelerate the rate of genetic gain in crop improvement. However, efficient and automatic phenotyping of traits across large populations is a challenge; which is further exacerbated by the necessity of sampling multiple environments and growing replicated trials. A promising approach is to leverage current advances in imaging technology, data analytics and machine learning to enable automated and fast phenotyping and subsequent decision support. In this context, the workflow for phenotyping( image capture → data storage and curation → trait extraction → machine learning/classification → models/apps for decision support) has to be carefully designed and efficiently executed to minimize resource usage and maximize utility. We illustrate such an end-to-end phenotyping workflow for the case of plant stress severity phenotyping in soybean, with a specific focus on the rapid and automatic assessment of iron deficiency chlorosis( IDC) severity on thousands of field plots. We showcase this analytics framework by extracting IDC features from a set of ~4500 unique canopies representing a diverse germplasm base that have different levels of IDC, and subsequently training a variety of classification models to predict plant stress severity. The best classifier is then deployed as a smartphone app for rapid and real time severity rating in the field. Results We investigated 10 different classification approaches, with the best classifier being a hierarchical classifier with a mean per-class accuracy of ~96%. We construct a phenotypically meaningful ‘population canopy graph’, connecting the automatically extracted canopy trait features with plant stress severity rating. We incorporated this image capture → image processing → classification workflow into a smartphone app that enables automated real-time evaluation of IDC scores using digital images of the canopy. Conclusion We expect this high-throughput framework to help increase the rate of genetic gain by providing a robust extendable framework for other abiotic and biotic stresses. We further envision this workflow embedded onto a high throughput phenotyping ground vehicle and unmanned aerial system that will allow real-time, automated stress trait detection and quantification for plant research, breeding and stress scouting applications.",10.1186/s13007-017-0173-7,http://dx.doi.org/10.1186/s13007-017-0173-7,Plant Methods,"Naik, Hsiang Sing;Zhang, Jiaoping;Lofquist, Alec;Assefa, Teshale;Sarkar, Soumik;Ackerman, David;Singh, Arti;Singh, Asheesh K.;Ganapathysubramanian, Baskar",2017,1280,"@article{2-28227,
  title={A real-time phenotyping framework using machine learning for plant stress severity rating in soybean},
  author={Naik, Hsiang Sing and Zhang, Jiaoping and Lofquist, Alec and Assefa, Teshale and Sarkar, Soumik and Ackerman, David and Singh, Arti and Singh, Asheesh K. and Ganapathysubramanian, Baskar},
  year={2017},
  doi={10.1186/s13007-017-0173-7},
  journal={Plant Methods}
}",System/Artifact contributions,Environment / Resources / Energy,Operational,"Forecasting, Analyzing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-285,aaai,Decoding AI’s Nudge: A Unified Framework to Predict Human Behavior in AI-Assisted Decision Making,"With the rapid development of AI-based decision aids, different forms of AI assistance have been increasingly integrated into the human decision making processes. To best support humans in decision making, it is essential to quantitatively understand how diverse forms of AI assistance influence humans decision making behavior. To this end, much of the current research focuses on the end-to-end prediction of human behavior using ``black-box models, often lacking interpretations of the nuanced ways in which AI assistance impacts the human decision making process. Meanwhile, methods that prioritize the interpretability of human behavior predictions are often tailored for one specific form of AI assistance, making adaptations to other forms of assistance difficult. In this paper, we propose a computational framework that can provide an interpretable characterization of the influence of different forms of AI assistance on decision makers in AI-assisted decision making. By conceptualizing AI assistance as the ``nudge in human decision making processes, our approach centers around modelling how different forms of AI assistance modify humans strategy in weighing different information in making their decisions. Evaluations on behavior data collected from real human decision makers show that the proposed framework outperforms various baselines in accurately predicting human behavior in AI-assisted decision making. Based on the proposed framework, we further provide insights into how individuals with different cognitive styles are nudged by AI assistance differently.",10.1609/aaai.v38i9.28872,https://ojs.aaai.org/index.php/AAAI/article/view/28872,AAAI Conference on Artificial Intelligence,Zhuoyan Li;Zhuoran Lu;Ming Yin,2024,0,"@inproceedings{2-285,
  title     = {Decoding AI’s Nudge: A Unified Framework to Predict Human Behavior in AI-Assisted Decision Making},
  author    = {Li, Zhuoyan and Lu, Zhuoran and Yin, Ming},
  year      = {2024},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi       = {10.1609/aaai.v38i9.28872}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,"Analyzing, Advising","Decision-subject, Decision-maker","Alter decision outcomes, Change trust, Change cognitive demands",no such info,textual explanations,NA,Textual,Yes,Yes
2-28545,springernature,Artificial intelligence for the diagnosis of clinically significant prostate cancer based on multimodal data: a multicenter study,"Background The introduction of multiparameter MRI and novel biomarkers has greatly improved the prediction of clinically significant prostate cancer( csPCa). However, decision-making regarding prostate biopsy and prebiopsy examinations is still difficult. We aimed to establish a quick and economic tool to improve the detection of csPCa based on routinely performed clinical examinations through an automated machine learning platform( AutoML). Methods This study included a multicenter retrospective cohort and two prospective cohorts with 4747 cases from 9 hospitals across China. The multimodal data, including demographics, clinical characteristics, laboratory tests, and ultrasound reports, of consecutive participants were retrieved using extract-transform-load tools. AutoML was applied to explore potential data processing patterns and the most suitable algorithm to build the Prostate Cancer Artificial Intelligence Diagnostic System( PCAIDS). The diagnostic performance was determined by the receiver operating characteristic curve( ROC) for discriminating csPCa from insignificant prostate cancer( PCa) and benign disease. The clinical utility was evaluated by decision curve analysis( DCA) and waterfall plots. Results The random forest algorithm was applied in the feature selection, and the AutoML algorithm was applied for model establishment. The area under the curve( AUC) value in identifying csPCa was 0. 853 in the training cohort, 0. 820 in the validation cohort, 0. 807 in the Changhai prospective cohort, and 0. 850 in the Zhongda prospective cohort. DCA showed that the PCAIDS was superior to PSA or fPSA/tPSA for diagnosing csPCa with a higher net benefit for all threshold probabilities in all cohorts. Setting a fixed sensitivity of 95%, a total of 32. 2%, 17. 6%, and 26. 3% of unnecessary biopsies could be avoided with less than 5% of csPCa missed in the validation cohort, Changhai and Zhongda prospective cohorts, respectively. Conclusions The PCAIDS was an effective tool to inform decision-making regarding the need for prostate biopsy and prebiopsy examinations such as mpMRI. Further prospective and international studies are warranted to validate the findings of this study. Trial registration Chinese Clinical Trial Registry ChiCTR2100048428. Registered on 06 July 2021.",10.1186/s12916-023-02964-x,http://dx.doi.org/10.1186/s12916-023-02964-x,BMC Medicine,"Zhang, Huiyong;Ji, Jin;Liu, Zhe;Lu, Huiru;Qian, Chong;Wei, Chunmeng;Chen, Shaohua;Lu, Wenhao;Wang, Chengbang;Xu, Huan;Xu, Yalong;Chen, Xi;He, Xing;Wang, Zuheng;Zhao, Xiaodong;Cheng, Wen;Chen, Xingfa;Pang, Guijian;Yu, Guopeng;Gu, Yue;Jiang, Kangxian;Xu, Bin;Chen, Junyi;Xu, Bin;Wei, Xuedong;Chen, Ming;Chen, Rui;Cheng, Jiwen;Wang, Fubo",2023,11,"@article{2-28545,
  title={Artificial intelligence for the diagnosis of clinically significant prostate cancer based on multimodal data: a multicenter study},
  author={Zhang, Huiyong and Ji, Jin and Liu, Zhe and Lu, Huiru and Qian, Chong and Wei, Chunmeng and Chen, Shaohua and Lu, Wenhao and Wang, Chengbang and Xu, Huan and Xu, Yalong and Chen, Xi and He, Xing and Wang, Zuheng and Zhao, Xiaodong and Cheng, Wen and Chen, Xingfa and Pang, Guijian and Yu, Guopeng and Gu, Yue and Jiang, Kangxian and Xu, Bin and Chen, Junyi and Xu, Bin and Wei, Xuedong and Chen, Ming and Chen, Rui and Cheng, Jiwen and Wang, Fubo},
  year={2023},
  doi={10.1186/s12916-023-02964-x},
  journal={BMC Medicine}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-28842,springernature,An AI approach for managing financial systemic risk via bank bailouts by taxpayers,"Bank bailouts are controversial governmental decisions, putting taxpayers’ money at risk to avoid a domino effect through the network of claims between financial institutions. Yet very few studies address quantitatively the convenience of government investments in failing banks from the taxpayers’ standpoint. We propose a dynamic financial network framework incorporating bailout decisions as a Markov Decision Process and an artificial intelligence technique that learns the optimal bailout actions to minimise the expected taxpayers’ losses. Considering the European global systemically important institutions, we find that bailout decisions become optimal only if the taxpayers’ stakes exceed some critical level, endogenously determined by all financial network’s characteristics. The convenience to intervene increases with the network’s distress, taxpayers’ stakes, bank bilateral credit exposures and crisis duration. Moreover, the government should optimally keep bailing-out banks that received previous investments, creating moral hazard for rescued banks that could increase their risk-taking, reckoning on government intervention. Systemic risk and bank bailout approaches have been the source of discussions on scientific, financial and governmental forums. An artificial intelligence technique is proposed to inform equitable bailout decisions that minimise taxpayers’ losses.",10.1038/s41467-022-34102-1,http://dx.doi.org/10.1038/s41467-022-34102-1,Nature Communications,"Petrone, Daniele;Rodosthenous, Neofytos;Latora, Vito",2022,11,"@article{2-28842,
  title={An AI approach for managing financial systemic risk via bank bailouts by taxpayers},
  author={Petrone, Daniele and Rodosthenous, Neofytos and Latora, Vito},
  year={2022},
  doi={10.1038/s41467-022-34102-1},
  journal={Nature Communications}
}",Algorithmic contributions,"Finance / Business / Economy, Law / Policy / Governance",Institutional,"Advising, Forecasting, Analyzing","Decision-maker, Guardian, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-28948,springernature,Multi-task Learning for Detection and Classification of Cancer in Screening Mammography,"Breast screening is an effective method to identify breast cancer in asymptomatic women; however, not all exams are read by radiologists specialized in breast imaging, and missed cancers are a reality. Deep learning provides a valuable tool to support this critical decision point. Algorithmically, accurate assessment of breast mammography requires both detection of abnormal findings( object detection) and a correct decision whether to recall a patient for additional imaging( image classification). In this paper, we present a multi-task learning approach, that we argue is ideally suited to this problem. We train a network for both object detection and image classification, based on state-of-the-art models, and demonstrate significant improvement in the recall vs no recall decision on a multi-site, multi-vendor data set, measured by concordance with biopsy proven malignancy. We also observe improved detection of microcalcifications, and detection of cancer cases that were missed by radiologists, demonstrating that this approach could provide meaningful support for radiologists in breast screening( especially non-specialists). Moreover, we argue that this multi-task framework is broadly applicable to a wide range of medical imaging problems that require a patient-level recommendation, based on specific imaging findings.",10.1007/978-3-030-59725-2_24,http://dx.doi.org/10.1007/978-3-030-59725-2_24,Medical Image Computing and Computer-Assisted Intervention Conference,"Sainz de Cea, Maria V.;Diedrich, Karl;Bakalo, Ran;Ness, Lior;Richmond, David",2020,35,"@inproceedings{2-28948,
  title={Multi-task Learning for Detection and Classification of Cancer in Screening Mammography},
  author={Sainz de Cea, Maria V. and Diedrich, Karl and Bakalo, Ran and Ness, Lior and Richmond, David},
  year={2020},
  doi={10.1007/978-3-030-59725-2_24},
  booktitle={Medical Image Computing and Computer-Assisted Intervention Conference}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-28967,springernature,ObjectivAIze: Measuring Performance and Biases in Augmented Business Decision Systems,"Business process management organizes flows of information and decisions in large organizations. These systems now integrate algorithmic decision aids leveraging machine learning: each time a stakeholder needs to make a decision, such as a purchase, a quote, or hiring someone, the software leverages the inputs and outcomes of similar past decisions to provide guidance, as a recommendation. If the confidence is high, the process may be automated. Otherwise, it may still help provide consistency in the decisions. Yet, we may question how these aids affect task performance. Can we measure an improvement? Can hidden biases influence decision makers negatively? What is the impact of various presentation options? To address those issues, we propose metrics of performance, automation bias and resistance. We validated those measures with an online study. Our aim is to instrument those systems to secure their benefits. In a first experiment, we study effective collaboration. Faced with a decision, subjects alone have a success rate of 72%; Aided by a recommender that has a 75% success rate, their success rate reaches 76%. The human-system collaboration had thus a greater success rate than each taken alone. However, we noted a complacency/authority bias that degraded the quality of decisions by 5% when the recommender was wrong. This suggests that any lingering algorithmic bias may be amplified by decision aids. In a second experiment, we evaluated the effectiveness of 5 presentation variants in reducing complacency bias. We found that optional presentation increases subjects’ resistance to wrong recommendations. We intend to leverage these findings to guide the design of human-algorithm collaboration in financial compliance alert filtering.",10.1007/978-3-030-85613-7_22,http://dx.doi.org/10.1007/978-3-030-85613-7_22,Human-Computer Interaction Conference (INTERACT),"Baudel, Thomas;Verbockhaven, Manon;Cousergue, Victoire;Roy, Guillaume;Laarach, Rida",2021,16,"@inproceedings{2-28967,
  title={ObjectivAIze: Measuring Performance and Biases in Augmented Business Decision Systems},
  author={Baudel, Thomas and Verbockhaven, Manon and Cousergue, Victoire and Roy, Guillaume and Laarach, Rida},
  year={2021},
  booktitle={Proceedings of the Human-Computer Interaction Conference (INTERACT)},
  doi={10.1007/978-3-030-85613-7_22}
}",Empirical contributions,Finance / Business / Economy,Operational,Advising,"Stakeholder, Decision-maker","Alter decision outcomes, Change affective-perceptual, Change trust",no such info,"recommendations, complacency/authority bias, success rate",NA,Interactive interface,Yes,Yes
2-29077,springernature,Ultra-fast deep-learned CNS tumour classification during surgery,"Central nervous system tumours represent one of the most lethal cancer types, particularly among children 1. Primary treatment includes neurosurgical resection of the tumour, in which a delicate balance must be struck between maximizing the extent of resection and minimizing risk of neurological damage and comorbidity 2, 3. However, surgeons have limited knowledge of the precise tumour type prior to surgery. Current standard practice relies on preoperative imaging and intraoperative histological analysis, but these are not always conclusive and occasionally wrong. Using rapid nanopore sequencing, a sparse methylation profile can be obtained during surgery 4. Here we developed Sturgeon, a patient-agnostic transfer-learned neural network, to enable molecular subclassification of central nervous system tumours based on such sparse profiles. Sturgeon delivered an accurate diagnosis within 40 minutes after starting sequencing in 45 out of 50 retrospectively sequenced samples( abstaining from diagnosis of the other 5 samples). Furthermore, we demonstrated its applicability in real time during 25 surgeries, achieving a diagnostic turnaround time of less than 90 min. Of these, 18( 72%) diagnoses were correct and 7 did not reach the required confidence threshold. We conclude that machine-learned diagnosis based on low-cost intraoperative sequencing can assist neurosurgical decision-making, potentially preventing neurological comorbidity and avoiding additional surgeries. Sturgeon is a pretrained neural network that uses incremental results from nanopore sequencing to rapidly classify central nervous system tumours and can be used to aid critical decision-making during surgery.",10.1038/s41586-023-06615-2,http://dx.doi.org/10.1038/s41586-023-06615-2,Nature,"Vermeulen, C.;Pagès-Gallego, M.;Kester, L.;Kranendonk, M. E. G.;Wesseling, P.;Verburg, N.;Witt Hamer, P.;Kooi, E. J.;Dankmeijer, L.;Lugt, J.;Baarsen, K.;Hoving, E. W.;Tops, B. B. J.;Ridder, J.",2023,0,"@article{2-29077,
  title={Ultra-fast deep-learned CNS tumour classification during surgery},
  author={Vermeulen, C. and Pag{\`e}s-Gallego, M. and Kester, L. and Kranendonk, M. E. G. and Wesseling, P. and Verburg, N. and Witt Hamer, P. and Kooi, E. J. and Dankmeijer, L. and Lugt, J. and Baarsen, K. and Hoving, E. W. and Tops, B. B. J. and Ridder, J.},
  year={2023},
  journal={Nature},
  doi={10.1038/s41586-023-06615-2}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-29173,springernature,Evaluation and mitigation of the limitations of large language models in clinical decision-making,"Clinical decision-making is one of the most impactful parts of a physician’s responsibilities and stands to benefit greatly from artificial intelligence solutions and large language models( LLMs) in particular. However, while LLMs have achieved excellent performance on medical licensing exams, these tests fail to assess many skills necessary for deployment in a realistic clinical decision-making environment, including gathering information, adhering to guidelines, and integrating into clinical workflows. Here we have created a curated dataset based on the Medical Information Mart for Intensive Care database spanning 2, 400 real patient cases and four common abdominal pathologies as well as a framework to simulate a realistic clinical setting. We show that current state-of-the-art LLMs do not accurately diagnose patients across all pathologies( performing significantly worse than physicians) , follow neither diagnostic nor treatment guidelines, and cannot interpret laboratory results, thus posing a serious risk to the health of patients. Furthermore, we move beyond diagnostic accuracy and demonstrate that they cannot be easily integrated into existing workflows because they often fail to follow instructions and are sensitive to both the quantity and order of information. Overall, our analysis reveals that LLMs are currently not ready for autonomous clinical decision-making while providing a dataset and framework to guide future studies. Using a curated dataset of 2, 400 cases and a framework to simulate a realistic clinical setting, current large language models are shown to incur substantial pitfalls when used for autonomous clinical decision-making.",10.1038/s41591-024-03097-1,http://dx.doi.org/10.1038/s41591-024-03097-1,Nature Medicine,"Hager, Paul;Jungmann, Friederike;Holland, Robbie;Bhagat, Kunal;Hubrecht, Inga;Knauer, Manuel;Vielhauer, Jakob;Makowski, Marcus;Braren, Rickmer;Kaissis, Georgios;Rueckert, Daniel",2024,1,"@article{2-29173,
  title={Evaluation and mitigation of the limitations of large language models in clinical decision-making},
  author={Hager, Paul and Jungmann, Friederike and Holland, Robbie and Bhagat, Kunal and Hubrecht, Inga and Knauer, Manuel and Vielhauer, Jakob and Makowski, Marcus and Braren, Rickmer and Kaissis, Georgios and Rueckert, Daniel},
  year={2024},
  doi={10.1038/s41591-024-03097-1},
  journal={Nature Medicine}
}","Empirical contributions, Dataset/Benchmark contributions",Healthcare / Medicine / Surgery,Operational,"Executing, Advising, Explaining","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-29174,springernature,Health improvement framework for actionable treatment planning using a surrogate Bayesian model,"Clinical decision-making regarding treatments based on personal characteristics leads to effective health improvements. Machine learning( ML) has been the primary concern of diagnosis support according to comprehensive patient information. A prominent issue is the development of objective treatment processes in clinical situations. This study proposes a framework to plan treatment processes in a data-driven manner. A key point of the framework is the evaluation of the actionability for personal health improvements by using a surrogate Bayesian model in addition to a high-performance nonlinear ML model. We first evaluate the framework from the viewpoint of its methodology using a synthetic dataset. Subsequently, the framework is applied to an actual health checkup dataset comprising data from 3132 participants, to lower systolic blood pressure and risk of chronic kidney disease at the individual level. We confirm that the computed treatment processes are actionable and consistent with clinical knowledge for improving these values. We also show that the improvement processes presented by the framework can be clinically informative. These results demonstrate that our framework can contribute toward decision-making in the medical field, providing clinicians with deeper insights. Clinical decision-making regarding treatments based on personal characteristics leads to effective health improvements. Here, the authors introduce a modeling framework to evaluate the actionability of treatment pathways.",10.1038/s41467-021-23319-1,http://dx.doi.org/10.1038/s41467-021-23319-1,Nature Communications,"Nakamura, Kazuki;Kojima, Ryosuke;Uchino, Eiichiro;Ono, Koh;Yanagita, Motoko;Murashita, Koichi;Itoh, Ken;Nakaji, Shigeyuki;Okuno, Yasushi",2021,20,"@article{2-29174,
  title = {Health improvement framework for actionable treatment planning using a surrogate Bayesian model},
  author = {Nakamura, Kazuki and Kojima, Ryosuke and Uchino, Eiichiro and Ono, Koh and Yanagita, Motoko and Murashita, Koichi and Itoh, Ken and Nakaji, Shigeyuki and Okuno, Yasushi},
  year = {2021},
  doi = {10.1038/s41467-021-23319-1},
  journal = {Nature Communications}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-subject, Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-29271,springernature,An interpretable RL framework for pre-deployment modeling in ICU hypotension management,"Computational methods from reinforcement learning have shown promise in inferring treatment strategies for hypotension management and other clinical decision-making challenges. Unfortunately, the resulting models are often difficult for clinicians to interpret, making clinical inspection and validation of these computationally derived strategies challenging in advance of deployment. In this work, we develop a general framework for identifying succinct sets of clinical contexts in which clinicians make very different treatment choices, tracing the effects of those choices, and inferring a set of recommendations for those specific contexts. By focusing on these few key decision points, our framework produces succinct, interpretable treatment strategies that can each be easily visualized and verified by clinical experts. This interrogation process allows clinicians to leverage the model’s use of historical data in tandem with their own expertise to determine which recommendations are worth investigating further e. g. at the bedside. We demonstrate the value of this approach via application to hypotension management in the ICU, an area with critical implications for patient outcomes that lacks data-driven individualized treatment strategies; that said, our framework has broad implications on how to use computational methods to assist with decision-making challenges on a wide range of clinical domains.",10.1038/s41746-022-00708-4,http://dx.doi.org/10.1038/s41746-022-00708-4,Nature Partner Journals Digital Medicine,"Zhang, Kristine;Wang, Henry;Du, Jianzhun;Chu, Brian;Arévalo, Aldo Robles;Kindle, Ryan;Celi, Leo Anthony;Doshi-Velez, Finale",2022,0,"@article{2-29271,
  title = {An interpretable RL framework for pre-deployment modeling in ICU hypotension management},
  author = {Zhang, Kristine and Wang, Henry and Du, Jianzhun and Chu, Brian and Ar{\'e}valo, Aldo Robles and Kindle, Ryan and Celi, Leo Anthony and Doshi-Velez, Finale},
  year = {2022},
  doi = {10.1038/s41746-022-00708-4},
  journal = {Nature Partner Journals Digital Medicine}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Explaining, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-29334,springernature,The effect of transparency and trust on intelligent system acceptance: Evidence from a user-based study,"Contemporary decision support systems are increasingly relying on artificial intelligence technology such as machine learning algorithms to form intelligent systems. These systems have human-like decision capacity for selected applications based on a decision rationale which cannot be looked-up conveniently and constitutes a black box. As a consequence, acceptance by end-users remains somewhat hesitant. While lacking transparency has been said to hinder trust and enforce aversion towards these systems, studies that connect user trust to transparency and subsequently acceptance are scarce. In response, our research is concerned with the development of a theoretical model that explains end-user acceptance of intelligent systems. We utilize the unified theory of acceptance and use in information technology as well as explanation theory and related theories on initial trust and user trust in information systems. The proposed model is tested in an industrial maintenance workplace scenario using maintenance experts as participants to represent the user group. Results show that acceptance is performance-driven at first sight. However, transparency plays an important indirect role in regulating trust and the perception of performance.",10.1007/s12525-022-00593-5,http://dx.doi.org/10.1007/s12525-022-00593-5,Electronic Markets,"Wanner, Jonas;Herm, Lukas-Valentin;Heinrich, Kai;Janiesch, Christian",2022,161,"@article{2-29334,
  title={The effect of transparency and trust on intelligent system acceptance: Evidence from a user-based study},
  author={Wanner, Jonas and Herm, Lukas-Valentin and Heinrich, Kai and Janiesch, Christian},
  year={2022},
  journal={Electronic Markets},
  doi={10.1007/s12525-022-00593-5}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Manufacturing / Industry / Automation",Operational,Advising,"Decision-maker, Knowledge provider, Guardian","Change affective-perceptual, Alter decision outcomes, Change trust",no such info,"recommendations, visual explanations, textual explanations, confidence score",domain knowledge,"Textual, Visual",Yes,Yes
2-29611,springernature,Making Large Language Models Better Planners with Reasoning-Decision Alignment,"Data-driven approaches for autonomous driving( AD) have been widely adopted in the past decade but are confronted with dataset bias and uninterpretability. Inspired by the knowledge-driven nature of human driving, recent approaches explore the potential of large language models( LLMs) to improve understanding and decision-making in traffic scenarios. They find that the pretrain-finetune paradigm of LLMs on downstream data with the Chain-of-Thought( CoT) reasoning process can enhance explainability and scene understanding. However, such a popular strategy proves to suffer from the notorious problems of misalignment between the crafted CoTs against the consequent decision-making, which remains untouched by previous LLM-based AD methods. To address this problem, we motivate an end-to-end decision-making model based on multimodality-augmented LLM, which simultaneously executes CoT reasoning and carries out planning results. Furthermore, we propose a reasoning-decision alignment constraint between the paired CoTs and planning results, imposing the correspondence between reasoning and decision-making. Moreover, we redesign the CoTs to enable the model to comprehend complex scenarios and enhance decision-making performance. We dub our proposed large language planners with reasoning-decision alignment as RDA-Driver. Experimental evaluations on the nuScenes and DriveLM-nuScenes benchmarks demonstrate the effectiveness of our RDA-Driver in enhancing the performance of end-to-end AD systems. Specifically, our RDA-Driver achieves state-of-the-art planning performance on the nuScenes dataset with 0. 80 L2 error and 0. 32 collision rate, and also achieves leading results on challenging DriveLM-nuScenes benchmarks with 0. 82 L2 error and 0. 38 collision rate.",10.1007/978-3-031-72764-1_5,http://dx.doi.org/10.1007/978-3-031-72764-1_5,European Conference on Computer Vision,"Huang, Zhijian;Tang, Tao;Chen, Shaoxiang;Lin, Sihao;Jie, Zequn;Ma, Lin;Wang, Guangrun;Liang, Xiaodan",2025,31,"@inproceedings{2-29611,
  title     = {Making Large Language Models Better Planners with Reasoning-Decision Alignment},
  author    = {Huang, Zhijian and Tang, Tao and Chen, Shaoxiang and Lin, Sihao and Jie, Zequn and Ma, Lin and Wang, Guangrun and Liang, Xiaodan},
  year      = {2025},
  booktitle = {European Conference on Computer Vision},
  doi       = {10.1007/978-3-031-72764-1\_5}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Explaining, Analyzing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-29656,springernature,How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection,"Decision support systems embodying machine learning models offer the promise of an improved standard of care for major depressive disorder, but little is known about how clinicians’ treatment decisions will be influenced by machine learning recommendations and explanations. We used a within-subject factorial experiment to present 220 clinicians with patient vignettes, each with or without a machine-learning( ML) recommendation and one of the multiple forms of explanation. We found that interacting with ML recommendations did not significantly improve clinicians’ treatment selection accuracy, assessed as concordance with expert psychopharmacologist consensus, compared to baseline scenarios in which clinicians made treatment decisions independently. Interacting with incorrect recommendations paired with explanations that included limited but easily interpretable information did lead to a significant reduction in treatment selection accuracy compared to baseline questions. These results suggest that incorrect ML recommendations may adversely impact clinician treatment selections and that explanations are insufficient for addressing overreliance on imperfect ML algorithms. More generally, our findings challenge the common assumption that clinicians interacting with ML tools will perform better than either clinicians or ML algorithms individually.",10.1038/s41398-021-01224-x,http://dx.doi.org/10.1038/s41398-021-01224-x,Translational Psychiatry,"Jacobs, Maia;Pradier, Melanie F.;McCoy, Thomas H., Jr.;Perlis, Roy H.;Doshi-Velez, Finale;Gajos, Krzysztof Z.",2021,30,"@article{2-29656,
  title = {How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection},
  author = {Jacobs, Maia and Pradier, Melanie F. and McCoy, Thomas H., Jr. and Perlis, Roy H. and Doshi-Velez, Finale and Gajos, Krzysztof Z.},
  year = {2021},
  doi = {10.1038/s41398-021-01224-x},
  journal = {Translational Psychiatry}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising","Knowledge provider, Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-29687,springernature,Search-based Automatic Repair for Fairness and Accuracy in Decision-making Software,"Decision-making software mainly based on Machine Learning( ML) may contain fairness issues( e. g. , providing favourable treatment to certain people rather than others based on sensitive attributes such as gender or race). Various mitigation methods have been proposed to automatically repair fairness issues to achieve fairer ML software and help software engineers to create responsible software. However, existing bias mitigation methods trade accuracy for fairness( i. e. , trade a reduction in accuracy for better fairness). In this paper, we present a novel search-based method for repairing ML-based decision making software to simultaneously increase both its fairness and accuracy. As far as we know, this is the first bias mitigation approach based on multi-objective search that aims to repair fairness issues without trading accuracy for binary classification methods. We apply our approach to two widely studied ML models in the software fairness literature( i. e. , Logistic Regression and Decision Trees) , and compare it with seven publicly available state-of-the-art bias mitigation methods by using three different fairness measurements. The results show that our approach successfully increases both accuracy and fairness for 61% of the cases studied, while the state-of-the-art always decrease accuracy when attempting to reduce bias. With our proposed approach, software engineers that previously were concerned with accuracy losses when considering fairness, are now enabled to improve the fairness of binary classification models without sacrificing accuracy.",10.1007/s10664-023-10419-3,http://dx.doi.org/10.1007/s10664-023-10419-3,Empirical Software Engineering,"Hort, Max;Zhang, Jie M.;Sarro, Federica;Harman, Mark",2024,0,"@article{2-29687,
  title={Search-based Automatic Repair for Fairness and Accuracy in Decision-making Software},
  author={Hort, Max and Zhang, Jie M. and Sarro, Federica and Harman, Mark},
  year={2024},
  doi={10.1007/s10664-023-10419-3},
  journal={Empirical Software Engineering}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Software / Systems / Security",Operational,Executing,Developer,NA,NA,NA,NA,NA,Yes,No
2-29720,springernature,Triage-driven diagnosis of Barrett’s esophagus for early detection of esophageal adenocarcinoma using deep learning,"Deep learning methods have been shown to achieve excellent performance on diagnostic tasks, but how to optimally combine them with expert knowledge and existing clinical decision pathways is still an open challenge. This question is particularly important for the early detection of cancer, where high-volume workflows may benefit from( semi-) automated analysis. Here we present a deep learning framework to analyze samples of the Cytosponge-TFF3 test, a minimally invasive alternative to endoscopy, for detecting Barrett’s esophagus, which is the main precursor of esophageal adenocarcinoma. We trained and independently validated the framework on data from two clinical trials, analyzing a combined total of 4, 662 pathology slides from 2, 331 patients. Our approach exploits decision patterns of gastrointestinal pathologists to define eight triage classes of varying priority for manual expert review. By substituting manual review with automated review in low-priority classes, we can reduce pathologist workload by 57% while matching the diagnostic performance of experienced pathologists. A clinician-in-the-loop deep learning system streamlines the workflow of pathologists for detection of Barrett’s esophagus and retains the diagnostic accuracy of full manual review.",10.1038/s41591-021-01287-9,http://dx.doi.org/10.1038/s41591-021-01287-9,Nature Medicine,"Gehrung, Marcel;Crispin-Ortuzar, Mireia;Berman, Adam G.;O’Donovan, Maria;Fitzgerald, Rebecca C.;Markowetz, Florian",2021,115,"@article{2-29720,
  title={Triage-driven diagnosis of Barrett’s esophagus for early detection of esophageal adenocarcinoma using deep learning},
  author={Gehrung, Marcel and Crispin-Ortuzar, Mireia and Berman, Adam G. and O’Donovan, Maria and Fitzgerald, Rebecca C. and Markowetz, Florian},
  year={2021},
  doi={10.1038/s41591-021-01287-9},
  journal={Nature Medicine}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Collaborating, Analyzing","Decision-subject, Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-29758,springernature,A value-based deep reinforcement learning model with human expertise in optimal treatment of sepsis,"Deep Reinforcement Learning( DRL) has been increasingly attempted in assisting clinicians for real-time treatment of sepsis. While a value function quantifies the performance of policies in such decision-making processes, most value-based DRL algorithms cannot evaluate the target value function precisely and are not as safe as clinical experts. In this study, we propose a Weighted Dueling Double Deep Q-Network with embedded human Expertise( WD3QNE). A target Q value function with adaptive dynamic weight is designed to improve the estimate accuracy and human expertise in decision-making is leveraged. In addition, the random forest algorithm is employed for feature selection to improve model interpretability. We test our algorithm against state-of-the-art value function methods in terms of expected return, survival rate, action distribution and external validation. The results demonstrate that WD3QNE obtains the highest survival rate of 97. 81% in MIMIC-III dataset. Our proposed method is capable of providing reliable treatment decisions with embedded clinician expertise.",10.1038/s41746-023-00755-5,http://dx.doi.org/10.1038/s41746-023-00755-5,Nature Partner Journals Digital Medicine,"Wu, XiaoDan;Li, RuiChang;He, Zhen;Yu, TianZhi;Cheng, ChangQing",2023,0,"@article{2-29758,
  title = {A value-based deep reinforcement learning model with human expertise in optimal treatment of sepsis},
  author = {Wu, XiaoDan and Li, RuiChang and He, Zhen and Yu, TianZhi and Cheng, ChangQing},
  year = {2023},
  doi = {10.1038/s41746-023-00755-5},
  journal = {Nature Partner Journals Digital Medicine}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-29770,springernature,Multisite implementation of a workflow-integrated machine learning system to optimize COVID-19 hospital admission decisions,"Demand has outstripped healthcare supply during the coronavirus disease 2019( COVID-19) pandemic. Emergency departments( EDs) are tasked with distinguishing patients who require hospital resources from those who may be safely discharged to the community. The novelty and high variability of COVID-19 have made these determinations challenging. In this study, we developed, implemented and evaluated an electronic health record( EHR) embedded clinical decision support( CDS) system that leverages machine learning( ML) to estimate short-term risk for clinical deterioration in patients with or under investigation for COVID-19. The system translates model-generated risk for critical care needs within 24 h and inpatient care needs within 72 h into rapidly interpretable COVID-19 Deterioration Risk Levels made viewable within ED clinician workflow. ML models were derived in a retrospective cohort of 21, 452 ED patients who visited one of five ED study sites and were prospectively validated in 15, 670 ED visits that occurred before( n = 4322) or after( n = 11, 348) CDS implementation; model performance and numerous patient-oriented outcomes including in-hospital mortality were measured across study periods. Incidence of critical care needs within 24 h and inpatient care needs within 72 h were 10. 7% and 22. 5%, respectively and were similar across study periods. ML model performance was excellent under all conditions, with AUC ranging from 0. 85 to 0. 91 for prediction of critical care needs and 0. 80–0. 90 for inpatient care needs. Total mortality was unchanged across study periods but was reduced among high-risk patients after CDS implementation.",10.1038/s41746-022-00646-1,http://dx.doi.org/10.1038/s41746-022-00646-1,Nature Partner Journals Digital Medicine,"Hinson, Jeremiah S.;Klein, Eili;Smith, Aria;Toerper, Matthew;Dungarani, Trushar;Hager, David;Hill, Peter;Kelen, Gabor;Niforatos, Joshua D.;Stephens, R. Scott;Strauss, Alexandra T.;Levin, Scott",2022,20,"@article{2-29770,
  title = {Multisite implementation of a workflow-integrated machine learning system to optimize COVID-19 hospital admission decisions},
  author = {Hinson, Jeremiah S. and Klein, Eili and Smith, Aria and Toerper, Matthew and Dungarani, Trushar and Hager, David and Hill, Peter and Kelen, Gabor and Niforatos, Joshua D. and Stephens, R. Scott and Strauss, Alexandra T. and Levin, Scott},
  year = {2022},
  doi = {10.1038/s41746-022-00646-1},
  journal = {Nature Partner Journals Digital Medicine}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-29819,springernature,Deploying artificial intelligence in services to AID vulnerable consumers,"Despite offering substantial opportunities to tailor services to consumers’ wants and needs, artificial intelligence( AI) technologies often come with ethical and operational challenges. One salient instance of such challenges emerges when vulnerable consumers, consumers who temporarily or permanently lack resource access or control, are unknowingly discriminated against, or excluded from the marketplace. By integrating the literature on consumer vulnerability, AI for social good, and the calls for rethinking marketing for a better world, the current work builds a framework on how to leverage AI technologies to detect, better serve, and empower vulnerable consumers. Specifically, our AID framework advocates for designing AI technologies that make services more accessible, optimize customer experiences and journeys interactively, and to dynamically improve consumer decision-making. Adopting a multi-stakeholder perspective, we also discuss the respective implications for researchers, managers, consumers, and public policy makers.",10.1007/s11747-023-00986-8,http://dx.doi.org/10.1007/s11747-023-00986-8,Journal of the Academy of Marketing Science,"Hermann, Erik;Williams, Gizem Yalcin;Puntoni, Stefano",2024,0,"@article{2-29819,
  title={Deploying artificial intelligence in services to AID vulnerable consumers},
  author={Hermann, Erik and Williams, Gizem Yalcin and Puntoni, Stefano},
  year={2024},
  journal={Journal of the Academy of Marketing Science},
  doi={10.1007/s11747-023-00986-8}
}",Theoretical contributions,"Finance / Business / Economy, Everyday / Employment / Public Service",Operational,"Analyzing, Advising","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-29847,springernature,Insulin dose optimization using an automated artificial intelligence-based decision support system in youths with type 1 diabetes,"Despite the increasing adoption of insulin pumps and continuous glucose monitoring devices, most people with type 1 diabetes do not achieve their glycemic goals 1. This could be related to a lack of expertise or inadequate time for clinicians to analyze complex sensor-augmented pump data. We tested whether frequent insulin dose adjustments guided by an automated artificial intelligence-based decision support system( AI-DSS) is as effective and safe as those guided by physicians in controlling glucose levels. ADVICE4U was a six-month, multicenter, multinational, parallel, randomized controlled, non-inferiority trial in 108 participants with type 1 diabetes, aged 10–21 years and using insulin pump therapy( ClinicalTrials. gov no. NCT03003806). Participants were randomized 1:1 to receive remote insulin dose adjustment every three weeks guided by either an AI-DSS, ( AI-DSS arm, n = 54) or by physicians( physician arm, n = 54). The results for the primary efficacy measure—the percentage of time spent within the target glucose range( 70–180 mg dl −1( 3. 9–10. 0 mmol l −1) ) —in the AI-DSS arm were statistically non-inferior to those in the physician arm( 50. 2 ± 11. 1% versus 51. 6 ± 11. 3%, respectively, P < 1 × 10 −7). The percentage of readings below 54 mg dl −1( <3. 0 mmol l −1) within the AI-DSS arm was statistically non-inferior to that in the physician arm( 1. 3 ± 1. 4% versus 1. 0 ± 0. 9%, respectively, P < 0. 0001). Three severe adverse events related to diabetes( two severe hypoglycemia, one diabetic ketoacidosis) were reported in the physician arm and none in the AI-DSS arm. In conclusion, use of an automated decision support tool for optimizing insulin pump settings was non-inferior to intensive insulin titration provided by physicians from specialized academic diabetes centers. The randomized-controlled trial ADVICE4U demonstrates non-inferiority of an automated AI-based decision support system compared with advice from expert physicians for optimal insulin dosing in youths with type 1 diabetes.",10.1038/s41591-020-1045-7,http://dx.doi.org/10.1038/s41591-020-1045-7,Nature Medicine,"Nimri, Revital;Battelino, Tadej;Laffel, Lori M.;Slover, Robert H.;Schatz, Desmond;Weinzimer, Stuart A.;Dovc, Klemen;Danne, Thomas;Phillip, Moshe;NextDREAM Consortium",2020,1,"@article{2-29847,
  title={Insulin dose optimization using an automated artificial intelligence-based decision support system in youths with type 1 diabetes},
  author={Nimri, Revital and Battelino, Tadej and Laffel, Lori M. and Slover, Robert H. and Schatz, Desmond and Weinzimer, Stuart A. and Dovc, Klemen and Danne, Thomas and Phillip, Moshe and NextDREAM Consortium},
  year={2020},
  doi={10.1038/s41591-020-1045-7},
  journal={Nature Medicine}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Executing, Analyzing","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-30021,springernature,A nascent design theory for explainable intelligent systems,"Due to computational advances in the past decades, so-called intelligent systems can learn from increasingly complex data, analyze situations, and support users in their decision-making to address them. However, in practice, the complexity of these intelligent systems renders the user hardly able to comprehend the inherent decision logic of the underlying machine learning model. As a result, the adoption of this technology, especially for high-stake scenarios, is hampered. In this context, explainable artificial intelligence offers numerous starting points for making the inherent logic explainable to people. While research manifests the necessity for incorporating explainable artificial intelligence into intelligent systems, there is still a lack of knowledge about how to socio-technically design these systems to address acceptance barriers among different user groups. In response, we have derived and evaluated a nascent design theory for explainable intelligent systems based on a structured literature review, two qualitative expert studies, a real-world use case application, and quantitative research. Our design theory includes design requirements, design principles, and design features covering the topics of global explainability, local explainability, personalized interface design, as well as psychological/emotional factors.",10.1007/s12525-022-00606-3,http://dx.doi.org/10.1007/s12525-022-00606-3,Electronic Markets,"Herm, Lukas-Valentin;Steinbach, Theresa;Wanner, Jonas;Janiesch, Christian",2022,30,"@article{2-30021,
  title={A nascent design theory for explainable intelligent systems},
  author={Herm, Lukas-Valentin and Steinbach, Theresa and Wanner, Jonas and Janiesch, Christian},
  year={2022},
  doi={10.1007/s12525-022-00606-3},
  journal={Electronic Markets}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Institutional,"Forecasting, Analyzing, Explaining","Decision-maker, Guardian","Change trust, Change cognitive demands, Change affective-perceptual",Update AI competence,"explanations, a personalized interface design, support human in own decision-making, incorporating features that rise the motivation",NA,Semi-Autonomous System,Yes,Yes
2-30022,springernature,Explainable product backorder prediction exploiting CNN: Introducing explainable models in businesses,"Due to expected positive impacts on business, the application of artificial intelligence has been widely increased. The decision-making procedures of those models are often complex and not easily understandable to the company’s stakeholders, i. e. the people having to follow up on recommendations or try to understand automated decisions of a system. This opaqueness and black-box nature might hinder adoption, as users struggle to make sense and trust the predictions of AI models. Recent research on eXplainable Artificial Intelligence( XAI) focused mainly on explaining the models to AI experts with the purpose of debugging and improving the performance of the models. In this article, we explore how such systems could be made explainable to the stakeholders. For doing so, we propose a new convolutional neural network( CNN) -based explainable predictive model for product backorder prediction in inventory management. Backorders are orders that customers place for products that are currently not in stock. The company now takes the risk to produce or acquire the backordered products while in the meantime, customers can cancel their orders if that takes too long, leaving the company with unsold items in their inventory. Hence, for their strategic inventory management, companies need to make decisions based on assumptions. Our argument is that these tasks can be improved by offering explanations for AI recommendations. Hence, our research investigates how such explanations could be provided, employing Shapley additive explanations to explain the overall models’ priority in decision-making. Besides that, we introduce locally interpretable surrogate models that can explain any individual prediction of a model. The experimental results demonstrate effectiveness in predicting backorders in terms of standard evaluation metrics and outperform known related works with AUC 0. 9489. Our approach demonstrates how current limitations of predictive technologies can be addressed in the business domain.",10.1007/s12525-022-00599-z,http://dx.doi.org/10.1007/s12525-022-00599-z,Electronic Markets,"Shajalal, Md;Boden, Alexander;Stevens, Gunnar",2022,30,"@article{2-30022,
  title = {Explainable product backorder prediction exploiting CNN: Introducing explainable models in businesses},
  author = {Shajalal, Md and Boden, Alexander and Stevens, Gunnar},
  year = {2022},
  journal = {Electronic Markets},
  doi = {10.1007/s12525-022-00599-z}
}",Methodological contributions,Finance / Business / Economy,Operational,"Forecasting, Advising, Explaining","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-30142,springernature,Graph augmented triplet architecture for fine-grained patient similarity,"Electronic Health Records( EHRs) provide rich information for the research of multiple healthcare applications that improve chance of survival in Intensive Care Units( ICU) , especially the case-based decision support system, which helps physicians make effective clinical decisions in the rapidly changing environment of ICUs according to similar historical patient records. Thus, an efficient approach being able to measure clinically similarities among patients is a fundamental and critical module for the decision support system. In this paper, we propose a novel framework that derives informative EHR graphs from patient records to augment information transmission in Recurrent Neural Networks( RNNs) for fine-grained patient similarity learning, named Graph Augmented Triplet Architecture( GATA). Specifically, GATA firstly derives Dynamic Bayesian Networks( DBNs) from EHRs to reveal correlations among medical variables, then it constructs graph augmented RNNs where each unit aggregate information from variables that it conditionally dependent in DBNs. After that, the specially designed RNNs will act as the fundamental components of the Triplet architecture to measure similarities among patients. GATA has been compared to different baselines based on a real-world ICU dataset MIMIC III, and the experimental results illustrate the effectiveness of GATA in fine-grained patient similarity learning, providing a promising direction for the research on clinical decision support.",10.1007/s11280-020-00794-y,http://dx.doi.org/10.1007/s11280-020-00794-y,World Wide Web Journal,"Wang, Yanda;Chen, Weitong;Pi, Dechang;Boots, Robert",2020,12,"@article{2-30142,
  title = {Graph Augmented Triplet Architecture for Fine-Grained Patient Similarity},
  author = {Wang, Yanda and Chen, Weitong and Pi, Dechang and Boots, Robert},
  year = {2020},
  journal = {World Wide Web Journal},
  doi = {10.1007/s11280-020-00794-y}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising, Executing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-30259,springernature,Decision Support for Intoxication Prediction Using Graph Convolutional Networks,"Every day, poison control centers( PCC) are called for immediate classification and treatment recommendations of acute intoxication cases. Due to their time-sensitive nature, a doctor is required to propose a correct diagnosis and intervention within a minimal time frame. Usually the toxin is known and recommendations can be made accordingly. However, in challenging cases only symptoms are mentioned and doctors have to rely on clinical experience. Medical experts and our analyses of regional intoxication records provide evidence that this is challenging, since occurring symptoms may not always match textbook descriptions due to regional distinctions or institutional workflow. Computer-aided diagnosis( CADx) can provide decision support, but approaches so far do not consider additional patient data like age or gender, despite their potential value for the diagnosis. In this work, we propose a new machine learning based CADx method which fuses patient symptoms and meta data using graph convolutional networks. We further propose a novel symptom matching method that allows the effective incorporation of prior knowledge into the network and evidently stabilizes the prediction. We validate our method against 10 medical doctors with different experience diagnosing intoxications for 10 different toxins from the PCC in Munich and show our method’s superiority for poison prediction.",10.1007/978-3-030-59713-9_61,http://dx.doi.org/10.1007/978-3-030-59713-9_61,Medical Image Computing and Computer-Assisted Intervention Conference,"Burwinkel, Hendrik;Keicher, Matthias;Bani-Harouni, David;Zellner, Tobias;Eyer, Florian;Navab, Nassir;Ahmadi, Seyed-Ahmad",2020,3,"@inproceedings{2-30259,
  title={Decision Support for Intoxication Prediction Using Graph Convolutional Networks},
  author={Burwinkel, Hendrik and Keicher, Matthias and Bani-Harouni, David and Zellner, Tobias and Eyer, Florian and Navab, Nassir and Ahmadi, Seyed-Ahmad},
  year={2020},
  booktitle={Medical Image Computing and Computer-Assisted Intervention Conference},
  doi={10.1007/978-3-030-59713-9_61}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-30293,springernature,Explanation Ontology: A Model of Explanations for User-Centered AI,"Explainability has been a goal for Artificial Intelligence( AI) systems since their conception, with the need for explainability growing as more complex AI models are increasingly used in critical, high-stakes settings such as healthcare. Explanations have often added to an AI system in a non-principled, post-hoc manner. With greater adoption of these systems and emphasis on user-centric explainability, there is a need for a structured representation that treats explainability as a primary consideration, mapping end user needs to specific explanation types and the system’s AI capabilities. We design an explanation ontology to model both the role of explanations, accounting for the system and user attributes in the process, and the range of different literature-derived explanation types. We indicate how the ontology can support user requirements for explanations in the domain of healthcare. We evaluate our ontology with a set of competency questions geared towards a system designer who might use our ontology to decide which explanation types to include, given a combination of users’ needs and a system’s capabilities, both in system design settings and in real-time operations. Through the use of this ontology, system designers will be able to make informed choices on which explanations AI systems can and should provide.",10.1007/978-3-030-62466-8_15,http://dx.doi.org/10.1007/978-3-030-62466-8_15,International Semantic Web Conference,"Chari, Shruthi;Seneviratne, Oshani;Gruen, Daniel M.;Foreman, Morgan A.;Das, Amar K.;McGuinness, Deborah L.",2020,139,"@inproceedings{2-30293,
  title     = {Explanation Ontology: A Model of Explanations for User-Centered AI},
  author    = {Chari, Shruthi and Seneviratne, Oshani and Gruen, Daniel M. and Foreman, Morgan A. and Das, Amar K. and McGuinness, Deborah L.},
  year      = {2020},
  doi       = {10.1007/978-3-030-62466-8_15},
  booktitle = {International Semantic Web Conference}
}",Theoretical contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising",Decision-maker,no such info,no such info,"counterfactual explanations, textual explanations, recommendations",domain knowledge,Textual,Yes,Yes
2-30303,springernature,Solving the explainable AI conundrum by bridging clinicians’ needs and developers’ goals,"Explainable artificial intelligence( XAI) has emerged as a promising solution for addressing the implementation challenges of AI/ML in healthcare. However, little is known about how developers and clinicians interpret XAI and what conflicting goals and requirements they may have. This paper presents the findings of a longitudinal multi-method study involving 112 developers and clinicians co-designing an XAI solution for a clinical decision support system. Our study identifies three key differences between developer and clinician mental models of XAI, including opposing goals( model interpretability vs. clinical plausibility) , different sources of truth( data vs. patient) , and the role of exploring new vs. exploiting old knowledge. Based on our findings, we propose design solutions that can help address the XAI conundrum in healthcare, including the use of causal inference models, personalized explanations, and ambidexterity between exploration and exploitation mindsets. Our study highlights the importance of considering the perspectives of both developers and clinicians in the design of XAI systems and provides practical recommendations for improving the effectiveness and usability of XAI in healthcare.",10.1038/s41746-023-00837-4,http://dx.doi.org/10.1038/s41746-023-00837-4,Nature Partner Journals Digital Medicine,"Bienefeld, Nadine;Boss, Jens Michael;Lüthy, Rahel;Brodbeck, Dominique;Azzati, Jan;Blaser, Mirco;Willms, Jan;Keller, Emanuela",2023,98,"@article{2-30303,
  title={Solving the explainable AI conundrum by bridging clinicians' needs and developers' goals},
  author={Bienefeld, Nadine and Boss, Jens Michael and L{\""u}thy, Rahel and Brodbeck, Dominique and Azzati, Jan and Blaser, Mirco and Willms, Jan and Keller, Emanuela},
  year={2023},
  journal={Nature Partner Journals Digital Medicine},
  doi={10.1038/s41746-023-00837-4}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Explaining",Decision-maker,"Alter decision outcomes, Change trust","Change AI responses, Update AI competence","causal explanations, personalized explanations","trust without model interpretability, relying instead on clinical plausibility","Textual, Visual, Interactive interface",Yes,Yes
2-30355,springernature,How people reason with counterfactual and causal explanations for Artificial Intelligence decisions in familiar and unfamiliar domains,"Few empirical studies have examined how people understand counterfactual explanations for other people’s decisions, for example, “ if you had asked for a lower amount, your loan application would have been approved”. Yet many current Artificial Intelligence( AI) decision support systems rely on counterfactual explanations to improve human understanding and trust. We compared counterfactual explanations to causal ones, i. e. , “because you asked for a high amount, your loan application was not approved ”, for an AI’s decisions in a familiar domain( alcohol and driving) and an unfamiliar one( chemical safety) in four experiments( n = 731). Participants were shown inputs to an AI system, its decisions, and an explanation for each decision; they attempted to predict the AI’s decisions, or to make their own decisions. Participants judged counterfactual explanations more helpful than causal ones, but counterfactuals did not improve the accuracy of their predictions of the AI’s decisions more than causals( Experiment 1). However, counterfactuals improved the accuracy of participants’ own decisions more than causals( Experiment 2). When the AI’s decisions were correct( Experiments 1 and 2) , participants considered explanations more helpful and made more accurate judgements in the familiar domain than in the unfamiliar one; but when the AI’s decisions were incorrect, they considered explanations less helpful and made fewer accurate judgements in the familiar domain than the unfamiliar one, whether they predicted the AI’s decisions( Experiment 3a) or made their own decisions( Experiment 3b). The results corroborate the proposal that counterfactuals provide richer information than causals, because their mental representation includes more possibilities.",10.3758/s13421-023-01407-5,http://dx.doi.org/10.3758/s13421-023-01407-5,Memory & Cognition,"Celar, Lenart;Byrne, Ruth M. J.",2023,37,"@article{2-30355,
  title={How people reason with counterfactual and causal explanations for Artificial Intelligence decisions in familiar and unfamiliar domains},
  author={Celar, Lenart and Byrne, Ruth M. J.},
  year={2023},
  journal={Memory \& Cognition},
  doi={10.3758/s13421-023-01407-5}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Manufacturing / Industry / Automation, Transportation / Mobility / Planning",Individual,"Explaining, Executing, Advising","Decision-maker, Guardian, Decision-subject","Alter decision outcomes, Change trust, Change affective-perceptual",no such info,"counterfactual explanations, causal explanations",NA,Textual,Yes,Yes
2-30375,springernature,Are Algorithmic Decisions Legitimate? The Effect of Process and Outcomes on Perceptions of Legitimacy of AI Decisions,"Firms use algorithms to make important business decisions. To date, the algorithmic accountability literature has elided a fundamentally empirical question important to business ethics and management: Under what circumstances, if any, are algorithmic decision-making systems considered legitimate ? The present study begins to answer this question. Using factorial vignette survey methodology, we explore the impact of decision importance, governance, outcomes, and data inputs on perceptions of the legitimacy of algorithmic decisions made by firms. We find that many of the procedural governance mechanisms in practice today, such as notices and impact statements, do not lead to algorithmic decisions being perceived as more legitimate in general, and, consistent with legitimacy theory, that algorithmic decisions with good outcomes are perceived as more legitimate than bad outcomes. Yet, robust governance, such as offering an appeal process, can create a legitimacy dividend for decisions with bad outcomes. However, when arbitrary or morally dubious factors are used to make decisions, most legitimacy dividends are erased. In other words, companies cannot overcome the legitimacy penalty of using arbitrary or morally dubious factors, such as race or the day of the week, with a good outcome or an appeal process for individuals. These findings add new perspectives to both the literature on legitimacy and policy discussions on algorithmic decision-making in firms.",10.1007/s10551-021-05032-7,http://dx.doi.org/10.1007/s10551-021-05032-7,Journal of Business Ethics,"Martin, Kirsten;Waldman, Ari",2023,1,"@article{2-30375,
  title = {Are Algorithmic Decisions Legitimate? The Effect of Process and Outcomes on Perceptions of Legitimacy of AI Decisions},
  author = {Martin, Kirsten and Waldman, Ari},
  year = {2023},
  doi = {10.1007/s10551-021-05032-7},
  journal = {Journal of Business Ethics}
}",Empirical contributions,Finance / Business / Economy,Institutional,Executing,"Decision-maker, Decision-subject, Guardian, Stakeholder",NA,NA,NA,NA,"Textual, Semi-Autonomous System",Yes,No
2-30651,springernature,Potential reduction in healthcare carbon footprint by autonomous artificial intelligence,"Healthcare is a large contributor to greenhouse gas( GHG) emissions around the world, given current power generation mix. Telemedicine, with its reduced travel for providers and patients, has been proposed to reduce emissions. Artificial intelligence( AI) , and especially autonomous AI, where the medical decision is made without human oversight, has the potential to further reduce healthcare GHG emissions, but concerns have also been expressed about GHG emissions from digital technology, and AI training and inference. In a real-world example, we compared the marginal GHG contribution of an encounter performed by an autonomous AI to that of an in-person specialist encounter. Results show that an 80% reduction may be achievable, and we conclude that autonomous AI has the potential to reduce healthcare GHG emissions.",10.1038/s41746-022-00605-w,http://dx.doi.org/10.1038/s41746-022-00605-w,Nature Partner Journals Digital Medicine,"Wolf, Risa M.;Abramoff, Michael D.;Channa, Roomasa;Tava, Chris;Clarida, Warren;Lehmann, Harold P.",2022,47,"@article{2-30651,
  title={Potential reduction in healthcare carbon footprint by autonomous artificial intelligence},
  author={Wolf, Risa M. and Abramoff, Michael D. and Channa, Roomasa and Tava, Chris and Clarida, Warren and Lehmann, Harold P.},
  year={2022},
  journal={Nature Partner Journals Digital Medicine},
  doi={10.1038/s41746-022-00605-w}
}",Empirical contributions,"Environment / Resources / Energy, Healthcare / Medicine / Surgery",Operational,Executing,"Stakeholder, Guardian",NA,NA,NA,NA,NA,Yes,No
2-30744,springernature,Boosting Human Decision-making with AI-Generated Decision Aids,"Human decision-making is plagued by many systematic errors. Many of these errors can be avoided by providing decision aids that guide decision-makers to attend to the important information and integrate it according to a rational decision strategy. Designing such decision aids used to be a tedious manual process. Advances in cognitive science might make it possible to automate this process in the future. We recently introduced machine learning methods for discovering optimal strategies for human decision-making automatically and an automatic method for explaining those strategies to people. Decision aids constructed by this method were able to improve human decision-making. However, following the descriptions generated by this method is very tedious. We hypothesized that this problem can be overcome by conveying the automatically discovered decision strategy as a series of natural language instructions for how to reach a decision. Experiment 1 showed that people do indeed understand such procedural instructions more easily than the decision aids generated by our previous method. Encouraged by this finding, we developed an algorithm for translating the output of our previous method into procedural instructions. We applied the improved method to automatically generate decision aids for a naturalistic planning task( i. e. , planning a road trip) and a naturalistic decision task( i. e. , choosing a mortgage). Experiment 2 showed that these automatically generated decision aids significantly improved people’s performance in planning a road trip and choosing a mortgage. These findings suggest that AI-powered boosting might have potential for improving human decision-making in the real world.",10.1007/s42113-022-00149-y,http://dx.doi.org/10.1007/s42113-022-00149-y,Computational Brain & Behavior,"Becker, Frederic;Skirzyński, Julian;Opheusden, Bas;Lieder, Falk",2022,31,"@article{2-30744,
  title = {Boosting Human Decision-making with AI-Generated Decision Aids},
  author = {Becker, Frederic and Skirzyński, Julian and Opheusden, Bas and Lieder, Falk},
  year = {2022},
  doi = {10.1007/s42113-022-00149-y},
  journal = {Computational Brain \& Behavior}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Transportation / Mobility / Planning, Finance / Business / Economy",Individual,"Explaining, Advising",Decision-maker,"Alter decision outcomes, Change trust",no such info,"procedural instructions, static descriptions",NA,Textual,Yes,Yes
2-30752,springernature,Wearable EEG electronics for a Brain–AI Closed-Loop System to enhance autonomous machine decision-making,"Human nonverbal communication tools are very ambiguous and difficult to transfer to machines or artificial intelligence( AI). If the AI understands the mental state behind a user’s decision, it can learn more appropriate decisions even in unclear situations. We introduce the Brain–AI Closed-Loop System( BACLoS) , a wireless interaction platform that enables human brain wave analysis and transfers results to AI to verify and enhance AI decision-making. We developed a wireless earbud-like electroencephalography( EEG) measurement device, combined with tattoo-like electrodes and connectors, which enables continuous recording of high-quality EEG signals, especially the error-related potential( ErrP). The sensor measures the ErrP signals, which reflects the human cognitive consequences of an unpredicted machine response. The AI corrects or reinforces decisions depending on the presence or absence of the ErrP signals, which is determined by deep learning classification of the received EEG data. We demonstrate the BACLoS for AI-based machines, including autonomous driving vehicles, maze solvers, and assistant interfaces.",10.1038/s41528-022-00164-w,http://dx.doi.org/10.1038/s41528-022-00164-w,npj Flexible Electronics,"Shin, Joo Hwan;Kwon, Junmo;Kim, Jong Uk;Ryu, Hyewon;Ok, Jehyung;Joon Kwon, S.;Park, Hyunjin;Kim, Tae-il",2022,95,"@article{2-30752,
  title={Wearable EEG electronics for a Brain--AI Closed-Loop System to enhance autonomous machine decision-making},
  author={Shin, Joo Hwan and Kwon, Junmo and Kim, Jong Uk and Ryu, Hyewon and Ok, Jehyung and Kwon, S. Joon and Park, Hyunjin and Kim, Tae-il},
  year={2022},
  journal={npj Flexible Electronics},
  doi={10.1038/s41528-022-00164-w}
}",System/Artifact contributions,"Generic / Abstract / Domain-agnostic, Transportation / Mobility / Planning",Individual,"Executing, Collaborating",Decision-maker,"Alter decision outcomes, Change cognitive demands","Shape AI for accountability, Change AI responses",decision suggestions,"clarification, human-impact-informed algorithm design","Autonomous System, Auditory, Visual",Yes,Yes
2-30800,springernature,A machine learning approach to support decision in insider trading detection,"Identifying market abuse activity from data on investors’ trading activity is very challenging both for the data volume and for the low signal to noise ratio. Here we propose two complementary unsupervised machine learning methods to support market surveillance aimed at identifying potential insider trading activities. The first one uses clustering to identify, in the vicinity of a price sensitive event such as a takeover bid, discontinuities in the trading activity of an investor with respect to her own past trading history and on the present trading activity of her peers. The second unsupervised approach aims at identifying( small) groups of investors that act coherently around price sensitive events, pointing to potential insider rings, i. e. a group of synchronised traders displaying strong directional trading in rewarding position in a period before the price sensitive event. As a case study, we apply our methods to investor resolved data of Italian stocks around takeover bids.",10.1140/epjds/s13688-024-00500-2,http://dx.doi.org/10.1140/epjds/s13688-024-00500-2,European Physical Journal Data Science,"Mazzarisi, Piero;Ravagnani, Adele;Deriu, Paola;Lillo, Fabrizio;Medda, Francesca;Russo, Antonio",2024,6,"@article{2-30800,
  title={A machine learning approach to support decision in insider trading detection},
  author={Mazzarisi, Piero and Ravagnani, Adele and Deriu, Paola and Lillo, Fabrizio and Medda, Francesca and Russo, Antonio},
  year={2024},
  journal={European Physical Journal Data Science},
  doi={10.1140/epjds/s13688-024-00500-2}
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Forecasting, Monitoring, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-30811,springernature,Developing well-calibrated illness severity scores for decision support in the critically ill,"Illness severity scores are regularly employed for quality improvement and benchmarking in the intensive care unit, but poor generalization performance, particularly with respect to probability calibration, has limited their use for decision support. These models tend to perform worse in patients at a high risk for mortality. We hypothesized that a sequential modeling approach wherein an initial regression model assigns risk and all patients deemed high risk then have their risk quantified by a second, high-risk-specific, regression model would result in a model with superior calibration across the risk spectrum. We compared this approach to a logistic regression model and a sophisticated machine learning approach, the gradient boosting machine. The sequential approach did not have an effect on the receiver operating characteristic curve or the precision-recall curve but resulted in improved reliability curves. The gradient boosting machine achieved a small improvement in discrimination performance and was similarly calibrated to the sequential models.",10.1038/s41746-019-0153-6,http://dx.doi.org/10.1038/s41746-019-0153-6,Nature Partner Journals Digital Medicine,"Cosgriff, Christopher V.;Celi, Leo Anthony;Ko, Stephanie;Sundaresan, Tejas;Armengol de la Hoz, Miguel Ángel;Kaufman, Aaron Russell;Stone, David J.;Badawi, Omar;Deliberato, Rodrigo Octavio",2019,31,"@article{2-30811,
  title = {Developing well-calibrated illness severity scores for decision support in the critically ill},
  author = {Cosgriff, Christopher V. and Celi, Leo Anthony and Ko, Stephanie and Sundaresan, Tejas and Armengol de la Hoz, Miguel {\'A}ngel and Kaufman, Aaron Russell and Stone, David J. and Badawi, Omar and Deliberato, Rodrigo Octavio},
  year = {2019},
  doi = {10.1038/s41746-019-0153-6},
  journal = {Nature Partner Journals Digital Medicine}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-30818,springernature,Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,"Image-based precision medicine aims to personalize treatment decisions based on an individual’s unique imaging features so as to improve their clinical outcome. Machine learning frameworks that integrate uncertainty estimation as part of their treatment recommendations would be safer and more reliable. However, little work has been done in adapting uncertainty estimation techniques and validation metrics for precision medicine. In this paper, we use Bayesian deep learning for estimating the posterior distribution over factual and counterfactual outcomes on several treatments. This allows for estimating the uncertainty for each treatment option and for the individual treatment effects( ITE) between any two treatments. We train and evaluate this model to predict future new and enlarging T2 lesion counts on a large, multi-center dataset of MR brain images of patients with multiple sclerosis, exposed to several treatments during randomized controlled trials. We evaluate the correlation of the uncertainty estimate with the factual error, and, given the lack of ground truth counterfactual outcomes, demonstrate how uncertainty for the ITE prediction relates to bounds on the ITE error. Lastly, we demonstrate how knowledge of uncertainty could modify clinical decision-making to improve individual patient and clinical trial outcomes.",10.1007/978-3-031-43904-9_46,http://dx.doi.org/10.1007/978-3-031-43904-9_46,Medical Image Computing and Computer Assisted Intervention (MICCAI),"Durso-Finley, Joshua;Falet, Jean-Pierre;Mehta, Raghav;Arnold, Douglas L.;Pawlowski, Nick;Arbel, Tal",2023,10,"@inproceedings{2-30818,
  title     = {Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models},
  author    = {Durso-Finley, Joshua and Falet, Jean-Pierre and Mehta, Raghav and Arnold, Douglas L. and Pawlowski, Nick and Arbel, Tal},
  year      = {2023},
  booktitle = {Medical Image Computing and Computer Assisted Intervention (MICCAI)},
  doi       = {10.1007/978-3-031-43904-9\_46}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-30846,springernature,Machine learning-based decision support model for selecting intra-arterial therapies for unresectable hepatocellular carcinoma: A national real-world evidence-based study,"Importance Intra-arterial therapies( IATs) are promising options for unresectable hepatocellular carcinoma( HCC). Stratifying the prognostic risk before administering IAT is important for clinical decision-making and for designing future clinical trials. Objective To develop and validate a machine learning( ML) -based decision support model( MLDSM) for recommending IAT modalities for unresectable HCC. Design, setting, and participants Between October 2014 and October 2022, a total of 2, 959 patients with HCC who underwent initial IATs were enroled retrospectively from 13 tertiary hospitals. These patients were divided into the training cohort( n = 1700) , validation cohort( n = 428) , and test cohort( n = 200). Main outcomes and measures Thirty-two clinical variables were input, and five supervised ML algorithms, including eXtreme Gradient Boosting( XGBoost) , Categorical Gradient Boosting( CatBoost) , Gradient Boosting Decision Tree( GBDT) , Light Gradient Boosting Machine( LGBM) and Random Forest( RF) , were compared using the areas under the receiver operating characteristic curve( AUC) with the DeLong test. Results A total of 1856 patients were assigned to the IAT alone Group( I-A) , and 1103 patients were assigned to the IAT combination Group( I-C). The 12-month death rates were 31. 9%( 352/1103) in the I-A group and 50. 4%( 936/1856) in the I-C group. For the test cohort, in the I-C group, the CatBoost model achieved the best discrimination when 30 variables were input, with an AUC of 0. 776( 95% confidence intervals [CI], 0. 833–0. 868). In the I-A group, the LGBM model achieved the best discrimination when 24 variables were input, with an AUC of 0. 776( 95% CI, 0. 833–0. 868). According to the decision trees, BCLC grade, local therapy, and diameter as to p three variables were used to guide clinical decisions between IAT modalities. Conclusions and relevance The MLDSM can accurately stratify prognostic risk for HCC patients who received IATs, thus helping physicians to make decisions about IAT and providing guidance for surveillance strategies in clinical practice.",10.1038/s41416-024-02784-7,http://dx.doi.org/10.1038/s41416-024-02784-7,British Journal of Cancer,"An, Chao;Wei, Ran;Liu, Wendao;Fu, Yan;Gong, Xiaolong;Li, Chengzhi;Yao, Wang;Zuo, Mengxuan;Li, Wang;Li, Yansheng;Wu, Fatian;Liu, Kejia;Yan, Dong;Wu, Peihong;Han, Jianjun",2024,0,"@article{2-30846,
  title={Machine learning-based decision support model for selecting intra-arterial therapies for unresectable hepatocellular carcinoma: A national real-world evidence-based study},
  author={An, Chao and Wei, Ran and Liu, Wendao and Fu, Yan and Gong, Xiaolong and Li, Chengzhi and Yao, Wang and Zuo, Mengxuan and Li, Wang and Li, Yansheng and Wu, Fatian and Liu, Kejia and Yan, Dong and Wu, Peihong and Han, Jianjun},
  year={2024},
  journal={British Journal of Cancer},
  doi={10.1038/s41416-024-02784-7}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-30954,springernature,Smartphone-based DNA diagnostics for malaria detection using deep learning for local decision support and blockchain technology for security,"In infectious disease diagnosis, results need to be communicated rapidly to healthcare professionals once testing has been completed so that care pathways can be implemented. This represents a particular challenge when testing in remote, low-resource rural communities, in which such diseases often create the largest burden. Here, we report a smartphone-based end-to-end platform for multiplexed DNA diagnosis of malaria. The approach uses a low-cost paper-based microfluidic diagnostic test, which is combined with deep learning algorithms for local decision support and blockchain technology for secure data connectivity and management. We validated the approach via field tests in rural Uganda, where it correctly identified more than 98% of tested cases. Our platform also provides secure geotagged diagnostic information, which creates the possibility of integrating infectious disease data within surveillance frameworks. A smartphone-based system that uses deep learning algorithms for local decision support, and incorporates blockchain technology to provide secure data connectivity and management, can be used for multiplexed DNA diagnosis of malaria.",10.1038/s41928-021-00612-x,http://dx.doi.org/10.1038/s41928-021-00612-x,Nature Electronics,"Guo, Xin;Khalid, Muhammad Arslan;Domingos, Ivo;Michala, Anna Lito;Adriko, Moses;Rowel, Candia;Ajambo, Diana;Garrett, Alice;Kar, Shantimoy;Yan, Xiaoxiang;Reboud, Julien;Tukahebwa, Edridah M.;Cooper, Jonathan M.",2021,110,"@article{2-30954,
  title = {Smartphone-based DNA diagnostics for malaria detection using deep learning for local decision support and blockchain technology for security},
  author = {Guo, Xin and Khalid, Muhammad Arslan and Domingos, Ivo and Michala, Anna Lito and Adriko, Moses and Rowel, Candia and Ajambo, Diana and Garrett, Alice and Kar, Shantimoy and Yan, Xiaoxiang and Reboud, Julien and Tukahebwa, Edridah M. and Cooper, Jonathan M.},
  year = {2021},
  doi = {10.1038/s41928-021-00612-x},
  journal = {Nature Electronics}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-31003,springernature,Collaborative Work with Highly Automated Marine Navigation Systems,"In navigation applications, Artificial Intelligence( AI) can improve efficiency and decision making. It is not clear, however, how designers should account for human cooperation when integrating AI systems in navigation work. In a novel empirical study, we examine the transition in the maritime domain towards higher levels of machine autonomy. Our method involved interviewing technology designers( n = 9) and navigators aboard two partially automated ferries( n = 5) , as well as collecting field observations aboard one of the ferries. The results indicated a discrepancy between how designers construed human-AI collaboration compared to navigators’ own accounts in the field. Navigators reflected upon their role as one of ‘backup, ’ defined by ad-hoc control takeovers from the automation. Designers positioned navigators ‘in the loop’ of a larger control system but discounted the role of in-situ skills and heuristic decision making in all but the most controlled takeover actions. The discrepancy shed light on how integration of AI systems may be better aligned to human cooperation in navigation. This included designing AI systems that render computational activities more visible and that incorporate social cues that articulate human work in its natural setting. Positioned within the field of AI alignment research, the main contribution is a formulation of human-AI interaction design insights for future navigation and control room work.",10.1007/s10606-022-09450-7,http://dx.doi.org/10.1007/s10606-022-09450-7,Computer Supported Cooperative Work (CSCW),"Veitch, Erik;Dybvik, Henrikke;Steinert, Martin;Alsos, Ole Andreas",2024,18,"@article{2-31003,
  title = {Collaborative Work with Highly Automated Marine Navigation Systems},
  author = {Veitch, Erik and Dybvik, Henrikke and Steinert, Martin and Alsos, Ole Andreas},
  year = {2024},
  doi = {10.1007/s10606-022-09450-7},
  journal = {Computer Supported Cooperative Work (CSCW)}
}",Empirical contributions,Transportation / Mobility / Planning,Organizational,"Executing, Collaborating","Decision-maker, Guardian","Change cognitive demands, Restrict human agency","Shape AI for accountability, Update AI competence, Change AI responses",NA,"domain knowledge, navigators' own skills and experience",Autonomous System,Yes,Yes
2-31185,springernature,Everyday Diagnostic Work in the Histopathology Lab: CSCW Perspectives on the Utilization of Data-Driven Clinical Decision Support Systems,"In this paper we present an ethnographic study of the work of histopathologists as they grapple with the twin innovations of transitioning to digital biopsy images and the prospective adoption of an AI-based clinical decision support system( CDSS). We explore how they are adapting to the former and their expectations of the latter. The study’s ethnomethodologically-informed ethnography approach brings to light some key issues regarding the nature of diagnostic work, and accountability and trust that are central to the successful adoption of technological innovations in clinical settings.",10.1007/s10606-024-09496-9,http://dx.doi.org/10.1007/s10606-024-09496-9,Computer Supported Cooperative Work (CSCW),"Procter, Rob;Rouncefield, Mark;Tolmie, Peter;Verrill, Clare",2024,5,"@article{2-31185,
  title = {Everyday Diagnostic Work in the Histopathology Lab: CSCW Perspectives on the Utilization of Data-Driven Clinical Decision Support Systems},
  author = {Procter, Rob and Rouncefield, Mark and Tolmie, Peter and Verrill, Clare},
  year = {2024},
  journal = {Computer Supported Cooperative Work (CSCW)},
  doi = {10.1007/s10606-024-09496-9}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Knowledge provider","Alter decision outcomes, Change trust, Change cognitive demands, Shift responsibility",no such info,"preliminary diagnoses, highlighting and extraction, procedural instructions, reasoning",NA,"Textual, Visual",Yes,Yes
2-31289,springernature,An Empirical Evaluation of Predicted Outcomes as Explanations in Human-AI Decision-Making,"In this work, we empirically examine human-AI decision-making in the presence of explanations based on predicted outcomes. This type of explanation provides a human decision-maker with expected consequences for each decision alternative at inference time—where the predicted outcomes are typically measured in a problem-specific unit( e. g. , profit in U. S. dollars). We conducted a pilot study in the context of peer-to-peer lending to assess the effects of providing predicted outcomes as explanations to lay study participants. Our preliminary findings suggest that people’s reliance on AI recommendations increases compared to cases where no explanation or feature-based explanations are provided, especially when the AI recommendations are incorrect. This results in a hampered ability to distinguish correct from incorrect AI recommendations, which can ultimately affect decision quality in a negative way.",10.1007/978-3-031-23618-1_24,http://dx.doi.org/10.1007/978-3-031-23618-1_24,European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases,"Jakubik, Johannes;Schöffer, Jakob;Hoge, Vincent;Vössing, Michael;Kühl, Niklas",2023,2,"@inproceedings{2-31289,
  title     = {An Empirical Evaluation of Predicted Outcomes as Explanations in Human-AI Decision-Making},
  author    = {Jakubik, Johannes and Sch\""offer, Jakob and Hoge, Vincent and V\""ossing, Michael and K\""uhl, Niklas},
  year      = {2023},
  doi       = {10.1007/978-3-031-23618-1_24},
  booktitle = {European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases}
}",Empirical contributions,Finance / Business / Economy,Operational,"Advising, Forecasting, Explaining",Decision-maker,"Alter decision outcomes, Change trust",no such info,"recommendations, feature-based explanations",NA,Textual,Yes,Yes
2-31299,springernature,Personalized insulin dosing using reinforcement learning for high-fat meals and aerobic exercises in type 1 diabetes: a proof-of-concept trial,"In type 1 diabetes, high-fat meals require more insulin to prevent hyperglycemia while meals followed by aerobic exercises require less insulin to prevent hypoglycemia, but the adjustments needed vary between individuals. We propose a decision support system with reinforcement learning to personalize insulin doses for high-fat meals and postprandial aerobic exercises. We test this system in a single-arm 16-week study in 15 adults on multiple daily injections therapy( NCT05041621). The primary objective of this study is to assess the feasibility of the novel learning algorithm. This study looks at glucose outcomes and patient reported outcomes. The postprandial incremental area under the glucose curve is improved from the baseline to the evaluation period for high-fat meals( 378 ± 222 vs 38 ± 223 mmol/L/min, p = 0. 03) and meals followed by exercises( −395 ± 192 vs 132 ± 181 mmol/L/min, p = 0. 007). The postprandial time spent below 3. 9 mmol/L is reduced after high-fat meals( 5. 3 ± 1. 6 vs 1. 8 ± 1. 5%, p = 0. 003) and meals followed by exercises( 5. 3 ± 1. 2 vs 1. 4 ± 1. 1%, p = 0. 003). Our study shows the feasibility of automatically personalizing insulin doses for high-fat meals and postprandial exercises. Randomized controlled trials are warranted. Individuals with type 1 diabetes face challenges in determining the insulin doses they need before high-fat meals and meals followed by aerobic exercises. Here, the authors show that it is clinically feasible to automatically determine these insulin doses for each individual using a reinforcement learning algorithm.",10.1038/s41467-024-50764-5,http://dx.doi.org/10.1038/s41467-024-50764-5,Nature Communications,"Jafar, Adnan;Kobayati, Alessandra;Tsoukas, Michael A.;Haidar, Ahmad",2024,13,"@article{2-31299,
  title = {Personalized insulin dosing using reinforcement learning for high-fat meals and aerobic exercises in type 1 diabetes: a proof-of-concept trial},
  author = {Jafar, Adnan and Kobayati, Alessandra and Tsoukas, Michael A. and Haidar, Ahmad},
  year = {2024},
  doi = {10.1038/s41467-024-50764-5},
  journal = {Nature Communications}
}","Algorithmic contributions, Empirical contributions",Healthcare / Medicine / Surgery,Individual,"Executing, Forecasting, Analyzing","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-31327,springernature,Matching code and law: achieving algorithmic fairness with optimal transport,"Increasingly, discrimination by algorithms is perceived as a societal and legal problem. As a response, a number of criteria for implementing algorithmic fairness in machine learning have been developed in the literature. This paper proposes the continuous fairness algorithm $$( \\hbox {CFA}\\theta) $$( CFA θ) which enables a continuous interpolation between different fairness definitions. More specifically, we make three main contributions to the existing literature. First, our approach allows the decision maker to continuously vary between specific concepts of individual and group fairness. As a consequence, the algorithm enables the decision maker to adopt intermediate “worldviews” on the degree of discrimination encoded in algorithmic processes, adding nuance to the extreme cases of “we’re all equal” and “what you see is what you get” proposed so far in the literature. Second, we use optimal transport theory, and specifically the concept of the barycenter, to maximize decision maker utility under the chosen fairness constraints. Third, the algorithm is able to handle cases of intersectionality, i. e. , of multi-dimensional discrimination of certain groups on grounds of several criteria. We discuss three main examples( credit applications; college admissions; insurance contracts) and map out the legal and policy implications of our approach. The explicit formalization of the trade-off between individual and group fairness allows this post-processing approach to be tailored to different situational contexts in which one or the other fairness criterion may take precedence. Finally, we evaluate our model experimentally.",10.1007/s10618-019-00658-8,http://dx.doi.org/10.1007/s10618-019-00658-8,Data Mining and Knowledge Discovery,"Zehlike, Meike;Hacker, Philipp;Wiedemann, Emil",2020,83,"@article{2-31327,
  title = {Matching code and law: achieving algorithmic fairness with optimal transport},
  author = {Zehlike, Meike and Hacker, Philipp and Wiedemann, Emil},
  year = {2020},
  doi = {10.1007/s10618-019-00658-8},
  journal = {Data Mining and Knowledge Discovery}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy, Everyday / Employment / Public Service","Institutional, Operational","Advising, Executing, Explaining",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-31404,springernature,Still doing it yourself? Investigating determinants for the adoption of intelligent process automation,"Intelligent process automation( IPA) augments symbolic process automation using artificial intelligence. Emulating human decision-making, IPA enables the execution of complex processes requiring decision-making capacities. IPA promises great economic potential as it enables more efficient use of the human workforce. However, the adoption rate in practice falls behind these potentials. Our study aims to investigate reasons and identify areas for action towards IPA adoption. To this end, we identified 13 determinants and created an extended UTAUT model. We tested the model with partial least squares structural equation modeling for significant influential relationships between the determinants based on a user study. We contribute to theory and practice finding a special role of trust and transparency for the adoption of IPA. Likewise, we show that organizations should cultivate a positive attitude towards IPA diffusion. Further, our results contribute with a focus on the potential adopters as IPA adoption is contingent upon their characteristics, such as experience and job level.",10.1007/s12525-024-00737-9,http://dx.doi.org/10.1007/s12525-024-00737-9,Electronic Markets,"Mayr, Alexander;Stahmann, Philip;Nebel, Maximilian;Janiesch, Christian",2024,8,"@article{2-31404,
  title={Still doing it yourself? Investigating determinants for the adoption of intelligent process automation},
  author={Mayr, Alexander and Stahmann, Philip and Nebel, Maximilian and Janiesch, Christian},
  year={2024},
  journal={Electronic Markets},
  doi={10.1007/s12525-024-00737-9}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Manufacturing / Industry / Automation",no such info,Executing,"Decision-maker, Guardian, Stakeholder",NA,NA,NA,NA,Autonomous System,Yes,No
2-31433,springernature,A case-based interpretable deep learning model for classification of mass lesions in digital mammography,"Interpretability in machine learning models is important in high-stakes decisions such as whether to order a biopsy based on a mammographic exam. Mammography poses important challenges that are not present in other computer vision tasks: datasets are small, confounding information is present and it can be difficult even for a radiologist to decide between watchful waiting and biopsy based on a mammogram alone. In this work we present a framework for interpretable machine learning-based mammography. In addition to predicting whether a lesion is malignant or benign, our work aims to follow the reasoning processes of radiologists in detecting clinically relevant semantic features of each image, such as the characteristics of the mass margins. The framework includes a novel interpretable neural network algorithm that uses case-based reasoning for mammography. Our algorithm can incorporate a combination of data with whole image labelling and data with pixel-wise annotations, leading to better accuracy and interpretability even with a small number of images. Our interpretable models are able to highlight the classification-relevant parts of the image, whereas other methods highlight healthy tissue and confounding information. Our models are decision aids—rather than decision makers—and aim for better overall human–machine collaboration. We do not observe a loss in mass margin classification accuracy over a black box neural network trained on the same data. The black-box nature of neural networks is a concern for high-stakes medical applications in which decisions must be based on medically relevant features. The authors develop an interpretable machine learning-based framework that aims to follow the reasoning processes of radiologists in providing predictions for cancer diagnosis in mammography.",10.1038/s42256-021-00423-x,http://dx.doi.org/10.1038/s42256-021-00423-x,Nature Machine Intelligence,"Barnett, Alina Jade;Schwartz, Fides Regina;Tao, Chaofan;Chen, Chaofan;Ren, Yinhao;Lo, Joseph Y.;Rudin, Cynthia",2021,168,"@article{2-31433,
  title={A case-based interpretable deep learning model for classification of mass lesions in digital mammography},
  author={Barnett, Alina Jade and Schwartz, Fides Regina and Tao, Chaofan and Chen, Chaofan and Ren, Yinhao and Lo, Joseph Y. and Rudin, Cynthia},
  year={2021},
  doi={10.1038/s42256-021-00423-x},
  journal={Nature Machine Intelligence}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Explaining, Forecasting","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-31447,springernature,Multi-modality 3D CNN Transformer for Assisting Clinical Decision in Intracerebral Hemorrhage,"Intracerebral hemorrhage( ICH) is a cerebrovascular disease with high mortality and morbidity rates. Early-stage ICH patients often lack clear surgical indications, which is quite challenging for neurosurgeons to make treatment decisions. Currently, early treatment decisions for ICH primarily rely on the clinical experience of neurosurgeons. Although there have been attempts to combine local CT imaging with clinical data for decision-making, these approaches fail to provide deep semantic analysis and do not fully leverage the synergistic effects between different modalities. To address this issue, this paper introduces a novel multi-modality predictive model that combines CT images and clinical data to provide reliable treatment decisions for ICH patients. Specifically, this model employs a combination of 3D CNN and Transformer to analyze patients’ brain CT scans, effectively capturing the 3D spatial information of intracranial hematomas and surrounding brain tissue. In addition, it utilizes a contrastive language-image pre-training( CLIP) module to extract demographic features and important clinical data and integrates with CT imaging data through a cross-attention mechanism. Furthermore, a novel CNN-based multilayer perceptron( MLP) layer is designed to enhance the understanding of the 3D spatial features. Extensive experiments conducted on real clinical datasets demonstrate that the proposed method significantly improves the accuracy of treatment decisions compared to existing state-of-the-art methods. Code is available at https://github. com/Henry-Xiong/3DCT-ICH.",10.1007/978-3-031-72086-4_49,http://dx.doi.org/10.1007/978-3-031-72086-4_49,Medical Image Computing and Computer-Assisted Intervention Conference,"Xiong, Zicheng;Zhao, Kai;Ji, Like;Shu, Xujun;Long, Dazhi;Chen, Shengbo;Yang, Fuxing",2024,9,"@inproceedings{2-31447,
  title = {Multi-modality 3D CNN Transformer for Assisting Clinical Decision in Intracerebral Hemorrhage},
  author = {Xiong, Zicheng and Zhao, Kai and Ji, Like and Shu, Xujun and Long, Dazhi and Chen, Shengbo and Yang, Fuxing},
  year = {2024},
  doi = {10.1007/978-3-031-72086-4_49},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention Conference}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-31496,springernature,"Personalized treatment supported by automated quantitative fluid analysis in active neovascular age-related macular degeneration( nAMD) —a phase III, prospective, multicentre, randomized study: design and methods","Introduction In neovascular age-related macular degeneration( nAMD) the exact amount of fluid and its location on optical coherence tomography( OCT) have been defined as crucial biomarkers for disease activity and therapeutic decisions. Yet in the absence of quantitative evaluation tools, real-world care outcomes are disappointing. Artificial intelligence( AI) offers a practical option for clinicians to enhance point-of-care management by analysing OCT volumes in a short time. In this protocol we present the prospective implementation of an AI-algorithm providing automated real-time fluid quantifications in a clinical real-world setting. Methods This is a prospective, multicentre, randomized( 1:1) and double masked phase III clinical trial. Two-hundred-ninety patients with active nAMD will be randomized between a study arm using AI-supported fluid quantifications and another arm using conventional qualitative assessments, i. e. state-of-the-art disease management. The primary outcome is defined as the mean number of injections over 1 year. Change in BCVA is defined as a secondary outcome. Discussion Automated measurement of fluid volumes in all retinal compartments such as intraretinal fluid( IRF) , and subretinal fluid( SRF) will serve as an objective tool for clinical investigators on which to base retreatment decisions. Compared to qualitative fluid assessment, retreatment decisions will be plausible and less prone to error or large variability. The underlying hypothesis is that fluid should be treated, while residual persistent or stable amounts of fluid may not benefit from further therapy. Reducing injection numbers without diminishing the visual benefit will increase overall patient safety and relieve the burden for healthcare providers. Trial-registration EudraCT-Number: 2019-003133-42",10.1038/s41433-022-02154-8,http://dx.doi.org/10.1038/s41433-022-02154-8,Eye,"Coulibaly, Leonard M.;Sacu, Stefan;Fuchs, Philipp;Bogunovic, Hrvoje;Faustmann, Georg;Unterrainer, Christian;Reiter, Gregor S.;Schmidt-Erfurth, Ursula",2023,17,"@article{2-31496,
  title = {Personalized treatment supported by automated quantitative fluid analysis in active neovascular age-related macular degeneration (nAMD) —a phase III, prospective, multicentre, randomized study: design and methods},
  author = {Coulibaly, Leonard M. and Sacu, Stefan and Fuchs, Philipp and Bogunovic, Hrvoje and Faustmann, Georg and Unterrainer, Christian and Reiter, Gregor S. and Schmidt-Erfurth, Ursula},
  year = {2023},
  doi = {10.1038/s41433-022-02154-8},
  journal = {Eye}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Decision-maker, Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-31584,springernature,"Systematic analysis of ChatGPT, Google search and Llama 2 for clinical decision support tasks","It is likely that individuals are turning to Large Language Models( LLMs) to seek health advice, much like searching for diagnoses on Google. We evaluate clinical accuracy of GPT-3·5 and GPT-4 for suggesting initial diagnosis, examination steps and treatment of 110 medical cases across diverse clinical disciplines. Moreover, two model configurations of the Llama 2 open source LLMs are assessed in a sub-study. For benchmarking the diagnostic task, we conduct a naïve Google search for comparison. Overall, GPT-4 performed best with superior performances over GPT-3·5 considering diagnosis and examination and superior performance over Google for diagnosis. Except for treatment, better performance on frequent vs rare diseases is evident for all three approaches. The sub-study indicates slightly lower performances for Llama models. In conclusion, the commercial LLMs show growing potential for medical question answering in two successive major releases. However, some weaknesses underscore the need for robust and regulated AI models in health care. Open source LLMs can be a viable option to address specific needs regarding data privacy and transparency of training. People will likely use ChatGPT to seek health advice. Here, the authors show promising performance of ChatGPT and open source models, but a lack of high accuracy considering medical question answering. Improvements are expected over time via domain-specific finetuning and integration of regulations.",10.1038/s41467-024-46411-8,http://dx.doi.org/10.1038/s41467-024-46411-8,Nature Communications,"Sandmann, Sarah;Riepenhausen, Sarah;Plagwitz, Lucas;Varghese, Julian",2024,181,"@article{2-31584,
  title={Systematic analysis of ChatGPT, Google search and Llama 2 for clinical decision support tasks},
  author={Sandmann, Sarah and Riepenhausen, Sarah and Plagwitz, Lucas and Varghese, Julian},
  year={2024},
  journal={Nature Communications},
  doi={10.1038/s41467-024-46411-8}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-31635,springernature,Noninvasive Determination of Gene Mutations in Clear Cell Renal Cell Carcinoma Using Multiple Instance Decisions Aggregated CNN,"Kidney clear cell renal cell carcinoma( ccRCC) is the major sub-type of RCC, constituting one the most common cancers worldwide accounting for a steadily increasing mortality rate with 350, 000 new cases recorded in 2012. Understanding the underlying genetic mutations in ccRCC provides crucial information enabling malignancy staging and patient survival estimation thus plays a vital role in accurate ccRCC diagnosis, prognosis, treatment planning, and response assessment. Although the underlying gene mutations can be identified by whole genome sequencing of the ccRCC following invasive nephrectomy or kidney biopsy procedures, recent studies have suggested that such mutations may be noninvasively identified by studying image features of the ccRCC from Computed Tomography( CT) data. Such image feature identification currently relies on laborious manual processes based on visual inspection of 2D image slices that are time-consuming and subjective. In this paper, we propose a convolutional neural network approach for automatic detection of underlying ccRCC gene mutations from 3D CT volumes. We aggregate the mutation-presence/absence decisions for all the ccRCC slices in a kidney into a robust singular decision that determines whether the interrogated kidney bears a specific mutation or not. When validated on clinical CT datasets of 267 patients from the TCIA database, our method detected gene mutations with 94% accuracy.",10.1007/978-3-030-00934-2_73,http://dx.doi.org/10.1007/978-3-030-00934-2_73,Medical Image Computing and Computer Assisted Intervention,"Hussain, Mohammad Arafat;Hamarneh, Ghassan;Garbi, Rafeef",2018,16,"@inproceedings{2-31635,
  title={Noninvasive Determination of Gene Mutations in Clear Cell Renal Cell Carcinoma Using Multiple Instance Decisions Aggregated CNN},
  author={Hussain, Mohammad Arafat and Hamarneh, Ghassan and Garbi, Rafeef},
  year={2018},
  booktitle={Medical Image Computing and Computer Assisted Intervention},
  doi={10.1007/978-3-030-00934-2_73}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-31792,springernature,User-Centered Design Approach for a Machine Learning Platform for Medical Purpose,"Machine learning is increasingly present in different sectors. Decision-making processes that occur in all types of companies and entities can be improved with the use of AI algorithms and machine learning. Furthermore, the application of machine learning algorithms enables the possibility of providing support to automate the undertaking of complex tasks. However, not all users who want to use machine learning are skilled enough from a technological and data science point of view to use many of the tools that are already available on the market. In particular, the health sector is taking advantage of AI algorithms to enhance the decision-making processes and to support complex common activities. Nonetheless, physicians have the domain knowledge but are not deeply trained in data science. This is the case of the cardiology department of the University Hospital of Salamanca, where the large amount of anonymized data makes it possible to improve certain tasks and decision-making. This work describes a machine learning platform to assist non-expert users in the definition and application of ML pipelines. The platform aims to fill data science gaps while automatizing ML pipelines and provides a baseline to integrate it with other developed applications for the cardiology department.",10.1007/978-3-030-92325-9_18,http://dx.doi.org/10.1007/978-3-030-92325-9_18,Human-Computer Interaction,"García-Holgado, Alicia;Vázquez-Ingelmo, Andrea;Alonso-Sánchez, Julia;García-Peñalvo, Francisco José;Therón, Roberto;Sampedro-Gómez, Jesús;Sánchez-Puente, Antonio;Vicente-Palacios, Víctor;Dorado-Díaz, P. Ignacio;Sánchez, Pedro L.",2021,15,"@inproceedings{2-31792,
  title     = {User-Centered Design Approach for a Machine Learning Platform for Medical Purpose},
  author    = {García-Holgado, Alicia and Vázquez-Ingelmo, Andrea and Alonso-Sánchez, Julia and García-Peñalvo, Francisco José and Therón, Roberto and Sampedro-Gómez, Jesús and Sánchez-Puente, Antonio and Vicente-Palacios, Víctor and Dorado-Díaz, P. Ignacio and Sánchez, Pedro L.},
  year      = {2021},
  doi       = {10.1007/978-3-030-92325-9_18},
  booktitle = {Human-Computer Interaction}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Executing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-31793,springernature,Fairness-aware machine learning engineering: how far are we?,"Machine learning is part of the daily life of people and companies worldwide. Unfortunately, bias in machine learning algorithms risks unfairly influencing the decision-making process and reiterating possible discrimination. While the interest of the software engineering community in software fairness is rapidly increasing, there is still a lack of understanding of various aspects connected to fair machine learning engineering, i. e. , the software engineering process involved in developing fairness-critical machine learning systems. Questions connected to the practitioners’ awareness and maturity about fairness, the skills required to deal with the matter, and the best development phase( s) where fairness should be faced more are just some examples of the knowledge gaps currently open. In this paper, we provide insights into how fairness is perceived and managed in practice, to shed light on the instruments and approaches that practitioners might employ to properly handle fairness. We conducted a survey with 117 professionals who shared their knowledge and experience highlighting the relevance of fairness in practice, and the skills and tools required to handle it. The key results of our study show that fairness is still considered a second-class quality aspect in the development of artificial intelligence systems. The building of specific methods and development environments, other than automated validation tools, might help developers to treat fairness throughout the software lifecycle and revert this trend.",10.1007/s10664-023-10402-y,http://dx.doi.org/10.1007/s10664-023-10402-y,Empirical Software Engineering,"Ferrara, Carmine;Sellitto, Giulia;Ferrucci, Filomena;Palomba, Fabio;De Lucia, Andrea",2023,0,"@article{2-31793,
  title={Fairness-aware machine learning engineering: how far are we?},
  author={Ferrara, Carmine and Sellitto, Giulia and Ferrucci, Filomena and Palomba, Fabio and De Lucia, Andrea},
  year={2023},
  journal={Empirical Software Engineering},
  doi={10.1007/s10664-023-10402-y}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Institutional,Advising,"Developer, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-31800,springernature,"Machine learning for human learners: opportunities, issues, tensions and threats","Machine learning systems are infiltrating our lives and are beginning to become important in our education systems. This article, developed from a synthesis and analysis of previous research, examines the implications of recent developments in machine learning for human learners and learning. In this article we first compare deep learning in computers and humans to examine their similarities and differences. Deep learning is identified as a sub-set of machine learning, which is itself a component of artificial intelligence. Deep learning often depends on backwards propagation in weighted neural networks, so is non-deterministic—the system adapts and changes through practical experience or training. This adaptive behaviour predicates the need for explainability and accountability in such systems. Accountability is the reverse of explainability. Explainability flows through the system from inputs to output( decision) whereas accountability flows backwards, from a decision to the person taking responsibility for it. Both explainability and accountability should be incorporated in machine learning system design from the outset to meet social, ethical and legislative requirements. For students to be able to understand the nature of the systems that may be supporting their own learning as well as to act as responsible citizens in contemplating the ethical issues that machine learning raises, they need to understand key aspects of machine learning systems and have opportunities to adapt and create such systems. Therefore, some changes are needed to school curricula. The article concludes with recommendations about machine learning for teachers, students, policymakers, developers and researchers.",10.1007/s11423-020-09858-2,http://dx.doi.org/10.1007/s11423-020-09858-2,Educational Technology Research and Development,"Webb, Mary E.;Fluck, Andrew;Magenheim, Johannes;Malyn-Smith, Joyce;Waters, Juliet;Deschênes, Michelle;Zagami, Jason",2021,54,"@article{2-31800,
  title={Machine learning for human learners: opportunities, issues, tensions and threats},
  author={Webb, Mary E. and Fluck, Andrew and Magenheim, Johannes and Malyn-Smith, Joyce and Waters, Juliet and Desch{\^e}nes, Michelle and Zagami, Jason},
  year={2021},
  doi={10.1007/s11423-020-09858-2},
  journal={Educational Technology Research and Development}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research, Law / Policy / Governance",no such info,"Explaining, Auditing","Decision-maker, Guardian, Developer",NA,NA,NA,NA,NA,Yes,No
2-31807,springernature,Making machine learning matter to clinicians: model actionability in medical decision-making,"Machine learning( ML) has the potential to transform patient care and outcomes. However, there are important differences between measuring the performance of ML models in silico and usefulness at the point of care. One lens to use to evaluate models during early development is actionability, which is currently undervalued. We propose a metric for actionability intended to be used before the evaluation of calibration and ultimately decision curve analysis and calculation of net benefit. Our metric should be viewed as part of an overarching effort to increase the number of pragmatic tools that identify a model’s possible clinical impacts.",10.1038/s41746-023-00753-7,http://dx.doi.org/10.1038/s41746-023-00753-7,Nature Partner Journals Digital Medicine,"Ehrmann, Daniel E.;Joshi, Shalmali;Goodfellow, Sebastian D.;Mazwi, Mjaye L.;Eytan, Danny",2023,94,"@article{2-31807,
  title = {Making machine learning matter to clinicians: model actionability in medical decision-making},
  author = {Ehrmann, Daniel E. and Joshi, Shalmali and Goodfellow, Sebastian D. and Mazwi, Mjaye L. and Eytan, Danny},
  year = {2023},
  doi = {10.1038/s41746-023-00753-7},
  journal = {Nature Partner Journals Digital Medicine}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-31838,springernature,PnP: Integrated Prediction and Planning for Interactive Lane Change in Dense Traffic,"Making human-like decisions for autonomous driving in interactive scenarios is crucial and difficult, requiring the self-driving vehicle to reason about the reactions of interactive vehicles to its behavior. To handle this challenge, we provide an integrated prediction and planning( PnP) decision-making approach. A reactive trajectory prediction model is developed to predict the future states of other actors in order to account for the interactive nature of the behaviors. Then, n -step temporal-difference search is used to make a tactical decision and plan the tracking trajectory for the self-driving vehicle by combining the value estimation network with the reactive prediction model. The proposed PnP method is evaluated using the CARLA simulator, and the results demonstrate that PnP obtains superior performance compared to popular model-free and model-based reinforcement learning baselines.",10.1007/978-981-99-8076-5_22,http://dx.doi.org/10.1007/978-981-99-8076-5_22,Neural Information Processing,"Liu, Xueyi;Zhang, Qichao;Gao, Yinfeng;Xia, Zhongpu",2024,1,"@inproceedings{2-31838,
  title     = {PnP: Integrated Prediction and Planning for Interactive Lane Change in Dense Traffic},
  author    = {Liu, Xueyi and Zhang, Qichao and Gao, Yinfeng and Xia, Zhongpu},
  booktitle = {Neural Information Processing},
  year      = {2024},
  doi       = {10.1007/978-981-99-8076-5_22}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Forecasting, Executing","Knowledge provider, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-31857,springernature,Clinical decision support for bipolar depression using large language models,"Management of depressive episodes in bipolar disorder remains challenging for clinicians despite the availability of treatment guidelines. In other contexts, large language models have yielded promising results for supporting clinical decisionmaking. We developed 50 sets of clinical vignettes reflecting bipolar depression and presented them to experts in bipolar disorder, who were asked to identify 5 optimal next-step pharmacotherapies and 5 poor or contraindicated choices. The same vignettes were then presented to a large language model( GPT4-turbo; gpt-4-1106-preview) , with or without augmentation by prompting with recent bipolar treatment guidelines, and asked to identify the optimal next-step pharmacotherapy. Overlap between model output and gold standard was estimated. The augmented model prioritized the expert-designated optimal choice for 508/1000 vignettes( 50. 8%, 95% CI 47. 7–53. 9%; Cohen’s kappa = 0. 31, 95% CI 0. 28–0. 35). For 120 vignettes( 12. 0%) , at least one model choice was among the poor or contraindicated treatments. Results were not meaningfully different when gender or race of the vignette was permuted to examine risk for bias. By comparison, an un-augmented model identified the optimal treatment for 234( 23. 0%, 95% CI 20. 8–26. 0%; McNemar’s p < 0. 001 versus augmented model) of the vignettes. A sample of community clinicians scoring the same vignettes identified the optimal choice for 23. 1%( 95% CI 15. 7–30. 5%) of vignettes, on average; McNemar’s p < 0. 001 versus augmented model. Large language models prompted with evidence-based guidelines represent a promising, scalable strategy for clinical decision support. In addition to prospective studies of efficacy, strategies to avoid clinician overreliance on such models, and address the possibility of bias, will be needed.",10.1038/s41386-024-01841-2,http://dx.doi.org/10.1038/s41386-024-01841-2,Neuropsychopharmacology,"Perlis, Roy H.;Goldberg, Joseph F.;Ostacher, Michael J.;Schneck, Christopher D.",2024,0,"@article{2-31857,
  title={Clinical decision support for bipolar depression using large language models},
  author={Perlis, Roy H. and Goldberg, Joseph F. and Ostacher, Michael J. and Schneck, Christopher D.},
  year={2024},
  doi={10.1038/s41386-024-01841-2},
  journal={Neuropsychopharmacology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-31863,springernature,Automated decision making in Barrett’s oesophagus: development and deployment of a natural language processing tool,"Manual decisions regarding the timing of surveillance endoscopy for premalignant Barrett’s oesophagus( BO) is error-prone. This leads to inefficient resource usage and safety risks. To automate decision-making, we fine-tuned Bidirectional Encoder Representations from Transformers( BERT) models to categorize BO length( EndoBERT) and worst histopathological grade( PathBERT) on 4, 831 endoscopy and 4, 581 pathology reports from Guy’s and St Thomas’ Hospital( GSTT). The accuracies for EndoBERT test sets from GSTT, King’s College Hospital( KCH) , and Sandwell and West Birmingham Hospitals( SWB) were 0. 95, 0. 86, and 0. 99, respectively. Average accuracies for PathBERT were 0. 93, 0. 91, and 0. 92, respectively. A retrospective analysis of 1640 GSTT reports revealed a 27% discrepancy between endoscopists’ decisions and model recommendations. This study underscores the development and deployment of NLP-based software in BO surveillance, demonstrating high performance at multiple sites. The analysis emphasizes the potential efficiency of automation in enhancing precision and guideline adherence in clinical decision-making.",10.1038/s41746-024-01302-6,http://dx.doi.org/10.1038/s41746-024-01302-6,Nature Partner Journals Digital Medicine,"Zecevic, Agathe;Jackson, Laurence;Zhang, Xinyue;Pavlidis, Polychronis;Dunn, Jason;Trudgill, Nigel;Ahmed, Shahd;Visaggi, Pierfrancesco;YoonusNizar, Zanil;Roberts, Angus;Zeki, Sebastian S.",2024,0,"@article{2-31863,
  title={Automated decision making in Barrett’s oesophagus: development and deployment of a natural language processing tool},
  author={Zecevic, Agathe and Jackson, Laurence and Zhang, Xinyue and Pavlidis, Polychronis and Dunn, Jason and Trudgill, Nigel and Ahmed, Shahd and Visaggi, Pierfrancesco and YoonusNizar, Zanil and Roberts, Angus and Zeki, Sebastian S.},
  year={2024},
  journal={Nature Partner Journals Digital Medicine},
  doi={10.1038/s41746-024-01302-6}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-31961,springernature,"Understanding, explaining, and utilizing medical artificial intelligence","Medical artificial intelligence is cost-effective and scalable and often outperforms human providers, yet people are reluctant to use it. We show that resistance to the utilization of medical artificial intelligence is driven by both the subjective difficulty of understanding algorithms( the perception that they are a ‘black box’) and by an illusory subjective understanding of human medical decision-making. In five pre-registered experiments( 1–3B: N = 2, 699) , we find that people exhibit an illusory understanding of human medical decision-making( study 1). This leads people to believe they better understand decisions made by human than algorithmic healthcare providers( studies 2A, B) , which makes them more reluctant to utilize algorithmic than human providers( studies 3A, B). Fortunately, brief interventions that increase subjective understanding of algorithmic decision processes increase willingness to utilize algorithmic healthcare providers( studies 3A, B). A sixth study on Google Ads for an algorithmic skin cancer detection app finds that the effectiveness of such interventions generalizes to field settings( study 4: N = 14, 013). Cadario et al. identify potential reasons underlying the resistance to use medical artificial intelligence and test interventions to overcome this resistance.",10.1038/s41562-021-01146-0,http://dx.doi.org/10.1038/s41562-021-01146-0,Nature Human Behaviour,"Cadario, Romain;Longoni, Chiara;Morewedge, Carey K.",2021,0,"@article{2-31961,
  title = {Understanding, Explaining, and Utilizing Medical Artificial Intelligence},
  author = {Cadario, Romain and Longoni, Chiara and Morewedge, Carey K.},
  year = {2021},
  doi = {10.1038/s41562-021-01146-0},
  journal = {Nature Human Behaviour}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising","Decision-subject, Decision-maker",Alter decision outcomes,no such info,"brief interventions that increase subjective understanding of algorithmic decision
processes","subjective difficulty of understanding algorithms, an illusory subjective understanding of human medical decision-making, averse to using algorithms for tasks",Textual,Yes,Yes
2-32041,springernature,Validation of MSIntuit as an AI-based pre-screening tool for MSI detection from colorectal cancer histology slides,"Mismatch Repair Deficiency( dMMR) /Microsatellite Instability( MSI) is a key biomarker in colorectal cancer( CRC). Universal screening of CRC patients for MSI status is now recommended, but contributes to increased workload for pathologists and delayed therapeutic decisions. Deep learning has the potential to ease dMMR/MSI testing and accelerate oncologist decision making in clinical practice, yet no comprehensive validation of a clinically approved tool has been conducted. We developed MSIntuit, a clinically approved artificial intelligence( AI) based pre-screening tool for MSI detection from haematoxylin-eosin( H&E) stained slides. After training on samples from The Cancer Genome Atlas( TCGA) , a blind validation is performed on an independent dataset of 600 consecutive CRC patients. Inter-scanner reliability is studied by digitising each slide using two different scanners. MSIntuit yields a sensitivity of 0. 96–0. 98, a specificity of 0. 47-0. 46, and an excellent inter-scanner agreement( Cohen’s κ: 0. 82). By reaching high sensitivity comparable to gold standard methods while ruling out almost half of the non-MSI population, we show that MSIntuit can effectively serve as a pre-screening tool to alleviate MSI testing burden in clinical practice. Microsatellite instability is a known risk factor for colorectal cancer development and treatment response. Here, the authors utilise deep learning to develop MSIntuit, a pre-screening tool to detect MSI from H&E stained slides.",10.1038/s41467-023-42453-6,http://dx.doi.org/10.1038/s41467-023-42453-6,Nature Communications,"Saillard, Charlie;Dubois, Rémy;Tchita, Oussama;Loiseau, Nicolas;Garcia, Thierry;Adriansen, Aurélie;Carpentier, Séverine;Reyre, Joelle;Enea, Diana;Loga, Katharina;Kamoun, Aurélie;Rossat, Stéphane;Wiscart, Corentin;Sefta, Meriem;Auffret, Michaël;Guillou, Lionel;Fouillet, Arnaud;Kather, Jakob Nikolas;Svrcek, Magali",2023,87,"@article{2-32041,
  title = {Validation of {MSIntuit} as an {AI}-based pre-screening tool for {MSI} detection from colorectal cancer histology slides},
  author = {Saillard, Charlie and Dubois, R{\'e}my and Tchita, Oussama and Loiseau, Nicolas and Garcia, Thierry and Adriansen, Aur{\'e}lie and Carpentier, S{\'e}verine and Reyre, Joelle and Enea, Diana and Loga, Katharina and Kamoun, Aur{\'e}lie and Rossat, St{\'e}phane and Wiscart, Corentin and Sefta, Meriem and Auffret, Micha{\""e}l and Guillou, Lionel and Fouillet, Arnaud and Kather, Jakob Nikolas and Svrcek, Magali},
  year = {2023},
  doi = {10.1038/s41467-023-42453-6},
  journal = {Nature Communications}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Auditing, Analyzing, Forecasting","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-32307,springernature,Development and external validation of a pretrained deep learning model for the prediction of non-accidental trauma,"Non-accidental trauma( NAT) is deadly and difficult to predict. Transformer models pretrained on large datasets have recently produced state of the art performance on diverse prediction tasks, but the optimal pretraining strategies for diagnostic predictions are not known. Here we report the development and external validation of Pretrained and Adapted BERT for Longitudinal Outcomes( PABLO) , a transformer-based deep learning model with multitask clinical pretraining, to identify patients who will receive a diagnosis of NAT in the next year. We develop a clinical interface to visualize patient trajectories, model predictions, and individual risk factors. In two comprehensive statewide databases, approximately 1% of patients experience NAT within one year of prediction. PABLO predicts NAT events with area under the receiver operating characteristic curve( AUROC) of 0. 844( 95% CI 0. 838–0. 851) in the California test set, and 0. 849( 95% CI 0. 846–0. 851) on external validation in Florida, outperforming comparator models. Multitask pretraining significantly improves model performance. Attribution analysis shows substance use, psychiatric, and injury diagnoses, in the context of age and racial demographics, as influential predictors of NAT. As a clinical decision support system, PABLO can identify high-risk patients and patient-specific risk factors, which can be used to target secondary screening and preventive interventions at the point-of-care.",10.1038/s41746-023-00875-y,http://dx.doi.org/10.1038/s41746-023-00875-y,Nature Partner Journals Digital Medicine,"Huang, David;Cogill, Steven;Hsia, Renee Y.;Yang, Samuel;Kim, David",2023,13,"@article{2-32307,
  title={Development and external validation of a pretrained deep learning model for the prediction of non-accidental trauma},
  author={Huang, David and Cogill, Steven and Hsia, Renee Y. and Yang, Samuel and Kim, David},
  year={2023},
  doi={10.1038/s41746-023-00875-y},
  journal={Nature Partner Journals Digital Medicine}
}","Algorithmic contributions, System/Artifact contributions",Healthcare / Medicine / Surgery,Operational,"Explaining, Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-32414,springernature,A 2-year investigation of the impact of the computed tomography–derived fractional flow reserve calculated using a deep learning algorithm on routine decision-making for coronary artery disease management,"Objective This study aims to investigate the safety and feasibility of using a deep learning algorithm to calculate computed tomography angiography–based fractional flow reserve( DL-FFRCT) as an alternative to invasive coronary angiography( ICA) in the selection of patients for coronary intervention. Materials and methods Patients( N = 296) with symptomatic coronary artery disease identified by coronary computed tomography angiography( CTA) with stenosis over 50% were retrospectively enrolled from a single centre in this study. ICA-guided interventions were performed in patients at admission, and DL-FFRCT was conducted retrospectively. The influences on decision-making by using DL-FFRCT and the clinical outcome were compared to those of ICA-guided care for symptomatic CAD at the 2-year follow-up evaluation. Result Two hundred forty-three patients were evaluated. Up to 72% of diagnostic ICA studies could have been avoided by using a DL-FFRCT value > 0. 8 as a cut-off for intervention. A similar major adverse cardiovascular event( MACE) rate was observed in patients who underwent revascularisation with a DL-FFRCT value ≤ 0. 8( 2. 9%) compared to that of ICA-guided interventions( 3. 3%) ( stented lesions with ICA stenosis > 75%) ( p = 0. 838). Conclusion DL-FFRCT can reduce the need for diagnostic coronary angiography when identifying patients suitable for coronary intervention. A low MACE rate was found in a 2-year follow-up investigation. Key Points • Seventy-two percent of diagnostic ICA studies could have been avoided by using a DL-FFRCT value > 0. 8 as a cut-off for intervention. • Coronary artery stenting based on the diagnosis by using a 320-detector row CT scanner and a positive DL-FFRCT value could potentially be associated with a lower occurrence rate of major adverse cardiovascular events( 2. 9%) within the first 2 years. • A low event rate was found when intervention was performed in tandem lesions with haemodynamic significance based on DL-FFRCT < 0. 8 as a cut-off value.",10.1007/s00330-021-07771-7,http://dx.doi.org/10.1007/s00330-021-07771-7,European Radiology,"Liu, Xin;Mo, Xukai;Zhang, Heye;Yang, Guang;Shi, Changzheng;Hau, William Kongtou",2021,25,"@article{2-32414,
  title = {A 2-year investigation of the impact of the computed tomography--derived fractional flow reserve calculated using a deep learning algorithm on routine decision-making for coronary artery disease management},
  author = {Liu, Xin and Mo, Xukai and Zhang, Heye and Yang, Guang and Shi, Changzheng and Hau, William Kongtou},
  year = {2021},
  doi = {10.1007/s00330-021-07771-7},
  journal = {European Radiology}
}","Algorithmic contributions, Empirical contributions",Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-32475,springernature,"Can incorrect artificial intelligence( AI) results impact radiologists, and if so, what can we do about it? A multi-reader pilot study of lung cancer detection with chest radiography","Objective To examine whether incorrect AI results impact radiologist performance, and if so, whether human factors can be optimized to reduce error. Methods Multi-reader design, 6 radiologists interpreted 90 identical chest radiographs( follow-up CT needed: yes/no) on four occasions( 09/20–01/22). No AI result was provided for session 1. Sham AI results were provided for sessions 2–4, and AI for 12 cases were manipulated to be incorrect( 8 false positives( FP) , 4 false negatives( FN) ) ( 0. 87 ROC-AUC). In the Delete AI( No Box) condition, radiologists were told AI results would not be saved for the evaluation. In Keep AI( No Box) and Keep AI( Box) , radiologists were told results would be saved. In Keep AI( Box) , the ostensible AI program visually outlined the region of suspicion. AI results were constant between conditions. Results Relative to the No AI condition( FN = 2. 7%, FP = 51. 4%) , FN and FPs were higher in the Keep AI( No Box) ( FN = 33. 0%, FP = 86. 0%) , Delete AI( No Box) ( FN = 26. 7%, FP = 80. 5%) , and Keep AI( Box) ( FN = to 20. 7%, FP = 80. 5%) conditions( all p s < 0. 05). FNs were higher in the Keep AI( No Box) condition( 33. 0%) than in the Keep AI( Box) condition( 20. 7%) ( p = 0. 04). FPs were higher in the Keep AI( No Box) ( 86. 0%) condition than in the Delete AI( No Box) condition( 80. 5%) ( p = 0. 03). Conclusion Incorrect AI causes radiologists to make incorrect follow-up decisions when they were correct without AI. This effect is mitigated when radiologists believe AI will be deleted from the patient’s file or a box is provided around the region of interest. Clinical relevance statement When AI is wrong, radiologists make more errors than they would have without AI. Based on human factors psychology, our manuscript provides evidence for two AI implementation strategies that reduce the deleterious effects of incorrect AI. Key Points • When AI provided incorrect results, false negative and false positive rates among the radiologists increased. • False positives decreased when AI results were deleted, versus kept, in the patient’s record. • False negatives and false positives decreased when AI visually outlined the region of suspicion.",10.1007/s00330-023-09747-1,http://dx.doi.org/10.1007/s00330-023-09747-1,European Radiology,"Bernstein, Michael H.;Atalay, Michael K.;Dibble, Elizabeth H.;Maxwell, Aaron W. P.;Karam, Adib R.;Agarwal, Saurabh;Ward, Robert C.;Healey, Terrance T.;Baird, Grayson L.",2023,110,"@article{2-32475,
  title = {Can incorrect artificial intelligence (AI) results impact radiologists, and if so, what can we do about it? A multi-reader pilot study of lung cancer detection with chest radiography},
  author = {Bernstein, Michael H. and Atalay, Michael K. and Dibble, Elizabeth H. and Maxwell, Aaron W. P. and Karam, Adib R. and Agarwal, Saurabh and Ward, Robert C. and Healey, Terrance T. and Baird, Grayson L.},
  year = {2023},
  doi = {10.1007/s00330-023-09747-1},
  journal = {European Radiology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Advising,Decision-maker,"Alter decision outcomes, Change cognitive demands",no such info,preliminary diagnoses,NA,"Textual, Visual",Yes,Yes
2-32507,springernature,An impact assessment of machine learning risk forecasts on parole board decisions and recidivism,"Objectives : The Pennsylvania Board of Probation and Parole has begun using machine learning forecasts to help inform parole release decisions. In this paper, we evaluate the impact of the forecasts on those decisions and subsequent recidivism. Methods : A close approximation to a natural, randomized experiment is used to evaluate the impact of the forecasts on parole release decisions. A generalized regression discontinuity design is used to evaluate the impact of the forecasts on recidivism. Results : The forecasts apparently had no effect on the overall parole release rate, but did appear to alter the mix of inmates released. Important distinctions were made between offenders forecasted to be re-arrested for nonviolent crime and offenders forecasted to be re-arrested for violent crime. The balance of evidence indicates that the forecasts led to reductions in re-arrests for both nonviolent and violent crimes. Conclusions : Risk assessments based on machine learning forecasts can improve parole release decisions, especially when distinctions are made between re-arrests for violent and nonviolent crime.",10.1007/s11292-017-9286-2,http://dx.doi.org/10.1007/s11292-017-9286-2,Journal of Experimental Criminology,"Berk, Richard",2017,273,"@article{2-32507,
  title={An impact assessment of machine learning risk forecasts on parole board decisions and recidivism},
  author={Berk, Richard},
  year={2017},
  doi={10.1007/s11292-017-9286-2},
  journal={Journal of Experimental Criminology}
}",Empirical contributions,Law / Policy / Governance,Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-32545,springernature,"AI support for accurate and fast radiological diagnosis of COVID-19: an international multicenter, multivendor CT study","Objectives Differentiation between COVID-19 and community-acquired pneumonia( CAP) in computed tomography( CT) is a task that can be performed by human radiologists and artificial intelligence( AI). The present study aims to( 1) develop an AI algorithm for differentiating COVID-19 from CAP and( 2) evaluate its performance. ( 3) Evaluate the benefit of using the AI result as assistance for radiological diagnosis and the impact on relevant parameters such as accuracy of the diagnosis, diagnostic time, and confidence. Methods We included n = 1591 multicenter, multivendor chest CT scans and divided them into AI training and validation datasets to develop an AI algorithm( n = 991 CT scans; n = 462 COVID-19, and n = 529 CAP) from three centers in China. An independent Chinese and German test dataset of n = 600 CT scans from six centers( COVID-19 / CAP; n = 300 each) was used to test the performance of eight blinded radiologists and the AI algorithm. A subtest dataset( 180 CT scans; n = 90 each) was used to evaluate the radiologists’ performance without and with AI assistance to quantify changes in diagnostic accuracy, reporting time, and diagnostic confidence. Results The diagnostic accuracy of the AI algorithm in the Chinese-German test dataset was 76. 5%. Without AI assistance, the eight radiologists’ diagnostic accuracy was 79. 1% and increased with AI assistance to 81. 5%, going along with significantly shorter decision times and higher confidence scores. Conclusion This large multicenter study demonstrates that AI assistance in CT-based differentiation of COVID-19 and CAP increases radiological performance with higher accuracy and specificity, faster diagnostic time, and improved diagnostic confidence. Key Points • AI can help radiologists to get higher diagnostic accuracy, make faster decisions, and improve diagnostic confidence. • The China-German multicenter study demonstrates the advantages of a human-machine interaction using AI in clinical radiology for diagnostic differentiation between COVID-19 and CAP in CT scans.",10.1007/s00330-022-09335-9,http://dx.doi.org/10.1007/s00330-022-09335-9,European Radiology,"Meng, Fanyang;Kottlors, Jonathan;Shahzad, Rahil;Liu, Haifeng;Fervers, Philipp;Jin, Yinhua;Rinneburger, Miriam;Le, Dou;Weisthoff, Mathilda;Liu, Wenyun;Ni, Mengzhe;Sun, Ye;An, Liying;Huai, Xiaochen;Móré, Dorottya;Giannakis, Athanasios;Kaltenborn, Isabel;Bucher, Andreas;Maintz, David;Zhang, Lei;Thiele, Frank;Li, Mingyang;Perkuhn, Michael;Zhang, Huimao;Persigehl, Thorsten",2023,19,"@article{2-32545,
  title = {AI support for accurate and fast radiological diagnosis of COVID-19: an international multicenter, multivendor CT study},
  author = {Meng, Fanyang and Kottlors, Jonathan and Shahzad, Rahil and Liu, Haifeng and Fervers, Philipp and Jin, Yinhua and Rinneburger, Miriam and Le, Dou and Weisthoff, Mathilda and Liu, Wenyun and Ni, Mengzhe and Sun, Ye and An, Liying and Huai, Xiaochen and Móré, Dorottya and Giannakis, Athanasios and Kaltenborn, Isabel and Bucher, Andreas and Maintz, David and Zhang, Lei and Thiele, Frank and Li, Mingyang and Perkuhn, Michael and Zhang, Huimao and Persigehl, Thorsten},
  year = {2023},
  doi = {10.1007/s00330-022-09335-9},
  journal = {European Radiology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-32550,springernature,Predicting peritumoral edema development after gamma knife radiosurgery of meningiomas using machine learning methods: a multicenter study,"Objectives Edema is a complication of gamma knife radiosurgery( GKS) in meningioma patients that leads to a variety of consequences. The aim of this study is to construct radiomics-based machine learning models to predict post-GKS edema development. Methods In total, 445 meningioma patients who underwent GKS in our institution were enrolled and partitioned into training and internal validation datasets( 8:2). A total of 150 cases from multicenter data were included as the external validation dataset. In each case, 1132 radiomics features were extracted from each pre-treatment MRI sequence( contrast-enhanced T1WI, T2WI, and ADC maps). Nine clinical features and eight semantic features were also generated. Nineteen random survival forest( RSF) and nineteen neural network( DeepSurv) models with different combinations of radiomics, clinical, and semantic features were developed with the training dataset, and evaluated with internal and external validation. A nomogram was derived from the model achieving the highest C-index in external validation. Results All the models were successfully validated on both validation datasets. The RSF model incorporating clinical, semantic, and ADC radiomics features achieved the best performance with a C-index of 0. 861( 95% CI: 0. 748–0. 975) in internal validation, and 0. 780( 95% CI: 0. 673–0. 887) in external validation. It stratifies high-risk and low-risk cases effectively. The nomogram based on the predicted risks provided personalized prediction with a C-index of 0. 962( 95%CI: 0. 951–0. 973) and satisfactory calibration. Conclusion This RSF model with a nomogram could represent a non-invasive and cost-effective tool to predict post-GKS edema risk, thus facilitating personalized decision-making in meningioma treatment. Clinical relevance statement The RSF model with a nomogram built in this study represents a handy, non-invasive, and cost-effective tool for meningioma patients to assist in better counselling on the risks, appropriate individual treatment decisions, and customized follow-up plans. Key Points • Machine learning models were built to predict post-GKS edema in meningioma. The random survival forest model with clinical, semantic, and ADC radiomics features achieved excellent performance. • The nomogram based on the predicted risks provides personalized prediction with a C-index of 0. 962( 95%CI: 0. 951–0. 973) and satisfactory calibration and shows the potential to assist in better counselling, appropriate treatment decisions, and customized follow-up plans. • Given the excellent performance and convenient acquisition of the conventional sequence, we envision that this non-invasive and cost-effective tool will facilitate personalized medicine in meningioma treatment.",10.1007/s00330-023-09955-9,http://dx.doi.org/10.1007/s00330-023-09955-9,European Radiology,"Li, Xuanxuan;Lu, Yiping;Liu, Li;Wang, Dongdong;Zhao, Yajing;Mei, Nan;Geng, Daoying;Ma, Xin;Zheng, Weiwei;Duan, Shaofeng;Wu, Pu-Yeh;Wen, Hongkai;Tan, Yongli;Sun, Xiaogang;Sun, Shibin;Li, Zhiwei;Yu, Tonggang;Yin, Bo",2023,8,"@article{2-32550,
  title = {Predicting peritumoral edema development after gamma knife radiosurgery of meningiomas using machine learning methods: a multicenter study},
  author = {Li, Xuanxuan and Lu, Yiping and Liu, Li and Wang, Dongdong and Zhao, Yajing and Mei, Nan and Geng, Daoying and Ma, Xin and Zheng, Weiwei and Duan, Shaofeng and Wu, Pu-Yeh and Wen, Hongkai and Tan, Yongli and Sun, Xiaogang and Sun, Shibin and Li, Zhiwei and Yu, Tonggang and Yin, Bo},
  year = {2023},
  doi = {10.1007/s00330-023-09955-9},
  journal = {European Radiology}
}","Algorithmic contributions, Empirical contributions",Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-32617,springernature,Reader bias in breast cancer screening related to cancer prevalence and artificial intelligence decision support—a reader study,"Objectives The aim of our study was to examine how breast radiologists would be affected by high cancer prevalence and the use of artificial intelligence( AI) for decision support. Materials and method This reader study was based on selection of screening mammograms, including the original radiologist assessment, acquired in 2010 to 2013 at the Karolinska University Hospital, with a ratio of 1:1 cancer versus healthy based on a 2-year follow-up. A commercial AI system generated an exam-level positive or negative read, and image markers. Double-reading and consensus discussions were first performed without AI and later with AI, with a 6-week wash-out period in between. The chi-squared test was used to test for differences in contingency tables. Results Mammograms of 758 women were included, half with cancer and half healthy. 52% were 40–55 years; 48% were 56–75 years. In the original non-enriched screening setting, the sensitivity was 61%( 232/379) at specificity 98%( 323/379). In the reader study, the sensitivity without and with AI was 81%( 307/379) and 75%( 284/379) respectively( p < 0. 001). The specificity without and with AI was 67%( 255/379) and 86%( 326/379) respectively( p < 0. 001). The tendency to change assessment from positive to negative based on erroneous AI information differed between readers and was affected by type and number of image signs of malignancy. Conclusion Breast radiologists reading a list with high cancer prevalence performed at considerably higher sensitivity and lower specificity than the original screen-readers. Adding AI information, calibrated to a screening setting, decreased sensitivity and increased specificity. Clinical relevance statement Radiologist screening mammography assessments will be biased towards higher sensitivity and lower specificity by high-risk triaging and nudged towards the sensitivity and specificity setting of AI reads. After AI implementation in clinical practice, there is reason to carefully follow screening metrics to ensure the impact is desired. Key Points • Breast radiologists’ sensitivity and specificity will be affected by changes brought by artificial intelligence. • Reading in a high cancer prevalence setting markedly increased sensitivity and decreased specificity. • Reviewing the binary reads by AI, negative or positive, biased screening radiologists towards the sensitivity and specificity of the AI system. Graphical abstract",10.1007/s00330-023-10514-5,http://dx.doi.org/10.1007/s00330-023-10514-5,European Radiology,"Al-Bazzaz, Hanen;Janicijevic, Marina;Strand, Fredrik",2024,22,"@article{2-32617,
  title={Reader bias in breast cancer screening related to cancer prevalence and artificial intelligence decision support---a reader study},
  author={Al-Bazzaz, Hanen and Janicijevic, Marina and Strand, Fredrik},
  year={2024},
  journal={European Radiology},
  doi={10.1007/s00330-023-10514-5}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-subject, Decision-maker","Alter decision outcomes, Change cognitive demands",no such info,information prompts,NA,"Interactive interface, Visual",Yes,Yes
2-32635,springernature,Development and validation of a radiomics-based prediction pipeline for the response to stereotactic radiosurgery therapy in brain metastases,"Objectives The first treatment strategy for brain metastases( BM) plays a pivotal role in the prognosis of patients. Among all strategies, stereotactic radiosurgery( SRS) is considered a promising therapy method. Therefore, we developed and validated a radiomics-based prediction pipeline to prospectively identify BM patients who are insensitive to SRS therapy, especially those who are at potential risk of progressive disease. Methods A total of 337 BM patients( 277, 30, and 30 in the training set, internal validation set, and external validation set, respectively) were enrolled in the study. 19, 377 radiomics features( 3 masks × 3 MRI sequences × 2153 features) extracted from 9 ROIs were filtered through LASSO and Max-Relevance and Min-Redundancy( mRMR) algorithms. The selected radiomics features were combined with 4 clinical features to construct a two-stage cascaded model for the prediction of BM patients response to SRS therapy using SVM and an ensemble learning classifier. The performance of the model was evaluated by its accuracy, specificity, sensitivity, and AUC curve. Results Radiomics features were integrated with the clinical features of patients in our optimal model, which showed excellent discriminative performance in the training set( AUC: 0. 95, 95% CI: 0. 88–0. 98). The model was also verified in the internal validation set and external validation set( AUC 0. 93, 95% CI: 0. 76–0. 95 and AUC 0. 90, 95% CI: 0. 73–0. 93, respectively). Conclusions The proposed prediction pipeline could non-invasively predict the response to SRS therapy in patients with brain metastases thus assisting doctors to precisely designate individualized first treatment decisions. Clinical relevance statement The proposed prediction pipeline combines the radiomics features of multi-modal MRI with clinical features to construct machine learning models that noninvasively predict the response of patients with brain metastases to stereotactic radiosurgery therapy, assisting neuro-oncologists to develop personalized first treatment plans. Key Points • The proposed prediction pipeline can non-invasively predict the response to SRS therapy. • The combination of multi-modality and multi-mask contributes significantly to the prediction. • The edema index also shows a certain predictive value.",10.1007/s00330-023-09930-4,http://dx.doi.org/10.1007/s00330-023-09930-4,European Radiology,"Du, Peng;Liu, Xiao;Xiang, Rui;Lv, Kun;Chen, Hongyi;Liu, Weifan;Cao, Aihong;Chen, Lang;Wang, Xuefeng;Yu, Tonggang;Ding, Jian;Li, Wuchao;Li, Jie;Li, Yuxin;Yu, Zekuan;Zhu, Li;Liu, Jie;Geng, Daoying",2023,0,"@article{2-32635,
  title={Development and validation of a radiomics-based prediction pipeline for the response to stereotactic radiosurgery therapy in brain metastases},
  author={Du, Peng and Liu, Xiao and Xiang, Rui and Lv, Kun and Chen, Hongyi and Liu, Weifan and Cao, Aihong and Chen, Lang and Wang, Xuefeng and Yu, Tonggang and Ding, Jian and Li, Wuchao and Li, Jie and Li, Yuxin and Yu, Zekuan and Zhu, Li and Liu, Jie and Geng, Daoying},
  year={2023},
  doi={10.1007/s00330-023-09930-4},
  journal={European Radiology}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-32693,springernature,Patient perspectives on the use of artificial intelligence in prostate cancer diagnosis on MRI,"Objectives This study investigated patients’ acceptance of artificial intelligence( AI) for diagnosing prostate cancer( PCa) on MRI scans and the factors influencing their trust in AI diagnoses. Materials and methods A prospective, multicenter study was conducted between January and November 2023. Patients undergoing prostate MRI were surveyed about their opinions on hypothetical AI assessment of their MRI scans. The questionnaire included nine items: four on hypothetical scenarios of combinations between AI and the radiologist, two on trust in the diagnosis, and three on accountability for misdiagnosis. Relationships between the items and independent variables were assessed using multivariate analysis. Results A total of 212 PCa suspicious patients undergoing prostate MRI were included. The majority preferred AI involvement in their PCa diagnosis alongside a radiologist, with 91% agreeing with AI as the primary reader and 79% as the secondary reader. If AI has a high certainty diagnosis, 15% of the respondents would accept it as the sole decision-maker. Autonomous AI outperforming radiologists would be accepted by 52%. Higher educated persons tended to accept AI when it would outperform radiologists( p < 0. 05). The respondents indicated that the hospital( 76%) , radiologist( 70%) , and program developer( 55%) should be held accountable for misdiagnosis. Conclusions Patients favor AI involvement alongside radiologists in PCa diagnosis. Trust in AI diagnosis depends on the patient’s education level and the AI performance, with autonomous AI acceptance by a small majority on the condition that AI outperforms a radiologist. Respondents held the hospital, radiologist, and program developers accountable for misdiagnosis in descending order of accountability. Clinical relevance statement Patients show a high level of acceptance for AI-assisted prostate cancer diagnosis on MRI, either alongside radiologists or fully autonomous, particularly if it demonstrates superior performance to radiologists alone. Key Points Prostate cancer suspicious patients may accept autonomous AI based on performance. Patients prefer AI involvement alongside a radiologist in diagnosing prostate cancer. Patients indicate accountability for AI should be shared among multiple stakeholders.",10.1007/s00330-024-11012-y,http://dx.doi.org/10.1007/s00330-024-11012-y,European Radiology,"Fransen, Stefan J.;Kwee, T. C.;Rouw, D.;Roest, C.;Lohuizen, Q. Y.;Simonis, F. F. J.;Leeuwen, P. J.;Heijmink, S.;Ongena, Y. P.;Haan, M.;Yakar, D.",2024,15,"@article{2-32693,
  title = {Patient perspectives on the use of artificial intelligence in prostate cancer diagnosis on MRI},
  author = {Fransen, Stefan J. and Kwee, T. C. and Rouw, D. and Roest, C. and Lohuizen, Q. Y. and Simonis, F. F. J. and Leeuwen, P. J. and Heijmink, S. and Ongena, Y. P. and Haan, M. and Yakar, D.},
  year = {2024},
  doi = {10.1007/s00330-024-11012-y},
  journal = {European Radiology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Analyzing","Decision-subject, Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-32694,springernature,Impact of machine learning–based coronary computed tomography angiography fractional flow reserve on treatment decisions and clinical outcomes in patients with suspected coronary artery disease,"Objectives This study investigated the impact of machine learning( ML) –based fractional flow reserve derived from computed tomography( FFR CT) compared to invasive coronary angiography( ICA) for therapeutic decision-making and patient outcome in patients with suspected coronary artery disease( CAD). Methods One thousand one hundred twenty-one consecutive patients with stable chest pain who underwent coronary computed tomography angiography( CCTA) followed ICA within 90 days between January 2007 and December 2016 were included in this retrospective study. Medical records were reviewed for the endpoint of major adverse cardiac events( MACEs). FFR CT values were calculated using an artificial intelligence( AI) ML platform. Disagreements between hemodynamic significant stenosis via FFR CT and severe stenosis on qualitative CCTA and ICA were also evaluated. Results After FFR CT results were revealed, a change in the proposed treatment regimen chosen based on ICA results was seen in 167 patients( 14. 9%). Over a median follow-up time of 26 months( 4–48 months) , FFR CT ≤ 0. 80 was associated with MACE( HR, 6. 84( 95% CI, 3. 57 to 13. 11) ; p < 0. 001) , with superior prognostic value compared to severe stenosis on ICA( HR, 1. 84( 95% CI, 1. 24 to 2. 73) , p = 0. 002) and CCTA( HR, 1. 47( 95% CI, 1. 01 to 2. 14, p = 0. 045). Reserving ICA and revascularization for vessels with positive FFR CT could have reduced the rate of ICA by 54. 5% and lead to 4. 4% fewer percutaneous interventions. Conclusions This study indicated ML-based FFR CT had superior prognostic value when compared to severe anatomic stenosis on CCTA and adding FFR CT may direct therapeutic decision-making with the potential to improve efficiency of ICA. Key Points • ML-based FFR CT shows superior outcome prediction value when compared to severe anatomic stenosis on CCTA. • FFR CT noninvasively informs therapeutic decision-making with potential to change diagnostic workflows and enhance efficiencies in patients with suspected CAD. • Reserving ICA and revascularization for vessels with positive FFR CT may reduce the normalcy rate of ICA and improve its efficiency.",10.1007/s00330-020-06964-w,http://dx.doi.org/10.1007/s00330-020-06964-w,European Radiology,"Qiao, Hong Yan;Tang, Chun Xiang;Schoepf, U. Joseph;Tesche, Christian;Bayer, Richard R., 2nd;Giovagnoli, Dante A;Todd Hudson, H., Jr;Zhou, Chang Sheng;Yan, Jing;Lu, Meng Jie;Zhou, Fan;Lu, Guang Ming;Jiang, Jian Wei;Zhang, Long Jiang",2020,0,"@article{2-32694,
  title={Impact of machine learning--based coronary computed tomography angiography fractional flow reserve on treatment decisions and clinical outcomes in patients with suspected coronary artery disease},
  author={Qiao, Hong Yan and Tang, Chun Xiang and Schoepf, U. Joseph and Tesche, Christian and Bayer, Richard R., 2nd and Giovagnoli, Dante A and Hudson, Todd H., Jr and Zhou, Chang Sheng and Yan, Jing and Lu, Meng Jie and Zhou, Fan and Lu, Guang Ming and Jiang, Jian Wei and Zhang, Long Jiang},
  year={2020},
  doi={10.1007/s00330-020-06964-w},
  journal={European Radiology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-32729,springernature,Trust and stakeholder perspectives on the implementation of AI tools in clinical radiology,"Objectives To define requirements that condition trust in artificial intelligence( AI) as clinical decision support in radiology from the perspective of various stakeholders and to explore ways to fulfil these requirements. Methods Semi-structured interviews were conducted with twenty-five respondents—nineteen directly involved in the development, implementation, or use of AI applications in radiology and six working with AI in other areas of healthcare. We designed the questions to explore three themes: development and use of AI, professional decision-making, and management and organizational procedures connected to AI. The transcribed interviews were analysed in an iterative coding process from open coding to theoretically informed thematic coding. Results We identified four aspects of trust that relate to reliability, transparency, quality verification, and inter-organizational compatibility. These aspects fall under the categories of substantial and procedural requirements. Conclusions Development of appropriate levels of trust in AI in healthcare is complex and encompasses multiple dimensions of requirements. Various stakeholders will have to be involved in developing AI solutions for healthcare and radiology to fulfil these requirements. Clinical relevance statement For AI to achieve advances in radiology, it must be given the opportunity to support, rather than replace, human expertise. Support requires trust. Identification of aspects and conditions for trust allows developing AI implementation strategies that facilitate advancing the field. Key Points • Dimensions of procedural and substantial demands that need to be fulfilled to foster appropriate levels of trust in AI in healthcare are conditioned on aspects related to reliability, transparency, quality verification, and inter-organizational compatibility. •Creating the conditions for trust to emerge requires the involvement of various stakeholders, who will have to compensate the problem’s inherent complexity by finding and promoting well-defined solutions.",10.1007/s00330-023-09967-5,http://dx.doi.org/10.1007/s00330-023-09967-5,European Radiology,"Bergquist, Magnus;Rolandsson, Bertil;Gryska, Emilia;Laesser, Mats;Hoefling, Nickoleta;Heckemann, Rolf;Schneiderman, Justin F.;Björkman-Burtscher, Isabella M.",2024,0,"@article{2-32729,
  title={Trust and stakeholder perspectives on the implementation of AI tools in clinical radiology},
  author={Bergquist, Magnus and Rolandsson, Bertil and Gryska, Emilia and Laesser, Mats and Hoefling, Nickoleta and Heckemann, Rolf and Schneiderman, Justin F. and Bj{\""o}rkman-Burtscher, Isabella M.},
  year={2024},
  doi={10.1007/s00330-023-09967-5},
  journal={European Radiology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Decision-subject, Stakeholder","Change cognitive demands, Shape ethical norms, Change trust, Alter decision outcomes",Update AI competence,NA,NA,NA,Yes,Yes
2-32746,springernature,Deep learning enables the differentiation between early and late stages of hip avascular necrosis,"Objectives To develop a deep learning methodology that distinguishes early from late stages of avascular necrosis of the hip( AVN) to determine treatment decisions. Methods Three convolutional neural networks( CNNs) VGG-16, Inception ResnetV2, InceptionV3 were trained with transfer learning( ImageNet) and finetuned with a retrospectively collected cohort of( n = 104) MRI examinations of AVN patients, to differentiate between early( ARCO 1–2) and late( ARCO 3–4) stages. A consensus CNN ensemble decision was recorded as the agreement of at least two CNNs. CNN and ensemble performance was benchmarked on an independent cohort of 49 patients from another country and was compared to the performance of two MSK radiologists. CNN performance was expressed with areas under the curve( AUC) , the respective 95% confidence intervals( CIs) and precision, and recall and f1-scores. AUCs were compared with DeLong’s test. Results On internal testing, Inception-ResnetV2 achieved the highest individual performance with an AUC of 99. 7%( 95%CI 99–100%) , followed by InceptionV3 and VGG-16 with AUCs of 99. 3%( 95%CI 98. 4–100%) and 97. 3%( 95%CI 95. 5–99. 2%) respectively. The CNN ensemble the same AUCs Inception ResnetV2. On external validation, model performance dropped with VGG-16 achieving the highest individual AUC of 78. 9%( 95%CI 51. 6–79. 6%) The best external performance was achieved by the model ensemble with an AUC of 85. 5%( 95%CI 72. 2–93. 9%). No significant difference was found between the CNN ensemble and expert MSK radiologists( p = 0. 22 and 0. 092 respectively). Conclusion An externally validated CNN ensemble accurately distinguishes between the early and late stages of AVN and has comparable performance to expert MSK radiologists. Clinical relevance statement This paper introduces the use of deep learning for the differentiation between early and late avascular necrosis of the hip, assisting in a complex clinical decision that can determine the choice between conservative and surgical treatment. Key Points • A convolutional neural network ensemble achieved excellent performance in distinguishing between early and late avascular necrosis. • The performance of the deep learning method was similar to the performance of expert readers.",10.1007/s00330-023-10104-5,http://dx.doi.org/10.1007/s00330-023-10104-5,European Radiology,"Klontzas, Michail E.;Vassalou, Evangelia E.;Spanakis, Konstantinos;Meurer, Felix;Woertler, Klaus;Zibis, Aristeidis;Marias, Kostas;Karantanas, Apostolos H.",2024,16,"@article{2-32746,
  title={Deep learning enables the differentiation between early and late stages of hip avascular necrosis},
  author={Klontzas, Michail E. and Vassalou, Evangelia E. and Spanakis, Konstantinos and Meurer, Felix and Woertler, Klaus and Zibis, Aristeidis and Marias, Kostas and Karantanas, Apostolos H.},
  year={2024},
  journal={European Radiology},
  doi={10.1007/s00330-023-10104-5}
}","Algorithmic contributions, Empirical contributions",Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-32755,springernature,Deep learning for liver tumor diagnosis part II: convolutional neural network interpretation using radiologic imaging features,"Objectives To develop a proof-of-concept “interpretable” deep learning prototype that justifies aspects of its predictions from a pre-trained hepatic lesion classifier. Methods A convolutional neural network( CNN) was engineered and trained to classify six hepatic tumor entities using 494 lesions on multi-phasic MRI, described in Part 1. A subset of each lesion class was labeled with up to four key imaging features per lesion. A post hoc algorithm inferred the presence of these features in a test set of 60 lesions by analyzing activation patterns of the pre-trained CNN model. Feature maps were generated that highlight regions in the original image that correspond to particular features. Additionally, relevance scores were assigned to each identified feature, denoting the relative contribution of a feature to the predicted lesion classification. Results The interpretable deep learning system achieved 76. 5% positive predictive value and 82. 9% sensitivity in identifying the correct radiological features present in each test lesion. The model misclassified 12% of lesions. Incorrect features were found more often in misclassified lesions than correctly identified lesions( 60. 4% vs. 85. 6%). Feature maps were consistent with original image voxels contributing to each imaging feature. Feature relevance scores tended to reflect the most prominent imaging criteria for each class. Conclusions This interpretable deep learning system demonstrates proof of principle for illuminating portions of a pre-trained deep neural network’s decision-making, by analyzing inner layers and automatically describing features contributing to predictions. Key Points • An interpretable deep learning system prototype can explain aspects of its decision-making by identifying relevant imaging features and showing where these features are found on an image, facilitating clinical translation. • By providing feedback on the importance of various radiological features in performing differential diagnosis, interpretable deep learning systems have the potential to interface with standardized reporting systems such as LI-RADS, validating ancillary features and improving clinical practicality. • An interpretable deep learning system could potentially add quantitative data to radiologic reports and serve radiologists with evidence-based decision support.",10.1007/s00330-019-06214-8,http://dx.doi.org/10.1007/s00330-019-06214-8,European Radiology,"Wang, Clinton J.;Hamm, Charlie A.;Savic, Lynn J.;Ferrante, Marc;Schobert, Isabel;Schlachter, Todd;Lin, MingDe;Weinreb, Jeffrey C.;Duncan, James S.;Chapiro, Julius;Letzen, Brian",2019,164,"@article{2-32755,
  title = {Deep learning for liver tumor diagnosis part II: convolutional neural network interpretation using radiologic imaging features},
  author = {Wang, Clinton J. and Hamm, Charlie A. and Savic, Lynn J. and Ferrante, Marc and Schobert, Isabel and Schlachter, Todd and Lin, MingDe and Weinreb, Jeffrey C. and Duncan, James S. and Chapiro, Julius and Letzen, Brian},
  year = {2019},
  doi = {10.1007/s00330-019-06214-8},
  journal = {European Radiology}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-32817,springernature,Impact of real-life use of artificial intelligence as support for human reading in a population-based breast cancer screening program with mammography and tomosynthesis,"Objectives To evaluate the impact of using an artificial intelligence( AI) system as support for human double reading in a real-life scenario of a breast cancer screening program with digital mammography( DM) or digital breast tomosynthesis( DBT). Material and methods We analyzed the performance of double reading screening with mammography and tomosynthesis after implementarion of AI as decision support. The study group consisted of a consecutive cohort of 1 year screening between March 2021 and March 2022 where double reading was performed with concurrent AI support that automatically detects and highlights lesions suspicious of breast cancer in mammography and tomosynthesis. Screening performance was measured as cancer detection rate( CDR) , recall rate( RR) , and positive predictive value( PPV) of recalls. Performance in the study group was compared using a McNemar test to a control group that included a screening cohort of the same size, recorded just prior to the implementation of AI. Results A total of 11, 998 women( mean age 57. 59 years ± 5. 8 [sd]) were included in the study group( 5049 DM and 6949 DBT). Comparing global results( including DM and DBT) of double reading with vs. without AI support, we observed an increase in CDR, PPV, and RR by 3. 2/‰( 5. 8 vs. 9; p < 0. 001) , 4%( 10. 6 vs. 14. 6; p < 0. 001) , and 0. 7%( 5. 4 vs. 6. 1; p < 0. 001) respectively. Conclusion AI used as support for human double reading in a real-life breast cancer screening program with DM and DBT increases CDR and PPV of the recalled women. Clinical relevance statement Artificial intelligence as support for human double reading improves accuracy in a real-life breast cancer screening program both in digital mammography and digital breast tomosynthesis. Key Points • AI systems based on deep learning technology offer potential for improving breast cancer screening programs. • Using artificial intelligence as support for reading improves radiologists’ performance in breast cancer screening programs with mammography or tomosynthesis. • Artificial intelligence used concurrently with human reading in clinical screening practice increases breast cancer detection rate and positive predictive value of the recalled women.",10.1007/s00330-023-10426-4,http://dx.doi.org/10.1007/s00330-023-10426-4,European Radiology,"Elías-Cabot, Esperanza;Romero-Martín, Sara;Raya-Povedano, José Luis;Brehl, A.-K.;Álvarez-Benito, Marina",2024,23,"@article{2-32817,
  title={Impact of real-life use of artificial intelligence as support for human reading in a population-based breast cancer screening program with mammography and tomosynthesis},
  author={El{\'\i}as-Cabot, Esperanza and Romero-Mart{\'\i}n, Sara and Raya-Povedano, Jos{\'e} Luis and Brehl, A-K and {\'A}lvarez-Benito, Marina},
  year={2024},
  doi={10.1007/s00330-023-10426-4},
  journal={European Radiology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-32858,springernature,Development of image-based decision support systems utilizing information extracted from radiological free-text report databases with text-based transformers,"Objectives To investigate the potential and limitations of utilizing transformer-based report annotation for on-site development of image-based diagnostic decision support systems( DDSS). Methods The study included 88, 353 chest X-rays from 19, 581 intensive care unit( ICU) patients. To label the presence of six typical findings in 17, 041 images, the corresponding free-text reports of the attending radiologists were assessed by medical research assistants( “gold labels”). Automatically generated “silver” labels were extracted for all reports by transformer models trained on gold labels. To investigate the benefit of such silver labels, the image-based models were trained using three approaches: with gold labels only( M G) , with silver labels first, then with gold labels( M S/G) , and with silver and gold labels together( M S+G). To investigate the influence of invested annotation effort, the experiments were repeated with different numbers( N) of gold-annotated reports for training the transformer and image-based models and tested on 2099 gold-annotated images. Significant differences in macro-averaged area under the receiver operating characteristic curve( AUC) were assessed by non-overlapping 95% confidence intervals. Results Utilizing transformer-based silver labels showed significantly higher macro-averaged AUC than training solely with gold labels( N = 1000: M G 67. 8 [66. 0–69. 6], M S/G 77. 9 [76. 2–79. 6]; N = 14, 580: M G 74. 5 [72. 8–76. 2], M S/G 80. 9 [79. 4–82. 4]). Training with silver and gold labels together was beneficial using only 500 gold labels( M S+G 76. 4 [74. 7–78. 0], M S/G 75. 3 [73. 5–77. 0]). Conclusions Transformer-based annotation has potential for unlocking free-text report databases for the development of image-based DDSS. However, on-site development of image-based DDSS could benefit from more sophisticated annotation pipelines including further information than a single radiological report. Clinical relevance statement Leveraging clinical databases for on-site development of artificial intelligence( AI) –based diagnostic decision support systems by text-based transformers could promote the application of AI in clinical practice by circumventing highly regulated data exchanges with third parties. Key Points • The amount of data from a database that can be used to develop AI-assisted diagnostic decision systems is often limited by the need for time-consuming identification of pathologies by radiologists. • The transformer-based structuring of free-text radiological reports shows potential to unlock corresponding image databases for on-site development of image-based diagnostic decision support systems. • However, the quality of image annotations generated solely on the content of a single radiology report may be limited by potential inaccuracies and incompleteness of this report.",10.1007/s00330-023-10373-0,http://dx.doi.org/10.1007/s00330-023-10373-0,European Radiology,"Nowak, Sebastian;Schneider, Helen;Layer, Yannik C.;Theis, Maike;Biesner, David;Block, Wolfgang;Wulff, Benjamin;Attenberger, Ulrike I.;Sifa, Rafet;Sprinkart, Alois M.",2024,9,"@article{2-32858,
  title={Development of image-based decision support systems utilizing information extracted from radiological free-text report databases with text-based transformers},
  author={Nowak, Sebastian and Schneider, Helen and Layer, Yannik C. and Theis, Maike and Biesner, David and Block, Wolfgang and Wulff, Benjamin and Attenberger, Ulrike I. and Sifa, Rafet and Sprinkart, Alois M.},
  year={2024},
  doi={10.1007/s00330-023-10373-0},
  journal={European Radiology}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-32863,springernature,A deep learning model integrating mammography and clinical factors facilitates the malignancy prediction of BI-RADS 4 microcalcifications in breast cancer screening,"Objectives To investigate the value of full-field digital mammography-based deep learning( DL) in predicting malignancy of Breast Imaging Reporting and Data System( BI-RADS) 4 microcalcifications. Methods A total of 384 patients with 414 pathologically confirmed microcalcifications( 221 malignant and 193 benign) were randomly allocated into the training, validation, and testing datasets( 272/71/71 lesions) in this retrospective study. A combined DL model was developed incorporating mammography and clinical variables. Model performance was evaluated by using areas under the receiver operating characteristic curve( AUC) and compared with the clinical model, stand-alone DL image model, and BI-RADS approach. The predictive performance for malignancy was also compared between the combined model and human readers( 2 juniors and 2 seniors). Results The combined DL model demonstrated favorable AUC, sensitivity, and specificity of 0. 910, 85. 3%, and 91. 9% in predicting BI-RADS 4 malignant microcalcifications in the testing dataset, which outperformed the clinical model, DL image model, and BI-RADS with AUCs of 0. 799, 0. 841, and 0. 804, respectively. The combined model achieved non-inferior performance as senior radiologists( p = 0. 860, p = 0. 800) and outperformed junior radiologists( p = 0. 155, p = 0. 029). The diagnostic performance of two junior radiologists was improved after artificial intelligence assistance with AUCs increased to 0. 854 and 0. 901 from 0. 816( p = 0. 556) and 0. 773( p = 0. 046) , while the interobserver agreement was improved with a kappa value increased to 0. 843 from 0. 331. Conclusions The combined deep learning model can improve the malignancy prediction of BI-RADS 4 microcalcifications in screening mammography and assist junior radiologists to achieve better performance, which can facilitate clinical decision-making. Key Points • The combined deep learning model demonstrated high diagnostic power, sensitivity, and specificity for predicting malignant BI-RADS 4 mammographic microcalcifications. • The combined model achieved similar performance with senior breast radiologists, while it outperformed junior breast radiologists. • Deep learning could improve the diagnostic performance of junior radiologists and facilitate clinical decision-making.",10.1007/s00330-020-07659-y,http://dx.doi.org/10.1007/s00330-020-07659-y,European Radiology,"Liu, Huanhuan;Chen, Yanhong;Zhang, Yuzhen;Wang, Lijun;Luo, Ran;Wu, Haoting;Wu, Chenqing;Zhang, Huiling;Tan, Weixiong;Yin, Hongkun;Wang, Dengbin",2021,0,"@article{2-32863,
  title={A deep learning model integrating mammography and clinical factors facilitates the malignancy prediction of BI-RADS 4 microcalcifications in breast cancer screening},
  author={Liu, Huanhuan and Chen, Yanhong and Zhang, Yuzhen and Wang, Lijun and Luo, Ran and Wu, Haoting and Wu, Chenqing and Zhang, Huiling and Tan, Weixiong and Yin, Hongkun and Wang, Dengbin},
  year={2021},
  journal={European Radiology},
  doi={10.1007/s00330-020-07659-y}
}","Algorithmic contributions, Empirical contributions",Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-32888,springernature,Machine learning and deep learning for classifying the justification of brain CT referrals,"Objectives To train the machine and deep learning models to automate the justification analysis of radiology referrals in accordance with iGuide categorisation, and to determine if prediction models can generalise across multiple clinical sites and outperform human experts. Methods Adult brain computed tomography( CT) referrals from scans performed in three CT centres in Ireland in 2020 and 2021 were retrospectively collected. Two radiographers analysed the justification of 3000 randomly selected referrals using iGuide, with two consultant radiologists analysing the referrals with disagreement. Insufficient or duplicate referrals were discarded. The inter-rater agreement among radiographers and consultants was computed. A random split( 4:1) was performed to apply machine learning( ML) and deep learning( DL) techniques to unstructured clinical indications to automate retrospective justification auditing with multi-class classification. The accuracy and macro-averaged F1 score of the best-performing classifier of each type on the training set were computed on the test set. Results 42 referrals were ignored. 1909( 64. 5%) referrals were justified, 811( 27. 4%) were potentially justified, and 238( 8. 1%) were unjustified. The agreement between radiographers( κ = 0. 268) was lower than radiologists( κ = 0. 460). The best-performing ML model was the bag-of-words-based gradient-boosting classifier achieving a 94. 4% accuracy and a macro F1 of 0. 94. DL models were inferior, with bi-directional long short-term memory achieving 92. 3% accuracy, a macro F1 of 0. 92, and outperforming multilayer perceptrons. Conclusion Interpreting unstructured clinical indications is challenging necessitating clinical decision support. ML and DL can generalise across multiple clinical sites, outperform human experts, and be used as an artificial intelligence-based iGuide interpreter when retrospectively vetting radiology referrals. Clinical relevance statement Healthcare vendors and clinical sites should consider developing and utilising artificial intelligence-enabled systems for justifying medical exposures. This would enable better implementation of imaging referral guidelines in clinical practices and reduce population dose burden, CT waiting lists, and wasteful use of resources. Key Points Significant variations exist among human experts in interpreting unstructured clinical indications/patient presentations. Machine and deep learning can automate the justification analysis of radiology referrals according to iGuide categorisation. Machine and deep learning can improve retrospective and prospective justification auditing for better implementation of imaging referral guidelines.",10.1007/s00330-024-10851-z,http://dx.doi.org/10.1007/s00330-024-10851-z,European Radiology,"Potočnik, Jaka;Thomas, Edel;Lawlor, Aonghus;Kearney, Dearbhla;Heffernan, Eric J.;Killeen, Ronan P.;Foley, Shane J.",2024,4,"@article{2-32888,
  title = {Machine Learning and Deep Learning for Classifying the Justification of Brain CT Referrals},
  author = {Potočnik, Jaka and Thomas, Edel and Lawlor, Aonghus and Kearney, Dearbhla and Heffernan, Eric J. and Killeen, Ronan P. and Foley, Shane J.},
  year = {2024},
  doi = {10.1007/s00330-024-10851-z},
  journal = {European Radiology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-32895,springernature,Development and validation of a deep learning radiomics nomogram for preoperatively differentiating thymic epithelial tumor histologic subtypes,"Objectives Using contrast-enhanced computed tomography( CECT) and deep learning technology to develop a deep learning radiomics nomogram( DLRN) to preoperative predict risk status of patients with thymic epithelial tumors( TETs). Methods Between October 2008 and May 2020, 257 consecutive patients with surgically and pathologically confirmed TETs were enrolled from three medical centers. We extracted deep learning features from all lesions using a transformer-based convolutional neural network and created a deep learning signature( DLS) using selector operator regression and least absolute shrinkage. The predictive capability of a DLRN incorporating clinical characteristics, subjective CT findings and DLS was evaluated by the area under the curve( AUC) of a receiver operating characteristic curve. Results To construct a DLS, 25 deep learning features with non-zero coefficients were selected from 116 low-risk TETs( subtypes A, AB, and B1) and 141 high-risk TETs( subtypes B2, B3, and C). The combination of subjective CT features such as infiltration and DLS demonstrated the best performance in differentiating TETs risk status. The AUCs in the training, internal validation, external validation 1 and 2 cohorts were 0. 959( 95% confidence interval [CI]: 0. 924–0. 993) , 0. 868( 95% CI: 0. 765–0. 970) , 0. 846( 95% CI: 0. 750–0. 942) , and 0. 846( 95% CI: 0. 735–0. 957) , respectively. The DeLong test and decision in curve analysis revealed that the DLRN was the most predictive and clinically useful model. Conclusions The DLRN comprised of CECT-derived DLS and subjective CT findings showed a high performance in predicting risk status of patients with TETs. Clinical relevance statement Accurate risk status assessment of thymic epithelial tumors( TETs) may aid in determining whether preoperative neoadjuvant treatment is necessary. A deep learning radiomics nomogram incorporating enhancement CT-based deep learning features, clinical characteristics, and subjective CT findings has the potential to predict the histologic subtypes of TETs, which can facilitate decision-making and personalized therapy in clinical practice. Key Points • A non-invasive diagnostic method that can predict the pathological risk status may be useful for pretreatment stratification and prognostic evaluation in TET patients. • DLRN demonstrated superior performance in differentiating the risk status of TETs when compared to the deep learning signature, radiomics signature, or clinical model. • The DeLong test and decision in curve analysis revealed that the DLRN was the most predictive and clinically useful in differentiating the risk status of TETs.",10.1007/s00330-023-09690-1,http://dx.doi.org/10.1007/s00330-023-09690-1,European Radiology,"Chen, Xiangmeng;Feng, Bao;Xu, Kuncai;Chen, Yehang;Duan, Xiaobei;Jin, Zhifa;Li, Kunwei;Li, Ronggang;Long, Wansheng;Liu, Xueguo",2023,15,"@article{2-32895,
  title={Development and validation of a deep learning radiomics nomogram for preoperatively differentiating thymic epithelial tumor histologic subtypes},
  author={Chen, Xiangmeng and Feng, Bao and Xu, Kuncai and Chen, Yehang and Duan, Xiaobei and Jin, Zhifa and Li, Kunwei and Li, Ronggang and Long, Wansheng and Liu, Xueguo},
  year={2023},
  doi={10.1007/s00330-023-09690-1},
  journal={European Radiology}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-32905,springernature,Deep learning-based detection and quantification of brain metastases on black-blood imaging can provide treatment suggestions: a clinical cohort study,"Objectives We aimed to evaluate whether deep learning–based detection and quantification of brain metastasis( BM) may suggest treatment options for patients with BMs. Methods The deep learning system( DLS) for detection and quantification of BM was developed in 193 patients and applied to 112 patients that were newly detected on black-blood contrast-enhanced T1-weighted imaging. Patients were assigned to one of 3 treatment suggestion groups according to the European Association of Neuro-Oncology( EANO) -European Society for Medical Oncology( ESMO) recommendations using number and volume of the BMs detected by the DLS: short-term imaging follow-up without treatment( group A) , surgery or stereotactic radiosurgery( limited BM, group B) , or whole-brain radiotherapy or systemic chemotherapy( extensive BM, group C). The concordance between the DLS-based groups and clinical decisions was analyzed with or without consideration of targeted agents. The performance of distinguishing high-risk( B + C) was calculated. Results Among 112 patients( mean age 64. 3 years, 63 men) , group C had the largest number and volume of BM, followed by group B( 4. 4 and 851. 6 mm 3) and A( 1. 5 and 15. 5 mm 3). The DLS-based groups were concordant with the actual clinical decisions, with an accuracy of 76. 8%( 86 of 112). Modified accuracy considering targeted agents was 81. 3%( 91 of 112). The DLS showed 95%( 82/86) sensitivity and 81%( 21/26) specificity for distinguishing the high risk. Conclusion DLS-based detection and quantification of BM have the potential to be helpful in the determination of treatment options for both lowand high-risk groups of limited and extensive BMs. Clinical relevance statement For patients with newly diagnosed brain metastasis, deep learning–based detection and quantification may be used in clinical settings where prompt and accurate treatment decisions are required, which can lead to better patient outcomes. Key Points • Deep learning–based brain metastasis detection and quantification showed excellent agreement with ground-truth classifications. • By setting an algorithm to suggest treatment based on the number and volume of brain metastases detected by the deep learning system, the concordance was 81. 3%. • When dividing patients into lowand high-risk groups, the sensitivity for detecting the latter was 95%.",10.1007/s00330-023-10120-5,http://dx.doi.org/10.1007/s00330-023-10120-5,European Radiology,"Jeong, Hana;Park, Ji Eun;Kim, NakYoung;Yoon, Shin-Kyo;Kim, Ho Sung",2024,3,"@article{2-32905,
  title = {Deep learning-based detection and quantification of brain metastases on black-blood imaging can provide treatment suggestions: a clinical cohort study},
  author = {Jeong, Hana and Park, Ji Eun and Kim, NakYoung and Yoon, Shin-Kyo and Kim, Ho Sung},
  year = {2024},
  doi = {10.1007/s00330-023-10120-5},
  journal = {European Radiology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-32918,springernature,A novel image deep learning–based sub-centimeter pulmonary nodule management algorithm to expedite resection of the malignant and avoid over-diagnosis of the benign,"Objectives With the popularization of chest computed tomography( CT) screening, there are more sub-centimeter( ≤ 1 cm) pulmonary nodules( SCPNs) requiring further diagnostic workup. This area represents an important opportunity to optimize the SCPN management algorithm avoiding “one-size fits all” approach. One critical problem is how to learn the discriminative multi-view characteristics and the unique context of each SCPN. Methods Here, we propose a multi-view coupled self-attention module( MVCS) to capture the global spatial context of the CT image through modeling the association order of space and dimension. Compared with existing self-attention methods, MVCS uses less memory consumption and computational complexity, unearths dimension correlations that previous methods have not found, and is easy to integrate with other frameworks. Results In total, a public dataset LUNA16 from LIDC-IDRI, 1319 SCPNs from 1069 patients presenting to a major referral center, and 160 SCPNs from 137 patients from three other major centers were analyzed to pre-train, train, and validate the model. Experimental results showed that performance outperforms the state-of-the-art models in terms of accuracy and stability and is comparable to that of human experts in classifying precancerous lesions and invasive adenocarcinoma. We also provide a fusion MVCS network( MVCSN) by combining the CT image with the clinical characteristics and radiographic features of patients. Conclusion This tool may ultimately aid in expediting resection of the malignant SCPNs and avoid over-diagnosis of the benign ones, resulting in improved management outcomes. Clinical relevance statement In the diagnosis of sub-centimeter lung adenocarcinoma, fusion MVCSN can help doctors improve work efficiency and guide their treatment decisions to a certain extent. Key Points • Advances in computed tomography( CT) not only increase the number of nodules detected, but also the nodules that are identified are smaller, such as sub-centimeter pulmonary nodules( SCPNs). • We propose a multi-view coupled self-attention module( MVCS) , which could model spatial and dimensional correlations sequentially for learning global spatial contexts, which is better than other attention mechanisms. • MVCS uses fewer huge memory consumption and computational complexity than the existing self-attention methods when dealing with 3D medical image data. Additionally, it reaches promising accuracy for SCPNs’ malignancy evaluation and has lower training cost than other models. Graphical abstract",10.1007/s00330-023-10026-2,http://dx.doi.org/10.1007/s00330-023-10026-2,European Radiology,"Yang, Xiongwen;Chu, Xiang-Peng;Huang, Shaohong;Xiao, Yi;Li, Dantong;Su, Xiaoyang;Qi, Yi-fan;Qiu, Zhen-bin;Wang, Yanqing;Tang, Wen-Fang;Wu, Yi-Long;Zhu, Qikui;Liang, Huiying;Zhong, Wen-Zhao",2024,9,"@article{2-32918,
  title = {A novel image deep learning–based sub-centimeter pulmonary nodule management algorithm to expedite resection of the malignant and avoid over-diagnosis of the benign},
  author = {Yang, Xiongwen and Chu, Xiang-Peng and Huang, Shaohong and Xiao, Yi and Li, Dantong and Su, Xiaoyang and Qi, Yi-fan and Qiu, Zhen-bin and Wang, Yanqing and Tang, Wen-Fang and Wu, Yi-Long and Zhu, Qikui and Liang, Huiying and Zhong, Wen-Zhao},
  year = {2024},
  doi = {10.1007/s00330-023-10026-2},
  journal = {European Radiology}
}","Algorithmic contributions, Empirical contributions",Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-32926,springernature,Towards Explainable Occupational Fraud Detection,"Occupational fraud within companies currently causes losses of around 5% of company revenue each year. While enterprise resource planning systems can enable automated detection of occupational fraud through recording large amounts of company data, the use of state-of-the-art machine learning approaches in this domain is limited by their untraceable decision process. In this study, we evaluate whether machine learning combined with explainable artificial intelligence can provide both strong performance and decision traceability in occupational fraud detection. We construct an evaluation setting that assesses the comprehensibility of machine learning-based occupational fraud detection approaches, and evaluate both performance and comprehensibility of multiple approaches with explainable artificial intelligence. Our study finds that high detection performance does not necessarily indicate good explanation quality, but specific approaches provide both satisfactory performance and decision traceability, highlighting the suitability of machine learning for practical application in occupational fraud detection and the importance of research evaluating both performance and comprehensibility together.",10.1007/978-3-031-23633-4_7,http://dx.doi.org/10.1007/978-3-031-23633-4_7,European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases,"Tritscher, Julian;Schlör, Daniel;Gwinner, Fabian;Krause, Anna;Hotho, Andreas",2023,11,"@inproceedings{2-32926,
  title={Towards Explainable Occupational Fraud Detection},
  author={Tritscher, Julian and Schl{\""o}r, Daniel and Gwinner, Fabian and Krause, Anna and Hotho, Andreas},
  year={2023},
  doi={10.1007/978-3-031-23633-4_7},
  booktitle={European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases}
}",Empirical contributions,Finance / Business / Economy,Organizational,"Explaining, Forecasting","Decision-maker, Stakeholder, Guardian",NA,NA,NA,NA,NA,Yes,No
2-32952,springernature,A flexible framework for evaluating user and item fairness in recommender systems,"One common characteristic of research works focused on fairness evaluation( in machine learning) is that they call for some form of parity( equality) either in treatment—meaning they ignore the information about users’ memberships in protected classes during training—or in impact—by enforcing proportional beneficial outcomes to users in different protected classes. In the recommender systems community, fairness has been studied with respect to both users’ and items’ memberships in protected classes defined by some sensitive attributes( e. g. , gender or race for users, revenue in a multi-stakeholder setting for items). Again here, the concept has been commonly interpreted as some form of equality —i. e. , the degree to which the system is meeting the information needs of all its users in an equal sense. In this work, we propose a probabilistic framework based on generalized cross entropy( GCE) to measure fairness of a given recommendation model. The framework comes with a suite of advantages: first, it allows the system designer to define and measure fairness for both users and items and can be applied to any classification task; second, it can incorporate various notions of fairness as it does not rely on specific and predefined probability distributions and they can be defined at design time; finally, in its design it uses a gain factor, which can be flexibly defined to contemplate different accuracy-related metrics to measure fairness upon decision-support metrics( e. g. , precision, recall) or rank-based measures( e. g. , NDCG, MAP). An experimental evaluation on four real-world datasets shows the nuances captured by our proposed metric regarding fairness on different user and item attributes, where nearest-neighbor recommenders tend to obtain good results under equality constraints. We observed that when the users are clustered based on both their interaction with the system and other sensitive attributes, such as age or gender, algorithms with similar performance values get different behaviors with respect to user fairness due to the different way they process data for each user cluster.",10.1007/s11257-020-09285-1,http://dx.doi.org/10.1007/s11257-020-09285-1,User Modeling and User-Adapted Interaction,"Deldjoo, Yashar;Anelli, Vito Walter;Zamani, Hamed;Bellogín, Alejandro;Noia, Tommaso",2021,113,"@article{2-32952,
  title = {A flexible framework for evaluating user and item fairness in recommender systems},
  author = {Deldjoo, Yashar and Anelli, Vito Walter and Zamani, Hamed and Bellogín, Alejandro and Noia, Tommaso},
  year = {2021},
  doi = {10.1007/s11257-020-09285-1},
  journal = {User Modeling and User-Adapted Interaction}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Institutional,Advising,"Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-32993,springernature,Improved prediction of immune checkpoint blockade efficacy across multiple cancer types,"Only a fraction of patients with cancer respond to immune checkpoint blockade( ICB) treatment, but current decision-making procedures have limited accuracy. In this study, we developed a machine learning model to predict ICB response by integrating genomic, molecular, demographic and clinical data from a comprehensively curated cohort( MSK-IMPACT) with 1, 479 patients treated with ICB across 16 different cancer types. In a retrospective analysis, the model achieved high sensitivity and specificity in predicting clinical response to immunotherapy and predicted both overall survival and progression-free survival in the test data across different cancer types. Our model significantly outperformed predictions based on tumor mutational burden, which was recently approved by the U. S. Food and Drug Administration for this purpose 1. Additionally, the model provides quantitative assessments of the model features that are most salient for the predictions. We anticipate that this approach will substantially improve clinical decision-making in immunotherapy and inform future interventions. A combination of genomic and clinical features improves predictions of response to immune checkpoint blockade.",10.1038/s41587-021-01070-8,http://dx.doi.org/10.1038/s41587-021-01070-8,Nature Biotechnology,"Chowell, Diego;Yoo, Seong-Keun;Valero, Cristina;Pastore, Alessandro;Krishna, Chirag;Lee, Mark;Hoen, Douglas;Shi, Hongyu;Kelly, Daniel W.;Patel, Neal;Makarov, Vladimir;Ma, Xiaoxiao;Vuong, Lynda;Sabio, Erich Y.;Weiss, Kate;Kuo, Fengshen;Lenz, Tobias L.;Samstein, Robert M.;Riaz, Nadeem;Adusumilli, Prasad S.;Balachandran, Vinod P.;Plitas, George;Ari Hakimi, A.;Abdel-Wahab, Omar;Shoushtari, Alexander N.;Postow, Michael A.;Motzer, Robert J.;Ladanyi, Marc;Zehir, Ahmet;Berger, Michael F.;Gönen, Mithat;Morris, Luc G. T.;Weinhold, Nils;Chan, Timothy A.",2022,254,"@article{2-32993,
  title={Improved prediction of immune checkpoint blockade efficacy across multiple cancer types},
  author={Chowell, Diego and Yoo, Seong-Keun and Valero, Cristina and Pastore, Alessandro and Krishna, Chirag and Lee, Mark and Hoen, Douglas and Shi, Hongyu and Kelly, Daniel W. and Patel, Neal and Makarov, Vladimir and Ma, Xiaoxiao and Vuong, Lynda and Sabio, Erich Y. and Weiss, Kate and Kuo, Fengshen and Lenz, Tobias L. and Samstein, Robert M. and Riaz, Nadeem and Adusumilli, Prasad S. and Balachandran, Vinod P. and Plitas, George and Ari Hakimi, A. and Abdel-Wahab, Omar and Shoushtari, Alexander N. and Postow, Michael A. and Motzer, Robert J. and Ladanyi, Marc and Zehir, Ahmet and Berger, Michael F. and Gönen, Mithat and Morris, Luc G. T. and Weinhold, Nils and Chan, Timothy A.},
  year={2022},
  doi={10.1038/s41587-021-01070-8},
  journal={Nature Biotechnology}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-33024,springernature,The Implications of Diverse Human Moral Foundations for Assessing the Ethicality of Artificial Intelligence,"Organizations are making massive investments in artificial intelligence( AI) , and recent demonstrations and achievements highlight the immense potential for AI to improve organizational and human welfare. Yet realizing the potential of AI necessitates a better understanding of the various ethical issues involved with deciding to use AI, training and maintaining it, and allowing it to make decisions that have moral consequences. People want organizations using AI and the AI systems themselves to behave ethically, but ethical behavior means different things to different people, and many ethical dilemmas require trade-offs such that no course of action is universally considered ethical. How should organizations using AI—and the AI itself—process ethical dilemmas where humans disagree on the morally right course of action? Though a variety of ethical AI frameworks have been suggested, these approaches do not adequately address how people make ethical evaluations of AI systems or how to incorporate the fundamental disagreements people have regarding what is and is not ethical behavior. Drawing on moral foundations theory, we theorize that a person will perceive an organization’s use of AI, its data procedures, and the resulting AI decisions as ethical to the extent that those decisions resonate with the person’s moral foundations. Since people hold diverse moral foundations, this highlights the crucial need to consider individual moral differences at multiple levels of AI. We discuss several unresolved issues and suggest potential approaches( such as moral reframing) for thinking about conflicts in moral judgments concerning AI.",10.1007/s10551-022-05057-6,http://dx.doi.org/10.1007/s10551-022-05057-6,Journal of Business Ethics,"Telkamp, Jake B.;Anderson, Marc H.",2022,75,"@article{2-33024,
  title={The Implications of Diverse Human Moral Foundations for Assessing the Ethicality of Artificial Intelligence},
  author={Telkamp, Jake B. and Anderson, Marc H.},
  year={2022},
  doi={10.1007/s10551-022-05057-6},
  journal={Journal of Business Ethics}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Individual,Executing,"Knowledge provider, Guardian",NA,NA,NA,NA,NA,Yes,No
2-33068,springernature,Towards Fairness Through Time,"Over the past decade, the development of Machine Learning( ML) algorithms to replace human decisions has raised concerns about potential bias issues. At the same time, significant advances have been made in the study of fairness in classification to prevent discrimination. However, only a few of these works have investigated how those techniques can have an impact on the society and if they remain reliable over time. This work aims( i) to shed light on traditional group fairness mitigation strategies that fail in real-time environments when financial data drifts affect only some classes of sensitive attributes;( ii) to investigate a strategy that encodes the human behaviour while retraining the model over time, favoring the convergence between individual and group fairness;( iii) to put the basis of strategies based on eXplainable AI( XAI) , to monitor the evolution of financial gaps between different population subgroups, like gender or race, observing whether the mitigation strategy is bringing benefits to society. Preliminary results are provided, processing about 800k personal loan granting from 2016 to 2019 for Intesa Sanpaolo bank.",10.1007/978-3-030-93736-2_46,http://dx.doi.org/10.1007/978-3-030-93736-2_46,European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases,"Castelnovo, Alessandro;Malandri, Lorenzo;Mercorio, Fabio;Mezzanzanica, Mario;Cosentini, Andrea",2021,32,"@inproceedings{2-33068,
  title={Towards Fairness Through Time},
  author={Castelnovo, Alessandro and Malandri, Lorenzo and Mercorio, Fabio and Mezzanzanica, Mario and Cosentini, Andrea},
  year={2021},
  doi={10.1007/978-3-030-93736-2_46},
  booktitle={European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases}
}",Methodological contributions,Finance / Business / Economy,Institutional,"Executing, Explaining, Monitoring","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-33244,springernature,Artificial intelligence assisted operative anatomy recognition in endoscopic pituitary surgery,"Pituitary tumours are surrounded by critical neurovascular structures and identification of these intra-operatively can be challenging. We have previously developed an AI model capable of sellar anatomy segmentation. This study aims to apply this model, and explore the impact of AI-assistance on clinician anatomy recognition. Participants were tasked with labelling the sella on six images, initially without assistance, then augmented by AI. Mean DICE scores and the proportion of annotations encompassing the centroid of the sella were calculated. Six medical students, six junior trainees, six intermediate trainees and six experts were recruited. There was an overall improvement in sella recognition from a DICE of score 70. 7% without AI assistance to 77. 5% with AI assistance( +6. 7; p < 0. 001). Medical students used and benefitted from AI assistance the most, improving from a DICE score of 66. 2% to 78. 9%( +12. 8; p = 0. 02). This technology has the potential to augment surgical education and eventually be used as an intra-operative decision support tool.",10.1038/s41746-024-01273-8,http://dx.doi.org/10.1038/s41746-024-01273-8,Nature Partner Journals Digital Medicine,"Khan, Danyal Z.;Valetopoulou, Alexandra;Das, Adrito;Hanrahan, John G.;Williams, Simon C.;Bano, Sophia;Borg, Anouk;Dorward, Neil L.;Barbarisi, Santiago;Culshaw, Lucy;Kerr, Karen;Luengo, Imanol;Stoyanov, Danail;Marcus, Hani J.",2024,10,"@article{2-33244,
  title = {Artificial intelligence assisted operative anatomy recognition in endoscopic pituitary surgery},
  author = {Khan, Danyal Z. and Valetopoulou, Alexandra and Das, Adrito and Hanrahan, John G. and Williams, Simon C. and Bano, Sophia and Borg, Anouk and Dorward, Neil L. and Barbarisi, Santiago and Culshaw, Lucy and Kerr, Karen and Luengo, Imanol and Stoyanov, Danail and Marcus, Hani J.},
  year = {2024},
  doi = {10.1038/s41746-024-01273-8},
  journal = {Nature Partner Journals Digital Medicine}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Decision-subject","Alter decision outcomes, Change trust",Update AI competence,recommendations,corrective feedback,Visual,Yes,Yes
2-33328,springernature,"Biased Humans, ( Un) Biased Algorithms?","Previous research has shown that algorithmic decisions can reflect gender bias. The increasingly widespread utilization of algorithms in critical decision-making domains( e. g. , healthcare or hiring) can thus lead to broad and structural disadvantages for women. However, women often experience bias and discrimination through human decisions and may turn to algorithms in the hope of receiving neutral and objective evaluations. Across three studies( N = 1107) , we examine whether women’s receptivity to algorithms is affected by situations in which they believe that their gender identity might disadvantage them in an evaluation process. In Study 1, we establish, in an incentive-compatible online setting, that unemployed women are more likely to choose to have their employment chances evaluated by an algorithm if the alternative is an evaluation by a man rather than a woman. Study 2 generalizes this effect by placing it in a hypothetical hiring context, and Study 3 proposes that relative algorithmic objectivity, i. e. , the perceived objectivity of an algorithmic evaluator over and against a human evaluator, is a driver of women’s preferences for evaluations by algorithms as opposed to men. Our work sheds light on how women make sense of algorithms in stereotype-relevant domains and exemplifies the need to provide education for those at risk of being adversely affected by algorithmic decisions. Our results have implications for the ethical management of algorithms in evaluation settings. We advocate for improving algorithmic literacy so that evaluators and evaluatees( e. g. , hiring managers and job applicants) can acquire the abilities required to reflect critically on algorithmic decisions.",10.1007/s10551-022-05071-8,http://dx.doi.org/10.1007/s10551-022-05071-8,Journal of Business Ethics,"Pethig, Florian;Kroenung, Julia",2023,3,"@article{2-33328,
  title={Biased Humans, (Un)Biased Algorithms?},
  author={Pethig, Florian and Kroenung, Julia},
  year={2023},
  doi={10.1007/s10551-022-05071-8},
  journal={Journal of Business Ethics}
}",Empirical contributions,Everyday / Employment / Public Service,Institutional,"Advising, Auditing","Guardian, Decision-maker, Decision-subject","Shape ethical norms, Change cognitive demands, Alter decision outcomes, Change affective-perceptual",no such info,NA,NA,Textual,Yes,Yes
2-3358,acm,Supporting High-Uncertainty Decisions through AI and Logic-Style Explanations,"A common criteria for Explainable AI (XAI) is to support users in establishing appropriate trust in the AI – rejecting advice when it is incorrect, and accepting advice when it is correct. Previous findings suggest that explanations can cause an over-reliance on AI (overly accepting advice). Explanations that evoke appropriate trust are even more challenging for decision-making tasks that are difficult for humans and AI. For this reason, we study decision-making by non-experts in the high-uncertainty domain of stock trading. We compare the effectiveness of three different explanation styles (influenced by inductive, abductive, and deductive reasoning) and the role of AI confidence in terms of a) the users’ reliance on the XAI interface elements (charts with indicators, AI prediction, explanation), b) the correctness of the decision (task performance), and c) the agreement with the AI’s prediction. In contrast to previous work, we look at interactions between different aspects of decision-making, including AI correctness, and the combined effects of AI confidence and explanations styles. Our results show that specific explanation styles (abductive and deductive) improve the user’s task performance in the case of high AI confidence compared to inductive explanations. In other words, these styles of explanations were able to invoke correct decisions (for both positive and negative decisions) when the system was certain. In such a condition, the agreement between the user’s decision and the AI prediction confirms this finding, highlighting a significant agreement increase when the AI is correct. This suggests that both explanation styles are suitable for evoking appropriate trust in a confident AI. Our findings further indicate a need to consider AI confidence as a criterion for including or excluding explanations from AI interfaces. In addition, this paper highlights the importance of carefully selecting an explanation style according to the characteristics of the task and data.",10.1145/3581641.3584080,https://doi.org/10.1145/3581641.3584080,ACM International Conference on Intelligent User Interfaces (IUI),"Cau, Federico Maria; Hauptmann, Hanna; Spano, Lucio Davide; Tintarev, Nava",2023,45,"@inproceedings{2-3358,
  title={Supporting High-Uncertainty Decisions through AI and Logic-Style Explanations},
  author={Cau, Federico Maria and Hauptmann, Hanna and Spano, Lucio Davide and Tintarev, Nava},
  year={2023},
  doi={10.1145/3581641.3584080},
  booktitle={Proceedings of the ACM International Conference on Intelligent User Interfaces (IUI)}
}",Empirical contributions,"Finance / Business / Economy, Generic / Abstract / Domain-agnostic",Operational,"Explaining, Forecasting, Advising",Decision-maker,"Alter decision outcomes, Change trust",no such info,"inductive explanations, deductive explanations, abductive explanations, confidence score, AI correctness, prediction of alternative, stock charts",NA,Interactive interface,Yes,Yes
2-3359,acm,"Inform, Explain, or Control: Techniques to Adjust End-User Performance Expectations for a Conversational Agent Facilitating Group Chat Discussions","A conversational agent (CA) effectively facilitates online group discussions at scale. However, users may have expectations about how well the CA would perform that do not match with the actual performance, compromising technology acceptance. We built a facilitator CA that detects a member who has low contribution during a synchronous group chat discussion and asks the person to participate more. We designed three techniques to set end-user expectations about how accurately the CA identifies an under-contributing member: 1)information: explicitly communicating the accuracy of the detection algorithm, 2)explanation: providing an overview of the algorithm and the data used for the detection, and 3)adjustment: enabling users to gain a feeling of control over the algorithm. We conducted an online experiment with 163 crowdworkers in which each group completed a collaborative decision-making task and experienced one of the techniques. Through surveys and interviews, we found that the explanation technique was the most effective strategy overall as it reduced user embarrassment, increased the perceived intelligence of the CA, and helped users better understand the detection algorithm. In contrast, the information technique reduced members' contributions and the adjustment technique led to a more negative perceived discussion experience. We also discovered that the interactions with other team members diluted the effects of the techniques on users' performance expectations and acceptance of the CA. We discuss implications for better designing expectation-setting techniques for AI-team collaboration such as ways to improve collaborative decision outcomes and quality of contributions.",10.1145/3610192,https://doi.org/10.1145/3610192,Proceedings of the ACM on Human-Computer Interaction,"Do, Hyo Jin; Kong, Ha-Kyung; Tetali, Pooja; Karahalios, Karrie; Bailey, Brian P.",2023,23,"@article{2-3359,
  title={Inform, Explain, or Control: Techniques to Adjust End-User Performance Expectations for a Conversational Agent Facilitating Group Chat Discussions},
  author={Do, Hyo Jin and Kong, Ha-Kyung and Tetali, Pooja and Karahalios, Karrie and Bailey, Brian P.},
  year={2023},
  doi={10.1145/3610192},
  journal={Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Media / Communication / Entertainment,Operational,"Advising, Explaining","Decision-maker, Decision-subject","Change trust, Change cognitive demands, Change affective-perceptual, Alter decision outcomes",no such info,"explanations, system accuracy",user agency,"Conversational/Natural Language, Textual",Yes,Yes
2-3360,acm,“I Want It That Way”: Enabling Interactive Decision Support Using Large Language Models and Constraint Programming,"A critical factor in the success of many decision support systems is the accurate modeling of user preferences. Psychology research has demonstrated that users often develop their preferences during the elicitation process, highlighting the pivotal role of system-user interaction in developing personalized systems. This paper introduces a novel approach, combining Large Language Models (LLMs) with Constraint Programming to facilitate interactive decision support. We study this hybrid framework through the lens of meeting scheduling, a time-consuming daily activity faced by a multitude of information workers. We conduct three studies to evaluate the novel framework, including a diary study to characterize contextual scheduling preferences, a quantitative evaluation of the system’s performance, and a user study to elicit insights with a technology probe that encapsulates our framework. Our work highlights the potential for a hybrid LLM and optimization approach for iterative preference elicitation, and suggests design considerations for building systems that support human-system collaborative decision-making processes.",10.1145/3685053,https://doi.org/10.1145/3685053,ACM Transactions on Interactive Intelligent Systems,"Lawless, Connor; Schoeffer, Jakob; Le, Lindy; Rowan, Kael; Sen, Shilad; St. Hill, Cristina; Suh, Jina; Sarrafzadeh, Bahareh",2024,0,"@article{2-3360,
  title = {“I Want It That Way”: Enabling Interactive Decision Support Using Large Language Models and Constraint Programming},
  author = {Lawless, Connor and Schoeffer, Jakob and Le, Lindy and Rowan, Kael and Sen, Shilad and St. Hill, Cristina and Suh, Jina and Sarrafzadeh, Bahareh},
  year = {2024},
  doi = {10.1145/3685053},
  journal = {ACM Transactions on Interactive Intelligent Systems}
}",System/Artifact contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service",Individual,"Advising, Collaborating",Decision-maker,"Change affective-perceptual, Change trust, Change cognitive demands, Restrict human agency","Update AI competence, Change AI responses","AI suggestions, explanations",NA,Interactive interface,Yes,Yes
2-3362,acm,Ground(less) Truth: A Causal Framework for Proxy Labels in Human-Algorithm Decision-Making,"A growing literature on human-AI decision-making investigates strategies for combining human judgment with statistical models to improve decision-making. Research in this area often evaluates proposed improvements to models, interfaces, or workflows by demonstrating improved predictive performance on “ground truth’’ labels. However, this practice overlooks a key difference between human judgments and model predictions. Whereas humans commonly reason about broader phenomena of interest in a decision – including latent constructs that are not directly observable, such as disease status, the “toxicity” of online comments, or future “job performance” – predictive models target proxy labels that are readily available in existing datasets. Predictive models’ reliance on simplistic proxies for these nuanced phenomena makes them vulnerable to various sources of statistical bias. In this paper, we identify five sources of target variable bias that can impact the validity of proxy labels in human-AI decision-making tasks. We develop a causal framework to disentangle the relationship between each bias and clarify which are of concern in specific human-AI decision-making tasks. We demonstrate how our framework can be used to articulate implicit assumptions made in prior modeling work, and we recommend evaluation strategies for verifying whether these assumptions hold in practice. We then leverage our framework to re-examine the designs of prior human subjects experiments that investigate human-AI decision-making, finding that only a small fraction of studies examine factors related to target variable bias. We conclude by discussing opportunities to better address target variable bias in future research.",10.1145/3593013.3594036,https://doi.org/10.1145/3593013.3594036,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Guerdan, Luke; Coston, Amanda; Wu, Zhiwei Steven; Holstein, Kenneth",2023,49,"@inproceedings{2-3362,
  title = {Ground(less) Truth: A Causal Framework for Proxy Labels in Human-Algorithm Decision-Making},
  author = {Guerdan, Luke and Coston, Amanda and Wu, Zhiwei Steven and Holstein, Kenneth},
  year = {2023},
  doi = {10.1145/3593013.3594036},
  booktitle = {ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Institutional,"Forecasting, Advising","Decision-maker, Developer",no such info,no such info,"prediction of alternative, target variable bias",NA,Semi-Autonomous System,Yes,Yes
2-3363,acm,Evaluating a Learned Admission-Prediction Model as a Replacement for Standardized Tests in College Admissions,"A growing number of college applications has presented an annual challenge for college admissions in the United States. Admission offices have historically relied on standardized test scores to organize large applicant pools into viable subsets for review. However, this approach may be subject to bias in test scores and selection bias in test-taking with recent trends toward test-optional admission. We explore a machine learning-based approach to replace the role of standardized tests in subset generation while taking into account a wide range of factors extracted from student applications to support a more holistic review. We evaluate the approach on data from an undergraduate admission office at a selective US institution (13,248 applications). We find that a prediction model trained on past admission data outperforms an SAT-based heuristic and matches the demographic composition of the last admitted class. We discuss the risks and opportunities for how such a learned model could be leveraged to support human decision-making in college admissions.",10.1145/3573051.3593382,https://doi.org/10.1145/3573051.3593382,ACM Conference on Learning at Scale,"Lee, Hansol; Kizilcec, René F.; Joachims, Thorsten",2023,17,"@inproceedings{2-3363,
author = {Lee, Hansol and Kizilcec, Ren\'{e} F. and Joachims, Thorsten},
title = {Evaluating a Learned Admission-Prediction Model as a Replacement for Standardized Tests in College Admissions},
year = {2023},
url = {https://doi.org/10.1145/3573051.3593382},
doi = {10.1145/3573051.3593382},
pages = {195–203},
numpages = {9}
}",Empirical contributions,"Education / Teaching / Research, Everyday / Employment / Public Service",Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3364,acm,Would you do it?: Enacting Moral Dilemmas in Virtual Reality for Understanding Ethical Decision-Making,"A moral dilemma is a decision-making paradox without unambiguously acceptable or preferable options. This paper investigates if and how the virtual enactment of two renowned moral dilemmas—the Trolley and the Mad Bomber—influence decision-making when compared with mentally visualizing such situations. We conducted two user studies with two gender-balanced samples of 60 participants in total that compared between paper-based and virtual-reality (VR) conditions, while simulating 5 distinct scenarios for the Trolley dilemma, and 4 storyline scenarios for the Mad Bomber's dilemma. Our findings suggest that the VR enactment of moral dilemmas further fosters utilitarian decision-making, while it amplifies biases such as sparing juveniles and seeking retribution. Ultimately, we theorize that the VR enactment of renowned moral dilemmas can yield ecologically-valid data for training future Artificial Intelligence (AI) systems on ethical decision-making, and we elicit early design principles for the training of such systems.",10.1145/3313831.3376788,https://doi.org/10.1145/3313831.3376788,Conference on Human Factors in Computing Systems,"Niforatos, Evangelos; Palma, Adam; Gluszny, Roman; Vourvopoulos, Athanasios; Liarokapis, Fotis",2020,4,"@inproceedings{2-3364,
  title = {Would you do it?: Enacting Moral Dilemmas in Virtual Reality for Understanding Ethical Decision-Making},
  author = {Niforatos, Evangelos and Palma, Adam and Gluszny, Roman and Vourvopoulos, Athanasios and Liarokapis, Fotis},
  year = {2020},
  doi = {10.1145/3313831.3376788},
  booktitle = {Conference on Human Factors in Computing Systems}
}","Empirical contributions, Theoretical contributions",Generic / Abstract / Domain-agnostic,Institutional,"Analyzing, Advising",Decision-maker,NA,NA,NA,NA,"3D, Textual, Physical / Embodiment",Yes,No
2-3366,acm,Sociotechnical Systems and Ethics in the Large,"Advances in AI techniques and computing platforms have triggered a lively and expanding discourse on ethical decision making by autonomous agents. Much recent work in AI concentrates on the challenges of moral decision making from a decision-theoretic perspective, and especially the representation of various ethical dilemmas. Such approaches may be useful but in general are not productive because moral decision making is as context-driven as other forms of decision making, if not more. In contrast, we consider ethics not from the standpoint of an individual agent but of the wider sociotechnical systems (STS) in which the agent operates. Our contribution in this paper is the conception of ethical STS founded on governance that takes into account stakeholder values, normative constraints on agents, and outcomes (states of the STS) that obtain due to actions taken by agents. An important element of our conception is accountability, which is necessary for adequate consideration of outcomes that prima facie appear ethical or unethical. Focusing on STS provides a basis for tackling the difficult problems of ethics because the norms of an STS give an operational basis for agent decision making.",10.1145/3278721.3278740,https://doi.org/10.1145/3278721.3278740,"AAAI/ACM Conference on AI, Ethics, and Society","Chopra, Amit K.; SIngh, Munindar P.",2018,22,"@inproceedings{2-3366,
  title     = {Sociotechnical Systems and Ethics in the Large},
  author    = {Chopra, Amit K. and Singh, Munindar P.},
  year      = {2018},
  doi       = {10.1145/3278721.3278740},
  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Institutional,Executing,"Decision-maker, Guardian, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-3367,acm,A Human-AI Collaborative Approach for Clinical Decision Making on Rehabilitation Assessment,"Advances in artificial intelligence (AI) have made it increasingly applicable to supplement expert’s decision-making in the form of a decision support system on various tasks. For instance, an AI-based system can provide therapists quantitative analysis on patient’s status to improve practices of rehabilitation assessment. However, there is limited knowledge on the potential of these systems. In this paper, we present the development and evaluation of an interactive AI-based system that supports collaborative decision making with therapists for rehabilitation assessment. This system automatically identifies salient features of assessment to generate patient-specific analysis for therapists, and tunes with their feedback. In two evaluations with therapists, we found that our system supports therapists significantly higher agreement on assessment (0.71 average F1-score) than a traditional system without analysis (0.66 average F1-score, p &lt; 0.05). After tuning with therapist’s feedback, our system significantly improves its performance from 0.8377 to 0.9116 average F1-scores (p &lt; 0.01). This work discusses the potential of a human-AI collaborative system to support more accurate decision making while learning from each other’s strengths.",10.1145/3411764.3445472,https://doi.org/10.1145/3411764.3445472,CHI Conference on Human Factors in Computing Systems,"Lee, Min Hun; Siewiorek, Daniel P.; Smailagic, Asim; Bernardino, Alexandre; Bermúdez i Badia, Sergi Bermúdez",2021,186,"@inproceedings{2-3367,
  title     = {A Human-AI Collaborative Approach for Clinical Decision Making on Rehabilitation Assessment},
  author    = {Lee, Min Hun and Siewiorek, Daniel P. and Smailagic, Asim and Bernardino, Alexandre and Bermúdez i Badia, Sergi},
  year      = {2021},
  doi       = {10.1145/3411764.3445472},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Collaborating","Decision-maker, Knowledge provider","Change cognitive demands, Alter decision outcomes","Update AI competence, Change AI responses",patient-specific analysis for therapists,"simple feedback, corrective feedback",Interactive interface,Yes,Yes
2-33677,springernature,Structured reports of videofluoroscopic swallowing studies have the potential to improve overall report quality compared to free text reports,"Purpose To compare free text( FTR) and structured reports( SR) of videofluoroscopic swallowing studies( VFSS) and evaluate satisfaction of referring otolaryngologists and speech therapists. Materials and methods Both standard FTR and SR of 26 patients with VFSS were acquired. A dedicated template focusing on oropharyngeal phases was created for SR using online software with clickable decision-trees and concomitant generation of semantically structured reports. All reports were evaluated regarding overall quality and content, information extraction and clinical decision support( 10-point Likert scale( 0 = I completely disagree, 10 = I completely agree) ). Results Two otorhinolaryngologists and two speech therapists evaluated FTR and SR. SR received better ratings than FTR in all items. SR were perceived to contain more details on the swallowing phases( median rating: 10 vs. 5; P < 0. 001) , penetration and aspiration( 10 vs. 5; P < 0. 001) and facilitated information extraction compared to FTR( 10 vs. 4; P < 0. 001). Overall quality was rated significantly higher in SR than FTR( P < 0. 001). Conclusion SR of VFSS provide more detailed information and facilitate information extraction. SR better assist in clinical decision-making, might enhance the quality of the report and, thus, are recommended for the evaluation of VFSS. Key Points • Structured reports on videofluoroscopic exams of deglutition lead to improved report quality. • Information extraction is facilitated when using structured reports based on decision trees. • Template-based reports add more value to clinical decision-making than free text reports. • Structured reports receive better ratings by speech therapists and otolaryngologists. • Structured reports on videofluoroscopic exams may improve the comparability between exams.",10.1007/s00330-017-4971-0,http://dx.doi.org/10.1007/s00330-017-4971-0,European Radiology,"Schoeppe, Franziska;Sommer, Wieland H.;Haack, Mareike;Havel, Miriam;Rheinwald, Marika;Wechtenbruch, Juliane;Fischer, Martin R.;Meinel, Felix G.;Sabel, Bastian O.;Sommer, Nora N.",2018,0,"@article{2-33677,
  title={Structured reports of videofluoroscopic swallowing studies have the potential to improve overall report quality compared to free text reports},
  author={Schoeppe, Franziska and Sommer, Wieland H. and Haack, Mareike and Havel, Miriam and Rheinwald, Marika and Wechtenbruch, Juliane and Fischer, Martin R. and Meinel, Felix G. and Sabel, Bastian O. and Sommer, Nora N.},
  year={2018},
  doi={10.1007/s00330-017-4971-0},
  journal={European Radiology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Decision-maker, Guardian",NA,NA,procedural instructions,NA,NA,Yes,No
2-3368,acm,Understanding the Role of Explanation Modality in AI-assisted Decision-making,"Advances in artificial intelligence and machine learning have led to a steep rise in the adoption of AI to augment or support human decision-making across domains. There has been an increasing body of work addressing the benefits of model interpretability and explanations to help end-users or other stakeholders decipher the inner workings of the so-called ”black box AI systems”. Yet, little is currently understood about the role of modalities through which explanations can be communicated (e.g., text, visualizations, or audio) to inform, augment, and shape human decision-making. In our work, we address this research gap through the lens of a credibility assessment system. Considering the deluge of information available through various channels, people constantly make decisions while considering the perceived credibility of the information they consume. However, with an increasing information overload, assessing the credibility of the information we encounter is a non-trivial task. To help users in this task, automated credibility assessment systems have been devised as decision support systems in various contexts (e.g., assessing the credibility of news or social media posts). However, for these systems to be effective in supporting users, they need to be trusted and understood. Explanations have been shown to play an essential role in informing users’ reliance on decision support systems. In this paper, we investigate the influence of explanation modalities on an AI-assisted credibility assessment task. We use a between-subjects experiment (N = 375), spanning six different explanation modalities, to evaluate the role of explanation modality on the accuracy of AI-assisted decision outcomes, the perceived system trust among users, and system usability. Our results indicate that explanations play a significant role in shaping users’ reliance on the decision support system and, thereby, the accuracy of decisions made. We found that users performed with higher accuracy while assessing the credibility of statements in the presence of explanations. We also found that users had a significantly harder time agreeing on statement credibility without explanations. With explanations present, text and audio explanations were more effective than graphic explanations. Additionally, we found that combining graphical with text and/or audio explanations were significantly effective. Such combinations of modalities led to a higher user performance than using graphical explanations alone.",10.1145/3503252.3531311,https://doi.org/10.1145/3503252.3531311,"ACM Conference on User Modeling, Adaptation and Personalization (UMAP)","Robbemond, Vincent; Inel, Oana; Gadiraju, Ujwal",2022,0,"@inproceedings{2-3368,
  title={Understanding the Role of Explanation Modality in AI-assisted Decision-making},
  author={Robbemond, Vincent and Inel, Oana and Gadiraju, Ujwal},
  year={2022},
  doi={10.1145/3503252.3531311},
  booktitle={Proceedings of the ACM Conference on User Modeling, Adaptation and Personalization (UMAP)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Media / Communication / Entertainment",Individual,"Advising, Explaining","Decision-maker, Stakeholder","Change affective-perceptual, Alter decision outcomes, Change trust, Change cognitive demands",no such info,"visual explanations, textual explanations, audio explanations",NA,"Textual, Visual, Auditory",Yes,Yes
2-3369,acm,Appropriate Reliance on AI Advice: Conceptualization and the Effect of Explanations,"AI advice is becoming increasingly popular, e.g., in investment and medical treatment decisions. As this advice is typically imperfect, decision-makers have to exert discretion as to whether actually follow that advice: they have to “appropriately” rely on correct and turn down incorrect advice. However, current research on appropriate reliance still lacks a common definition as well as an operational measurement concept. Additionally, no in-depth behavioral experiments have been conducted that help understand the factors influencing this behavior. In this paper, we propose Appropriateness of Reliance (AoR) as an underlying, quantifiable two-dimensional measurement concept. We develop a research model that analyzes the effect of providing explanations for AI advice. In an experiment with 200 participants, we demonstrate how these explanations influence the AoR, and, thus, the effectiveness of AI advice. Our work contributes fundamental concepts for the analysis of reliance behavior and the purposeful design of AI advisors.",10.1145/3581641.3584066,https://doi.org/10.1145/3581641.3584066,ACM International Conference on Intelligent User Interfaces (IUI),"Schemmer, Max; Kuehl, Niklas; Benz, Carina; Bartos, Andrea; Satzger, Gerhard",2023,65,"@inproceedings{2-3369,
  title     = {Appropriate Reliance on {AI} Advice: Conceptualization and the Effect of Explanations},
  author    = {Schemmer, Max and Kuehl, Niklas and Benz, Carina and Bartos, Andrea and Satzger, Gerhard},
  year      = {2023},
  doi       = {10.1145/3581641.3584066},
  booktitle = {Proceedings of the ACM International Conference on Intelligent User Interfaces (IUI)}
}","Methodological contributions, Theoretical contributions",Generic / Abstract / Domain-agnostic,Operational,Advising,Decision-maker,"Change trust, Change affective-perceptual, Alter decision outcomes",no such info,"LIME feature importance explanations, AI advice",NA,"Textual, Visual, Conversational/Natural Language",Yes,Yes
2-3371,acm,Does More Advice Help? The Effects of Second Opinions in AI-Assisted Decision Making,"AI assistance in decision-making has become popular, yet people's inappropriate reliance on AI often leads to unsatisfactory human-AI collaboration performance. In this paper, through three pre-registered, randomized human subject experiments, we explore whether and how the provision of second opinions may affect decision-makers' behavior and performance in AI-assisted decision-making. We find that if both the AI model's decision recommendation and a second opinion are always presented together, decision-makers reduce their over-reliance on AI while increase their under-reliance on AI, regardless whether the second opinion is generated by a peer or another AI model. However, if decision-makers have the control to decide when to solicit a peer's second opinion, we find that their active solicitations of second opinions have the potential to mitigate over-reliance on AI without inducing increased under-reliance in some cases. We conclude by discussing the implications of our findings for promoting effective human-AI collaborations in decision-making.",10.1145/3653708,https://doi.org/10.1145/3653708,Proceedings of the ACM on Human-Computer Interaction,"Lu, Zhuoran; Wang, Dakuo; Yin, Ming",2024,36,"@article{2-3371,
  title = {Does More Advice Help? The Effects of Second Opinions in AI-Assisted Decision Making},
  author = {Lu, Zhuoran and Wang, Dakuo and Yin, Ming},
  year = {2024},
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  doi = {10.1145/3653708}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Advising, Collaborating",Decision-maker,"Alter decision outcomes, Change trust, Change cognitive demands",no such info,"recommendations, second opinion presentation",second opinion presentation,Textual,Yes,Yes
2-3373,acm,Understanding the Role of Human Intuition on Reliance in Human-AI Decision-Making with Explanations,"AI explanations are often mentioned as a way to improve human-AI decision-making, but empirical studies have not found consistent evidence of explanations' effectiveness and, on the contrary, suggest that they can increase overreliance when the AI system is wrong. While many factors may affect reliance on AI support, one important factor is how decision-makers reconcile their own intuition—beliefs or heuristics, based on prior knowledge, experience, or pattern recognition, used to make judgments—with the information provided by the AI system to determine when to override AI predictions. We conduct a think-aloud, mixed-methods study with two explanation types (feature- and example-based) for two prediction tasks to explore how decision-makers' intuition affects their use of AI predictions and explanations, and ultimately their choice of when to rely on AI. Our results identify three types of intuition involved in reasoning about AI predictions and explanations: intuition about the task outcome, features, and AI limitations. Building on these, we summarize three observed pathways for decision-makers to apply their own intuition and override AI predictions. We use these pathways to explain why (1) the feature-based explanations we used did not improve participants' decision outcomes and increased their overreliance on AI, and (2) the example-based explanations we used improved decision-makers' performance over feature-based explanations and helped achieve complementary human-AI performance. Overall, our work identifies directions for further development of AI decision-support systems and explanation methods that help decision-makers effectively apply their intuition to achieve appropriate reliance on AI.",10.1145/3610219,https://doi.org/10.1145/3610219,Proceedings of the ACM on Human-Computer Interaction,"Chen, Valerie; Liao, Q. Vera; Wortman Vaughan, Jennifer; Bansal, Gagan",2023,216,"@article{2-3373,
  title={Understanding the Role of Human Intuition on Reliance in Human-AI Decision-Making with Explanations},
  author={Chen, Valerie and Liao, Q. Vera and Wortman Vaughan, Jennifer and Bansal, Gagan},
  year={2023},
  doi={10.1145/3610219},
  journal={Proceedings of the ACM on Human-Computer Interaction}
}","Empirical contributions, Theoretical contributions",Generic / Abstract / Domain-agnostic,Institutional,"Explaining, Forecasting, Advising",Decision-maker,Alter decision outcomes,no such info,"textual explanations, prediction of alternative",intuition,"Textual, Conversational/Natural Language",Yes,Yes
2-3374,acm,Watch Out for Updates: Understanding the Effects of Model Explanation Updates in AI-Assisted Decision Making,"AI explanations have been increasingly used to help people better utilize AI recommendations in AI-assisted decision making. While AI explanations may change over time due to updates of the AI model, little is known about how these changes may affect people’s perceptions and usage of the model. In this paper, we study how varying levels of similarity between the AI explanations before and after a model update affects people’s trust in and satisfaction with the AI model. We conduct randomized human-subject experiments on two decision making contexts where people have different levels of domain knowledge. Our results show that changes in AI explanation during the model update do not affect people’s tendency to adopt AI recommendations. However, they may change people’s subjective trust in and satisfaction with the AI model via changing both their perceived model accuracy and perceived consistency of AI explanations with their prior knowledge.",10.1145/3544548.3581366,https://doi.org/10.1145/3544548.3581366,ACM CHI Conference on Human Factors in Computing Systems,"Wang, Xinru; Yin, Ming",2023,49,"@inproceedings{2-3374,
  title = {Watch Out for Updates: Understanding the Effects of Model Explanation Updates in AI-Assisted Decision Making},
  author = {Wang, Xinru and Yin, Ming},
  year = {2023},
  doi = {10.1145/3544548.3581366},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,no such info,"Explaining, Advising",Decision-maker,"Alter decision outcomes, Change trust, Change affective-perceptual",no such info,"textual explanations, varying levels of similarity between the AI explanations",domain knowledge,"Textual, Conversational/Natural Language",Yes,Yes
2-3375,acm,Beyond Recommendations: From Backward to Forward AI Support of Pilots' Decision-Making Process,"AI is anticipated to enhance human decision-making in high-stakes domains like aviation, but adoption is often hindered by challenges such as inappropriate reliance and poor alignment with users' decision-making. Recent research suggests that a core underlying issue is the recommendation-centric design of many AI systems, i.e., they give end-to-end recommendations and ignore the rest of the decision-making process. Alternative support paradigms are rare, and it remains unclear how the few that do exist compare to recommendation-centric support. In this work, we aimed to empirically compare recommendation-centric support to an alternative paradigm, continuous support, in the context of diversions in aviation. We conducted a mixed-methods study with 32 professional pilots in a realistic setting. To ensure the quality of our study scenarios, we conducted a focus group with four additional pilots prior to the study. We found that continuous support can support pilots' decision-making in a forward direction, allowing them to think more beyond the limits of the system and make faster decisions when combined with recommendations, though the forward support can be disrupted. Participants' statements further suggest a shift in design goal away from providing recommendations, to supporting quick information gathering. Our results show ways to design more helpful and effective AI decision support that goes beyond end-to-end recommendations.",10.1145/3687024,https://doi.org/10.1145/3687024,Proceedings of the ACM on Human-Computer Interaction,"Zhang, Zelun Tony; Feger, Sebastian S.; Dullenkopf, Lucas; Liao, Rulu; Süsslin, Lou; Liu, Yuanting; Butz, Andreas",2024,15,"@article{2-3375,
  title = {Beyond Recommendations: From Backward to Forward AI Support of Pilots' Decision-Making Process},
  author = {Zhang, Zelun Tony and Feger, Sebastian S. and Dullenkopf, Lucas and Liao, Rulu and S{\""u}sslin, Lou and Liu, Yuanting and Butz, Andreas},
  year = {2024},
  doi = {10.1145/3687024},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Transportation / Mobility / Planning,Operational,"Advising, Collaborating",Decision-maker,"Alter decision outcomes, Change cognitive demands, Change trust",Change AI responses,"recommendation-centric support, (continuous) support",NA,Interactive interface,Yes,Yes
2-3377,acm,How Stated Accuracy of an AI System and Analogies to Explain Accuracy Affect Human Reliance on the System,"AI systems are increasingly being used to support human decision making. It is important that AI advice is followed appropriately. However, according to existing literature, users typically under-rely or over-rely on AI systems, and this leads to sub-optimal team performance. In this context, we investigate the role of stated system accuracy by contrasting the lack of system information with the presence of system accuracy in a loan prediction task. We explore how the degree to which humans understand system accuracy influences their reliance on the AI system, by investigating numeracy levels and with the aid of analogies to explain system accuracy in a first of its kind between-subjects study (N=281). We found that explaining the stated accuracy of a system using analogies failed to help users rely on the AI systemappropriately (i.e., the tendency of users to rely on the system when the system is correct, or on themselves otherwise). To eliminate the impact of subjective attitudes towards analogy domains, we conducted a within-subjects study (N=248) where each participant worked on tasks with analogy-based explanations from different domains. Results from this second study confirmed that explaining stated accuracy of the system with analogies was not sufficient to facilitate appropriate reliance on the AI system in the context of loan prediction tasks, irrespective of individual user differences. Based on our findings from the two studies, we reason that the under-reliance on the AI system may be a result of users' overestimation of their own ability to solve the given task. Thus, although familiar analogies can be effective in improving the intelligibility of stated accuracy of the system, an improved understanding of system accuracy does not necessarily lead to improved system reliance and team performance.",10.1145/3610067,https://doi.org/10.1145/3610067,Proceedings of the ACM on Human-Computer Interaction,"He, Gaole; Buijsman, Stefan; Gadiraju, Ujwal",2023,36,"@article{2-3377,
  title = {How Stated Accuracy of an AI System and Analogies to Explain Accuracy Affect Human Reliance on the System},
  author = {He, Gaole and Buijsman, Stefan and Gadiraju, Ujwal},
  year = {2023},
  doi = {10.1145/3610067},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Operational,"Forecasting, Advising, Explaining",Decision-maker,"Change trust, Change affective-perceptual, Alter decision outcomes",Update AI competence,"system accuracy, prediction of alternative",NA,"Textual, Conversational/Natural Language",Yes,Yes
2-3379,acm,The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features,"AI systems have been known to amplify biases in real-world data. Explanations may help human-AI teams address these biases for fairer decision-making. Typically, explanations focus on salient input features. If a model is biased against some protected group, explanations may include features that demonstrate this bias, but when biases are realized through proxy features, the relationship between this proxy feature and the protected one may be less clear to a human. In this work, we study the effect of the presence of protected and proxy features on participants’ perception of model fairness and their ability to improve demographic parity over an AI alone. Further, we examine how different treatments—explanations, model bias disclosure and proxy correlation disclosure—affect fairness perception and parity. We find that explanations help people detect direct but not indirect biases. Additionally, regardless of bias type, explanations tend to increase agreement with model biases. Disclosures can help mitigate this effect for indirect biases, improving both unfairness recognition and decision-making fairness. We hope that our findings can help guide further research into advancing explanations in support of fair human-AI decision-making.",10.1145/3640543.3645210,https://doi.org/10.1145/3640543.3645210,International Conference on Intelligent User Interfaces (IUI),"Goyal, Navita; Baumler, Connor; Nguyen, Tin; Daumé III, Hal",2024,0,"@inproceedings{2-3379,
  title = {The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features},
  author = {Goyal, Navita and Baumler, Connor and Nguyen, Tin and Daumé III, Hal},
  year = {2024},
  doi = {10.1145/3640543.3645210},
  booktitle = {International Conference on Intelligent User Interfaces (IUI)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Operational,"Explaining, Advising","Decision-subject, Decision-maker","Shape ethical norms, Alter decision outcomes, Change trust",Change AI responses,"protected/proxy features, textual explanations, bias disclosure, proxy feature correlation disclosure",NA,"Textual, Conversational/Natural Language",Yes,Yes
2-3382,acm,Play for Real(ism) - Using Games to Predict Human-AI interactions in the Real World,"AI-enabled decision support systems have repeatedly failed in real world applications despite the underlying model operating as designed. Often this was because the system was used in an unexpected manner. Our goal is to enable better prediction of how systems will be used prior to their implementation as well as to improve existing designs, by taking human behavior into account. There are several challenges to collecting such data. Not having access to an existing prediction engine requires the simulation of such a system's behavior. This simulation must include not just the behavior of the underlying model but also the context in which the decision will be made in the real world. Additionally, collecting statistically valid samples requires that test subjects make repeated choices under slightly varied conditions. Unfortunately, in such repetitious conditions fatigue can quickly set in. Games provide us the ability to address both of these challenges by providing both systems context and narrative context. Systems context can be used to convey some or all of the information the player needs to make a decision in the game environment itself, which can help avoid the onset of fatigue. Narrative context can provide a broader environment within which the simulated system operates, adding a sense of progress, showing the effect of decisions, adding perceived social norms, and setting incentives and stakes. This broader environment can further prevent player fatigue while replicating many of the external factors that might affect choices in the real world. In this paper we describe the design of the Human-AI Decision Evaluation System (HADES), a test harness capable of interfacing with a game environment, simulating the behavior of an AI-enabled decision support system, and collecting the results of human decision making based upon such a system's predictions. Additionally, we present an analysis of data collected by HADES while interfaced with a visual novel game focused on software cyber-risk assessment.",10.1145/3474655,https://doi.org/10.1145/3474655,Proceedings of the ACM on Human-Computer Interaction,"Guttman, Rotem D.; Hammer, Jessica; Harpstead, Erik; Smith, Carol J.",2021,11,"@article{2-3382,
  title={Play for Real(ism) - Using Games to Predict Human-AI interactions in the Real World},
  author={Guttman, Rotem D. and Hammer, Jessica and Harpstead, Erik and Smith, Carol J.},
  year={2021},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  doi={10.1145/3474655}
}",System/Artifact contributions,Generic / Abstract / Domain-agnostic,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3383,acm,Designing for Control in Nurse-AI Collaboration During Emergency Medical Calls,"AI-powered symptom checkers are automating the work of telephone triage nurses in assessing patient urgency. Yet, these systems exclude several vulnerable patient groups and overlook telenurses’ competent interaction with their patients. This study, conducted in collaboration with telenurses, examines how AI can support their clinical assessment and was carried out in four phases: 1) interviews that revealed telenurses’ challenge of juggling decision-support and documentation interfaces, 2) a co-design workshop that conceptualized continuous nurse-AI interaction, 3) development of a prototype that suggested questions for nurses to ask callers, and 4) a role-play workshop that demonstrated nurse-AI interaction in practice. The study addresses how we can design for control in human-AI collaboration in order to enhance, rather than replace, human decision-making processes.",10.1145/3563657.3596110,https://doi.org/10.1145/3563657.3596110,ACM Designing Interactive Systems Conference,"Berge, Arngeir; Guribye, Frode; Fotland, Siri-Linn Schmidt; Fonnes, Gro; Johansen, Ingrid H.; Trattner, Christoph",2023,17,"@inproceedings{2-3383,
  title = {Designing for Control in Nurse-AI Collaboration During Emergency Medical Calls},
  author = {Berge, Arngeir and Guribye, Frode and Fotland, Siri-Linn Schmidt and Fonnes, Gro and Johansen, Ingrid H. and Trattner, Christoph},
  year = {2023},
  doi = {10.1145/3563657.3596110},
  booktitle = {Proceedings of the ACM Designing Interactive Systems Conference}
}","Empirical contributions, System/Artifact contributions",Healthcare / Medicine / Surgery,Organizational,"Advising, Analyzing, Collaborating","Decision-maker, Developer","Change trust, Alter decision outcomes, Change cognitive demands, Restrict human agency","Change AI responses, Update AI competence","recommendations, AI suggestions",documentation,Interactive interface,Yes,Yes
2-3385,acm,"Implications of AI (un-)fairness in higher education admissions: the effects of perceived AI (un-)fairness on exit, voice and organizational reputation","Algorithmic decision-making (ADM) is becoming increasingly important in all areas of social life. In higher education, machine-learning systems have manifold uses because they can efficiently process large amounts of student data and use these data to arrive at effective decisions. Despite the potential upsides of ADM systems, fairness concerns are gaining momentum in academic and public discourses. The criticism largely focuses on the disparate effects of ADM. That is, algorithms may not serve as objective and fair decision-makers but, rather, reproduce biases existing within the respective training data. This study adopted a different approach by focusing on individual perceptions of fairness. Specifically, we looked at two different dimensions of perceived fairness: (i) procedural fairness and (ii) distributive fairness. Using cross-sectional survey data (n = 304) from a large German university, we tested whether students' assessments of fairness differ with respect to algorithmic vs. human decision-making (HDM) within the higher education context. Furthermore, we investigated whether fairness perceptions have subsequent effects on three different outcome variables, which are hugely important for universities: (1) exit, (2) voice, and (3) organizational reputation. The results of our survey suggest that participants evaluated ADM higher than HDM in terms of both procedural and distributive fairness. Concerning the subsequent effects of fairness perceptions, we find that (1) distributive fairness as well as procedural fairness perceptions have a negative impact on the intention to protest against an ADM system, whereas (2) only procedural fairness perceptions negatively affect the likelihood of exiting. Finally, (3) distributive fairness, but not procedural fairness perceptions have a positive effect on organizational reputation. For universities aiming to implement ADM systems, it is crucial, therefore, to take possible fairness issues and their further implications into account.",10.1145/3351095.3372867,https://doi.org/10.1145/3351095.3372867,"ACM Conference on Fairness, Accountability, and Transparency (FAccT)","Marcinkowski, Frank; Kieslich, Kimon; Starke, Christopher; Lünich, Marco",2020,0,"@inproceedings{2-3385,
  title={Implications of AI (un-)fairness in higher education admissions: the effects of perceived AI (un-)fairness on exit, voice and organizational reputation},
  author={Marcinkowski, Frank and Kieslich, Kimon and Starke, Christopher and L{\""u}nich, Marco},
  year={2020},
  doi={10.1145/3351095.3372867},
  booktitle={ACM Conference on Fairness, Accountability, and Transparency (FAccT)}
}",Empirical contributions,Everyday / Employment / Public Service,Operational,Executing,"Decision-maker, Decision-subject","Shape ethical norms, Alter decision outcomes",Shape AI for accountability,"biased training data, recommendations",NA,"Textual, Conversational/Natural Language",Yes,Yes
2-3388,acm,Understanding the Effect of Out-of-distribution Examples and Interactive Explanations on Human-AI Decision Making,"Although AI holds promise for improving human decision making in societally critical domains, it remains an open question how human-AI teams can reliably outperform AI alone and human alone in challenging prediction tasks (also known as complementary performance). We explore two directions to understand the gaps in achieving complementary performance. First, we argue that the typical experimental setup limits the potential of human-AI teams. To account for lower AI performance out-of-distribution than in-distribution because of distribution shift, we design experiments with different distribution types and investigate human performance for both in-distribution and out-of-distribution examples. Second, we develop novel interfaces to support interactive explanations so that humans can actively engage with AI assistance. Using virtual pilot studies and large-scale randomized experiments across three tasks, we demonstrate a clear difference between in-distribution and out-of-distribution, and observe mixed results for interactive explanations: while interactive explanations improve human perception of AI assistance's usefulness, they may reinforce human biases and lead to limited performance improvement. Overall, our work points out critical challenges and future directions towards enhancing human performance with AI assistance.",10.1145/3479552,https://doi.org/10.1145/3479552,Proceedings of the ACM on Human-Computer Interaction,"Liu, Han; Lai, Vivian; Tan, Chenhao",2021,208,"@article{2-3388,
  title={Understanding the Effect of Out-of-distribution Examples and Interactive Explanations on Human-AI Decision Making},
  author={Liu, Han and Lai, Vivian and Tan, Chenhao},
  year={2021},
  doi={10.1145/3479552},
  journal={Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Explaining, Advising, Forecasting",Decision-maker,"Alter decision outcomes, Change trust, Change affective-perceptual",no such info,"interactive explanations, real-time assistance, static assistance",NA,Interactive interface,Yes,Yes
2-3389,acm,"""Hello AI"": Uncovering the Onboarding Needs of Medical Practitioners for Human-AI Collaborative Decision-Making","Although rapid advances in machine learning have made it increasingly applicable to expert decision-making, the delivery of accurate algorithmic predictions alone is insufficient for effective human-AI collaboration. In this work, we investigate the key types of information medical experts desire when they are first introduced to a diagnostic AI assistant. In a qualitative lab study, we interviewed 21 pathologists before, during, and after being presented deep neural network (DNN) predictions for prostate cancer diagnosis, to learn the types of information that they desired about the AI assistant. Our findings reveal that, far beyond understanding the local, case-specific reasoning behind any model decision, clinicians desired upfront information about basic, global properties of the model, such as its known strengths and limitations, its subjective point-of-view, and its overall design objective--what it's designed to be optimized for. Participants compared these information needs to the collaborative mental models they develop of their medical colleagues when seeking a second opinion: the medical perspectives and standards that those colleagues embody, and the compatibility of those perspectives with their own diagnostic patterns. These findings broaden and enrich discussions surrounding AI transparency for collaborative decision-making, providing a richer understanding of what experts find important in their introduction to AI assistants before integrating them into routine practice.",10.1145/3359206,https://doi.org/10.1145/3359206,Proceedings of the ACM on Human-Computer Interaction,"Cai, Carrie J.; Winter, Samantha; Steiner, David; Wilcox, Lauren; Terry, Michael",2019,648,"@article{2-3389,
  title = {Hello AI: Uncovering the Onboarding Needs of Medical Practitioners for Human-AI Collaborative Decision-Making},
  author = {Cai, Carrie J. and Winter, Samantha and Steiner, David and Wilcox, Lauren and Terry, Michael},
  year = {2019},
  doi = {10.1145/3359206},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising, Collaborating","Decision-maker, Knowledge provider","Change cognitive demands, Change trust","Update AI competence, Change AI responses","known strengths and limitations of AI, subjective point-of-view and overall design objective of AI, prediction of alternative",NA,Visual,Yes,Yes
2-3391,acm,"Designing for Appropriate Reliance: The Roles of AI Uncertainty Presentation, Initial User Decision, and User Demographics in AI-Assisted Decision-Making","Appropriate reliance is critical to achieving synergistic human-AI collaboration. For instance, when users over-rely on AI assistance, their human-AI team performance is bounded by the model's capability. This work studies how the presentation of model uncertainty may steer users' decision-making toward fostering appropriate reliance. Our results demonstrate that showing the calibrated model uncertainty alone is inadequate. Rather, calibrating model uncertainty and presenting it in a frequency format allow users to adjust their reliance accordingly and help reduce the effect of confirmation bias on their decisions. Furthermore, the critical nature of our skin cancer screening task skews participants' judgment, causing their reliance to vary depending on their initial decision. Additionally, step-wise multiple regression analyses revealed how user demographics such as age and familiarity with probability and statistics influence human-AI collaborative decision-making. We discuss the potential for model uncertainty presentation, initial user decision, and user demographics to be incorporated in designing personalized AI aids for appropriate reliance.",10.1145/3637318,https://doi.org/10.1145/3637318,Proceedings of the ACM on Human-Computer Interaction,"Cao, Shiye; Liu, Anqi; Huang, Chien-Ming",2024,28,"@article{2-3391,
  title = {Designing for Appropriate Reliance: The Roles of AI Uncertainty Presentation, Initial User Decision, and User Demographics in AI-Assisted Decision-Making},
  author = {Cao, Shiye and Liu, Anqi and Huang, Chien-Ming},
  year = {2024},
  doi = {10.1145/3637318},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject","Change trust, Change affective-perceptual, Alter decision outcomes",no such info,uncertainty,contextual knowledge,Textual,Yes,Yes
2-33919,springernature,Increasing transparency of computer-aided detection impairs decision-making in visual search,"Recent developments in artificial intelligence( AI) have led to changes in healthcare. Government and regulatory bodies have advocated the need for transparency in AI systems with recommendations to provide users with more details about AI accuracy and how AI systems work. However, increased transparency could lead to negative outcomes if humans become overreliant on the technology. This study investigated how changes in AI transparency affected human decision-making in a medical-screening visual search task. Transparency was manipulated by either giving or withholding knowledge about the accuracy of an ‘AI system’. We tested performance in seven simulated lab mammography tasks, in which observers searched for a cancer which could be correctly or incorrectly flagged by computer-aided detection( CAD) ‘AI prompts’. Across tasks, the CAD systems varied in accuracy. In the ‘transparent’ condition, participants were told the accuracy of the CAD system, in the ‘not transparent’ condition, they were not. The results showed that increasing CAD transparency impaired task performance, producing an increase in false alarms, decreased sensitivity, an increase in recall rate, and a decrease in positive predictive value. Along with increasing investment in AI, this research shows that it is important to investigate how transparency of AI systems affect human decision-making. Increased transparency may lead to overtrust in AI systems, which can impact clinical outcomes.",10.3758/s13423-024-02601-5,http://dx.doi.org/10.3758/s13423-024-02601-5,Psychonomic Bulletin & Review,"Kunar, Melina A.;Montana, Giovanni;Watson, Derrick G.",2024,2,"@article{2-33919,
  title     = {Increasing transparency of computer-aided detection impairs decision-making in visual search},
  author    = {Kunar, Melina A. and Montana, Giovanni and Watson, Derrick G.},
  year      = {2024},
  journal   = {Psychonomic Bulletin \& Review},
  doi       = {10.3758/s13423-024-02601-5}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Explaining","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3392,acm,"Towards Efficient Annotations for a Human-AI Collaborative, Clinical Decision Support System: A Case Study on Physical Stroke Rehabilitation Assessment","Artificial intelligence (AI) and machine learning (ML) algorithms are increasingly being explored to support various decision-making tasks in health (e.g. rehabilitation assessment). However, the development of such AI/ML-based decision support systems is challenging due to the expensive process to collect an annotated dataset. In this paper, we describe the development process of a human-AI collaborative, clinical decision support system that augments an ML model with a rule-based (RB) model from domain experts. We conducted its empirical evaluation in the context of assessing physical stroke rehabilitation with the dataset of three exercises from 15 post-stroke survivors and therapists. Our results bring new insights on the efficient development and annotations of a decision support system: when an annotated dataset is not available initially, the RB model can be used to assess post-stroke survivor’s quality of motion and identify samples with low confidence scores to support efficient annotations for training an ML model. Specifically, our system requires only 22 - 33% of annotations from therapists to train an ML model that achieves equally good performance with an ML model with all annotations from a therapist. Our work discusses the values of a human-AI collaborative approach for effectively collecting an annotated dataset and supporting a complex decision-making task.",10.1145/3490099.3511112,https://doi.org/10.1145/3490099.3511112,International Conference on Intelligent User Interfaces (IUI),"Lee, Min Hun; Siewiorek, Daniel P.; Smailagic, Asim; Bernardino, Alexandre; Bermúdez i Badia, Sergi",2022,21,"@inproceedings{2-3392,
  title = {Towards Efficient Annotations for a Human-AI Collaborative, Clinical Decision Support System: A Case Study on Physical Stroke Rehabilitation Assessment},
  author = {Lee, Min Hun and Siewiorek, Daniel P. and Smailagic, Asim and Bernardino, Alexandre and Bermúdez i Badia, Sergi},
  year = {2022},
  doi = {10.1145/3490099.3511112},
  booktitle = {International Conference on Intelligent User Interfaces (IUI)}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Knowledge provider, Decision-subject",no such info,"Update AI competence, Change AI responses",confidence score,NA,"Visual, Autonomous System, Interactive interface",Yes,Yes
2-3393,acm,Ambiguity-aware AI Assistants for Medical Data Analysis,"Artificial intelligence (AI) assistants for clinical decision making show increasing promise in medicine. However, medical assessments can be contentious, leading to expert disagreement. This raises the question of how AI assistants should be designed to handle the classification of ambiguous cases. Our study compared two AI assistants that provide classification labels for medical time series data along with quantitative uncertainty estimates: conventional vs. ambiguity-aware. We simulated our ambiguity-aware AI based on real-world expert discussions to highlight cases likely to lead to expert disagreement, and to present arguments for conflicting classification choices. Our results demonstrate that ambiguity-aware AI can alter expert workflows by significantly increasing the proportion of contentious cases reviewed. We also found that the relevance of AI-provided arguments (selected from guidelines either randomly or by experts) affected experts' accuracy at revising AI-suggested labels. Our work contributes a novel perspective on the design of AI for contentious clinical assessments.",10.1145/3313831.3376506,https://doi.org/10.1145/3313831.3376506,Conference on Human Factors in Computing Systems,"Schaekermann, Mike; Beaton, Graeme; Sanoubari, Elaheh; Lim, Andrew; Larson, Kate; Law, Edith",2020,76,"@inproceedings{2-3393,
  title = {Ambiguity-aware AI Assistants for Medical Data Analysis},
  author = {Schaekermann, Mike and Beaton, Graeme and Sanoubari, Elaheh and Lim, Andrew and Larson, Kate and Law, Edith},
  year = {2020},
  doi = {10.1145/3313831.3376506},
  booktitle = {Conference on Human Factors in Computing Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting, Explaining","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-3395,acm,Public Health Calls for/with AI: An Ethnographic Perspective,"Artificial Intelligence (AI) based technologies are increasingly being integrated into public sector programs to help with decision-support and effective distribution of constrained resources. The field of Computer Supported Cooperative Work (CSCW) has begun to examine how the resultant sociotechnical systems may be designed appropriately when targeting underserved populations. We present an ethnographic study of a large-scale real-world integration of an AI system for resource allocation in a call-based maternal and child health program in India. Our findings uncover complexities around determining who benefits from the intervention, how the human-AI collaboration is managed, when intervention must take place in alignment with various priorities, and why the AI is sought, for what purpose. Our paper offers takeaways for human-centered AI integration in public health, drawing attention to the work done by the AI as actor, the work of configuring the human-AI partnership with multiple diverse stakeholders, and the work of aligning program goals for design and implementation through continual dialogue across stakeholders.",10.1145/3610203,https://doi.org/10.1145/3610203,Proceedings of the ACM on Human-Computer Interaction,"Ismail, Azra; Thakkar, Divy; Madhiwalla, Neha; Kumar, Neha",2023,28,"@article{2-3395,
  title = {Public Health Calls for/with AI: An Ethnographic Perspective},
  author = {Ismail, Azra and Thakkar, Divy and Madhiwalla, Neha and Kumar, Neha},
  year = {2023},
  doi = {10.1145/3610203},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Executing","Decision-maker, Stakeholder","Change cognitive demands, Change trust, Alter decision outcomes","Update AI competence, Change AI responses, Shape AI for accountability",NA,NA,NA,Yes,Yes
2-3396,acm,How Does Predictive Information Affect Human Ethical Preferences?,"Artificial intelligence (AI) has been increasingly involved in decision making in high-stakes domains, including loan applications, employment screening, and assistive clinical decision making. Meanwhile, involving AI in these high-stake decisions has created ethical concerns on how to balance different trade-offs to respect human values. One approach for aligning AIs with human values is to elicit human ethical preferences and incorporate this information in the design of computer systems. In this work, we explore how human ethical preferences are impacted by the information shown to humans during elicitation. In particular, we aim to provide a contrast between verifiable information (e.g., patient demographics or blood test results) and predictive information (e.g., the probability of organ transplant success). Using kidney transplant allocation as a case study, we conduct a randomized experiment to elicit human ethical preferences on scarce resource allocation to understand how human ethical preferences are impacted by the verifiable and predictive information. We find that the presence of predictive information significantly changes how humans take into account other verifiable information in their ethical preferences. We also find that the source of the predictive information (e.g., whether the predictions are made by AI or human doctors) plays a key role in how humans incorporate the predictive information into their own ethical judgements.",10.1145/3514094.3534165,https://doi.org/10.1145/3514094.3534165,"AAAI/ACM Conference on AI, Ethics, and Society","Narayanan, Saumik; Yu, Guanghui; Tang, Wei; Ho, Chien-Ju; Yin, Ming",2022,63,"@inproceedings{2-3396,
  title={How Does Predictive Information Affect Human Ethical Preferences?},
  author={Narayanan, Saumik and Yu, Guanghui and Tang, Wei and Ho, Chien-Ju and Yin, Ming},
  year={2022},
  booktitle={AAAI/ACM Conference on AI, Ethics, and Society},
  doi={10.1145/3514094.3534165}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising, Executing, Analyzing","Decision-maker, Decision-subject","Alter decision outcomes, Change trust, Shape ethical norms",no such info,prediction of alternative,NA,Interactive interface,Yes,Yes
2-33963,springernature,Reinforcement Learning-Based Algorithm for Real-Time Automated Parking Decision Making,"Recently, automated parking has gained attention for its ability to enhance parking accuracy and provide a comfortable experience for car owners. However, with the increasing number of vehicles in the parking lot, the traditional automatic parking algorithm face the dual challenges brought by narrow parking spaces and random vehicle obstacles. To address these issues, this paper proposes Curriculum Learning RL for automatic parking decision making in unregulated parking lots. Our approach involves SAC, a reinforcement learning( RL) algorithm, for curriculum learning, where the vehicle learns to park and avoid the obstacles separately through two courses. We incorporate a reward function that considers both location and safety, facilitating continuous learning of optimal actions. In addition, we develop a simulation platform for unregulated parking lots, and we train the algorithm on this platform. Comparing our algorithm with one that learns both actions simultaneously, we observe superior results in shorter timesteps. Furthermore, experiments conducted under various parking conditions demonstrate the algorithm’s strong generalization capabilities.",10.1007/978-981-99-9119-8_22,http://dx.doi.org/10.1007/978-981-99-9119-8_22,Artificial Intelligence,"Wei, Xiaoyi;Hou, Taixian;Zhao, Xiao;Tu, Jiaxin;Guan, Haiyang;Zhai, Peng;Zhang, Lihua",2024,3,"@inproceedings{2-33963,
  title={Reinforcement Learning-Based Algorithm for Real-Time Automated Parking Decision Making},
  author={Wei, Xiaoyi and Hou, Taixian and Zhao, Xiao and Tu, Jiaxin and Guan, Haiyang and Zhai, Peng and Zhang, Lihua},
  year={2024},
  doi={10.1007/978-981-99-9119-8_22},
  booktitle={Artificial Intelligence}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,"Stakeholder, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-33969,springernature,Adopting machine learning to automatically identify candidate patients for corneal refractive surgery,"Recently, it has become more important to screen candidates that undergo corneal refractive surgery to prevent complications. Until now, there is still no definitive screening method to confront the possibility of a misdiagnosis. We evaluate the possibilities of machine learning as a clinical decision support to determine the suitability to corneal refractive surgery. A machine learning architecture was built with the aim of identifying candidates combining the large multi-instrument data from patients and clinical decisions of highly experienced experts. Five heterogeneous algorithms were used to predict candidates for surgery. Subsequently, an ensemble classifier was developed to improve the performance. Training( 10, 561 subjects) and internal validation( 2640 subjects) were conducted using subjects who had visited between 2016 and 2017. External validation( 5279 subjects) was performed using subjects who had visited in 2018. The best model, i. e. , the ensemble classifier, had a high prediction performance with the area under the receiver operating characteristic curves of 0. 983( 95% CI, 0. 977–0. 987) and 0. 972( 95% CI, 0. 967–0. 976) when tested in the internal and external validation set, respectively. The machine learning models were statistically superior to classic methods including the percentage of tissue ablated and the Randleman ectatic score. Our model was able to correctly reclassify a patient with postoperative ectasia as an ectasia-risk group. Machine learning algorithms using a wide range of preoperative information achieved a comparable performance to screen candidates for corneal refractive surgery. An automated machine learning analysis of preoperative data can provide a safe and reliable clinical decision for refractive surgery.",10.1038/s41746-019-0135-8,http://dx.doi.org/10.1038/s41746-019-0135-8,Nature Partner Journals Digital Medicine,"Yoo, Tae Keun;Ryu, Ik Hee;Lee, Geunyoung;Kim, Youngnam;Kim, Jin Kuk;Lee, In Sik;Kim, Jung Sub;Rim, Tyler Hyungtaek",2019,26,"@article{2-33969,
  title = {Adopting machine learning to automatically identify candidate patients for corneal refractive surgery},
  author = {Yoo, Tae Keun and Ryu, Ik Hee and Lee, Geunyoung and Kim, Youngnam and Kim, Jin Kuk and Lee, In Sik and Kim, Jung Sub and Rim, Tyler Hyungtaek},
  year = {2019},
  doi = {10.1038/s41746-019-0135-8},
  journal = {Nature Partner Journals Digital Medicine}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-subject, Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-3397,acm,Modeling Epistemological Principles for Bias Mitigation in AI Systems: An Illustration in Hiring Decisions,"Artificial Intelligence (AI) has been used extensively in automatic decision making in a broad variety of scenarios, ranging from credit ratings for loans to recommendations of movies. Traditional design guidelines for AI models focus essentially on accuracy maximization, but recent work has shown that economically irrational and socially unacceptable scenarios of discrimination and unfairness are likely to arise unless these issues are explicitly addressed. This undesirable behavior has several possible sources, such as biased datasets used for training that may not be detected in black-box models. After pointing out connections between such bias of AI and the problem of induction, we focus on Popper's contributions after Hume's, which offer a logical theory of preferences. An AI model can be preferred over others on purely rational grounds after one or more attempts at refutation based on accuracy and fairness. Inspired by such epistemological principles, this paper proposes a structured approach to mitigate discrimination and unfairness caused by bias in AI systems. In the proposed computational framework, models are selected and enhanced after attempts at refutation. To illustrate our discussion, we focus on hiring decision scenarios where an AI system filters in which job applicants should go to the interview phase.",10.1145/3278721.3278751,https://doi.org/10.1145/3278721.3278751,"AAAI/ACM Conference on AI, Ethics, and Society","Vasconcelos, Marisa; Cardonha, Carlos; Gonçalves, Bernardo",2018,0,"@inproceedings{2-3397,
  title = {Modeling Epistemological Principles for Bias Mitigation in AI Systems: An Illustration in Hiring Decisions},
  author = {Vasconcelos, Marisa and Cardonha, Carlos and Gon{\c{c}}alves, Bernardo},
  year = {2018},
  doi = {10.1145/3278721.3278751},
  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society}
}",Theoretical contributions,Everyday / Employment / Public Service,Operational,"Executing, Forecasting, Monitoring","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3398,acm,"Ignore, Trust, or Negotiate: Understanding Clinician Acceptance of AI-Based Treatment Recommendations in Health Care","Artificial intelligence (AI) in healthcare has the potential to improve patient outcomes, but clinician acceptance remains a critical barrier. We developed a novel decision support interface that provides interpretable treatment recommendations for sepsis, a life-threatening condition in which decisional uncertainty is common, treatment practices vary widely, and poor outcomes can occur even with optimal decisions. This system formed the basis of a mixed-methods study in which 24 intensive care clinicians made AI-assisted decisions on real patient cases. We found that explanations generally increased confidence in the AI, but concordance with specific recommendations varied beyond the binary acceptance or rejection described in prior work. Although clinicians sometimes ignored or trusted the AI, they also often prioritized aspects of the recommendations to follow, reject, or delay in a process we term “negotiation.” These results reveal novel barriers to adoption of treatment-focused AI tools and suggest ways to better support differing clinician perspectives.",10.1145/3544548.3581075,https://doi.org/10.1145/3544548.3581075,ACM CHI Conference on Human Factors in Computing Systems,"Sivaraman, Venkatesh; Bukowski, Leigh A; Levin, Joel; Kahn, Jeremy M.; Perer, Adam",2023,166,"@inproceedings{2-3398,
  title = {Ignore, Trust, or Negotiate: Understanding Clinician Acceptance of AI-Based Treatment Recommendations in Health Care},
  author = {Sivaraman, Venkatesh and Bukowski, Leigh A and Levin, Joel and Kahn, Jeremy M. and Perer, Adam},
  year = {2023},
  doi = {10.1145/3544548.3581075},
  booktitle = {Proceedings of the ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Decision-subject","Change trust, Alter decision outcomes, Change affective-perceptual, Change cognitive demands","Update AI competence, Change AI responses","visual explanations, textual explanations, feature-based explanations",NA,"Interactive interface, Visual",Yes,Yes
2-3399,acm,Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making,"Artificial intelligence (AI) is increasingly being considered to assist human decision-making in high-stake domains (e.g. health). However, researchers have discussed an issue that humans can over-rely on wrong suggestions of the AI model instead of achieving human AI complementary performance. In this work, we utilized salient feature explanations along with what-if, counterfactual explanations to make humans review AI suggestions more analytically to reduce overreliance on AI and explored the effect of these explanations on trust and reliance on AI during clinical decision-making. We conducted an experiment with seven therapists and ten laypersons on the task of assessing post-stroke survivors' quality of motion, and analyzed their performance, agreement level on the task, and reliance on AI without and with two types of AI explanations. Our results showed that the AI model with both salient features and counterfactual explanations assisted therapists and laypersons to improve their performance and agreement level on the task when 'right' AI outputs are presented. While both therapists and laypersons over-relied on 'wrong' AI outputs, counterfactual explanations assisted both therapists and laypersons to reduce their over-reliance on 'wrong' AI outputs by 21% compared to salient feature explanations. Specifically, laypersons had higher performance degrades by 18.0 f1-score with salient feature explanations and 14.0 f1-score with counterfactual explanations than therapists with performance degrades of 8.6 and 2.8 f1-scores respectively. Our work discusses the potential of counterfactual explanations to better estimate the accuracy of an AI model and reduce over-reliance on 'wrong' AI outputs and implications for improving human-AI collaborative decision-making.",10.1145/3610218,https://doi.org/10.1145/3610218,Proceedings of the ACM on Human-Computer Interaction,"Lee, Min Hun; Chew, Chong Jun",2023,91,"@article{2-3399,
  title={Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making},
  author={Lee, Min Hun and Chew, Chong Jun},
  year={2023},
  doi={10.1145/3610218},
  journal={Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Explaining","Decision-maker, Knowledge provider","Change trust, Alter decision outcomes, Change cognitive demands",Update AI competence,"what-if, feature-based explanations",NA,"Textual, Visual, Interactive interface",Yes,Yes
2-3400,acm,Evaluating the Impact of Human Explanation Strategies on Human-AI Visual Decision-Making,"Artificial intelligence (AI) is increasingly being deployed in high-stakes domains, such as disaster relief and radiology, to aid practitioners during the decision-making process. Explainable AI techniques have been developed and deployed to provide users insights into why the AI made certain predictions. However, recent research suggests that these techniques may confuse or mislead users. We conducted a series of two studies to uncover strategies that humans use to explain decisions and then understand how those explanation strategies impact visual decision-making. In our first study, we elicit explanations from humans when assessing and localizing damaged buildings after natural disasters from satellite imagery and identify four core explanation strategies that humans employed. We then follow up by studying the impact of these explanation strategies by framing the explanations from Study 1 as if they were generated by AI and showing them to a different set of decision-makers performing the same task. We provide initial insights on how causal explanation strategies improve humans' accuracy and calibrate humans' reliance on AI when the AI is incorrect. However, we also find that causal explanation strategies may lead to incorrect rationalizations when AI presents a correct assessment with incorrect localization. We explore the implications of our findings for the design of human-centered explainable AI and address directions for future work.",10.1145/3579481,https://doi.org/10.1145/3579481,Proceedings of the ACM on Human-Computer Interaction,"Morrison, Katelyn; Shin, Donghoon; Holstein, Kenneth; Perer, Adam",2023,0,"@article{2-3400,
  title={Evaluating the Impact of Human Explanation Strategies on Human-AI Visual Decision-Making},
  author={Morrison, Katelyn and Shin, Donghoon and Holstein, Kenneth and Perer, Adam},
  year={2023},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  doi={10.1145/3579481}
}",Empirical contributions,Defense / Military / Emergency,Organizational,"Forecasting, Explaining, Advising",Decision-maker,"Alter decision outcomes, Change trust, Change affective-perceptual",Update AI competence,"causal explanations, global explanations, AI's damage assessment, localization, local explanations",NA,Visual,Yes,Yes
2-3401,acm,Hard Choices and Hard Limits in Artificial Intelligence,"Artificial intelligence (AI) is supposed to help us make better choices. Some of these choices are small, like what route to take to work, or what music to listen to. Others are big, like what treatment to administer for a disease or how long to sentence someone for a crime. If AI can assist with these big decisions, we might think it can also help with hard choices, cases where alternatives are neither better, worse nor equal but on a par. The aim of this paper, however, is to show that this view is mistaken: the fact of parity shows that there are hard limits on AI in decision making and choices that AI cannot, and should not, resolve.",10.1145/3461702.3462539,https://doi.org/10.1145/3461702.3462539,"AAAI/ACM Conference on AI, Ethics, and Society","Goodman, Bryce",2021,12,"@inproceedings{2-3401,
  title = {Hard Choices and Hard Limits in Artificial Intelligence},
  author = {Goodman, Bryce},
  year = {2021},
  doi = {10.1145/3461702.3462539},
  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,no such info,Advising,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-3402,acm,Healthcare AI Treatment Decision Support: Design Principles to Enhance Clinician Adoption and Trust,"Artificial intelligence (AI) supported clinical decision support (CDS) technologies can parse vast quantities of patient data into meaningful insights for healthcare providers. Much work is underway to determine the technical feasibility and the accuracy of AI-driven insights. Much less is known about what insights are considered useful and actionable by healthcare providers, their trust in the insights, and clinical workflow integration challenges. Our research team used a conceptual prototype based on AI-generated treatment insights for type 2 diabetes medications to elicit feedback from 41 U.S.-based clinicians, including primary care and internal medicine physicians, endocrinologists, nurse practitioners, physician assistants, and pharmacists. We contribute to the human-computer interaction (HCI) community by describing decision optimization and design objective tensions between population-level and personalized insights, and patterns of use and trust of AI systems. We also contribute a set of 6 design principles for AI-supported CDS.",10.1145/3544548.3581251,https://doi.org/10.1145/3544548.3581251,ACM CHI Conference on Human Factors in Computing Systems,"Burgess, Eleanor R.; Jankovic, Ivana; Austin, Melissa; Cai, Nancy; Kapuścińska, Adela; Currie, Suzanne; Overhage, J. Marc; Poole, Erika S; Kaye, Jofish",2023,0,"@inproceedings{2-3402,
  title     = {Healthcare AI Treatment Decision Support: Design Principles to Enhance Clinician Adoption and Trust},
  author    = {Burgess, Eleanor R. and Jankovic, Ivana and Austin, Melissa and Cai, Nancy and Kapu\'{s}ci\'{n}ska, Adela and Currie, Suzanne and Overhage, J. Marc and Poole, Erika S. and Kaye, Jofish},
  year      = {2023},
  doi       = {10.1145/3544548.3581251},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems}
}","Empirical contributions, System/Artifact contributions",Healthcare / Medicine / Surgery,Operational,"Advising, Executing, Analyzing","Decision-maker, Decision-subject, Knowledge provider","Change trust, Alter decision outcomes, Change cognitive demands",Update AI competence,"filtering, recommendations","personalized settings, domain knowledge","Textual, Visual, Interactive interface",Yes,Yes
2-3403,acm,"Cake, Death, and Trolleys: Dilemmas as benchmarks of ethical decision-making","Artificial intelligence (AI) systems are becoming part of our lives and societies. The more decisions such systems make for us, the more we need to ensure that the decisions they make have a positive individual and societal ethical impact. How can we estimate how good a system is at making ethical decisions? Benchmarking is used to evaluate how good a machine or a process performs with respect to industry bests. In this paper we argue that (some) ethical dilemmas can be used as benchmarks for estimating the ethical performance of an autonomous system. We advocate that an open source repository of such dilemmas should be maintained. We present a prototype of such a repository available at https://imdb. uib.no/dilemmaz/articles/all1.",10.1145/3278721.3278767,https://doi.org/10.1145/3278721.3278767,"AAAI/ACM Conference on AI, Ethics, and Society","Bjørgen, Edvard P.; Madsen, Simen; Bjørknes, Therese S.; Heimsæter, Fredrik V.; Håvik, Robin; Linderud, Morten; Longberg, Per-Niklas; Dennis, Louise A.; Slavkovik, Marija",2018,62,"@inproceedings{2-3403,
  title = {Cake, Death, and Trolleys: Dilemmas as Benchmarks of Ethical Decision-Making},
  author = {Bjørgen, Edvard P. and Madsen, Simen and Bjørknes, Therese S. and Heimsæter, Fredrik V. and Håvik, Robin and Linderud, Morten and Longberg, Per-Niklas and Dennis, Louise A. and Slavkovik, Marija},
  year = {2018},
  doi = {10.1145/3278721.3278767},
  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,no such info,"Executing, Explaining","Knowledge provider, Guardian",NA,NA,NA,NA,NA,Yes,No
2-3404,acm,“Brilliant AI Doctor” in Rural Clinics: Challenges in AI-Powered Clinical Decision Support System Deployment,"Artificial intelligence (AI) technology has been increasingly used in the implementation of advanced Clinical Decision Support Systems (CDSS). Research demonstrated the potential usefulness of AI-powered CDSS (AI-CDSS) in clinical decision making scenarios. However, post-adoption user perception and experience remain understudied, especially in developing countries. Through observations and interviews with 22 clinicians from 6 rural clinics in China, this paper reports the various tensions between the design of an AI-CDSS system (“Brilliant Doctor”) and the rural clinical context, such as the misalignment with local context and workflow, the technical limitations and usability barriers, as well as issues related to transparency and trustworthiness of AI-CDSS. Despite these tensions, all participants expressed positive attitudes toward the future of AI-CDSS, especially acting as “a doctor’s AI assistant” to realize a Human-AI Collaboration future in clinical settings. Finally we draw on our findings to discuss implications for designing AI-CDSS interventions for rural clinical contexts in developing countries.",10.1145/3411764.3445432,https://doi.org/10.1145/3411764.3445432,CHI Conference on Human Factors in Computing Systems,"Wang, Dakuo; Wang, Liuping; Zhang, Zhan; Wang, Ding; Zhu, Haiyi; Gao, Yvonne; Fan, Xiangmin; Tian, Feng",2021,0,"@inproceedings{2-3404,
  title = {“Brilliant AI Doctor” in Rural Clinics: Challenges in AI-Powered Clinical Decision Support System Deployment},
  author = {Wang, Dakuo and Wang, Liuping and Zhang, Zhan and Wang, Ding and Zhu, Haiyi and Gao, Yvonne and Fan, Xiangmin and Tian, Feng},
  year = {2021},
  doi = {10.1145/3411764.3445432},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Collaborating",Decision-maker,"Change trust, Change cognitive demands, Restrict human agency","Change AI responses, Update AI competence",NA,"domain knowledge, interoperability issue, usability barriers",Autonomous System,Yes,Yes
2-3405,acm,Keeping Designers in the Loop: Communicating Inherent Algorithmic Trade-offs Across Multiple Objectives,"Artificial intelligence algorithms have been used to enhance a wide variety of products and services, including assisting human decision making in high-stake contexts. However, these algorithms are complex and have trade-offs, notably between prediction accuracy and fairness to population subgroups. This makes it hard for designers to understand algorithms and design products or services in a way that respects users' goals, values, and needs. We proposed a method to help designers and users explore algorithms, visualize their trade-offs, and select algorithms with trade-offs consistent with their goals and needs. We evaluated our method on the problem of predicting criminal defendants' likelihood to re-offend through (i) a large-scale Amazon Mechanical Turk experiment, and (ii) in-depth interviews with domain experts. Our evaluations show that our method can help designers and users of these systems better understand and navigate algorithmic trade-offs. This paper contributes a new way of providing designers the ability to understand and control the outcomes of algorithmic systems they are creating.",10.1145/3357236.3395528,https://doi.org/10.1145/3357236.3395528,Designing Interactive Systems Conference (DIS),"Yu, Bowen; Yuan, Ye; Terveen, Loren; Wu, Zhiwei Steven; Forlizzi, Jodi; Zhu, Haiyi",2020,73,"@inproceedings{2-3405,
  title={Keeping Designers in the Loop: Communicating Inherent Algorithmic Trade-offs Across Multiple Objectives},
  author={Yu, Bowen and Yuan, Ye and Terveen, Loren and Wu, Zhiwei Steven and Forlizzi, Jodi and Zhu, Haiyi},
  booktitle={Designing Interactive Systems Conference (DIS)},
  year={2020},
  doi={10.1145/3357236.3395528}
}",Algorithmic contributions,Design / Creativity / Architecture,Individual,"Explaining, Advising","Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-3407,acm,"“Should I Follow the Human, or Follow the Robot?” — Robots in Power Can Have More Influence Than Humans on Decision-Making","Artificially intelligent (AI) agents such as robots are increasingly delegated power in work settings, yet it remains unclear how power functions in interactions with both humans and robots, especially when they directly compete for influence. Here we present an experiment where every participant was matched with one human and one robot to perform decision-making tasks. By manipulating who has power, we created three conditions: human as leader, robot as leader, and a no-power-difference control. The results showed that the participants were significantly more influenced by the leader, regardless of whether the leader was a human or a robot. However, they generally held a more positive attitude toward the human than the robot, although they considered whichever was in power as more competent. This study illustrates the importance of power for future Human-Robot Interaction (HRI) and Human-AI Interaction (HAI) research, as it addresses pressing concerns of society about AI-powered intelligent agents.",10.1145/3544548.3581066,https://doi.org/10.1145/3544548.3581066,ACM CHI Conference on Human Factors in Computing Systems,"Hou, Yoyo Tsung-Yu; Lee, Wen-Ying; Jung, Malte",2023,31,"@inproceedings{2-3407,
  title = {Should I Follow the Human, or Follow the Robot? — Robots in Power Can Have More Influence Than Humans on Decision-Making},
  author = {Hou, Yoyo Tsung-Yu and Lee, Wen-Ying and Jung, Malte},
  year = {2023},
  doi = {10.1145/3544548.3581066},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Advising, Collaborating, Executing",Decision-maker,"Alter decision outcomes, Change affective-perceptual, Change trust",no such info,"power, decision suggestions","power, domain knowledge",Physical / Embodiment,Yes,Yes
2-3409,acm,"More Similar Values, More Trust? - the Effect of Value Similarity on Trust in Human-Agent Interaction","As AI systems are increasingly involved in decision making, it also becomes important that they elicit appropriate levels of trust from their users. To achieve this, it is first important to understand which factors influence trust in AI. We identify that a research gap exists regarding the role of personal values in trust in AI. Therefore, this paper studies how human and agent Value Similarity (VS) influences a human's trust in that agent. To explore this, 89 participants teamed up with five different agents, which were designed with varying levels of value similarity to that of the participants. In a within-subjects, scenario-based experiment, agents gave suggestions on what to do when entering the building to save a hostage. We analyzed the agent's scores on subjective value similarity, trust and qualitative data from open-ended questions. Our results show that agents rated as having more similar values also scored higher on trust, indicating a positive effect between the two. With this result, we add to the existing understanding of human-agent trust by providing insight into the role of value-similarity.",10.1145/3461702.3462576,https://doi.org/10.1145/3461702.3462576,"AAAI/ACM Conference on AI, Ethics, and Society","Mehrotra, Siddharth; Jonker, Catholijn M.; Tielman, Myrthe L.",2021,37,"@inproceedings{2-3409,
  title = {More Similar Values, More Trust? - the Effect of Value Similarity on Trust in Human-Agent Interaction},
  author = {Mehrotra, Siddharth and Jonker, Catholijn M. and Tielman, Myrthe L.},
  year = {2021},
  doi = {10.1145/3461702.3462576},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Organizational,Advising,Decision-maker,"Change trust, Alter decision outcomes",no such info,NA,personal values,Interactive interface,Yes,Yes
2-3410,acm,Expanding Explainability: Towards Social Transparency in AI systems,"As AI-powered systems increasingly mediate consequential decision-making, their explainability is critical for end-users to take informed and accountable actions. Explanations in human-human interactions are socially-situated. AI systems are often socio-organizationally embedded. However, Explainable AI (XAI) approaches have been predominantly algorithm-centered. We take a developmental step towards socially-situated XAI by introducing and exploring Social Transparency (ST), a sociotechnically informed perspective that incorporates the socio-organizational context into explaining AI-mediated decision-making. To explore ST conceptually, we conducted interviews with 29 AI users and practitioners grounded in a speculative design scenario. We suggested constitutive design elements of ST and developed a conceptual framework to unpack ST’s effect and implications at the technical, decision-making, and organizational level. The framework showcases how ST can potentially calibrate trust in AI, improve decision-making, facilitate organizational collective actions, and cultivate holistic explainability. Our work contributes to the discourse of Human-Centered XAI by expanding the design space of XAI.",10.1145/3411764.3445188,https://doi.org/10.1145/3411764.3445188,CHI Conference on Human Factors in Computing Systems,"Ehsan, Upol; Liao, Q. Vera; Muller, Michael; Riedl, Mark O.; Weisz, Justin D.",2021,104,"@inproceedings{2-3410,
  title={Expanding Explainability: Towards Social Transparency in AI Systems},
  author={Ehsan, Upol and Liao, Q. Vera and Muller, Michael and Riedl, Mark O. and Weisz, Justin D.},
  year={2021},
  booktitle={CHI Conference on Human Factors in Computing Systems},
  doi={10.1145/3411764.3445188}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Institutional,"Advising, Explaining",Decision-maker,"Change trust, Change cognitive demands, Shape ethical norms, Alter decision outcomes","Shape AI for accountability, Change AI responses, Update AI competence","recommendations, appropriate level of details, social transparency","crew knowledge, privacy concern",NA,Yes,Yes
2-3411,acm,For What It’s Worth: Humans Overwrite Their Economic Self-interest to Avoid Bargaining With AI Systems,"As algorithms are increasingly augmenting and substituting human decision-making, understanding how the introduction of computational agents changes the fundamentals of human behavior becomes vital. This pertains to not only users, but also those parties who face the consequences of an algorithmic decision. In a controlled experiment with 480 participants, we exploit an extended version of two-player ultimatum bargaining where responders choose to bargain with either another human, another human with an AI decision aid or an autonomous AI-system acting on behalf of a passive human proposer. Our results show strong responder preferences against the algorithm, as most responders opt for a human opponent and demand higher compensation to reach a contract with autonomous agents. To map these preferences to economic expectations, we elicit incentivized subject beliefs about their opponent’s behavior. The majority of responders maximize their expected value when this is line with approaching the human proposer. In contrast, responders predicting income maximization for the autonomous AI-system overwhelmingly override economic self-interest to avoid the algorithm.",10.1145/3491102.3517734,https://doi.org/10.1145/3491102.3517734,ACM CHI Conference on Human Factors in Computing Systems,"Erlei, Alexander; Das, Richeek; Meub, Lukas; Anand, Avishek; Gadiraju, Ujwal",2022,40,"@inproceedings{2-3411,
  title = {For What It's Worth: Humans Overwrite Their Economic Self-interest to Avoid Bargaining With AI Systems},
  author = {Erlei, Alexander and Das, Richeek and Meub, Lukas and Anand, Avishek and Gadiraju, Ujwal},
  year = {2022},
  doi = {10.1145/3491102.3517734},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Advising, Executing","Decision-maker, Decision-subject","Alter decision outcomes, Restrict human agency, Change affective-perceptual",no such info,NA,incentivized beliefs,Autonomous System,Yes,Yes
2-3412,acm,Aligning Superhuman AI with Human Behavior: Chess as a Model System,"As artificial intelligence becomes increasingly intelligent—in some cases, achieving superhuman performance—there is growing potential for humans to learn from and collaborate with algorithms. However, the ways in which AI systems approach problems are often different from the ways people do, and thus may be uninterpretable and hard to learn from. A crucial step in bridging this gap between human and artificial intelligence is modeling the granular actions that constitute human behavior, rather than simply matching aggregate human performance. We pursue this goal in a model system with a long history in artificial intelligence: chess. The aggregate performance of a chess player unfolds as they make decisions over the course of a game. The hundreds of millions of games played online by players at every skill level form a rich source of data in which these decisions, and their exact context, are recorded in minute detail. Applying existing chess engines to this data, including an open-source implementation of AlphaZero, we find that they do not predict human moves well. We develop and introduce Maia, a customized version of AlphaZero trained on human chess games, that predicts human moves at a much higher accuracy than existing engines, and can achieve maximum accuracy when predicting decisions made by players at a specific skill level in a tuneable way. For a dual task of predicting whether a human will make a large mistake on the next move, we develop a deep neural network that significantly outperforms competitive baselines. Taken together, our results suggest that there is substantial promise in designing artificial intelligence systems with human collaboration in mind by first accurately modeling granular human decision-making.",10.1145/3394486.3403219,https://doi.org/10.1145/3394486.3403219,ACM SIGKDD Conference on Knowledge Discovery and Data Mining,"McIlroy-Young, Reid; Sen, Siddhartha; Kleinberg, Jon; Anderson, Ashton",2020,212,"@inproceedings{2-3412,
  title={Aligning Superhuman AI with Human Behavior: Chess as a Model System},
  author={McIlroy-Young, Reid and Sen, Siddhartha and Kleinberg, Jon and Anderson, Ashton},
  year={2020},
  doi={10.1145/3394486.3403219},
  booktitle={ACM SIGKDD Conference on Knowledge Discovery and Data Mining}
}",Algorithmic contributions,Media / Communication / Entertainment,Individual,"Forecasting, Advising","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-3413,acm,Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image Labeling,"As deep neural networks are more commonly deployed in high-stakes domains, their black-box nature makes uncertainty quantification challenging. We investigate the effects of presenting conformal prediction sets—a distribution-free class of methods for generating prediction sets with specified coverage—to express uncertainty in AI-advised decision-making. Through a large online experiment, we compare the utility of conformal prediction sets to displays of Top-1 and Top-k predictions for AI-advised image labeling. In a pre-registered analysis, we find that the utility of prediction sets for accuracy varies with the difficulty of the task: while they result in accuracy on par with or less than Top-1 and Top-k displays for easy images, prediction sets excel at assisting humans in labeling out-of-distribution (OOD) images, especially when the set size is small. Our results empirically pinpoint practical challenges of conformal prediction sets and provide implications on how to incorporate them for real-world decision-making.",10.1145/3613904.3642446,https://doi.org/10.1145/3613904.3642446,CHI Conference on Human Factors in Computing Systems,"Zhang, Dongping; Chatzimparmpas, Angelos; Kamali, Negar; Hullman, Jessica",2024,18,"@inproceedings{2-3413,
  title = {Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image Labeling},
  author = {Zhang, Dongping and Chatzimparmpas, Angelos and Kamali, Negar and Hullman, Jessica},
  year = {2024},
  doi = {10.1145/3613904.3642446},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Forecasting, Advising",Decision-maker,"Change cognitive demands, Alter decision outcomes",no such info,conformal prediction sets,NA,Visual,Yes,Yes
2-3415,acm,Fairness is not static: deeper understanding of long term fairness via simulation studies,"As machine learning becomes increasingly incorporated within high impact decision ecosystems, there is a growing need to understand the long-term behaviors of deployed ML-based decision systems and their potential consequences. Most approaches to understanding or improving the fairness of these systems have focused on static settings without considering long-term dynamics. This is understandable; long term dynamics are hard to assess, particularly because they do not align with the traditional supervised ML research framework that uses fixed data sets. To address this structural difficulty in the field, we advocate for the use of simulation as a key tool in studying the fairness of algorithms. We explore three toy examples of dynamical systems that have been previously studied in the context of fair decision making for bank loans, college admissions, and allocation of attention. By analyzing how learning agents interact with these systems in simulation, we are able to extend previous work, showing that static or single-step analyses do not give a complete picture of the long-term consequences of an ML-based decision system. We provide an extensible open-source software framework for implementing fairness-focused simulation studies and further reproducible research, available at https://github.com/google/ml-fairness-gym.",10.1145/3351095.3372878,https://doi.org/10.1145/3351095.3372878,"ACM Conference on Fairness, Accountability, and Transparency (FAccT)","D'Amour, Alexander; Srinivasan, Hansa; Atwood, James; Baljekar, Pallavi; Sculley, D.; Halpern, Yoni",2020,326,"@inproceedings{2-3415,
  title = {Fairness is not static: deeper understanding of long term fairness via simulation studies},
  author = {D'Amour, Alexander and Srinivasan, Hansa and Atwood, James and Baljekar, Pallavi and Sculley, D. and Halpern, Yoni},
  year = {2020},
  doi = {10.1145/3351095.3372878},
  booktitle = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (FAccT)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy, Everyday / Employment / Public Service, Manufacturing / Industry / Automation","Institutional, Operational","Advising, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-3420,acm,Studying Up Public Sector AI: How Networks of Power Relations Shape Agency Decisions Around AI Design and Use,"As public sector agencies rapidly introduce new AI tools in high-stakes domains like social services, it becomes critical to understand how decisions to adopt these tools are made in practice. We borrow from the anthropological practice to ""study up"" those in positions of power, and reorient our study of public sector AI around those who have the power and responsibility to make decisions about the role that AI tools will play in their agency. Through semi-structured interviews and design activities with 16 agency decision-makers, we examine how decisions about AI design and adoption are influenced by their interactions with and assumptions about other actors within these agencies (e.g., frontline workers and agency leaders), as well as those above (legal systems and contracted companies), and below (impacted communities). By centering these networks of power relations, our findings shed light on how infrastructural, legal, and social factors create barriers and disincentives to the involvement of a broader range of stakeholders in decisions about AI design and adoption. Agency decision-makers desired more practical support for stakeholder involvement around public sector AI to help overcome the knowledge and power differentials they perceived between them and other stakeholders (e.g., frontline workers and impacted community members). Building on these findings, we discuss implications for future research and policy around actualizing participatory AI approaches in public sector contexts.",10.1145/3686989,https://doi.org/10.1145/3686989,Proceedings of the ACM on Human-Computer Interaction,"Kawakami, Anna; Coston, Amanda; Heidari, Hoda; Holstein, Kenneth; Zhu, Haiyi",2024,25,"@article{2-3420,
  title = {Studying Up Public Sector AI: How Networks of Power Relations Shape Agency Decisions Around AI Design and Use},
  author = {Kawakami, Anna and Coston, Amanda and Heidari, Hoda and Holstein, Kenneth and Zhu, Haiyi},
  year = {2024},
  doi = {10.1145/3686989},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Law / Policy / Governance,Organizational,Advising,Decision-maker,no such info,"Shape AI for accountability, Change AI responses, Update AI competence",NA,limited agency,NA,Yes,Yes
2-3421,acm,“There Is Not Enough Information”: On the Effects of Explanations on Perceptions of Informational Fairness and Trustworthiness in Automated Decision-Making,"Automated decision systems (ADS) are increasingly used for consequential decision-making. These systems often rely on sophisticated yet opaque machine learning models, which do not allow for understanding how a given decision was arrived at. In this work, we conduct a human subject study to assess people’s perceptions of informational fairness (i.e., whether people think they are given adequate information on and explanation of the process and its outcomes) and trustworthiness of an underlying ADS when provided with varying types of information about the system. More specifically, we instantiate an ADS in the area of automated loan approval and generate different explanations that are commonly used in the literature. We randomize the amount of information that study participants get to see by providing certain groups of people with the same explanations as others plus additional explanations. From our quantitative analyses, we observe that different amounts of information as well as people’s (self-assessed) AI literacy significantly influence the perceived informational fairness, which, in turn, positively relates to perceived trustworthiness of the ADS. A comprehensive analysis of qualitative feedback sheds light on people’s desiderata for explanations, among which are (i) consistency (both with people’s expectations and across different explanations), (ii) disclosure of monotonic relationships between features and outcome, and (iii) actionability of recommendations.",10.1145/3531146.3533218,https://doi.org/10.1145/3531146.3533218,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Schoeffer, Jakob; Kuehl, Niklas; Machowski, Yvette",2022,101,"@inproceedings{2-3421,
author = {Schoeffer, Jakob and Kuehl, Niklas and Machowski, Yvette},
title = {“There Is Not Enough Information”: On the Effects of Explanations on Perceptions of Informational Fairness and Trustworthiness in Automated Decision-Making},
year = {2022},
doi = {10.1145/3531146.3533218},
pages = {1616–1628},
numpages = {13}
}",Empirical contributions,Finance / Business / Economy,Operational,"Explaining, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3422,acm,Preventing Discriminatory Decision-making in Evolving Data Streams,"Bias in machine learning has rightly received significant attention over the past decade. However, most fair machine learning (fair-ML) works to address bias in decision-making systems has focused solely on the offline setting. Despite the wide prevalence of online systems in the real world, work on identifying and correcting bias in the online setting is severely lacking. The unique challenges of the online environment make addressing bias more difficult than in the offline setting. First, Streaming Machine Learning (SML) algorithms must deal with the constantly evolving real-time data stream. Secondly, they need to adapt to changing data distributions (concept drift) to make accurate predictions on new incoming data. Incorporating fairness constraints into this already intricate task is not straightforward. In this work, we focus on the challenges of achieving fairness in biased data streams while accounting for the presence of concept drift, accessing one sample at a time. We present Fair Sampling over Stream (FS2), a novel fair rebalancing approach capable of being integrated with SML classification algorithms. Furthermore, we devise the first unified performance-fairness metric, Fairness Bonded Utility (FBU), to efficiently evaluate and compare the trade-offs between performance and fairness across various bias mitigation methods. FBU simplifies the comparison of fairness-performance trade-offs of multiple techniques through one unified and intuitive evaluation, allowing model designers to easily choose a technique. Overall, extensive evaluations show our measures surpass those of other fair online techniques previously reported in the literature.",10.1145/3593013.3593984,https://doi.org/10.1145/3593013.3593984,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Wang, Zichong; Saxena, Nripsuta; Yu, Tongjia; Karki, Sneha; Zetty, Tyler; Haque, Israat; Zhou, Shan; Kc, Dukka; Stockwell, Ian; Wang, Xuyu; Bifet, Albert; Zhang, Wenbin",2023,10,"@inproceedings{2-3422,
  title={Preventing Discriminatory Decision-making in Evolving Data Streams},
  author={Wang, Zichong and Saxena, Nripsuta and Yu, Tongjia and Karki, Sneha and Zetty, Tyler and Haque, Israat and Zhou, Shan and Kc, Dukka and Stockwell, Ian and Wang, Xuyu and Bifet, Albert and Zhang, Wenbin},
  year={2023},
  doi={10.1145/3593013.3593984},
  booktitle={Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Institutional,"Forecasting, Auditing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-3423,acm,INN: An Interpretable Neural Network for AI Incubation in Manufacturing,"Both artificial intelligence (AI) and domain knowledge from human experts play an important role in manufacturing decision making. Smart manufacturing emphasizes a fully automated data-driven decision-making; however, the AI incubation process involves human experts to enhance AI systems by integrating domain knowledge for modeling, data collection and annotation, and feature extraction. Such an AI incubation process not only enhances the domain knowledge discovery but also improves the interpretability and trustworthiness of AI methods. In this article, we focus on the knowledge transfer from human experts to a supervised learning problem by learning domain knowledge as interpretable features and rules, which can be used to construct rule-based systems to support manufacturing decision making, such as process modeling and quality inspection. Although many advanced statistical and machine learning methods have shown promising modeling accuracy and efficiency, rule-based systems are still highly preferred and widely adopted due to their interpretability for human experts to comprehend. However, most of the existing rule-based systems are constructed based on deterministic human-crafted rules, whose parameters, such as thresholds of decision rules, are suboptimal. Yet the machine learning methods, such as tree models or neural networks, can learn a decision rule based structure without much interpretation or agreement with domain knowledge. Therefore, the traditional machine learning models and human experts’ domain knowledge cannot be directly improved by learning from data. In this research, we propose an interpretable neural network (INN) model with a center-adjustable sigmoid activation function to efficiently optimize the rule-based systems. Using the rule-based system from domain knowledge to regulate the INN architecture not only improves the prediction accuracy with optimized parameters but also ensures the interpretability by adopting the interpretable rule-based systems from domain knowledge. The proposed INN will be effective for supervised learning problems when rule-based systems are available. The merits of the INN model are demonstrated via a simulation study and a real case study in the quality modeling of a semiconductor manufacturing process. The source code of this work is hosted here: .",10.1145/3519313,https://doi.org/10.1145/3519313,ACM Transactions on Intelligent Systems and Technology,"Chen, Xiaoyu; Zeng, Yingyan; Kang, Sungku; Jin, Ran",2022,22,"@article{2-3423,
  title={INN: An Interpretable Neural Network for AI Incubation in Manufacturing},
  author={Chen, Xiaoyu and Zeng, Yingyan and Kang, Sungku and Jin, Ran},
  year={2022},
  doi={10.1145/3519313},
  journal={ACM Transactions on Intelligent Systems and Technology}
}",Algorithmic contributions,Manufacturing / Industry / Automation,Operational,"Executing, Advising, Collaborating","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-3424,acm,"Measurements, Algorithms, and Presentations of Reality: Framing Interactions with AI-Enabled Decision Support","Bringing AI technology into clinical practice has proved challenging for system designers and medical professionals alike. The academic literature has, for example, highlighted the dangers of black-box decision-making and biased datasets. Furthermore, end-users’ ability to validate a system’s performance often disappears following the introduction of AI decision-making. We present the MAP model to understand and describe the three stages through which medical observations are interpreted and handled by AI systems. These stages are Measurement, in which information is gathered and converted into data points that can be stored and processed; Algorithm, in which computational processes transform the collected data; and Presentation, where information is returned to the user for interpretation. For each stage, we highlight possible challenges that need to be overcome to develop Human-Centred AI systems. We illuminate our MAP model through complementary case studies on colonoscopy practice and dementia diagnosis, providing examples of the challenges encountered in real-world settings. By defining Human-AI interaction across these three stages, we untangle some of the inherent complexities in designing AI technology for clinical decision-making, and aim to overcome misalignment between medical end-users and AI researchers and developers.",10.1145/3571815,https://doi.org/10.1145/3571815,ACM Transactions on Computer-Human Interaction,"van Berkel, Niels; Bellio, Maura; Skov, Mikael B.; Blandford, Ann",2023,34,"@article{2-3424,
  title={Measurements, Algorithms, and Presentations of Reality: Framing Interactions with AI-Enabled Decision Support},
  author={van Berkel, Niels and Bellio, Maura and Skov, Mikael B. and Blandford, Ann},
  year={2023},
  doi={10.1145/3571815},
  journal={ACM Transactions on Computer-Human Interaction}
}",Theoretical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting, Analyzing","Decision-maker, Developer","Change cognitive demands, Alter decision outcomes, Shape ethical norms","Update AI competence, Change AI responses","visual marks, recommendations",NA,"Visual, Interactive interface",Yes,Yes
2-3425,acm,INPREM: An Interpretable and Trustworthy Predictive Model for Healthcare,"Building a predictive model based on historical Electronic Health Records (EHRs) for personalized healthcare has become an active research area. Benefiting from the powerful ability of feature extraction, deep learning (DL) approaches have achieved promising performance in many clinical prediction tasks. However, due to the lack of interpretability and trustworthiness, it is difficult to apply DL in real clinical cases of decision making. To address this, in this paper, we propose an interpretable and trustworthy predictive model (INPREM) for healthcare. Firstly, INPREM is designed as a linear model for interpretability while encoding non-linear relationships into the learning weights for modeling the dependencies between and within each visit. This enables us to obtain the contribution matrix of the input variables, which is served as the evidence of the prediction result(s), and help physicians understand why the model gives such a prediction, thereby making the model more interpretable. Secondly, for trustworthiness, we place a random gate (which follows a Bernoulli distribution to turn on or off) over each weight of the model, as well as an additional branch to estimate data noises. With the help of the Monto Carlo sampling and an objective function accounting for data noises, the model can capture the uncertainty of each prediction. The captured uncertainty, in turn, allows physicians to know how confident the model is, thus making the model more trustworthy. We empirically demonstrate that the proposed INPREM outperforms existing approaches with a significant margin. A case study is also presented to show how the contribution matrix and the captured uncertainty are used to assist physicians in making robust decisions.",10.1145/3394486.3403087,https://doi.org/10.1145/3394486.3403087,ACM SIGKDD Conference on Knowledge Discovery and Data Mining,"Zhang, Xianli; Qian, Buyue; Cao, Shilei; Li, Yang; Chen, Hang; Zheng, Yefeng; Davidson, Ian",2020,44,"@inproceedings{2-3425,
  title = {INPREM: An Interpretable and Trustworthy Predictive Model for Healthcare},
  author = {Zhang, Xianli and Qian, Buyue and Cao, Shilei and Li, Yang and Chen, Hang and Zheng, Yefeng and Davidson, Ian},
  year = {2020},
  doi = {10.1145/3394486.3403087},
  booktitle = {Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing, Explaining","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-3427,acm,Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making,"Calls for heightened consideration of fairness and accountability in algorithmically-informed public decisions-like taxation, justice, and child protection-are now commonplace. How might designers support such human values? We interviewed 27 public sector machine learning practitioners across 5 OECD countries regarding challenges understanding and imbuing public values into their work. The results suggest a disconnect between organisational and institutional realities, constraints and needs, and those addressed by current research into usable, transparent and 'discrimination-aware' machine learning-absences likely to undermine practical initiatives unless addressed. We see design opportunities in this disconnect, such as in supporting the tracking of concept drift in secondary data sources, and in building usable transparency tools to identify risks and incorporate domain knowledge, aimed both at managers and at the 'street-level bureaucrats' on the frontlines of public service. We conclude by outlining ethical challenges and future directions for collaboration in these high-stakes applications.",10.1145/3173574.3174014,https://doi.org/10.1145/3173574.3174014,ACM CHI Conference on Human Factors in Computing Systems,"Veale, Michael; Van Kleek, Max; Binns, Reuben",2018,2,"@inproceedings{2-3427,
  title = {Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making},
  author = {Veale, Michael and Van Kleek, Max and Binns, Reuben},
  year = {2018},
  doi = {10.1145/3173574.3174014},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Everyday / Employment / Public Service,Institutional,"Analyzing, Advising","Decision-maker, Stakeholder, Knowledge provider","Alter decision outcomes, Change trust, Change affective-perceptual","Update AI competence, Change AI responses, Shape AI for accountability","the logic of the model, performance of the model, the target of accuracy","external or qualitative information, avoiding direct use of illegal-to-use protected characteristics",NA,Yes,Yes
2-3428,acm,Impossible Explanations? Beyond explainable AI in the GDPR from a COVID-19 use case scenario,"Can we achieve an adequate level of explanation for complex machine learning models in high-risk AI applications when applying the EU data protection framework? In this article, we address this question, analysing from a multidisciplinary point of view the connection between existing legal requirements for the explainability of AI systems and the current state of the art in the field of explainable AI.We present a case study of a real-life scenario designed to illustrate the application of an AI-based automated decision making process for the medical diagnosis of COVID-19 patients. The scenario exemplifies the trend in the usage of increasingly complex machine-learning algorithms with growing dimensionality of data and model parameters. Based on this setting, we analyse the challenges of providing human legible explanations in practice and we discuss their legal implications following the General Data Protection Regulation (GDPR).Although it might appear that there is just one single form of explanation in the GDPR, we conclude that the context in which the decision-making system operates requires that several forms of explanation are considered. Thus, we propose to design explanations in multiple forms, depending on: the moment of the disclosure of the explanation (either ex ante or ex post); the audience of the explanation (explanation for an expert or a data controller and explanation for the final data subject); the layer of granularity (such as general, group-based or individual explanations); the level of the risks of the automated decision regarding fundamental rights and freedoms. Consequently, explanations should embrace this multifaceted environment.Furthermore, we highlight how the current inability of complex, deep learning based machine learning models to make clear causal links between input data and final decisions represents a limitation for providing exact, human-legible reasons behind specific decisions. This makes the provision of satisfactorily, fair and transparent explanations a serious challenge. Therefore, there are cases where the quality of possible explanations might not be assessed as an adequate safeguard for automated decision-making processes under Article 22(3) GDPR. Accordingly, we suggest that further research should focus on alternative tools in the GDPR (such as algorithmic impact assessments from Article 35 GDPR or algorithmic lawfulness justifications) that might be considered to complement the explanations of automated decision-making.",10.1145/3442188.3445917,https://doi.org/10.1145/3442188.3445917,"ACM Conference on Fairness, Accountability, and Transparency (FAccT)","Hamon, Ronan; Junklewitz, Henrik; Malgieri, Gianclaudio; Hert, Paul De; Beslay, Laurent; Sanchez, Ignacio",2021,58,"@inproceedings{2-3428,
  title={Impossible Explanations? Beyond explainable AI in the GDPR from a COVID-19 use case scenario},
  author={Hamon, Ronan and Junklewitz, Henrik and Malgieri, Gianclaudio and Hert, Paul De and Beslay, Laurent and Sanchez, Ignacio},
  year={2021},
  doi={10.1145/3442188.3445917},
  booktitle={Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (FAccT)}
}","Empirical contributions, Theoretical contributions",Healthcare / Medicine / Surgery,Operational,"Executing, Forecasting, Explaining","Knowledge provider, Decision-subject, Guardian, Stakeholder",no such info,"Update AI competence, Shape AI for accountability","ex-ante explanations, ex-post explanations",NA,Autonomous System,Yes,Yes
2-3429,acm,Co-Design and Evaluation of an Intelligent Decision Support System for Stroke Rehabilitation Assessment,"Clinical decision support systems have the potential to improve work flows of experts in practice (e.g. therapist's evidence-based rehabilitation assessment). However, the adoption of these systems is challenging, and the gains of these systems have not fully demonstrated yet. In this paper, we identified the needs of therapists to assess patient's functional abilities (e.g. alternative perspectives with quantitative information on patient's exercise motions). As a result, we co-designed and developed an intelligent decision support system that automatically identifies salient features of assessment using reinforcement learning to assess the quality of motion and generate patient-specific analysis. We evaluated this system with seven therapists using the dataset from 15 patients performing three exercises. The results show that therapists have higher usage intent on our system than a traditional system without patient-specific analysis (p &lt; 0.05). While presenting richer information (p &lt; 0.10), our system significantly reduces therapists' effort on assessment (p &lt; 0.10) and improves their agreement on assessment from 0.66 to 0.71 F1-scores (p &lt; 0.01). This work discusses the importance of human centered design and development of a machine learning-based decision support system that presents contextually relevant information and salient explanations on its prediction for better adoption in practice.",10.1145/3415227,https://doi.org/10.1145/3415227,Proceedings of the ACM on Human-Computer Interaction,"Lee, Min Hun; Siewiorek, Daniel P.; Smailagic, Asim; Bernardino, Alexandre; Bermúdez i Badia, Sergi",2020,80,"@article{2-3429,
  title = {Co-Design and Evaluation of an Intelligent Decision Support System for Stroke Rehabilitation Assessment},
  author = {Lee, Min Hun and Siewiorek, Daniel P. and Smailagic, Asim and Bernardino, Alexandre and Bermúdez i Badia, Sergi},
  year = {2020},
  doi = {10.1145/3415227},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Executing, Advising","Decision-maker, Decision-subject, Knowledge provider","Alter decision outcomes, Change cognitive demands, Change trust","Change AI responses, Update AI competence","patient-specific analysis, predicted score",NA,"Autonomous System, Interactive interface",Yes,Yes
2-34297,springernature,Sample-to-answer platform for the clinical evaluation of COVID-19 using a deep learning-assisted smartphone-based assay,"Since many lateral flow assays( LFA) are tested daily, the improvement in accuracy can greatly impact individual patient care and public health. However, current self-testing for COVID-19 detection suffers from low accuracy, mainly due to the LFA sensitivity and reading ambiguities. Here, we present deep learning-assisted smartphone-based LFA( SMART AI -LFA) diagnostics to provide accurate decisions with higher sensitivity. Combining clinical data learning and two-step algorithms enables a cradle-free on-site assay with higher accuracy than the untrained individuals and human experts via blind tests of clinical data( n = 1500). We acquired 98% accuracy across 135 smartphone application-based clinical tests with different users/smartphones. Furthermore, with more low-titer tests, we observed that the accuracy of SMART AI -LFA was maintained at over 99% while there was a significant decrease in human accuracy, indicating the reliable performance of SMART AI -LFA. We envision a smartphone-based SMART AI -LFA that allows continuously enhanced performance by adding clinical tests and satisfies the new criterion for digitalized real-time diagnostics. The lateral flow assay( LFA) has been considered a rapid test tool but with low sensitivity hampering the precise diagnosis. Here, the authors report bioengineered enrichment tools for LFAs with enhanced sensitivity and specificity that can reinforce LFA’s clinical performance.",10.1038/s41467-023-38104-5,http://dx.doi.org/10.1038/s41467-023-38104-5,Nature Communications,"Lee , Seungmin;Kim , Sunmok;Yoon, Dae Sung;Park, Jeong Soo;Woo, Hyowon;Lee , Dongho;Cho, Sung-Yeon;Park, Chulmin;Yoo , Yong Kyoung;Lee, Ki- Baek;Lee, Jeong Hoon",2023,67,"@article{2-34297,
  title = {Sample-to-answer platform for the clinical evaluation of COVID-19 using a deep learning-assisted smartphone-based assay},
  author = {Lee, Seungmin and Kim, Sunmok and Yoon, Dae Sung and Park, Jeong Soo and Woo, Hyowon and Lee, Dongho and Cho, Sung-Yeon and Park, Chulmin and Yoo, Yong Kyoung and Lee, Ki-Baek and Lee, Jeong Hoon},
  year = {2023},
  doi = {10.1038/s41467-023-38104-5},
  journal = {Nature Communications}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Executing, Analyzing","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3430,acm,"Unremarkable AI: Fitting Intelligent Decision Support into Critical, Clinical Decision-Making Processes","Clinical decision support tools (DST) promise improved healthcare outcomes by offering data-driven insights. While effective in lab settings, almost all DSTs have failed in practice. Empirical research diagnosed poor contextual fit as the cause. This paper describes the design and field evaluation of a radically new form of DST. It automatically generates slides for clinicians' decision meetings with subtly embedded machine prognostics. This design took inspiration from the notion of Unremarkable Computing, that by augmenting the users' routines technology/AI can have significant importance for the users yet remain unobtrusive. Our field evaluation suggests clinicians are more likely to encounter and embrace such a DST. Drawing on their responses, we discuss the importance and intricacies of finding the right level of unremarkableness in DST design, and share lessons learned in prototyping critical AI systems as a situated experience.",10.1145/3290605.3300468,https://doi.org/10.1145/3290605.3300468,Conference on Human Factors in Computing Systems,"Yang, Qian; Steinfeld, Aaron; Zimmerman, John",2019,379,"@inproceedings{2-3430,
  title = {Unremarkable AI: Fitting Intelligent Decision Support into Critical, Clinical Decision-Making Processes},
  author = {Yang, Qian and Steinfeld, Aaron and Zimmerman, John},
  year = {2019},
  doi = {10.1145/3290605.3300468},
  booktitle = {Conference on Human Factors in Computing Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Knowledge provider","Alter decision outcomes, Change trust, Change cognitive demands, Change affective-perceptual, Shape ethical norms",no such info,machine prognostics,NA,"Visual, Textual, Autonomous System",Yes,Yes
2-3431,acm,Harnessing Biomedical Literature to Calibrate Clinicians’ Trust in AI Decision Support Systems,"Clinical decision support tools (DSTs), powered by Artificial Intelligence (AI), promise to improve clinicians’ diagnostic and treatment decision-making. However, no AI model is always correct. DSTs must enable clinicians to validate each AI suggestion, convincing them to take the correct suggestions while rejecting its errors. While prior work often tried to do so by explaining AI’s inner workings or performance, we chose a different approach: We investigated how clinicians validated each other’s suggestions in practice (often by referencing scientific literature) and designed a new DST that embraces these naturalistic interactions. This design uses GPT-3 to draw literature evidence that shows the AI suggestions’ robustness and applicability (or the lack thereof). A prototyping study with clinicians from three disease areas proved this approach promising. Clinicians’ interactions with the prototype also revealed new design and research opportunities around (1) harnessing the complementary strengths of literature-based and predictive decision supports; (2) mitigating risks of de-skilling clinicians; and (3) offering low-data decision support with literature.",10.1145/3544548.3581393,https://doi.org/10.1145/3544548.3581393,ACM CHI Conference on Human Factors in Computing Systems,"Yang, Qian; Hao, Yuexing; Quan, Kexin; Yang, Stephen; Zhao, Yiran; Kuleshov, Volodymyr; Wang, Fei",2023,96,"@inproceedings{2-3431,
  title = {Harnessing Biomedical Literature to Calibrate Clinicians’ Trust in AI Decision Support Systems},
  author = {Yang, Qian and Hao, Yuexing and Quan, Kexin and Yang, Stephen and Zhao, Yiran and Kuleshov, Volodymyr and Wang, Fei},
  year = {2023},
  booktitle = {Proceedings of the ACM CHI Conference on Human Factors in Computing Systems},
  doi = {10.1145/3544548.3581393}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising","Decision-maker, Knowledge provider","Change trust, Change cognitive demands, Alter decision outcomes","Update AI competence, Change AI responses","AI suggestions, a concise summary of a comprehensive set of evidence","evidence instead of explanations, imperfect evidence, additional sources of clinical evidence: medical images and -omics data, highly valuing the evidence of AI suggestion applicability, trust biomedical literature for debasing AI suggestions",NA,Yes,Yes
2-3432,acm,“If I Had All the Time in the World”: Ophthalmologists’ Perceptions of Anchoring Bias Mitigation in Clinical AI Support,"Clinical needs and technological advances have resulted in increased use of Artificial Intelligence (AI) in clinical decision support. However, such support can introduce new and amplify existing cognitive biases. Through contextual inquiry and interviews, we set out to understand the use of an existing AI support system by ophthalmologists. We identified concerns regarding anchoring bias and a misunderstanding of the AI’s capabilities. Following, we evaluated clinicians’ perceptions of three bias mitigation strategies as integrated into their existing decision support system. While clinicians recognised the danger of anchoring bias, we identified a concern around the impact of bias mitigation on procedure time. Our participants were divided in their expectations of any positive impact on diagnostic accuracy, stemming from varying reliance on the decision support. Our results provide insights into the challenges of integrating bias mitigation into AI decision support.",10.1145/3544548.3581513,https://doi.org/10.1145/3544548.3581513,ACM CHI Conference on Human Factors in Computing Systems,"Bach, Anne Kathrine Petersen; Nørgaard, Trine Munch; Brok, Jens Christian; van Berkel, Niels",2023,0,"@inproceedings{2-3432,
  title = {“If I Had All the Time in the World”: Ophthalmologists’ Perceptions of Anchoring Bias Mitigation in Clinical AI Support},
  author = {Bach, Anne Kathrine Petersen and Nørgaard, Trine Munch and Brok, Jens Christian and van Berkel, Niels},
  year = {2023},
  doi = {10.1145/3544548.3581513},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing",Decision-maker,"Alter decision outcomes, Change cognitive demands, Change trust, Change affective-perceptual",no such info,NA,anchoring bias,"Textual, Visual, Conversational/Natural Language",Yes,Yes
2-3433,acm,Write It Like You See It: Detectable Differences in Clinical Notes by Race Lead to Differential Model Recommendations,"Clinical notes are becoming an increasingly important data source for machine learning (ML) applications in healthcare. Prior research has shown that deploying ML models can perpetuate existing biases against racial minorities, as bias can be implicitly embedded in data. In this study, we investigate the level of implicit race information available to ML models and human experts and the implications of model-detectable differences in clinical notes. Our work makes three key contributions. First, we find that models can identify patient self-reported race from clinical notes even when the notes are stripped of explicit indicators of race. Second, we determine that human experts are not able to accurately predict patient race from the same redacted clinical notes. Finally, we demonstrate the potential harm of this implicit information in a simulation study, and show that models trained on these race-redacted clinical notes can still perpetuate existing biases in clinical treatment decisions.",10.1145/3514094.3534203,https://doi.org/10.1145/3514094.3534203,"AAAI/ACM Conference on AI, Ethics, and Society","Adam, Hammaad; Yang, Ming Ying; Cato, Kenrick; Baldini, Ioana; Senteio, Charles; Celi, Leo Anthony; Zeng, Jiaming; Singh, Moninder; Ghassemi, Marzyeh",2022,58,"@inproceedings{2-3433,
  title = {Write It Like You See It: Detectable Differences in Clinical Notes by Race Lead to Differential Model Recommendations},
  author = {Adam, Hammaad and Yang, Ming Ying and Cato, Kenrick and Baldini, Ioana and Senteio, Charles and Celi, Leo Anthony and Zeng, Jiaming and Singh, Moninder and Ghassemi, Marzyeh},
  year = {2022},
  doi = {10.1145/3514094.3534203},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3434,acm,The Trust Recovery Journey. The Effect of Timing of Errors on the Willingness to Follow AI Advice.,"Complementing human decision-making with AI advice offers substantial advantages. However, humans do not always trust AI advice appropriately and are overly sensitive to incidental AI errors, even in cases with overall good performance. Today’s research still needs to uncover the underlying aspects of trust decline and recovery over time in repeated human-AI interactions. Our work investigates the consequences of incidental AI error on (self-reported) trust and participants’ reliance on AI advice. Results from our experiment, where 208 participants evaluated 14 legal cases before and after receiving algorithmic advice, showed that trust significantly decreased after early and late errors but was rapidly restored in both scenarios. Reliance significantly dropped only for early errors but not for late errors. In both scenarios, reliance was able to be restored. Results suggest that late (compared to early) errors are less drastic in trust loss and allow quicker recovery. These findings align with an interpretation in which humans can build up trust over time if a system is performing well, making them more tolerant of incidental AI errors.",10.1145/3640543.3645167,https://doi.org/10.1145/3640543.3645167,International Conference on Intelligent User Interfaces (IUI),"Kahr, Patricia K.; Rooks, Gerrit; Snijders, Chris; Willemsen, Martijn C.",2024,19,"@inproceedings{2-3434,
  title = {The Trust Recovery Journey: The Effect of Timing of Errors on the Willingness to Follow AI Advice},
  author = {Kahr, Patricia K. and Rooks, Gerrit and Snijders, Chris and Willemsen, Martijn C.},
  booktitle = {Proceedings of the International Conference on Intelligent User Interfaces (IUI)},
  year = {2024},
  doi = {10.1145/3640543.3645167}
}",Empirical contributions,Law / Policy / Governance,Individual,"Advising, Analyzing","Decision-maker, Knowledge provider","Change trust, Alter decision outcomes","Update AI competence, Change AI responses","incidental AI error, recommendations",NA,Interactive interface,Yes,Yes
2-3435,acm,Who Should Take This Task? Dynamic Decision Support for Crowd Workers,"Context: The success of crowdsourced software development (CSD) depends on a large crowd of trustworthy software workers who are registering and submitting for their interested tasks in exchange of financial gains. Preliminary analysis on software worker behaviors reveals an alarming task-quitting rate of 82.9%.Goal: The objective of this study is to empirically investigate worker decision factors and provide better decision support in order to improve the success and efficiency of CSD.Method: We propose a novel problem formulation, DCW-DS, and an analytics-based decision support methodology to guide workers in acceptance of offered development tasks. DCS-DS is evaluated using more than one year's real-world data from TopCoder, the leading CSD platform.Results: Applying Random Forest based machine learning with dynamic updates, we can predict a worker as being a likely quitter with 99% average precision and 99% average recall accuracy. Similarly, we achieved 78% average precision and 88% average recall for the worker winner class. For workers just following the top three task recommendations, we have shown that the average quitting rate goes down below 6%.Conclusions: In total, the proposed method can be used to improve total success rate as well as reduce quitting rate of tasks performed.",10.1145/2961111.2962594,https://doi.org/10.1145/2961111.2962594,International Symposium on Empirical Software Engineering and Measurement (ESEM),"Yang, Ye; Karim, Muhammad Rezaul; Saremi, Razieh; Ruhe, Guenther",2016,77,"@inproceedings{2-3435,
  title = {Who Should Take This Task? Dynamic Decision Support for Crowd Workers},
  author = {Yang, Ye and Karim, Muhammad Rezaul and Saremi, Razieh and Ruhe, Guenther},
  year = {2016},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement (ESEM)},
  doi = {10.1145/2961111.2962594}
}",Empirical contributions,Software / Systems / Security,Operational,"Forecasting, Advising",Decision-maker,Alter decision outcomes,no such info,"static features, dynamic features",NA,Textual,Yes,Yes
2-3437,acm,How Do Data Analysts Respond to AI Assistance? A Wizard-of-Oz Study,"Data analysis is challenging as analysts must navigate nuanced decisions that may yield divergent conclusions. AI assistants have the potential to support analysts in planning their analyses, enabling more robust decision making. Though AI-based assistants that target code execution (e.g., Github Copilot) have received significant attention, limited research addresses assistance for both analysis execution and planning. In this work, we characterize helpful planning suggestions and their impacts on analysts’ workflows. We first review the analysis planning literature and crowd-sourced analysis studies to categorize suggestion content. We then conduct a Wizard-of-Oz study (n=13) to observe analysts’ preferences and reactions to planning assistance in a realistic scenario. Our findings highlight subtleties in contextual factors that impact suggestion helpfulness, emphasizing design implications for supporting different abstractions of assistance, forms of initiative, increased engagement, and alignment of goals between analysts and assistants.",10.1145/3613904.3641891,https://doi.org/10.1145/3613904.3641891,CHI Conference on Human Factors in Computing Systems,"Gu, Ken; Grunde-McLaughlin, Madeleine; McNutt, Andrew; Heer, Jeffrey; Althoff, Tim",2024,0,"@inproceedings{2-3437,
  title = {How Do Data Analysts Respond to AI Assistance? A Wizard-of-Oz Study},
  author = {Gu, Ken and Grunde-McLaughlin, Madeleine and McNutt, Andrew and Heer, Jeffrey and Althoff, Tim},
  year = {2024},
  doi = {10.1145/3613904.3641891},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Education / Teaching / Research,Operational,"Advising, Collaborating","Decision-maker, Knowledge provider","Alter decision outcomes, Change cognitive demands, Change trust","Update AI competence, Change AI responses","AI suggestions, planning, explanations","domain knowledge, prior experience",Interactive interface,Yes,Yes
2-3438,acm,“Why Do I Care What’s Similar?” Probing Challenges in AI-Assisted Child Welfare Decision-Making through Worker-AI Interface Design Concepts,"Data-driven AI systems are increasingly used to augment human decision-making in complex, social contexts, such as social work or legal practice. Yet, most existing design knowledge regarding how to best support AI-augmented decision-making comes from studies in comparatively well-defined settings. In this paper, we present findings from design interviews with 12 social workers who use an algorithmic decision support tool (ADS) to assist their day-to-day child maltreatment screening decisions. We generated a range of design concepts, each envisioning different ways of redesigning or augmenting the ADS interface. Overall, workers desired ways to understand the risk score and incorporate contextual knowledge, which move beyond existing notions of AI interpretability. Conversations around our design concepts also surfaced more fundamental concerns around the assumptions underlying statistical prediction, such as inference based on similar historical cases and statistical notions of uncertainty. Based on our findings, we discuss how ADS may be better designed to support the roles of human decision-makers in social decision-making contexts.",10.1145/3532106.3533556,https://doi.org/10.1145/3532106.3533556,ACM Designing Interactive Systems Conference,"Kawakami, Anna; Sivaraman, Venkatesh; Stapleton, Logan; Cheng, Hao-Fei; Perer, Adam; Wu, Zhiwei Steven; Zhu, Haiyi; Holstein, Kenneth",2022,0,"@inproceedings{2-3438,
  title = {Why Do I Care What's Similar? Probing Challenges in AI-Assisted Child Welfare Decision-Making through Worker-AI Interface Design Concepts},
  author = {Kawakami, Anna and Sivaraman, Venkatesh and Stapleton, Logan and Cheng, Hao-Fei and Perer, Adam and Wu, Zhiwei Steven and Zhu, Haiyi and Holstein, Kenneth},
  year = {2022},
  doi = {10.1145/3532106.3533556},
  booktitle = {ACM Designing Interactive Systems Conference}
}",Empirical contributions,"Law / Policy / Governance, Everyday / Employment / Public Service",Operational,Advising,"Decision-maker, Knowledge provider","Change trust, Restrict human agency","Change AI responses, Shape AI for accountability",NA,"need to know how the algorithmic reasoning diverges from their own, inability to distinguish between positive versus negative engagement with services",Interactive interface,Yes,Yes
2-3439,acm,Understanding Uncertainty: How Lay Decision-makers Perceive and Interpret Uncertainty in Human-AI Decision Making,"Decision Support Systems (DSS) based on Machine Learning (ML) often aim to assist lay decision-makers, who are not math-savvy, in making high-stakes decisions. However, existing ML-based DSS are not always transparent about the probabilistic nature of ML predictions and how uncertain each prediction is. This lack of transparency could give lay decision-makers a false sense of reliability. Growing calls for AI transparency have led to increasing efforts to quantify and communicate model uncertainty. However, there are still gaps in knowledge regarding how and why the decision-makers utilize ML uncertainty information in their decision process. Here, we conducted a qualitative, think-aloud user study with 17 lay decision-makers who interacted with three different DSS: 1) interactive visualization, 2) DSS based on an ML model that provides predictions without uncertainty information, and 3) the same DSS with uncertainty information. Our qualitative analysis found that communicating uncertainty about ML predictions forced participants to slow down and think analytically about their decisions. This in turn made participants more vigilant, resulting in reduction in over-reliance on ML-based DSS. Our work contributes empirical knowledge on how lay decision-makers perceive, interpret, and make use of uncertainty information when interacting with DSS. Such foundational knowledge informs the design of future ML-based DSS that embrace transparent uncertainty communication.",10.1145/3581641.3584033,https://doi.org/10.1145/3581641.3584033,ACM International Conference on Intelligent User Interfaces (IUI),"Prabhudesai, Snehal; Yang, Leyao; Asthana, Sumit; Huan, Xun; Liao, Q. Vera; Banovic, Nikola",2023,1,"@inproceedings{2-3439,
  title = {Understanding Uncertainty: How Lay Decision-makers Perceive and Interpret Uncertainty in Human-AI Decision Making},
  author = {Prabhudesai, Snehal and Yang, Leyao and Asthana, Sumit and Huan, Xun and Liao, Q. Vera and Banovic, Nikola},
  year = {2023},
  doi = {10.1145/3581641.3584033},
  booktitle = {Proceedings of the ACM International Conference on Intelligent User Interfaces (IUI)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Individual,"Analyzing, Forecasting, Advising",Decision-maker,"Change trust, Change cognitive demands",no such info,"uncertainty, prediction of alternative",NA,"Visual, Interactive interface",Yes,Yes
2-3442,acm,AI-Moderated Decision-Making: Capturing and Balancing Anchoring Bias in Sequential Decision Tasks,"Decision-making involves biases from past experiences, which are difficult to perceive and eliminate. We investigate a specific type of anchoring bias, in which decision-makers are anchored by their own recent decisions, e.g.&nbsp;a college admission officer sequentially reviewing students. We propose an algorithm that identifies existing anchored decisions, reduces sequential dependencies to previous decisions, and mitigates decision inaccuracies post-hoc with 2% increased agreement to ground-truth on a large-scale college admission decision data set. A crowd-sourced study validates this algorithm on product preferences (5% increased agreement). To avoid biased decisions ex-ante, we propose a procedure that presents instances in an order that reduces anchoring bias in real-time. Tested in another crowd-sourced study, it reduces bias and increases agreement to ground-truth by 7%. Our work reinforces individuals with similar characteristics to be treated similarly, independent of when they were reviewed in the decision-making process.",10.1145/3491102.3517443,https://doi.org/10.1145/3491102.3517443,ACM CHI Conference on Human Factors in Computing Systems,"Echterhoff, Jessica Maria; Yarmand, Matin; McAuley, Julian",2022,126,"@inproceedings{2-3442,
  title={AI-Moderated Decision-Making: Capturing and Balancing Anchoring Bias in Sequential Decision Tasks},
  author={Echterhoff, Jessica Maria and Yarmand, Matin and McAuley, Julian},
  year={2022},
  doi={10.1145/3491102.3517443},
  booktitle={Proceedings of the ACM CHI Conference on Human Factors in Computing Systems}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service, Education / Teaching / Research",Operational,"Monitoring, Advising, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3443,acm,Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness,"Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.",10.1145/3442188.3445912,https://doi.org/10.1145/3442188.3445912,"ACM Conference on Fairness, Accountability, and Transparency (FAccT)","Finocchiaro, Jessie; Maio, Roland; Monachou, Faidra; Patro, Gourab K; Raghavan, Manish; Stoica, Ana-Andreea; Tsirtsis, Stratis",2021,98,"@inproceedings{2-3443,
  title = {Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness},
  author = {Finocchiaro, Jessie and Maio, Roland and Monachou, Faidra and Patro, Gourab K and Raghavan, Manish and Stoica, Ana-Andreea and Tsirtsis, Stratis},
  year = {2021},
  doi = {10.1145/3442188.3445912},
  booktitle = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (FAccT)}
}","Vision contributions, Empirical contributions","Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service, Finance / Business / Economy, Law / Policy / Governance",Institutional,"Advising, Forecasting","Decision-maker, Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3448,acm,Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation,"Despite impressive performance in many benchmark datasets, AI models can still make mistakes, especially among out-of-distribution examples. It remains an open question how such imperfect models can be used effectively in collaboration with humans. Prior work has focused on AI assistance that helps people make individual high-stakes decisions, which is not scalable for a large amount of relatively low-stakes decisions, e.g., moderating social media comments. Instead, we propose conditional delegation as an alternative paradigm for human-AI collaboration where humans create rules to indicate trustworthy regions of a model. Using content moderation as a testbed, we develop novel interfaces to assist humans in creating conditional delegation rules and conduct a randomized experiment with two datasets to simulate in-distribution and out-of-distribution scenarios. Our study demonstrates the promise of conditional delegation in improving model performance and provides insights into design for this novel paradigm, including the effect of AI explanations.",10.1145/3491102.3501999,https://doi.org/10.1145/3491102.3501999,ACM CHI Conference on Human Factors in Computing Systems,"Lai, Vivian; Carton, Samuel; Bhatnagar, Rajat; Liao, Q. Vera; Zhang, Yunfeng; Tan, Chenhao",2022,2,"@inproceedings{2-3448,
  title = {Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation},
  author = {Lai, Vivian and Carton, Samuel and Bhatnagar, Rajat and Liao, Q. Vera and Zhang, Yunfeng and Tan, Chenhao},
  year = {2022},
  doi = {10.1145/3491102.3501999},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Media / Communication / Entertainment,Operational,"Explaining, Advising, Collaborating",Decision-maker,"Alter decision outcomes, Change cognitive demands","Change AI responses, Update AI competence","predicted labels, local explanations, global explanations",conditional delegation,Interactive interface,Yes,Yes
2-3449,acm,How Much Decision Power Should (A)I Have?: Investigating Patients’ Preferences Towards AI Autonomy in Healthcare Decision Making,"Despite the growing potential of artificial intelligence (AI) in improving clinical decision making, patients' perspectives on the use of AI for their care decision making are underexplored. In this paper, we investigate patients’ preferences towards the autonomy of AI in assisting healthcare decision making. We conducted interviews and an online survey using an interactive narrative and speculative AI prototypes to elicit participants’ preferred choices of using AI in a pregnancy care context. The analysis of the interviews and in-story responses reveals that patients’ preferences for AI autonomy vary per person and context, and may change over time. This finding suggests the need for involving patients in defining and reassessing the appropriate level of AI assistance for healthcare decision making. Departing from these varied preferences for AI autonomy, we discuss implications for incorporating patient-centeredness in designing AI-powered healthcare decision making.",10.1145/3613904.3642883,https://doi.org/10.1145/3613904.3642883,CHI Conference on Human Factors in Computing Systems,"Kim, Dajung; Vegt, Niko; Visch, Valentijn; Bos-De Vos, Marina",2024,0,"@inproceedings{2-3449,
  title = {How Much Decision Power Should (A)I Have?: Investigating Patients’ Preferences Towards {AI} Autonomy in Healthcare Decision Making},
  author = {Kim, Dajung and Vegt, Niko and Visch, Valentijn and Bos-De Vos, Marina},
  year = {2024},
  doi = {10.1145/3613904.3642883},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-subject, Decision-maker, Knowledge provider","Change trust, Change affective-perceptual, Restrict human agency","Change AI responses, Shape AI for accountability",prediction of alternative,"potential vulnerability, reducing autonomy, HCP Involvement in All Levels of AI Autonomy, balancing the distribution of decision power among different decision-making stakeholders, varying preferences for AI autonomy",Interactive interface,Yes,Yes
2-3450,acm,Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments,"Despite vigorous debates about the technical characteristics of risk assessments being deployed in the U.S. criminal justice system, remarkably little research has studied how these tools affect actual decision-making processes. After all, risk assessments do not make definitive decisions—they inform judges, who are the final arbiters. It is therefore essential that considerations of risk assessments be informed by rigorous studies of how judges actually interpret and use them. This paper takes a first step toward such research on human interactions with risk assessments through a controlled experimental study on Amazon Mechanical Turk. We found several behaviors that call into question the supposed efficacy and fairness of risk assessments: our study participants 1) underperformed the risk assessment even when presented with its predictions, 2) could not effectively evaluate the accuracy of their own or the risk assessment's predictions, and 3) exhibited behaviors fraught with ""disparate interactions,"" whereby the use of risk assessments led to higher risk predictions about black defendants and lower risk predictions about white defendants. These results suggest the need for a new ""algorithm-in-the-loop"" framework that places machine learning decision-making aids into the sociotechnical context of improving human decisions rather than the technical context of generating the best prediction in the abstract. If risk assessments are to be used at all, they must be grounded in rigorous evaluations of their real-world impacts instead of in their theoretical potential.",10.1145/3287560.3287563,https://doi.org/10.1145/3287560.3287563,"Conference on Fairness, Accountability, and Transparency (FAccT)","Green, Ben; Chen, Yiling",2019,14,"@inproceedings{2-3450,
  title = {Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments},
  author = {Green, Be and Chen, Yiling},
  year = {2019},
  doi = {10.1145/3287560.3287563},
  booktitle = {Conference on Fairness, Accountability, and Transparency (FAccT)}
}",Empirical contributions,Law / Policy / Governance,Operational,"Forecasting, Advising",Decision-maker,"Alter decision outcomes, Shape ethical norms, Change trust, Change affective-perceptual","Update AI competence, Shape AI for accountability",prediction of alternative,NA,Interactive interface,Yes,Yes
2-3451,acm,Who Goes First? Influences of Human-AI Workflow on Decision Making in Clinical Imaging,"Details of the designs and mechanisms in support of human-AI collaboration must be considered in the real-world fielding of AI technologies. A critical aspect of interaction design for AI-assisted human decision making are policies about the display and sequencing of AI inferences within larger decision-making workflows. We have a poor understanding of the influences of making AI inferences available before versus after human review of a diagnostic task at hand. We explore the effects of providing AI assistance at the start of a diagnostic session in radiology versus after the radiologist has made a provisional decision. We conducted a user study where 19 veterinary radiologists identified radiographic findings present in patients’ X-ray images, with the aid of an AI tool. We employed two workflow configurations to analyze (i) anchoring effects, (ii) human-AI team diagnostic performance and agreement, (iii) time spent and confidence in decision making, and (iv) perceived usefulness of the AI. We found that participants who are asked to register provisional responses in advance of reviewing AI inferences are less likely to agree with the AI regardless of whether the advice is accurate and, in instances of disagreement with the AI, are less likely to seek the second opinion of a colleague. These participants also reported that the AI advice to be less useful. Surprisingly, requiring provisional decisions on cases in advance of the display of AI inferences did not lengthen the time participants spent on the task. The study provides generalizable and actionable insights for the deployment of clinical AI tools in human-in-the-loop systems and introduces a methodology for studying alternative designs for human-AI collaboration. We make our experimental platform available as open source to facilitate future research on the influence of alternate designs on human-AI workflows.",10.1145/3531146.3533193,https://doi.org/10.1145/3531146.3533193,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Fogliato, Riccardo; Chappidi, Shreya; Lungren, Matthew; Fisher, Paul; Wilson, Diane; Fitzke, Michael; Parkinson, Mark; Horvitz, Eric; Inkpen, Kori; Nushi, Besmira",2022,114,"@inproceedings{2-3451,
  title = {Who Goes First? Influences of Human-AI Workflow on Decision Making in Clinical Imaging},
  author = {Fogliato, Riccardo and  Chappidi, Shreya and Lungren, Matthew; Fisher, Paul and Wilson, Diane and Fitzke, Michael and Parkinson, Mark and Horvitz, Eric; Inkpen, Kori and Nushi, Besmira},
  year = {2022},
  doi = {10.1145/3531146.3533193},
  booktitle = {ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Analyzing, Advising","Decision-maker, Knowledge provider","Alter decision outcomes, Change affective-perceptual, Change trust, Change cognitive demands",no such info,"AI confidence, AI flags","provisional responses, one-step workflow, two-step workflow","Visual, Interactive interface",Yes,Yes
2-3453,acm,Learning to Prescribe Interventions for Tuberculosis Patients Using Digital Adherence Data,"Digital Adherence Technologies (DATs) are an increasingly popular method for verifying patient adherence to many medications. We analyze data from one city served by 99DOTS, a phone-call-based DAT deployed for Tuberculosis (TB) treatment in India where nearly 3 million people are afflicted with the disease each year. The data contains nearly 17,000 patients and 2.1M dose records. We lay the groundwork for learning from this real-world data, including a method for avoiding the effects of unobserved interventions in training data used for machine learning. We then construct a deep learning model, demonstrate its interpretability, and show how it can be adapted and trained in three different clinical scenarios to better target and improve patient care. In the real-time risk prediction setting our model could be used to proactively intervene with 21% more patients and before 76% more missed doses than current heuristic baselines. For outcome prediction, our model performs 40% better than baseline methods, allowing cities to target more resources to clinics with a heavier burden of patients at risk of failure. Finally, we present a case study demonstrating how our model can be trained in an end-to-end decision focused learning setting to achieve 15% better solution quality in an example decision problem faced by health workers.",10.1145/3292500.3330777,https://doi.org/10.1145/3292500.3330777,ACM SIGKDD Conference on Knowledge Discovery and Data Mining,"Killian, Jackson A.; Wilder, Bryan; Sharma, Amit; Choudhary, Vinod; Dilkina, Bistra; Tambe, Milind",2019,82,"@inproceedings{2-3453,
  title = {Learning to Prescribe Interventions for Tuberculosis Patients Using Digital Adherence Data},
  author = {Killian, Jackson A. and Wilder, Bryan and Sharma, Amit and Choudhary, Vinod and Dilkina, Bistra and Tambe, Milind},
  year = {2019},
  doi = {10.1145/3292500.3330777},
  booktitle = {Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3456,acm,Avoiding Decision Fatigue with AI-Assisted Decision-Making,"During online browsing, e.g.&nbsp;when looking to select a movie to watch, we are often confronted with multiple rejection-selection steps which can lead to tens or hundreds of decisions made in quick succession. It is unclear if showing the next “best” item, as often employed by standard recommenders, is the most efficient way to help users select an item. In this work, we show that we can reduce the number of decisions to selection with a reinforcement learning-based Decision Minimizer Network (DMN). By implementing a step-aware reward function we can penalize long sequences, leading to fewer decisions having to be made by humans. Using a task to select a movie to watch, we show that we can reduce the number of decisions to selection by 39% compared to heuristic strategies and by 20% compared to standard recommender while increasing user selection satisfaction. Minimizing the number of decision steps can finally help to reduce decision fatigue, which refers to the deteriorating quality of decisions made by an individual after a long session of decision steps, and help to prevent infinite scrolling.",10.1145/3627043.3659569,https://doi.org/10.1145/3627043.3659569,"ACM Conference on User Modeling, Adaptation and Personalization (UMAP)","Echterhoff, Jessica Maria; Melkote, Aditya; Kancherla, Sujen; McAuley, Julian",2024,7,"@inproceedings{2-3456,
  title = {Avoiding Decision Fatigue with AI-Assisted Decision-Making},
  author = {Echterhoff, Jessica Maria and Melkote, Aditya and Kancherla, Sujen and McAuley, Julian},
  year = {2024},
  doi = {10.1145/3627043.3659569},
  booktitle = {ACM Conference on User Modeling, Adaptation and Personalization (UMAP)}
}",Algorithmic contributions,"Media / Communication / Entertainment, Everyday / Employment / Public Service",Individual,"Advising, Executing",Decision-maker,"Alter decision outcomes, Change affective-perceptual, Change cognitive demands",no such info,recommendations,NA,Interactive interface,Yes,Yes
2-3457,acm,Who Is Included in Human Perceptions of AI?: Trust and Perceived Fairness around Healthcare AI and Cultural Mistrust,"Emerging research suggests that people trust algorithmic decisions less than human decisions. However, different populations, particularly in marginalized communities, may have different levels of trust in human decision-makers. Do people who mistrust human decision-makers perceive human decisions to be more trustworthy and fairer than algorithmic decisions? Or do they trust algorithmic decisions as much as or more than human decisions? We examine the role of mistrust in human systems in people’s perceptions of algorithmic decisions. We focus on healthcare Artificial Intelligence (AI), group-based medical mistrust, and Black people in the United States. We conducted a between-subjects online experiment to examine people’s perceptions of skin cancer screening decisions made by an AI versus a human physician depending on their medical mistrust, and we conducted interviews to understand how to cultivate trust in healthcare AI. Our findings highlight that research around human experiences of AI should consider critical differences in social groups.",10.1145/3411764.3445570,https://doi.org/10.1145/3411764.3445570,CHI Conference on Human Factors in Computing Systems,"Lee, Min Kyung; Rich, Katherine",2021,211,"@inproceedings{2-3457,
  title = {Who Is Included in Human Perceptions of AI?: Trust and Perceived Fairness around Healthcare AI and Cultural Mistrust},
  author = {Lee, Min Kyung and Rich, Katherine},
  year = {2021},
  doi = {10.1145/3411764.3445570},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,"Institutional, Operational","Forecasting, Executing","Decision-maker, Decision-subject","Change trust, Shape ethical norms",no such info,"AI description, data-driven AI description, data-driven AI description with different statements about the AI fairness, system accuracy","mistrust levels depends on previous experiences, mistrust and racial backgrounds are noticeably associated with pre-existing notion of how biased or unbiased AI is",NA,Yes,Yes
2-34585,springernature,Leveraging Machine Learning to Automatically Derive Robust Decision Strategies from Imperfect Knowledge of the Real World,"Teaching people clever heuristics is a promising approach to improve decision-making under uncertainty. The theory of resource rationality makes it possible to leverage machine learning to discover optimal heuristics automatically. One bottleneck of this approach is that the resulting decision strategies are only as good as the model of the decision problem that the machine learning methods were applied to. This is problematic because even domain experts cannot give complete and fully accurate descriptions of the decisions they face. To address this problem, we develop strategy discovery methods that are robust to potential inaccuracies in the description of the scenarios in which people will use the discovered decision strategies. The basic idea is to derive the strategy that will perform best in expectation across all possible real-world problems that could have given rise to the likely erroneous description that a domain expert provided. To achieve this, our method uses a probabilistic model of how the description of a decision problem might be corrupted by biases in human judgment and memory. Our method uses this model to perform Bayesian inference on which real-world scenarios might have given rise to the provided descriptions. We applied our Bayesian approach to robust strategy discovery in two domains: planning and risky choice. In both applications, we find that our approach is more robust to errors in the description of the decision problem and that teaching the strategies it discovers significantly improves human decision-making in scenarios where approaches ignoring the risk that the description might be incorrect are ineffective or even harmful. The methods developed in this article are an important step towards leveraging machine learning to improve human decision-making in the real world because they tackle the problem that the real world is fundamentally uncertain.",10.1007/s42113-022-00141-6,http://dx.doi.org/10.1007/s42113-022-00141-6,Computational Brain & Behavior,"Mehta, Aashay;Jain, Yash Raj;Kemtur, Anirudha;Stojcheski, Jugoslav;Consul, Saksham;Tošić, Mateo;Lieder, Falk",2022,7,"@article{2-34585,
  title={Leveraging Machine Learning to Automatically Derive Robust Decision Strategies from Imperfect Knowledge of the Real World},
  author={Mehta, Aashay and Jain, Yash Raj and Kemtur, Anirudha and Stojcheski, Jugoslav and Consul, Saksham and Tošić, Mateo and Lieder, Falk},
  year={2022},
  doi={10.1007/s42113-022-00141-6},
  journal={Computational Brain \& Behavior}
}",Methodological contributions,"Education / Teaching / Research, Generic / Abstract / Domain-agnostic",Operational,"Executing, Analyzing, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-3459,acm,VIME: Visual Interactive Model Explorer for Identifying Capabilities and Limitations of Machine Learning Models for Sequential Decision-Making,"Ensuring that Machine Learning (ML) models make correct and meaningful inferences is necessary for the broader adoption of such models into high-stakes decision-making scenarios. Thus, ML model engineers increasingly use eXplainable AI (XAI) tools to investigate the capabilities and limitations of their ML models before deployment. However, explaining sequential ML models, which make a series of decisions at each timestep, remains challenging. We present Visual Interactive Model Explorer (VIME), an XAI toolbox that enables ML model engineers to explain decisions of sequential models in different “what-if” scenarios. Our evaluation with 14 ML experts, who investigated two existing sequential ML models using VIME and a baseline XAI toolbox to explore “what-if” scenarios, showed that VIME made it easier to identify and explain instances when the models made wrong decisions compared to the baseline. Our work informs the design of future interactive XAI mechanisms for evaluating sequential ML-based decision support systems.",10.1145/3654777.3676323,https://doi.org/10.1145/3654777.3676323,ACM Symposium on User Interface Software and Technology (UIST),"Das Antar, Anindya; Molaei, Somayeh; Chen, Yan-Ying; Lee, Matthew L; Banovic, Nikola",2024,0,"@inproceedings{2-3459,
  title = {VIME: Visual Interactive Model Explorer for Identifying Capabilities and Limitations of Machine Learning Models for Sequential Decision-Making},
  author = {Das Antar, Anindya and Molaei, Somayeh and Chen, Yan-Ying and Lee, Matthew L and Banovic, Nikola},
  year = {2024},
  booktitle = {Proceedings of the ACM Symposium on User Interface Software and Technology (UIST)},
  doi = {10.1145/3654777.3676323}
}",System/Artifact contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery, Transportation / Mobility / Planning",Operational,"Explaining, Advising","Decision-maker, Knowledge provider",Alter decision outcomes,Update AI competence,scenario-based interactive model exploration,domain knowledge,Visual,Yes,Yes
2-3461,acm,On the Impact of Explanations on Understanding of Algorithmic Decision-Making,"Ethical principles for algorithms are gaining importance as more and more stakeholders are affected by ""high-risk"" algorithmic decision-making (ADM) systems. Understanding how these systems work enables stakeholders to make informed decisions and to assess the systems’ adherence to ethical values. Explanations are a promising way to create understanding, but current explainable artificial intelligence (XAI) research does not always consider existent theories on how understanding is formed and evaluated. In this work, we aim to contribute to a better understanding of understanding by conducting a qualitative task-based study with 30 participants, including users and affected stakeholders. We use three explanation modalities (textual, dialogue, and interactive) to explain a ""high-risk"" ADM system to participants and analyse their responses both inductively and deductively, using the ""six facets of understanding"" framework by Wiggins &amp; McTighe [63]. Our findings indicate that the ""six facets"" framework is a promising approach to analyse participants’ thought processes in understanding, providing categories for both rational and emotional understanding. We further introduce the ""dialogue"" modality as a valid explanation approach to increase participant engagement and interaction with the ""explainer"", allowing for more insight into their understanding in the process. Our analysis further suggests that individuality in understanding affects participants’ perceptions of algorithmic fairness, demonstrating the interdependence between understanding and ADM assessment that previous studies have outlined. We posit that drawing from theories on learning and understanding like the ""six facets"" and leveraging explanation modalities can guide XAI research to better suit explanations to learning processes of individuals and consequently enable their assessment of ethical values of ADM systems.",10.1145/3593013.3594054,https://doi.org/10.1145/3593013.3594054,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Schmude, Timothée; Koesten, Laura; Möller, Torsten; Tschiatschek, Sebastian",2023,26,"@inproceedings{2-3461,
  title={On the Impact of Explanations on Understanding of Algorithmic Decision-Making},
  author={Schmude, Timothée and Koesten, Laura and Möller, Torsten and Tschiatschek, Sebastian},
  year={2023},
  doi={10.1145/3593013.3594054},
  booktitle={Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}",Empirical contributions,"Everyday / Employment / Public Service, Generic / Abstract / Domain-agnostic",Institutional,"Explaining, Advising","Decision-maker, Stakeholder","Change cognitive demands, Change trust",no such info,"prediction of alternative, textual explanations",NA,"Textual, Conversational/Natural Language, Interactive interface, Auditory",Yes,Yes
2-3462,acm,The Selective Labels Problem: Evaluating Algorithmic Predictions in the Presence of Unobservables,"Evaluating whether machines improve on human performance is one of the central questions of machine learning. However, there are many domains where the data is selectively labeled, in the sense that the observed outcomes are themselves a consequence of the existing choices of the human decision-makers. For instance, in the context of judicial bail decisions, we observe the outcome of whether a defendant fails to return for their court appearance only if the human judge decides to release the defendant on bail. This selective labeling makes it harder to evaluate predictive models as the instances for which outcomes are observed do not represent a random sample of the population. Here we propose a novel framework for evaluating the performance of predictive models on selectively labeled data. We develop an approach called contraction which allows us to compare the performance of predictive models and human decision-makers without resorting to counterfactual inference. Our methodology harnesses the heterogeneity of human decision-makers and facilitates effective evaluation of predictive models even in the presence of unmeasured confounders (unobservables) which influence both human decisions and the resulting outcomes. Experimental results on real world datasets spanning diverse domains such as health care, insurance, and criminal justice demonstrate the utility of our evaluation metric in comparing human decisions and machine predictions.",10.1145/3097983.3098066,https://doi.org/10.1145/3097983.3098066,ACM SIGKDD Conference on Knowledge Discovery and Data Mining,"Lakkaraju, Himabindu; Kleinberg, Jon; Leskovec, Jure; Ludwig, Jens; Mullainathan, Sendhil",2017,246,"@inproceedings{2-3462,
  title = {The Selective Labels Problem: Evaluating Algorithmic Predictions in the Presence of Unobservables},
  author = {Lakkaraju, Himabindu and Kleinberg, Jon and Leskovec, Jure and Ludwig, Jens and Mullainathan, Sendhil},
  year = {2017},
  doi = {10.1145/3097983.3098066},
  booktitle = {Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery, Law / Policy / Governance, Finance / Business / Economy",Operational,"Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-3464,acm,Effects of AI and Logic-Style Explanations on Users’ Decisions Under Different Levels of Uncertainty,"Existing eXplainable Artificial Intelligence (XAI) techniques support people in interpreting AI advice. However, although previous work evaluates the users’ understanding of explanations, factors influencing the decision support are largely overlooked in the literature. This article addresses this gap by studying the impact of user uncertainty, AI correctness, and the interaction between AI uncertainty and explanation logic-styles for classification tasks. We conducted two separate studies: one requesting participants to recognize handwritten digits and one to classify the sentiment of reviews. To assess the decision making, we analyzed the task performance, agreement with the AI suggestion, and the user’s reliance on the XAI interface elements. Participants make their decision relying on three pieces of information in the XAI interface (image or text instance, AI prediction, and explanation). Participants were shown one explanation style (between-participants design) according to three styles of logical reasoning (inductive, deductive, and abductive). This allowed us to study how different levels of AI uncertainty influence the effectiveness of different explanation styles. The results show that user uncertainty and AI correctness on predictions significantly affected users’ classification decisions considering the analyzed metrics. In both domains (images and text), users relied mainly on the instance to decide. Users were usually overconfident about their choices, and this evidence was more pronounced for text. Furthermore, the inductive style explanations led to overreliance on the AI advice in both domains—it was the most persuasive, even when the AI was incorrect. The abductive and deductive styles have complex effects depending on the domain and the AI uncertainty levels.",10.1145/3588320,https://doi.org/10.1145/3588320,ACM Transactions on Interactive Intelligent Systems,"Cau, Federico Maria; Hauptmann, Hanna; Spano, Lucio Davide; Tintarev, Nava",2023,45,"@article{2-3464,
  title={Effects of AI and Logic-Style Explanations on Users' Decisions Under Different Levels of Uncertainty},
  author={Cau, Federico Maria and Hauptmann, Hanna and Spano, Lucio Davide and Tintarev, Nava},
  year={2023},
  doi={10.1145/3588320},
  journal={ACM Transactions on Interactive Intelligent Systems}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research",Individual,"Forecasting, Explaining, Advising",Decision-maker,"Change affective-perceptual, Change cognitive demands, Change trust, Alter decision outcomes",no such info,"AI suggestions, prediction of alternative, image, text instance, inductive explanations, abductive explanations",NA,"Interactive interface, Textual, Visual",Yes,Yes
2-3467,acm,Competent but Rigid: Identifying the Gap in Empowering AI to Participate Equally in Group Decision-Making,"Existing research on human-AI collaborative decision-making focuses mainly on the interaction between AI and individual decision-makers. There is a limited understanding of how AI may perform in group decision-making. This paper presents a wizard-of-oz study in which two participants and an AI form a committee to rank three English essays. One novelty of our study is that we adopt a speculative design by endowing AI equal power to humans in group decision-making. We enable the AI to discuss and vote equally with other human members. We find that although the voice of AI is considered valuable, AI still plays a secondary role in the group because it cannot fully follow the dynamics of the discussion and make progressive contributions. Moreover, the divergent opinions of our participants regarding an “equal AI” shed light on the possible future of human-AI relations.",10.1145/3544548.3581131,https://doi.org/10.1145/3544548.3581131,ACM CHI Conference on Human Factors in Computing Systems,"Zheng, Chengbo; Wu, Yuheng; Shi, Chuhan; Ma, Shuai; Luo, Jiehui; Ma, Xiaojuan",2023,44,"@inproceedings{2-3467,
  title={Competent but Rigid: Identifying the Gap in Empowering AI to Participate Equally in Group Decision-Making},
  author={Zheng, Chengbo and Wu, Yuheng and Shi, Chuhan and Ma, Shuai and Luo, Jiehui and Ma, Xiaojuan},
  year={2023},
  doi={10.1145/3544548.3581131},
  booktitle={Proceedings of the ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,"Education / Teaching / Research, Generic / Abstract / Domain-agnostic",Operational,"Collaborating, Advising",Decision-maker,"Change cognitive demands, Alter decision outcomes, Change trust",no such info,"AI suggestions, textual explanations",NA,Autonomous System,Yes,Yes
2-3469,acm,The Impact of Imperfect XAI on Human-AI Decision-Making,"Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility of the explanations being incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making behavior in a bird species identification task, taking into account their level of expertise and an explanation's level of assertiveness. Our findings reveal the influence of imperfect XAI and humans' level of expertise on their reliance on AI and human-AI team performance. We also discuss how explanations can deceive decision-makers during human-AI collaboration. Hence, we shed light on the impacts of imperfect XAI in the field of computer-supported cooperative work and provide guidelines for designers of human-AI collaboration systems.",10.1145/3641022,https://doi.org/10.1145/3641022,Proceedings of the ACM on Human-Computer Interaction,"Morrison, Katelyn; Spitzer, Philipp; Turri, Violet; Feng, Michelle; Kühl, Niklas; Perer, Adam",2024,12,"@article{2-3469,
  title = {The Impact of Imperfect XAI on Human-AI Decision-Making},
  author = {Morrison, Katelyn and Spitzer, Philipp and Turri, Violet and Feng, Michelle and Kühl, Niklas and Perer, Adam},
  year = {2024},
  doi = {10.1145/3641022},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Education / Teaching / Research,Individual,"Explaining, Advising",Decision-maker,"Alter decision outcomes, Change trust, Change cognitive demands",no such info,"incorrect explanations, visual explanations, example-based explanations, natural language explanations",domain knowledge,Interactive interface,Yes,Yes
2-3470,acm,"Co-design of Human-centered, Explainable AI for Clinical Decision Support","eXplainable AI (XAI) involves two intertwined but separate challenges: the development of techniques to extract explanations from black-box AI models and the way such explanations are presented to users, i.e., the explanation user interface. Despite its importance, the second aspect has received limited attention so far in the literature. Effective AI explanation interfaces are fundamental for allowing human decision-makers to take advantage and oversee high-risk AI systems effectively. Following an iterative design approach, we present the first cycle of prototyping-testing-redesigning of an explainable AI technique and its explanation user interface for clinical Decision Support Systems (DSS). We first present an XAI technique that meets the technical requirements of the healthcare domain: sequential, ontology-linked patient data, and multi-label classification tasks. We demonstrate its applicability to explain a clinical DSS, and we design a first prototype of an explanation user interface. Next, we test such a prototype with healthcare providers and collect their feedback with a two-fold outcome: First, we obtain evidence that explanations increase users’ trust in the XAI system, and second, we obtain useful insights on the perceived deficiencies of their interaction with the system, so we can re-design a better, more human-centered explanation interface.",10.1145/3587271,https://doi.org/10.1145/3587271,ACM Transactions on Interactive Intelligent Systems,"Panigutti, Cecilia; Beretta, Andrea; Fadda, Daniele; Giannotti, Fosca; Pedreschi, Dino; Perotti, Alan; Rinzivillo, Salvatore",2023,101,"@article{2-3470,
  title = {Co-design of Human-centered, Explainable AI for Clinical Decision Support},
  author = {Panigutti, Cecilia and Beretta, Andrea and Fadda, Daniele and Giannotti, Fosca and Pedreschi, Dino and Perotti, Alan and Rinzivillo, Salvatore},
  year = {2023},
  doi = {10.1145/3587271},
  journal = {ACM Transactions on Interactive Intelligent Systems}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-3471,acm,Anchoring Bias Affects Mental Model Formation and User Reliance in Explainable AI Systems,"EXplainable Artificial Intelligence (XAI) approaches are used to bring transparency to machine learning and artificial intelligence models, and hence, improve the decision-making process for their end-users. While these methods aim to improve human understanding and their mental models, cognitive biases can still influence a user’s mental model and decision-making in ways that system designers do not anticipate. This paper presents research on cognitive biases due to ordering effects in intelligent systems. We conducted a controlled user study to understand how the order of observing system weaknesses and strengths can affect the user’s mental model, task performance, and reliance on the intelligent system, and we investigate the role of explanations in addressing this bias. Using an explainable video activity recognition tool in the cooking domain, we asked participants to verify whether a set of kitchen policies are being followed, with each policy focusing on a weakness or a strength. We controlled the order of the policies and the presence of explanations to test our hypotheses. Our main finding shows that those who observed system strengths early-on were more prone to automation bias and made significantly more errors due to positive first impressions of the system, while they built a more accurate mental model of the system competencies. On the other hand, those who encountered weaknesses earlier made significantly fewer errors since they tended to rely more on themselves, while they also underestimated model competencies due to having a more negative first impression of the model. Our work presents strong findings that aim to make intelligent system designers aware of such biases when designing such tools.",10.1145/3397481.3450639,https://doi.org/10.1145/3397481.3450639,International Conference on Intelligent User Interfaces (IUI),"Nourani, Mahsan; Roy, Chiradeep; Block, Jeremy E; Honeycutt, Donald R; Rahman, Tahrima; Ragan, Eric; Gogate, Vibhav",2021,116,"@inproceedings{2-3471,
  title = {Anchoring Bias Affects Mental Model Formation and User Reliance in Explainable AI Systems},
  author = {Nourani, Mahsan and Roy, Chiradeep and Block, Jeremy E and Honeycutt, Donald R and Rahman, Tahrima and Ragan, Eric and Gogate, Vibhav},
  year = {2021},
  doi = {10.1145/3397481.3450639},
  booktitle = {International Conference on Intelligent User Interfaces (IUI)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance",Operational,"Explaining, Advising",Decision-maker,"Shape ethical norms, Change trust, Alter decision outcomes, Change affective-perceptual",no such info,"the order of the policies, model explanations",cognitive biases,"Visual, Interactive interface",Yes,Yes
2-3472,acm,Directive Explanations for Monitoring the Risk of Diabetes Onset: Introducing Directive Data-Centric Explanations and Combinations to Support What-If Explorations,"Explainable artificial intelligence is increasingly used in machine learning (ML) based decision-making systems in healthcare. However, little research has compared the utility of different explanation methods in guiding healthcare experts for patient care. Moreover, it is unclear how useful, understandable, actionable and trustworthy these methods are for healthcare experts, as they often require technical ML knowledge. This paper presents an explanation dashboard that predicts the risk of diabetes onset and explains those predictions with data-centric, feature-importance, and example-based explanations. We designed an interactive dashboard to assist healthcare experts, such as nurses and physicians, in monitoring the risk of diabetes onset and recommending measures to minimize risk. We conducted a qualitative study with 11 healthcare experts and a mixed-methods study with 45 healthcare experts and 51 diabetic patients to compare the different explanation methods in our dashboard in terms of understandability, usefulness, actionability, and trust. Results indicate that our participants preferred our representation of data-centric explanations that provide local explanations with a global overview over other methods. Therefore, this paper highlights the importance of visually directive data-centric explanation method for assisting healthcare experts to gain actionable insights from patient health records. Furthermore, we share our design implications for tailoring the visual representation of different explanation methods for healthcare experts.",10.1145/3581641.3584075,https://doi.org/10.1145/3581641.3584075,ACM International Conference on Intelligent User Interfaces (IUI),"Bhattacharya, Aditya; Ooge, Jeroen; Stiglic, Gregor; Verbert, Katrien",2023,51,"@inproceedings{2-3472,
  title = {Directive Explanations for Monitoring the Risk of Diabetes Onset: Introducing Directive Data-Centric Explanations and Combinations to Support What-If Explorations},
  author = {Bhattacharya, Aditya and Ooge, Jeroen and Stiglic, Gregor and Verbert, Katrien},
  year = {2023},
  booktitle = {Proceedings of the ACM International Conference on Intelligent User Interfaces (IUI)},
  doi = {10.1145/3581641.3584075}
}","System/Artifact contributions, Empirical contributions",Healthcare / Medicine / Surgery,Operational,"Forecasting, Explaining, Advising","Decision-maker, Knowledge provider, Decision-subject","Change trust, Change affective-perceptual, Alter decision outcomes","Update AI competence, Change AI responses","data-centric explanations, example-based explanations, feature-importance explanations",NA,"Visual, Interactive interface",Yes,Yes
2-34722,springernature,Detection and evaluation of bias-inducing features in machine learning,"The cause-to-effect analysis can help us decompose all the likely causes of a problem, such as an undesirable business situation or unintended harm to the individual( s). This implies that we can identify how the problems are inherited, rank the causes to help prioritize fixes, simplify a complex problem and visualize them. In the context of machine learning( ML) , one can use cause-to-effect analysis to understand the reason for the biased behavior of the system. For example, we can examine the root causes of biases by checking each feature for a potential cause of bias in the model. To approach this, one can apply small changes to a given feature or a pair of features in the data, following some guidelines and observing how it impacts the decision made by the model( i. e. , model prediction). Therefore, we can use cause-to-effect analysis to identify the potential bias-inducing features, even when these features are originally are unknown. This is important since most current methods require a pre-identification of sensitive features for bias assessment and can actually miss other relevant bias-inducing features, which is why systematic identification of such features is necessary. Moreover, it often occurs that to achieve an equitable outcome, one has to take into account sensitive features in the model decision. Therefore, it should be up to the domain experts to decide based on their knowledge of the context of a decision whether bias induced by specific features is acceptable or not. In this study, we propose an approach for systematically identifying all bias-inducing features of a model to help support the decision-making of domain experts. Our technique is based on the idea of swapping the values of the features and computing the divergences in the distribution of the model prediction using different distance functions. We evaluated our technique using four well-known datasets to showcase how our contribution can help spearhead the standard procedure when developing, testing, maintaining, and deploying fair/equitable machine learning systems.",10.1007/s10664-023-10409-5,http://dx.doi.org/10.1007/s10664-023-10409-5,Empirical Software Engineering,"Openja, Moses;Laberge, Gabriel;Khomh, Foutse",2023,8,"@article{2-34722,
  title={Detection and evaluation of bias-inducing features in machine learning},
  author={Openja, Moses and Laberge, Gabriel and Khomh, Foutse},
  year={2023},
  journal={Empirical Software Engineering},
  doi={10.1007/s10664-023-10409-5}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Operational,"Auditing, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-3473,acm,Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems,"Explainable artificially intelligent (XAI) systems form part of sociotechnical systems, e.g., human+AI teams tasked with making decisions. Yet, current XAI systems are rarely evaluated by measuring the performance of human+AI teams on actual decision-making tasks. We conducted two online experiments and one in-person think-aloud study to evaluate two currently common techniques for evaluating XAI systems: (1) using proxy, artificial tasks such as how well humans predict the AI's decision from the given explanations, and (2) using subjective measures of trust and preference as predictors of actual performance. The results of our experiments demonstrate that evaluations with proxy tasks did not predict the results of the evaluations with the actual decision-making tasks. Further, the subjective measures on evaluations with actual decision-making tasks did not predict the objective performance on those same tasks. Our results suggest that by employing misleading evaluation methods, our field may be inadvertently slowing its progress toward developing human+AI teams that can reliably perform better than humans or AIs alone.",10.1145/3377325.3377498,https://doi.org/10.1145/3377325.3377498,International Conference on Intelligent User Interfaces (IUI),"Buçinca, Zana; Lin, Phoebe; Gajos, Krzysztof Z.; Glassman, Elena L.",2020,419,"@inproceedings{2-3473,
  title = {Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems},
  author = {Buçinca, Zana and Lin, Phoebe and Gajos, Krzysztof Z. and Glassman, Elena L.},
  year = {2020},
  doi = {10.1145/3377325.3377498},
  booktitle = {International Conference on Intelligent User Interfaces (IUI)}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Explaining, Advising",Decision-maker,"Change trust, Change cognitive demands, Alter decision outcomes",no such info,"textual explanations, recommendations, inductive explanations",NA,"Textual, Visual, Interactive interface",Yes,Yes
2-3477,acm,A Data-Driven Decision Support Framework for Player Churn Analysis in Online Games,"Faced with saturated market and fierce competition of online games, it is of great value to analyze the causes of the player churn for improving the game product, maintaining the player retention. A large number of research efforts on churn analysis have been made into churn prediction, which can achieve a sound accuracy benefiting from the booming of AI technologies. However, game publishers are usually unable to apply high-accuracy prediction methods in practice for preventing or relieving the churn due to the lack of the specific decision support (e.g., why they leave and what to do next). In this study, we fully exploit the expertise in online games and propose a comprehensive data-driven decision support framework for addressing game player churn. We first define the churn analysis in online games from a commercial perspective and elaborate the core demands of game publishers for churn analysis. Then we employ and improve the cutting-edge eXplainable AI (XAI) methods to predict player churn and analyze the potential churn causes. The possible churn causes can finally guide game publishers to make specific decisions of revision or intervention in our designed procedure. We demonstrate the effectiveness and high practical value of the framework by conducting extensive experiments on a real-world large-scale online game, Justice PC. The whole decision support framework, bringing interesting and valuable insights, also receives quite positive reviews from the game product and operation teams. Notably, the whole pipeline is readily transplanted to other online systems for decision support to address similar issues.",10.1145/3580305.3599759,https://doi.org/10.1145/3580305.3599759,ACM SIGKDD Conference on Knowledge Discovery and Data Mining,"Xiong, Yu; Wu, Runze; Zhao, Shiwei; Tao, Jianrong; Shen, Xudong; Lyu, Tangjie; Fan, Changjie; Cui, Peng",2023,4,"@inproceedings{2-3477,
  title = {A Data-Driven Decision Support Framework for Player Churn Analysis in Online Games},
  author = {Xiong, Yu and Wu, Runze and Zhao, Shiwei and Tao, Jianrong and Shen, Xudong and Lyu, Tangjie and Fan, Changjie and Cui, Peng},
  year = {2023},
  doi = {10.1145/3580305.3599759},
  booktitle = {ACM SIGKDD Conference on Knowledge Discovery and Data Mining}
}","Methodological contributions, System/Artifact contributions",Media / Communication / Entertainment,Organizational,"Forecasting, Explaining, Advising, Analyzing","Decision-maker, Developer",Alter decision outcomes,"Update AI competence, Change AI responses","potential churn causes, individual churn explanation, group churn explanation, global churn explanation, churn feature dependence explanation",NA,Interactive interface,Yes,Yes
2-34776,springernature,CPAS: the UK’s national machine learning-based hospital capacity planning system for COVID-19,"The coronavirus disease 2019( COVID-19) global pandemic poses the threat of overwhelming healthcare systems with unprecedented demands for intensive care resources. Managing these demands cannot be effectively conducted without a nationwide collective effort that relies on data to forecast hospital demands on the national, regional, hospital and individual levels. To this end, we developed the COVID-19 Capacity Planning and Analysis System( CPAS) —a machine learning-based system for hospital resource planning that we have successfully deployed at individual hospitals and across regions in the UK in coordination with NHS Digital. In this paper, we discuss the main challenges of deploying a machine learning-based decision support system at national scale, and explain how CPAS addresses these challenges by( 1) defining the appropriate learning problem, ( 2) combining bottom-up and top-down analytical approaches, ( 3) using state-of-the-art machine learning algorithms, ( 4) integrating heterogeneous data sources, and( 5) presenting the result with an interactive and transparent interface. CPAS is one of the first machine learning-based systems to be deployed in hospitals on a national scale to address the COVID-19 pandemic—we conclude the paper with a summary of the lessons learned from this experience.",10.1007/s10994-020-05921-4,http://dx.doi.org/10.1007/s10994-020-05921-4,Machine Learning,"Qian, Zhaozhi;Alaa, Ahmed M.;Schaar, Mihaela",2021,50,"@article{2-34776,
  title={CPAS: the UK’s national machine learning-based hospital capacity planning system for {COVID}-19},
  author={Qian, Zhaozhi and Alaa, Ahmed M. and Schaar, Mihaela},
  year={2021},
  journal={Machine Learning},
  doi={10.1007/s10994-020-05921-4}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Organizational,"Forecasting, Analyzing","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-3482,acm,Beyond Fairness: Reparative Algorithms to Address Historical Injustices of Housing Discrimination in the US,"Fairness in Machine Learning (ML) has mostly focused on interrogating the fairness of a particular decision point with assumptions made that the people represented in the data have been fairly treated throughout history. However, fairness cannot be ultimately achieved if such assumptions are not valid. This is the case for mortgage lending discrimination in the US, which should be critically understood as the result of historically accumulated injustices that were enacted through public policies and private practices including redlining, racial covenants, exclusionary zoning, and predatory inclusion, among others. With the erroneous assumptions of historical fairness in ML, Black borrowers with low income and low wealth are considered as a given condition in a lending algorithm, thus rejecting loans to them would be considered a “fair” decision even though Black borrowers were historically excluded from homeownership and wealth creation. To emphasize such issues, we introduce case studies using contemporary mortgage lending data as well as historical census data in the US. First, we show that historical housing discrimination has differentiated each racial group’s baseline wealth which is a critical input for algorithmically determining mortgage loans. The second case study estimates the cost of housing reparations in the algorithmic lending context to redress historical harms because of such discriminatory housing policies. Through these case studies, we envision what reparative algorithms would look like in the context of housing discrimination in the US. This work connects to emerging scholarship on how algorithmic systems can contribute to redressing past harms through engaging with reparations policies and programs.",10.1145/3531146.3533160,https://doi.org/10.1145/3531146.3533160,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","So, Wonyoung; Lohia, Pranay; Pimplikar, Rakesh; Hosoi, A.E.; D'Ignazio, Catherine",2022,31,"@inproceedings{2-3482,
  title = {Beyond Fairness: Reparative Algorithms to Address Historical Injustices of Housing Discrimination in the US},
  author = {So, Wonyoung and Lohia, Pranay and Pimplikar, Rakesh and Hosoi, A. E. and D'Ignazio, Catherine},
  year = {2022},
  doi = {10.1145/3531146.3533160},
  booktitle = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}","Vision contributions, Empirical contributions",Finance / Business / Economy,Institutional,"Analyzing, Advising","Decision-maker, Stakeholder, Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-34848,springernature,Clinical-grade computational pathology using weakly supervised deep learning on whole slide images,"The development of decision support systems for pathology and their deployment in clinical practice have been hindered by the need for large manually annotated datasets. To overcome this problem, we present a multiple instance learning-based deep learning system that uses only the reported diagnoses as labels for training, thereby avoiding expensive and time-consuming pixel-wise manual annotations. We evaluated this framework at scale on a dataset of 44, 732 whole slide images from 15, 187 patients without any form of data curation. Tests on prostate cancer, basal cell carcinoma and breast cancer metastases to axillary lymph nodes resulted in areas under the curve above 0. 98 for all cancer types. Its clinical application would allow pathologists to exclude 65–75% of slides while retaining 100% sensitivity. Our results show that this system has the ability to train accurate classification models at unprecedented scale, laying the foundation for the deployment of computational decision support systems in clinical practice. A deep learning model trained on real-world digital pathology data achieves clinical performance in cancer diagnosis.",10.1038/s41591-019-0508-1,http://dx.doi.org/10.1038/s41591-019-0508-1,Nature Medicine,"Campanella, Gabriele;Hanna, Matthew G.;Geneslaw, Luke;Miraflor, Allen;Werneck Krauss Silva, Vitor;Busam, Klaus J.;Brogi, Edi;Reuter, Victor E.;Klimstra, David S.;Fuchs, Thomas J.",2019,2734,"@article{2-34848,
  title = {Clinical-grade computational pathology using weakly supervised deep learning on whole slide images},
  author = {Campanella, Gabriele and Hanna, Matthew G. and Geneslaw, Luke and Miraflor, Allen and Werneck Krauss Silva, Vitor and Busam, Klaus J. and Brogi, Edi and Reuter, Victor E. and Klimstra, David S. and Fuchs, Thomas J.},
  year = {2019},
  journal = {Nature Medicine},
  doi = {10.1038/s41591-019-0508-1}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Executing, Analyzing","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-3485,acm,SearchEHR: A Family History Search System for Clinical Decision Support,"Finding patients with specific clinical conditions, such as having a familial disease history of diabetes, is an important task for clinical decision support. Clinical notes in Electronic Health Records (EHR), which document the patient medical history and familial disease history, are valuable resources for patient cohort selection. However, such information is difficult to discover in clinical text, and full-text search techniques often fail due to the unique characteristics of clinical language. We describe a system—SearchEHR—that combines Natural Language Processing (NLP) and Information Retrieval (IR) techniques to facilitate utilising clinical notes to find cohorts of patients, with a special focus on family disease history.",10.1145/3459637.3481986,https://doi.org/10.1145/3459637.3481986,ACM International Conference on Information and Knowledge Management (CIKM),"Dai, Xiang; Rybinski, Maciej; Karimi, Sarvnaz",2021,1,"@inproceedings{2-3485,
  title     = {SearchEHR: A Family History Search System for Clinical Decision Support},
  author    = {Dai, Xiang and Rybinski, Maciej and Karimi, Sarvnaz},
  year      = {2021},
  doi       = {10.1145/3459637.3481986},
  booktitle = {Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM)}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,Analyzing,"Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3486,acm,Invisible Influence: Artificial Intelligence and the Ethics of Adaptive Choice Architectures,"For several years, scholars have (for good reason) been largely preoccupied with worries about the use of artificial intelligence and machine learning (AI/ML) tools to make decisions about us. Only recently has significant attention turned to a potentially more alarming problem: the use of AI/ML to influence our decision-making. The contexts in which we make decisions–what behavioral economists call our choice architectures–are increasingly technologically-laden. Which is to say: algorithms increasingly determine, in a wide variety of contexts, both the sets of options we choose from and the way those options are framed. Moreover, artificial intelligence and machine learning (AI/ML) makes it possible for those options and their framings–the choice architectures–to be tailored to the individual chooser. They are constructed based on information collected about our individual preferences, interests, aspirations, and vulnerabilities, with the goal of influencing our decisions. At the same time, because we are habituated to these technologies we pay them little notice. They are, as philosophers of technology put it, transparent to us–effectively invisible. I argue that this invisible layer of technological mediation, which structures and influences our decision-making, renders us deeply susceptible to manipulation. Absent a guarantee that these technologies are not being used to manipulate and exploit, individuals will have little reason to trust them.",10.1145/3306618.3314286,https://doi.org/10.1145/3306618.3314286,"AAAI/ACM Conference on AI, Ethics, and Society","Susser, Daniel",2019,98,"@inproceedings{2-3486,
  title={Invisible Influence: Artificial Intelligence and the Ethics of Adaptive Choice Architectures},
  author={Susser, Daniel},
  year={2019},
  doi={10.1145/3306618.3314286},
  booktitle={AAAI/ACM Conference on AI, Ethics, and Society}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Individual,"Explaining, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3487,acm,Designing Theory-Driven User-Centric Explainable AI,"From healthcare to criminal justice, artificial intelligence (AI) is increasingly supporting high-consequence human decisions. This has spurred the field of explainable AI (XAI). This paper seeks to strengthen empirical application-specific investigations of XAI by exploring theoretical underpinnings of human decision making, drawing from the fields of philosophy and psychology. In this paper, we propose a conceptual framework for building human-centered, decision-theory-driven XAI based on an extensive review across these fields. Drawing on this framework, we identify pathways along which human cognitive patterns drives needs for building XAI and how XAI can mitigate common cognitive biases. We then put this framework into practice by designing and implementing an explainable clinical diagnostic tool for intensive care phenotyping and conducting a co-design exercise with clinicians. Thereafter, we draw insights into how this framework bridges algorithm-generated explanations and human decision-making theories. Finally, we discuss implications for XAI design and development.",10.1145/3290605.3300831,https://doi.org/10.1145/3290605.3300831,Conference on Human Factors in Computing Systems,"Wang, Danding; Yang, Qian; Abdul, Ashraf; Lim, Brian Y.",2019,11,"@inproceedings{2-3487,
  title = {Designing Theory-Driven User-Centric Explainable AI},
  author = {Wang, Danding and Yang, Qian and Abdul, Ashraf and Lim, Brian Y.},
  year = {2019},
  doi = {10.1145/3290605.3300831},
  booktitle = {Conference on Human Factors in Computing Systems}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,"Explaining, Advising","Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-3488,acm,The Hidden Rules of Hanabi: How Humans Outperform AI Agents,"Games that feature multiple players, limited communication, and partial information are particularly challenging for AI agents. In the cooperative card game Hanabi, which possesses all of these attributes, AI agents fail to achieve scores comparable to even first-time human players. Through an observational study of three mixed-skill Hanabi play groups, we identify the techniques used by humans that help to explain their superior performance compared to AI. These concern physical artefact manipulation, coordination play, role establishment, and continual rule negotiation. Our findings extend previous accounts of human performance in Hanabi, which are purely in terms of theory-of-mind reasoning, by revealing more precisely how this form of collective decision-making is enacted in skilled human play. Our interpretation points to a gap in the current capabilities of AI agents to perform cooperative tasks.",10.1145/3544548.3581550,https://doi.org/10.1145/3544548.3581550,ACM CHI Conference on Human Factors in Computing Systems,"Sidji, Matthew; Smith, Wally; Rogerson, Melissa J.",2023,14,"@inproceedings{2-3488,
  title = {The Hidden Rules of Hanabi: How Humans Outperform AI Agents},
  author = {Sidji, Matthew and Smith, Wally and Rogerson, Melissa J.},
  year = {2023},
  doi = {10.1145/3544548.3581550},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Media / Communication / Entertainment,Individual,"Executing, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-3489,acm,Artificial Artificial Intelligence: Measuring Influence of AI 'Assessments' on Moral Decision-Making,"Given AI's growing role in modeling and improving decision-making, how and when to present users with feedback is an urgent topic to address. We empirically examined the effect of feedback from false AI on moral decision-making about donor kidney allocation. We found some evidence that judgments about whether a patient should receive a kidney can be influenced by feedback about participants' own decision-making perceived to be given by AI, even if the feedback is entirely random. We also discovered different effects between assessments presented as being from human experts and assessments presented as being from AI.",10.1145/3375627.3375870,https://doi.org/10.1145/3375627.3375870,"AAAI/ACM Conference on AI, Ethics, and Society","Chan, Lok; Doyle, Kenzie; McElfresh, Duncan; Conitzer, Vincent; Dickerson, John P.; Schaich Borg, Jana; Sinnott-Armstrong, Walter",2020,18,"@inproceedings{2-3489,
  title = {Artificial Artificial Intelligence: Measuring Influence of AI 'Assessments' on Moral Decision-Making},
  author = {Chan, Lok and Doyle, Kenzie and McElfresh, Duncan and Conitzer, Vincent and Dickerson, John P. and Schaich Borg, Jana and Sinnott-Armstrong, Walter},
  year = {2020},
  doi = {10.1145/3375627.3375870},
  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Decision-subject, Knowledge provider","Change trust, Alter decision outcomes",no such info,identity cues,evaluation,Textual,Yes,Yes
2-3491,acm,Escalation Risks from Language Models in Military and Diplomatic Decision-Making,"Governments are increasingly considering integrating autonomous AI agents in high-stakes military and foreign-policy decision-making, especially with the emergence of advanced generative AI models like GPT-4. Our work aims to scrutinize the behavior of multiple AI agents in simulated wargames, specifically focusing on their predilection to take escalatory actions that may exacerbate multilateral conflicts. Drawing on political science and international relations literature about escalation dynamics, we design a novel wargame simulation and scoring framework to assess the escalation risks of actions taken by these agents in different scenarios. Contrary to prior studies, our research provides both qualitative and quantitative insights and focuses on large language models (LLMs). We find that all five studied off-the-shelf LLMs show forms of escalation and difficult-to-predict escalation patterns. We observe that models tend to develop arms-race dynamics, leading to greater conflict, and in rare cases, even to the deployment of nuclear weapons. Qualitatively, we also collect the models’ reported reasoning for chosen actions and observe worrying justifications based on deterrence and first-strike tactics. Given the high stakes of military and foreign-policy contexts, we recommend further examination and cautious consideration before deploying autonomous language model agents for strategic military or diplomatic decision-making.",10.1145/3630106.3658942,https://doi.org/10.1145/3630106.3658942,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Rivera, Juan-Pablo; Mukobi, Gabriel; Reuel, Anka; Lamparth, Max; Smith, Chandler; Schneider, Jacquelyn",2024,95,"@inproceedings{2-3491,
  title = {Escalation Risks from Language Models in Military and Diplomatic Decision-Making},
  author = {Rivera, Juan-Pablo and Mukobi, Gabriel and Reuel, Anka and Lamparth, Max and Smith, Chandler and Schneider, Jacquelyn},
  year = {2024},
  doi = {10.1145/3630106.3658942},
  booktitle = {ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}",Empirical contributions,Defense / Military / Emergency,Institutional,"Forecasting, Advising, Analyzing, Executing","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-3492,acm,Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil's Advocate,"Group decision making plays a crucial role in our complex and interconnected world. The rise of AI technologies has the potential to provide data-driven insights to facilitate group decision making, although it is found that groups do not always utilize AI assistance appropriately. In this paper, we aim to examine whether and how the introduction of a devil’s advocate in the AI-assisted group decision making processes could help groups better utilize AI assistance and change the perceptions of group processes during decision making. Inspired by the exceptional conversational capabilities exhibited by modern large language models (LLMs), we design four different styles of devil’s advocate powered by LLMs, varying their interactivity (i.e., interactive vs. non-interactive) and their target of objection (i.e., challenge the AI recommendation or the majority opinion within the group). Through a randomized human-subject experiment, we find evidence suggesting that LLM-powered devil’s advocates that argue against the AI model’s decision recommendation have the potential to promote groups’ appropriate reliance on AI. Meanwhile, the introduction of LLM-powered devil’s advocate usually does not lead to substantial increases in people’s perceived workload for completing the group decision making tasks, while interactive LLM-powered devil’s advocates are perceived as more collaborating and of higher quality. We conclude by discussing the practical implications of our findings.",10.1145/3640543.3645199,https://doi.org/10.1145/3640543.3645199,International Conference on Intelligent User Interfaces (IUI),"Chiang, Chun-Wei; Lu, Zhuoran; Li, Zhuoyan; Yin, Ming",2024,106,"@inproceedings{2-3492,
  title = {Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil's Advocate},
  author = {Chiang, Chun-Wei and Lu, Zhuoran and Li, Zhuoyan and Yin, Ming},
  year = {2024},
  doi = {10.1145/3640543.3645199},
  booktitle = {International Conference on Intelligent User Interfaces (IUI)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance",Operational,"Advising, Collaborating",Decision-maker,"Alter decision outcomes, Change trust, Change cognitive demands, Change affective-perceptual",no such info,"prediction of alternative, critiques, recommendations",NA,"Textual, Conversational/Natural Language",Yes,Yes
2-3494,acm,Human Perceptions on Moral Responsibility of AI: A Case Study in AI-Assisted Bail Decision-Making,"How to attribute responsibility for autonomous artificial intelligence (AI) systems’ actions has been widely debated across the humanities and social science disciplines. This work presents two experiments (N=200 each) that measure people’s perceptions of eight different notions of moral responsibility concerning AI and human agents in the context of bail decision-making. Using real-life adapted vignettes, our experiments show that AI agents are held causally responsible and blamed similarly to human agents for an identical task. However, there was a meaningful difference in how people perceived these agents’ moral responsibility; human agents were ascribed to a higher degree of present-looking and forward-looking notions of responsibility than AI agents. We also found that people expect both AI and human decision-makers and advisors to justify their decisions regardless of their nature. We discuss policy and HCI implications of these findings, such as the need for explainable AI in high-stakes scenarios.",10.1145/3411764.3445260,https://doi.org/10.1145/3411764.3445260,CHI Conference on Human Factors in Computing Systems,"Lima, Gabriel; Grgić-Hlača, Nina; Cha, Meeyoung",2021,112,"@inproceedings{2-3494,
  title = {Human Perceptions on Moral Responsibility of AI: A Case Study in AI-Assisted Bail Decision-Making},
  author = {Lima, Gabriel and Grgić-Hlača, Nina and Cha, Meeyoung},
  year = {2021},
  doi = {10.1145/3411764.3445260},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Law / Policy / Governance,Operational,"Advising, Executing",Decision-maker,"Shape ethical norms, Shift responsibility",no such info,NA,NA,Autonomous System,Yes,Yes
2-3495,acm,How Time Pressure in Different Phases of Decision-Making Influences Human-AI Collaboration,"Human cognitive and decision-making abilities depreciate under pressure, motivating the emergence of artificial intelligence (AI) systems as decision support tools to assist people in performing tasks under stress. In this work, we study human decision-making behavior and task performance under time pressure—induced from limitedinitial observation time (time to perform the task before providing an initial response without AI input) andfinal decision time (time to weigh an AI's suggestion before reaching a collective human-AI team answer)—for spatial reasoning and count estimation tasks. Our results show that, while the impact of initial observation time on AI-assisted decision-making was dependent on task nature, participants were more likely to follow AI suggestions when they were provided with longer final decision time; moreover, although participants generally tended to adhere to their initial responses, they had more agency when they were more logically engaged in a task. Our results offer a nuanced understanding of human-AI collaboration under time pressure in different phases of the decision-making process.",10.1145/3610068,https://doi.org/10.1145/3610068,Proceedings of the ACM on Human-Computer Interaction,"Cao, Shiye; Gomez, Catalina; Huang, Chien-Ming",2023,33,"@article{2-3495,
author = {Cao, Shiye and Gomez, Catalina and Huang, Chien-Ming},
title = {How Time Pressure in Different Phases of Decision-Making Influences Human-AI Collaboration},
year = {2023},,
volume = {7},
number = {CSCW2},
doi = {10.1145/3610068},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {277},
numpages = {26}
}",Empirical contributions,"Education / Teaching / Research, Generic / Abstract / Domain-agnostic",Individual,Advising,Decision-maker,"Alter decision outcomes, Change trust, Change affective-perceptual",no such info,AI suggestions,NA,Interactive interface,Yes,Yes
2-3497,acm,Advancing Human-AI Complementarity: The Impact of User Expertise and Algorithmic Tuning on Joint Decision Making,"Human-AI collaboration for decision-making strives to achieve team performance that exceeds the performance of humans or AI alone. However, many factors can impact success of Human-AI teams, including a user’s domain expertise, mental models of an AI system, trust in recommendations, and more. This article reports on a study that examines users’ interactions with three simulated algorithmic models, all with equivalent accuracy rates but each tuned differently in terms of true positive and true negative rates. Our study examined user performance in a non-trivial blood vessel labeling task where participants indicated whether a given blood vessel was flowing or stalled. Users completed 140 trials across multiple stages, first without an AI and then with recommendations from an AI-Assistant. Although all users had prior experience with the task, their levels of proficiency varied widely.Our results demonstrated that while recommendations from an AI-Assistant can aid in users’ decision making, several underlying factors, including user base expertise and complementary human-AI tuning, significantly impact the overall team performance. First, users’ base performance matters, particularly in comparison to the performance level of the AI. Novice users improved, but not to the accuracy level of the AI. Highly proficient users were generally able to discern when they should follow the AI recommendation and typically maintained or improved their performance. Mid-performers, who had a similar level of accuracy to the AI, were most variable in terms of whether the AI recommendations helped or hurt their performance. Second, tuning an AI algorithm to complement users’ strengths and weaknesses also significantly impacted users’ performance. For example, users in our study were better at detecting flowing blood vessels, so when the AI was tuned to reduce false negatives (at the expense of increasing false positives), users were able to reject those recommendations more easily and improve in accuracy. Finally, users’ perception of the AI’s performance relative to their own performance had an impact on whether users’ accuracy improved when given recommendations from the AI. Overall, this work reveals important insights on the complex interplay of factors influencing Human-AI collaboration and provides recommendations on how to design and tune AI algorithms to complement users in decision-making tasks.",10.1145/3534561,https://doi.org/10.1145/3534561,ACM Transactions on Computer-Human Interaction,"Inkpen, Kori; Chappidi, Shreya; Mallari, Keri; Nushi, Besmira; Ramesh, Divya; Michelucci, Pietro; Mandava, Vani; Vepřek, Libuše Hannah; Quinn, Gabrielle",2023,96,"@article{2-3497,
  title = {Advancing Human-AI Complementarity: The Impact of User Expertise and Algorithmic Tuning on Joint Decision Making},
  author = {Inkpen, Kori and Chappidi, Shreya and Mallari, Keri and Nushi, Besmira and Ramesh, Divya and Michelucci, Pietro and Mandava, Vani and Vepřek, Libuše Hannah and Quinn, Gabrielle},
  year = {2023},
  doi = {10.1145/3534561},
  journal = {ACM Transactions on Computer-Human Interaction}
}",Empirical contributions,"Healthcare / Medicine / Surgery, Generic / Abstract / Domain-agnostic",Individual,Advising,Decision-maker,Alter decision outcomes,no such info,"recommendations, human-complementary tuning",domain knowledge,Visual,Yes,Yes
2-3498,acm,The Explanation That Hits Home: The Characteristics of Verbal Explanations That Affect Human Perception in Subjective Decision-Making,"Human-AI collaborative decision-making can achieve better outcomes than either party individually. The success of this collaboration can depend on whether the human decision-maker perceives the AI contribution as beneficial to the decision-making process. Beneficial AI explanations are often described as relevant, convincing, and trustworthy. Yet, we know little about the characteristics of explanations that result in these perceptions. Focusing on collaborative subjective decision-making, using the context of subtle sexism, where explanations can surface new interpretations, we conducted a user study (N=20) to explore the structural and content characteristics that affect perceptions of human and AI-generated verbal (text and audio) explanations. We find four groups of characteristics (Tone, Grammatical Elements, Argumentative Sophistication and Relation to User), and that the effect of these characteristics on the perception of explanations for subtle sexism depends on the perceived author. Thus, we also identify which explanation characteristics participants use to identify the author of an explanation. Demonstrating the relationship between these characteristics and explanation perceptions, we present a categorized set of characteristics that system builders can leverage to produce the appropriate perception of an explanation for various sensitive contexts. We also highlight human perception biases and associated issues resulting from these perceptions.",10.1145/3687056,https://doi.org/10.1145/3687056,Proceedings of the ACM on Human-Computer Interaction,"Ferguson, Sharon; Aoyagui, Paula Akemi; Rizvi, Rimsha; Kim, Young-Ho; Kuzminykh, Anastasia",2024,9,"@article{2-3498,
  title={The Explanation That Hits Home: The Characteristics of Verbal Explanations That Affect Human Perception in Subjective Decision-Making},
  author={Ferguson, Sharon and Aoyagui, Paula Akemi and Rizvi, Rimsha and Kim, Young-Ho and Kuzminykh, Anastasia},
  year={2024},
  doi={10.1145/3687056},
  journal={Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Institutional,"Explaining, Collaborating","Decision-maker, Knowledge provider","Alter decision outcomes, Change trust, Change cognitive demands, Change affective-perceptual",no such info,"textual explanations, audio explanations",explanations,Interactive interface,Yes,Yes
2-3499,acm,Decision Making Strategies and Team Efficacy in Human-AI Teams,"Human-AI teams are increasingly prevalent in various domains. We investigate how the decision-making of a team member in a human-AI team impacts the outcome of the collaboration and perceived team-efficacy. In a large scale study on Mechanical Turk (n=125), we find significant differences across different decision making styles and disclosed AI identity disclosure in an AI-driven collaborative game. We find that autocratic decision-making negatively impacts team-efficacy in Human-AI teams, similar to its effects on human-only teams. We find that decision making style and AI-identity disclosure impacts how individuals make decisions in a collaborative context. We discuss our findings of the differences of collaborative behavior in human-human-AI teams and human-AI-AI teams.",10.1145/3579476,https://doi.org/10.1145/3579476,Proceedings of the ACM on Human-Computer Interaction,"Munyaka, Imani; Ashktorab, Zahra; Dugan, Casey; Johnson, J.; Pan, Qian",2023,40,"@article{2-3499,
author = {Munyaka, Imani and Ashktorab, Zahra and Dugan, Casey and Johnson, J. and Pan, Qian},
title = {Decision Making Strategies and Team Efficacy in Human-AI Teams},
year = {2023},
volume = {7},
number = {CSCW1},
doi = {10.1145/3579476},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {43},
numpages = {24}
}",Empirical contributions,Media / Communication / Entertainment,Individual,"Advising, Collaborating",Decision-maker,"Alter decision outcomes, Change affective-perceptual",no such info,"identity cues, AI-identity disclosure, decision-making style",NA,Interactive interface,Yes,Yes
2-3500,acm,Decision Rule Elicitation for Domain Adaptation,"Human-in-the-loop machine learning is widely used in artificial intelligence (AI) to elicit labels for data points from experts or to provide feedback on how close the predicted results are to the target. This simplifies away all the details of the decision-making process of the expert. In this work, we allow the experts to additionally produce decision rules describing their decision-making; the rules are expected to be imperfect but to give additional information. In particular, the rules can extend to new distributions, and hence enable significantly improving performance for cases where the training and testing distributions differ, such as in domain adaptation. We apply the proposed method to lifelong learning and domain adaptation problems and discuss applications in other branches of AI, such as knowledge acquisition problems in expert systems. In simulated and real-user studies, we show that decision rule elicitation improves domain adaptation of the algorithm and helps to propagate expert’s knowledge to the AI model.",10.1145/3397481.3450682,https://doi.org/10.1145/3397481.3450682,International Conference on Intelligent User Interfaces (IUI),"Nikitin, Alexander; Kaski, Samuel",2021,9,"@inproceedings{2-3500,
  title = {Decision Rule Elicitation for Domain Adaptation},
  author = {Nikitin, Alexander and Kaski, Samuel},
  year = {2021},
  booktitle = {International Conference on Intelligent User Interfaces (IUI)},
  doi = {10.1145/3397481.3450682}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Operational,"Advising, Collaborating","Decision-maker, Knowledge provider",no such info,"Change AI responses, Update AI competence",prediction of alternative,decision rules,Textual,Yes,Yes
2-3501,acm,On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection,"Humans are the final decision makers in critical tasks that involve ethical and legal concerns, ranging from recidivism prediction, to medical diagnosis, to fighting against fake news. Although machine learning models can sometimes achieve impressive performance in these tasks, these tasks are not amenable to full automation. To realize the potential of machine learning for improving human decisions, it is important to understand how assistance from machine learning models affects human performance and human agency.In this paper, we use deception detection as a testbed and investigate how we can harness explanations and predictions of machine learning models to improve human performance while retaining human agency. We propose a spectrum between full human agency and full automation, and develop varying levels of machine assistance along the spectrum that gradually increase the influence of machine predictions. We find that without showing predicted labels, explanations alone slightly improve human performance in the end task. In comparison, human performance is greatly improved by showing predicted labels (&gt;20% relative improvement) and can be further improved by explicitly suggesting strong machine performance. Interestingly, when predicted labels are shown, explanations of machine predictions induce a similar level of accuracy as an explicit statement of strong machine performance. Our results demonstrate a tradeoff between human performance and human agency and show that explanations of machine predictions can moderate this tradeoff.",10.1145/3287560.3287590,https://doi.org/10.1145/3287560.3287590,"Conference on Fairness, Accountability, and Transparency (FAccT)","Lai, Vivian; Tan, Chenhao",2019,568,"@inproceedings{2-3501,
  title = {On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection},
  author = {Lai, Vivian and Tan, Chenhao},
  year = {2019},
  doi = {10.1145/3287560.3287590},
  booktitle = {Conference on Fairness, Accountability, and Transparency (FAccT)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Institutional,"Forecasting, Explaining, Advising","Decision-maker, Guardian","Alter decision outcomes, Change trust, Change cognitive demands",no such info,"feature-based explanations, example-based explanations, predicted label without accuracy, predicted label with accuracy",user agency,"Semi-Autonomous System, Autonomous System",Yes,Yes
2-3502,acm,A Decision Theoretic Framework for Measuring AI Reliance,"Humans frequently make decisions with the aid of artificially intelligent (AI) systems. A common pattern is for the AI to recommend an action to the human who retains control over the final decision. Researchers have identified ensuring that a human has appropriate reliance on an AI as a critical component of achieving complementary performance. We argue that the current definition of appropriate reliance used in such research lacks formal statistical grounding and can lead to contradictions. We propose a formal definition of reliance, based on statistical decision theory, which separates the concepts of reliance as the probability the decision-maker follows the AI’s recommendation from challenges a human may face in differentiating the signals and forming accurate beliefs about the situation. Our definition gives rise to a framework that can be used to guide the design and interpretation of studies on human-AI complementarity and reliance. Using recent AI-advised decision making studies from literature, we demonstrate how our framework can be used to separate the loss due to mis-reliance from the loss due to not accurately differentiating the signals. We evaluate these losses by comparing to a baseline and a benchmark for complementary performance defined by the expected payoff achieved by a rational decision-maker facing the same decision task as the behavioral decision-makers.",10.1145/3630106.3658901,https://doi.org/10.1145/3630106.3658901,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Guo, Ziyang; Wu, Yifan; Hartline, Jason D.; Hullman, Jessica",2024,0,"@inproceedings{2-3502,
author = {Guo, Ziyang and Wu, Yifan and Hartline, Jason D. and Hullman, Jessica},
title = {A Decision Theoretic Framework for Measuring AI Reliance},
year = {2024},
doi = {10.1145/3630106.3658901},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {221–236},
numpages = {16}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Individual,Advising,Decision-maker,no such info,no such info,"example-based explanations, highlight explanations, heatmap explanation, predicted labels, system accuracy, explanation for the most confident AI recommendation, explanations for the top-2 most confident AI recommendations",delegation,Interactive interface,Yes,Yes
2-3503,acm,"It Seems Smart, but It Acts Stupid: Development of Trust in AI Advice in a Repeated Legal Decision-Making Task","Humans increasingly interact with AI systems, and successful interactions rely on individuals trusting such systems (when appropriate). Considering that trust is fragile and often cannot be restored quickly, we focus on how trust develops over time in a human-AI-interaction scenario. In a 2x2 between-subject experiment, we test how model accuracy (high vs. low) and type of explanation (human-like vs. not) affect trust in AI over time. We study a complex decision-making task in which individuals estimate jail time for 20 criminal law cases with AI advice. Results show that trust is significantly higher for high-accuracy models. Also, behavioral trust does not decline, and subjective trust even increases significantly with high accuracy. Human-like explanations did not generally affect trust but boosted trust in high-accuracy models.",10.1145/3581641.3584058,https://doi.org/10.1145/3581641.3584058,ACM International Conference on Intelligent User Interfaces (IUI),"Kahr, Patricia K.; Rooks, Gerrit; Willemsen, Martijn C.; Snijders, Chris C.P.",2023,51,"@inproceedings{2-3503,
  author    = {Kahr, Patricia K. and Rooks, Gerrit and Willemsen, Martijn C. and Snijders, Chris C.P.},
  title     = {It Seems Smart, but It Acts Stupid: Development of Trust in AI Advice in a Repeated Legal Decision-Making Task},
  year      = {2023},
  doi       = {10.1145/3581641.3584058},
  booktitle = {Proceedings of the ACM International Conference on Intelligent User Interfaces (IUI)}
}",Empirical contributions,Law / Policy / Governance,Organizational,Advising,Decision-maker,"Change trust, Alter decision outcomes",no such info,"system accuracy, human-like explanations, not human-like explanations, recommendations","domain knowledge, prosocialness, affinity for technology",Interactive interface,Yes,Yes
2-35049,springernature,A deep learning system for myopia onset prediction and intervention effectiveness evaluation in children,"The increasing prevalence of myopia worldwide presents a significant public health challenge. A key strategy to combat myopia is with early detection and prediction in children as such examination allows for effective intervention using readily accessible imaging technique. To this end, we introduced DeepMyopia, an artificial intelligence( AI) -enabled decision support system to detect and predict myopia onset and facilitate targeted interventions for children at risk using routine retinal fundus images. Based on deep learning architecture, DeepMyopia had been trained and internally validated on a large cohort of retinal fundus images( n = 1, 638, 315) and then externally tested on datasets from seven sites in China( n = 22, 060). Our results demonstrated robustness of DeepMyopia, with AUCs of 0. 908, 0. 813, and 0. 810 for 1-, 2-, and 3-year myopia onset prediction with the internal test set, and AUCs of 0. 796, 0. 808, and 0. 767 with the external test set. DeepMyopia also effectively stratified children into lowand high-risk groups( p < 0. 001) in both test sets. In an emulated randomized controlled trial( eRCT) on the Shanghai outdoor cohort( n = 3303) where DeepMyopia showed effectiveness in myopia prevention compared to NonCyc-based model, with an adjusted relative reduction( ARR) of −17. 8%, 95% CI: −29. 4%, −6. 4%. DeepMyopia-assisted interventions attained quality-adjusted life years( QALYs) of 0. 75( 95% CI: 0. 53, 1. 04) per person and avoided blindness years of 13. 54( 95% CI: 9. 57, 18. 83) per 1 million persons compared to natural lifestyle with no active intervention. Our findings demonstrated DeepMyopia as a reliable and efficient AI-based decision support system for intervention guidance for children.",10.1038/s41746-024-01204-7,http://dx.doi.org/10.1038/s41746-024-01204-7,Nature Partner Journals Digital Medicine,"Qi, Ziyi;Li, Tingyao;Chen, Jun;Yam, Jason C.;Wen, Yang;Huang, Gengyou;Zhong, Hua;He, Mingguang;Zhu, Dan;Dai, Rongping;Qian, Bo;Wang, Jingjing;Qian, Chaoxu;Wang, Wei;Zheng, Yanfei;Zhang, Jian;Yi, Xianglong;Wang, Zheyuan;Zhang, Bo;Liu, Chunyu;Cheng, Tianyu;Yang, Xiaokang;Li, Jun;Pan, Yan-Ting;Ding, Xiaohu;Xiong, Ruilin;Wang, Yan;Zhou, Yan;Feng, Dagan;Liu, Sichen;Du, Linlin;Yang, Jinliuxing;Zhu, Zhuoting;Bi, Lei;Kim, Jinman;Tang, Fangyao;Zhang, Yuzhou;Zhang, Xiujuan;Zou, Haidong;Ang, Marcus;Tham, Clement C.;Cheung, Carol Y.;Pang, Chi Pui;Sheng, Bin;He, Xiangui;Xu, Xun",2024,32,"@article{2-35049,
  title = {A deep learning system for myopia onset prediction and intervention effectiveness evaluation in children},
  author = {Qi, Ziyi and Li, Tingyao and Chen, Jun and Yam, Jason C. and Wen, Yang and Huang, Gengyou and Zhong, Hua and He, Mingguang and Zhu, Dan and Dai, Rongping and Qian, Bo and Wang, Jingjing and Qian, Chaoxu and Wang, Wei and Zheng, Yanfei and Zhang, Jian and Yi, Xianglong and Wang, Zheyuan and Zhang, Bo and Liu, Chunyu and Cheng, Tianyu and Yang, Xiaokang and Li, Jun and Pan, Yan-Ting and Ding, Xiaohu and Xiong, Ruilin and Wang, Yan and Zhou, Yan and Feng, Dagan and Liu, Sichen and Du, Linlin and Yang, Jinliuxing and Zhu, Zhuoting and Bi, Lei and Kim, Jinman and Tang, Fangyao and Zhang, Yuzhou and Zhang, Xiujuan and Zou, Haidong and Ang, Marcus and Tham, Clement C. and Cheung, Carol Y. and Pang, Chi Pui and Sheng, Bin and He, Xiangui and Xu, Xun},
  year = {2024},
  doi = {10.1038/s41746-024-01204-7},
  journal = {Nature Partner Journals Digital Medicine}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Auditing, Advising, Forecasting","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-3505,acm,Who Should I Trust: AI or Myself? Leveraging Human and AI Correctness Likelihood to Promote Appropriate Trust in AI-Assisted Decision-Making,"In AI-assisted decision-making, it is critical for human decision-makers to know when to trust AI and when to trust themselves. However, prior studies calibrated human trust only based on AI confidence indicating AI’s correctness likelihood (CL) but ignored humans’ CL, hindering optimal team decision-making. To mitigate this gap, we proposed to promote humans’ appropriate trust based on the CL of both sides at a task-instance level. We first modeled humans’ CL by approximating their decision-making models and computing their potential performance in similar instances. We demonstrated the feasibility and effectiveness of our model via two preliminary studies. Then, we proposed three CL exploitation strategies to calibrate users’ trust explicitly/implicitly in the AI-assisted decision-making process. Results from a between-subjects experiment (N=293) showed that our CL exploitation strategies promoted more appropriate human trust in AI, compared with only using AI confidence. We further provided practical implications for more human-compatible AI-assisted decision-making.",10.1145/3544548.3581058,https://doi.org/10.1145/3544548.3581058,ACM CHI Conference on Human Factors in Computing Systems,"Ma, Shuai; Lei, Ying; Wang, Xinru; Zheng, Chengbo; Shi, Chuhan; Yin, Ming; Ma, Xiaojuan",2023,68,"@inproceedings{2-3505,
  title = {Who Should I Trust: AI or Myself? Leveraging Human and AI Correctness Likelihood to Promote Appropriate Trust in AI-Assisted Decision-Making},
  author = {Ma, Shuai and Lei, Ying and Wang, Xinru and Zheng, Chengbo and Shi, Chuhan and Yin, Ming and Ma, Xiaojuan},
  year = {2023},
  doi = {10.1145/3544548.3581058},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems}
}","Empirical contributions, Theoretical contributions","Finance / Business / Economy, Generic / Abstract / Domain-agnostic",Individual,"Advising, Explaining",Decision-maker,"Alter decision outcomes, Change trust, Change cognitive demands, Change affective-perceptual",no such info,"AI confidence, CL exploitation",NA,Interactive interface,Yes,Yes
2-35056,springernature,Quantifying the impact of AI recommendations with explanations on prescription decision making,"The influence of AI recommendations on physician behaviour remains poorly characterised. We assess how clinicians’ decisions may be influenced by additional information more broadly, and how this influence can be modified by either the source of the information( human peers or AI) and the presence or absence of an AI explanation( XAI, here using simple feature importance). We used a modified between-subjects design where intensive care doctors( N = 86) were presented on a computer for each of 16 trials with a patient case and prompted to prescribe continuous values for two drugs. We used a multi-factorial experimental design with four arms, where each clinician experienced all four arms on different subsets of our 24 patients. The four arms were( i) baseline( control) , ( ii) peer human clinician scenario showing what doses had been prescribed by other doctors, ( iii) AI suggestion and( iv) XAI suggestion. We found that additional information( peer, AI or XAI) had a strong influence on prescriptions( significantly for AI, not so for peers) but simple XAI did not have higher influence than AI alone. There was no correlation between attitudes to AI or clinical experience on the AI-supported decisions and nor was there correlation between what doctors self-reported about how useful they found the XAI and whether the XAI actually influenced their prescriptions. Our findings suggest that the marginal impact of simple XAI was low in this setting and we also cast doubt on the utility of self-reports as a valid metric for assessing XAI in clinical experts.",10.1038/s41746-023-00955-z,http://dx.doi.org/10.1038/s41746-023-00955-z,Nature Partner Journals Digital Medicine,"Nagendran, Myura;Festor, Paul;Komorowski, Matthieu;Gordon, Anthony C.;Faisal, Aldo A.",2023,41,"@article{2-35056,
  title={Quantifying the impact of AI recommendations with explanations on prescription decision making},
  author={Nagendran, Myura and Festor, Paul and Komorowski, Matthieu and Gordon, Anthony C. and Faisal, Aldo A.},
  year={2023},
  doi={10.1038/s41746-023-00955-z},
  journal={Nature Partner Journals Digital Medicine}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Explaining","Decision-maker, Decision-subject, Knowledge provider",Alter decision outcomes,no such info,"AI suggestions, XAI suggestions",prescription from peer human clinician,Interactive interface,Yes,Yes
2-3506,acm,“Are You Really Sure?” Understanding the Effects of Human Self-Confidence Calibration in AI-Assisted Decision Making,"In AI-assisted decision-making, it is crucial but challenging for humans to achieve appropriate reliance on AI. This paper approaches this problem from a human-centered perspective, “human self-confidence calibration”. We begin by proposing an analytical framework to highlight the importance of calibrated human self-confidence. In our first study, we explore the relationship between human self-confidence appropriateness and reliance appropriateness. Then in our second study, We propose three calibration mechanisms and compare their effects on humans’ self-confidence and user experience. Subsequently, our third study investigates the effects of self-confidence calibration on AI-assisted decision-making. Results show that calibrating human self-confidence enhances human-AI team performance and encourages more rational reliance on AI (in some aspects) compared to uncalibrated baselines. Finally, we discuss our main findings and provide implications for designing future AI-assisted decision-making interfaces.",10.1145/3613904.3642671,https://doi.org/10.1145/3613904.3642671,CHI Conference on Human Factors in Computing Systems,"Ma, Shuai; Wang, Xinru; Lei, Ying; Shi, Chuhan; Yin, Ming; Ma, Xiaojuan",2024,68,"@inproceedings{2-3506,
  title = {“Are You Really Sure?” Understanding the Effects of Human Self-Confidence Calibration in AI-Assisted Decision Making},
  author = {Ma, Shuai and Wang, Xinru and Lei, Ying and Shi, Chuhan and Yin, Ming and Ma, Xiaojuan},
  year = {2024},
  doi = {10.1145/3613904.3642671},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}","Empirical contributions, Methodological contributions","Finance / Business / Economy, Generic / Abstract / Domain-agnostic",Operational,Advising,Decision-maker,"Alter decision outcomes, Change trust, Change affective-perceptual",no such info,"confidence score, AI suggestions",self-confidence calibration,Interactive interface,Yes,Yes
2-3507,acm,Do Humans Trust Advice More if it Comes from AI? An Analysis of Human-AI Interactions,"In decision support applications of AI, the AI algorithm's output is framed as a suggestion to a human user. The user may ignore this advice or take it into consideration to modify their decision. With the increasing prevalence of such human-AI interactions, it is important to understand how users react to AI advice. In this paper, we recruited over 1100 crowdworkers to characterize how humans use AI suggestions relative to equivalent suggestions from a group of peer humans across several experimental settings. We find that participants' beliefs about how human versus AI performance on a given task affects whether they heed the advice. When participants do heed the advice, they use it similarly for human and AI suggestions. Based on these results, we propose a two-stage, ""activation-integration"" model for human behavior and use it to characterize the factors that affect human-AI interactions.",10.1145/3514094.3534150,https://doi.org/10.1145/3514094.3534150,"AAAI/ACM Conference on AI, Ethics, and Society","Vodrahalli, Kailas; Daneshjou, Roxana; Gerstenberg, Tobias; Zou, James",2022,6,"@inproceedings{2-3507,
author = {Vodrahalli, Kailas and Daneshjou, Roxana and Gerstenberg, Tobias and Zou, James},
title = {Do Humans Trust Advice More if it Comes from AI? An Analysis of Human-AI Interactions},
year = {2022},
doi = {10.1145/3514094.3534150},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {763–777}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy, Healthcare / Medicine / Surgery, Everyday / Employment / Public Service",Operational,Advising,Decision-maker,"Alter decision outcomes, Change affective-perceptual, Change trust",no such info,AI suggestions,equivalent suggestions,Interactive interface,Yes,Yes
2-35072,springernature,A simulation-based evaluation of machine learning models for clinical decision support: application and analysis using hospital readmission,"The interest in applying machine learning in healthcare has grown rapidly in recent years. Most predictive algorithms requiring pathway implementations are evaluated using metrics focused on predictive performance, such as the c statistic. However, these metrics are of limited clinical value, for two reasons:( 1) they do not account for the algorithm’s role within a provider workflow; and( 2) they do not quantify the algorithm’s value in terms of patient outcomes and cost savings. We propose a model for simulating the selection of patients over time by a clinician using a machine learning algorithm, and quantifying the expected patient outcomes and cost savings. Using data on unplanned emergency department surgical readmissions, we show that factors such as the provider’s schedule and postoperative prediction timing can have major effects on the pathway cohort size and potential cost reductions from preventing hospital readmissions.",10.1038/s41746-021-00468-7,http://dx.doi.org/10.1038/s41746-021-00468-7,Nature Partner Journals Digital Medicine,"Mišić, Velibor V.;Rajaram, Kumar;Gabel, Eilon",2021,36,"@article{2-35072,
  title = {A simulation-based evaluation of machine learning models for clinical decision support: application and analysis using hospital readmission},
  author = {Mišić, Velibor V. and Rajaram, Kumar and Gabel, Eilon},
  year = {2021},
  journal = {Nature Partner Journals Digital Medicine},
  doi = {10.1038/s41746-021-00468-7}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-35082,springernature,Carbon trading supply chain management based on constrained deep reinforcement learning,"The issue of carbon emissions is a critical global concern, and how to effectively reduce energy consumption and emissions is a challenge faced by the industrial sector, which is highly emphasized in supply chain management. The complexity arises from the intricate coupling mechanism between carbon trading and ordering. T he large-scale state space involved and various constraints make cost optimization difficult. Carbon quota constraints and sequential decision-making exacerbate the challenges for businesses. Existing research implements rule-based and heuristic numerical simulation, which struggles to adapt to time-varying environments. We develop a unified framework from the perspective of Constrained Markov Decision Processes( CMDP). Constrained Deep Reinforcement Learning( DRL) with its powerful high-dimensional representations of neural networks and effective decision-making capabilities under constraints, provides a potential solution for supply chain management that includes carbon trading. DRL with constraints is a crucial tool to study cost optimization for enterprises. This paper constructs a DRL algorithm for Double Order based on PPO-Lagrangian( DOPPOL) , aimed at addressing a supply chain management model that integrates carbon trading decisions and ordering decisions. The results indicate that businesses can optimize both business and carbon costs, thereby increasing overall profits, as well as adapt to various demand uncertainties. DOPPOL outperforms the traditional method( s, S) in fluctuating demand scenarios. By introducing carbon trading, enterprises are able to adjust supply chain orders and carbon emissions through interaction, and improve operational efficiency. Finally, we emphasize the significant role of carbon pricing in enterprise contracts in terms of profitability, as reasonable prices can help control carbon emissions and reduce costs. Our research is of great importance in achieving climate change control, as well as promoting sustainability.",10.1007/s10458-024-09669-2,http://dx.doi.org/10.1007/s10458-024-09669-2,Autonomous Agents and Multi-Agent Systems,"Wang, Qinghao;Yang, Yaodong",2024,5,"@article{2-35082,
  title={Carbon trading supply chain management based on constrained deep reinforcement learning},
  author={Wang, Qinghao and Yang, Yaodong},
  year={2024},
  doi={10.1007/s10458-024-09669-2},
  journal={Autonomous Agents and Multi-Agent Systems}
}",Algorithmic contributions,"Environment / Resources / Energy, Manufacturing / Industry / Automation",Organizational,"Executing, Advising","Knowledge provider, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-3509,acm,Supporting Group Decision-Making: Insights from a Focus Group Study,"In everyday life, we make decisions in groups about a variety of issues. In group decision-making, group members discuss options, exchange preferences and opinions, and make a common decision. Decision support systems and group recommender systems facilitate this process by enabling preference elicitation, generating recommendations, and supporting the process. We are here interested in building a conversational system, namely, a chat app, enhanced with an AI agent supporting the group decision-making process. To design the system, rather than solely relying on our assumptions, we took one step back and conducted a comprehensive focus group study. This approach has allowed us to gain original insights into the specific needs and preferences of the future end-users, i.e., group members, ensuring that our system design aligns more closely with their requirements. The focus group study involved fourteen participants in three group compositions: friends, families, and couples. Our findings reveal that most of the group members define a good choice as one that maximizes overall satisfaction without leaving any member dissatisfied. Dealing with competing group members emerged as a primary concern, with study participants requesting specific help from the AI agent to address this challenge. Participants identified personality and group structure as crucial characteristics for the AI agent to properly operate, though some expressed privacy concerns. Lastly, participants expected an AI agent to provide private interactions with individual members, proactively guide discussions when necessary, continually analyze group interactions, and tailor support to those interactions.",10.1145/3627043.3659538,https://doi.org/10.1145/3627043.3659538,"ACM Conference on User Modeling, Adaptation and Personalization (UMAP)","Delić, Amra; Emamgholizadeh, Hanif; Ricci, Francesco; Masthoff, Judith",2024,8,"@inproceedings{2-3509,
  title = {Supporting Group Decision-Making: Insights from a Focus Group Study},
  author = {Deli{\'c}, Amra and Emamgholizadeh, Hanif and Ricci, Francesco and Masthoff, Judith},
  year = {2024},
  doi = {10.1145/3627043.3659538},
  booktitle = {Proceedings of the ACM Conference on User Modeling, Adaptation and Personalization (UMAP)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service",Organizational,"Analyzing, Executing, Advising",Decision-maker,"Alter decision outcomes, Shape ethical norms, Change cognitive demands",Change AI responses,"preference elicitation, contextual recommendation","diverging preferences of individual group members, disruptive group behavior, group contextual factors","Conversational/Natural Language, Autonomous System",Yes,Yes
2-3510,acm,Auditing Work: Exploring the New York City algorithmic bias audit regime,"In July 2023, New York City (NYC) implemented the first attempt to create an algorithm auditing regime for commercial machine-learning systems. Local Law 144 (LL 144), requires NYC-based employers using automated employment decision-making tools (AEDTs) in hiring to be subject to annual bias audits by an independent auditor. In this paper, we analyse what lessons can be learned from LL 144 for other national attempts to create algorithm auditing regimes. Using qualitative interviews with 17 experts and practitioners working within the regime, we find LL 144 has failed to create an effective auditing regime: the law fails to clearly define key aspects like AEDTs and what constitutes an independent auditor, leaving auditors, vendors who create AEDTs, and companies using AEDTs to define the law’s practical implementation in ways that failed to protect job applicants. Several factors contribute to this: first, the law was premised on a faulty transparency-driven theory of change that fails to stop biased AEDTs from being used by employers. Second, industry lobbying led to the definition of what constitutes an AEDT being narrowed to the point where most companies considered their tools exempt. Third, we find auditors face enormous practical and cultural challenges gaining access to data from employers and vendors building these tools. Fourth, we find wide disagreement over what constitutes a legitimate auditor and identify four different kinds of ‘auditor roles’ that serve different functions and offer different kinds of services. We conclude with four recommendations for policymakers seeking to create similar bias auditing regimes that use clearer definitions and metrics and more accountability. By exploring LL 144 through the lens of auditors, our paper advances the evidence base around audit as an accountability mechanism, and can provide guidance for policymakers seeking to create similar regimes.",10.1145/3630106.3658959,https://doi.org/10.1145/3630106.3658959,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Groves, Lara; Metcalf, Jacob; Kennedy, Alayna; Vecchione, Briana; Strait, Andrew",2024,54,"@inproceedings{2-3510,
  author    = {Groves, Lara and Metcalf, Jacob and Kennedy, Alayna and Vecchione, Briana and Strait, Andrew},
  title     = {Auditing Work: Exploring the New York City algorithmic bias audit regime},
  booktitle = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)},
  year      = {2024},
  doi       = {10.1145/3630106.3658959}
}",Empirical contributions,Law / Policy / Governance,Institutional,Executing,"Decision-subject, Guardian, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-3511,acm,Toward Supporting Perceptual Complementarity in Human-AI Collaboration via Reflection on Unobservables,"In many real world contexts, successful human-AI collaboration requires humans to productively integrate complementary sources of information into AI-informed decisions. However, in practice human decision-makers often lack understanding of what information an AI model has access to, in relation to themselves. There are few available guidelines regarding how to effectively communicate aboutunobservables: features that may influence the outcome, but which are unavailable to the model. In this work, we conducted an online experiment to understand whether and how explicitly communicating potentially relevant unobservables influences how people integrate model outputs and unobservables when making predictions. Our findings indicate that presenting prompts about unobservables can change how humans integrate model outputs and unobservables, but do not necessarily lead to improved performance. Furthermore, the impacts of these prompts can vary depending on decision-makers' prior domain expertise. We conclude by discussing implications for future research and design of AI-based decision support tools.",10.1145/3579628,https://doi.org/10.1145/3579628,Proceedings of the ACM on Human-Computer Interaction,"Holstein, Kenneth; De-Arteaga, Maria; Tumati, Lakshmi; Cheng, Yanghuidi",2023,65,"@article{2-3511,
  title = {Toward Supporting Perceptual Complementarity in Human-AI Collaboration via Reflection on Unobservables},
  author = {Holstein, Kenneth and De-Arteaga, Maria and Tumati, Lakshmi and Cheng, Yanghuidi},
  year = {2023},
  doi = {10.1145/3579628},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Individual,"Advising, Explaining, Analyzing, Collaborating",Decision-maker,"Alter decision outcomes, Change trust",no such info,prompts about unobservables,NA,Interactive interface,Yes,Yes
2-3512,acm,Towards Fairness in Practice: A Practitioner-Oriented Rubric for Evaluating Fair ML Toolkits,"In order to support fairness-forward thinking by machine learning (ML) practitioners, fairness researchers have created toolkits that aim to transform state-of-the-art research contributions into easily-accessible APIs. Despite these efforts, recent research indicates a disconnect between the needs of practitioners and the tools offered by fairness research. By engaging 20 ML practitioners in a simulated scenario in which they utilize fairness toolkits to make critical decisions, this work aims to utilize practitioner feedback to inform recommendations for the design and creation of fair ML toolkits. Through the use of survey and interview data, our results indicate that though fair ML toolkits are incredibly impactful on users’ decision-making, there is much to be desired in the design and demonstration of fairness results. To support the future development and evaluation of toolkits, this work offers a rubric that can be used to identify critical components of Fair ML toolkits.",10.1145/3411764.3445604,https://doi.org/10.1145/3411764.3445604,CHI Conference on Human Factors in Computing Systems,"Richardson, Brianna; Garcia-Gathright, Jean; Way, Samuel F.; Thom, Jennifer; Cramer, Henriette",2021,11,"@inproceedings{2-3512,
  title = {Towards Fairness in Practice: A Practitioner-Oriented Rubric for Evaluating Fair ML Toolkits},
  author = {Richardson, Brianna and Garcia-Gathright, Jean and Way, Samuel F. and Thom, Jennifer and Cramer, Henriette},
  year = {2021},
  doi = {10.1145/3411764.3445604},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Institutional,"Advising, Auditing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-3514,acm,Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time Pressure,"In settings where users both need high accuracy and are time-pressured, such as doctors working in emergency rooms, we want to provide AI assistance that both increases decision accuracy and reduces decision-making time. Current literature focusses on how users interact with AI assistance when there is no time pressure, finding that different AI assistances have different benefits: some can reduce time taken while increasing overreliance on AI, while others do the opposite. The precise benefit can depend on both the user and task. In time-pressured scenarios, adapting when we show AI assistance is especially important: relying on the AI assistance can save time, and can therefore be beneficial when the AI is likely to be right. We would ideally adapt what AI assistance we show depending on various properties (of the task and of the user) in order to best trade off accuracy and time. We introduce a study where users have to answer a series of logic puzzles. We find that time pressure affects how users use different AI assistances, making some assistances more beneficial than others when compared to no-time-pressure settings. We also find that a user’s overreliance rate is a key predictor of their behaviour: overreliers and not-overreliers use different AI assistance types differently. We find marginal correlations between a user’s overreliance rate (which is related to the user’s trust in AI recommendations) and their personality traits (Big Five Personality traits). Overall, our work suggests that AI assistances have different accuracy-time tradeoffs when people are under time pressure compared to no time pressure, and we explore how we might adapt AI assistances in this setting.",10.1145/3640543.3645206,https://doi.org/10.1145/3640543.3645206,International Conference on Intelligent User Interfaces (IUI),"Swaroop, Siddharth; Buçinca, Zana; Gajos, Krzysztof Z.; Doshi-Velez, Finale",2024,4,"@inproceedings{2-3514,
  title = {Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time Pressure},
  author = {Swaroop, Siddharth and Buçinca, Zana and Gajos, Krzysztof Z. and Doshi-Velez, Finale},
  year = {2024},
  doi = {10.1145/3640543.3645206},
  booktitle = {International Conference on Intelligent User Interfaces (IUI)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,Advising,Decision-maker,"Alter decision outcomes, Change trust, Change cognitive demands",no such info,"explanations, recommendations",domain knowledge,Interactive interface,Yes,Yes
2-3515,acm,Reinforcement Learning Enhances the Experts: Large-scale COVID-19 Vaccine Allocation with Multi-factor Contact Network,"In the fight against the COVID-19 pandemic, vaccines are the most critical resource but are still in short supply around the world. Therefore, efficient vaccine allocation strategies are urgently called for, especially in large-scale metropolis where uneven health risk is manifested in nearby neighborhoods. However, there exist several key challenges in solving this problem: (1) great complexity in the large scale scenario adds to the difficulty in experts' vaccine allocation decision making; (2) heterogeneous information from all aspects in the metropolis' contact network makes information utilization difficult in decision making; (3) when utilizing the strong decision-making ability of reinforcement learning (RL) to solve the problem, poor explainability limits the credibility of the RL strategies. In this paper, we propose a reinforcement learning enhanced experts method. We deal with the great complexity via a specially designed algorithm aggregating blocks in the metropolis into communities and we hierarchically integrate RL among the communities and experts solution within each community. We design a self-supervised contact network representation algorithm to fuse the heterogeneous information for efficient vaccine allocation decision making. We conduct extensive experiments in three metropolis with real-world data and prove that our method outperforms the best baseline, reducing 9.01% infections and 12.27% deaths.We further demonstrate the explainability of the RL model, adding to its credibility and also enlightening the experts in turn.",10.1145/3534678.3542679,https://doi.org/10.1145/3534678.3542679,ACM SIGKDD Conference on Knowledge Discovery and Data Mining,"Hao, Qianyue; Huang, Wenzhen; Xu, Fengli; Tang, Kun; Li, Yong",2022,13,"@inproceedings{2-3515,
  title = {Reinforcement Learning Enhances the Experts: Large-scale COVID-19 Vaccine Allocation with Multi-factor Contact Network},
  author = {Hao, Qianyue and Huang, Wenzhen and Xu, Fengli and Tang, Kun and Li, Yong},
  year = {2022},
  doi = {10.1145/3534678.3542679},
  booktitle = {ACM SIGKDD Conference on Knowledge Discovery and Data Mining}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Institutional,"Advising, Explaining, Analyzing","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-3516,acm,BiasEye: A Bias-Aware Real-time Interactive Material Screening System for Impartial Candidate Assessment,"In the process of evaluating competencies for job or student recruitment through material screening, decision-makers can be influenced by inherent cognitive biases, such as the screening order or anchoring information, leading to inconsistent outcomes. To tackle this challenge, we conducted interviews with seven experts to understand their challenges and needs for support in the screening process. Building on their insights, we introduce BiasEye, a bias-aware real-time interactive material screening visualization system. BiasEye enhances awareness of cognitive biases by improving information accessibility and transparency. It also aids users in identifying and mitigating biases through a machine learning (ML) approach that models individual screening preferences. Findings from a mixed-design user study with 20 participants demonstrate that, compared to a baseline system lacking our bias-aware features, BiasEye increases participants’ bias awareness and boosts their confidence in making final decisions. At last, we discuss the potential of ML and visualization in mitigating biases during human decision-making tasks.",10.1145/3640543.3645166,https://doi.org/10.1145/3640543.3645166,International Conference on Intelligent User Interfaces (IUI),"Liu, Qianyu; Jiang, Haoran; Pan, Zihao; Han, Qiushi; Peng, Zhenhui; Li, Quan",2024,0,"@inproceedings{2-3516,
  title     = {BiasEye: A Bias-Aware Real-time Interactive Material Screening System for Impartial Candidate Assessment},
  author    = {Liu, Qianyu and Jiang, Haoran and Pan, Zihao and Han, Qiushi and Peng, Zhenhui and Li, Quan},
  year      = {2024},
  booktitle = {International Conference on Intelligent User Interfaces (IUI)},
  doi       = {10.1145/3640543.3645166}
}",System/Artifact contributions,Everyday / Employment / Public Service,Operational,"Analyzing, Advising","Decision-maker, Knowledge provider","Shape ethical norms, Alter decision outcomes, Change affective-perceptual, Change trust, Change cognitive demands","Update AI competence, Change AI responses",information accessibility and transparency.,NA,Interactive interface,Yes,Yes
2-3517,acm,Development and translation of human-AI interaction models into working prototypes for clinical decision-making,"In the standard interaction model of clinical decision support systems, the system makes a recommendation, and the clinician decides whether to act on it. However, this model can compromise the patient-centeredness of care and the level of clinician involvement. There is scope to develop alternative interaction models, but we need methods for exploring and comparing these to assess how they may impact clinical decision-making. Through collaborating with clinical, AI safety, and HCI experts, and patient representatives, we co-designed a number of alternative human-AI interaction models for clinical decision-making. We then translated these models into ‘Wizard of Oz’ prototypes, where we created clinical scenarios and designed user interfaces with different types of AI output. In this paper, we present alternative models of human-AI interaction and illustrate how we used a co-design approach to translate them into functional prototypes that can be tested with users to explore potential impacts on clinical decision-making.",10.1145/3643834.3660697,https://doi.org/10.1145/3643834.3660697,ACM Designing Interactive Systems Conference,"Hussain, Muhammad; Iacovides, Ioanna; Lawton, Tom; Sharma, Vishal; Porter, Zoe; Cunningham, Alice; Habli, Ibrahim; Hickey, Shireen; Jia, Yan; Morgan, Phillip; Wong, Nee Ling",2024,12,"@inproceedings{2-3517,
  title = {Development and Translation of Human-AI Interaction Models into Working Prototypes for Clinical Decision-Making},
  author = {Hussain, Muhammad and Iacovides, Ioanna and Lawton, Tom and Sharma, Vishal and Porter, Zoe and Cunningham, Alice and Habli, Ibrahim and Hickey, Shireen and Jia, Yan and Morgan, Phillip and Wong, Nee Ling},
  year = {2024},
  doi = {10.1145/3643834.3660697},
  booktitle = {ACM Designing Interactive Systems Conference}
}","Empirical contributions, Methodological contributions",Healthcare / Medicine / Surgery,Operational,"Advising, Collaborating","Decision-maker, Knowledge provider","Alter decision outcomes, Restrict human agency","Change AI responses, Update AI competence, Shape AI for accountability","recommendations, underlying data, risk analysis","personalized settings, human value incorporation","Auditory, Textual, Conversational/Natural Language",Yes,Yes
2-3518,acm,AI Shall Have No Dominion: on How to Measure Technology Dominance in AI-supported Human decision-making,"In this article, we propose a conceptual and methodological framework for measuring the impact of the introduction of AI systems in decision settings, based on the concept of technological dominance, i.e. the influence that an AI system can exert on human judgment and decisions. We distinguish between a negative component of dominance (automation bias) and a positive one (algorithm appreciation) by focusing on and systematizing the patterns of interaction between human judgment and AI support, or reliance patterns, and their associated cognitive effects. We then define statistical approaches for measuring these dimensions of dominance, as well as corresponding qualitative visualizations. By reporting about four medical case studies, we illustrate how the proposed methods can be used to inform assessments of dominance and of related cognitive biases in real-world settings. Our study lays the groundwork for future investigations into the effects of introducing AI support into naturalistic and collaborative decision-making.",10.1145/3544548.3581095,https://doi.org/10.1145/3544548.3581095,ACM CHI Conference on Human Factors in Computing Systems,"Cabitza, Federico; Campagner, Andrea; Angius, Riccardo; Natali, Chiara; Reverberi, Carlo",2023,63,"@inproceedings{2-3518,
  author    = {Cabitza, Federico and Campagner, Andrea and Angius, Riccardo and Natali, Chiara and Reverberi, Carlo},
  title     = {AI Shall Have No Dominion: on How to Measure Technology Dominance in AI-supported Human decision-making},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems},
  year      = {2023},
  doi       = {10.1145/3544548.3581095}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,"Advising, Analyzing",Decision-maker,"Alter decision outcomes, Change trust, Restrict human agency, Shift responsibility",no such info,"AI support, visual explanations, textual explanations, classifcation advice",NA,Interactive interface,Yes,Yes
2-3520,acm,A Hierarchical Imitation Learning-based Decision Framework for Autonomous Driving,"In this paper, we focus on the decision-making challenge in autonomous driving, a central and intricate problem influencing the safety and practicality of autonomous vehicles. We propose an innovative hierarchical imitation learning framework that effectively alleviates the complexity of learning in autonomous driving decision-making problems by decoupling decision-making tasks into sub-problems. Specifically, the decision-making process is divided into two levels of sub-problems: the upper level directs the vehicle's lane selection and qualitative speed management, while the lower level implements precise control of the driving speed and direction. We harness Transformer-based models for solving each sub-problem, enabling overall hierarchical framework to comprehend and navigate diverse and various road conditions, ultimately resulting in improved decision-making. Through an evaluation in several typical driving scenarios within the SMARTS autonomous driving simulation environment, our proposed hierarchical decision-making framework significantly outperforms end-to-end reinforcement learning algorithms and behavior cloning algorithm, achieving an average pass rate of over 90%. Our framework's effectiveness is substantiated by its commendable achievements at the NeurIPS 2022 Driving SMARTS competition, where it secures dual track championships.",10.1145/3583780.3615454,https://doi.org/10.1145/3583780.3615454,ACM International Conference on Information and Knowledge Management (CIKM),"Liang, Hebin; Dong, Zibin; Ma, Yi; Hao, Xiaotian; Zheng, Yan; Hao, Jianye",2023,6,"@inproceedings{2-3520,
  title = {A Hierarchical Imitation Learning-based Decision Framework for Autonomous Driving},
  author = {Liang, Hebin and Dong, Zibin and Ma, Yi and Hao, Xiaotian and Zheng, Yan and Hao, Jianye},
  year = {2023},
  doi = {10.1145/3583780.3615454},
  booktitle = {ACM International Conference on Information and Knowledge Management (CIKM)}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Executing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-35209,springernature,What we owe to decision-subjects: beyond transparency and explanation in automated decision-making,"The ongoing explosion of interest in artificial intelligence is fueled in part by recently developed techniques in machine learning. Those techniques allow automated systems to process huge amounts of data, utilizing mathematical methods that depart from traditional statistical approaches, and resulting in impressive advancements in our ability to make predictions and uncover correlations across a host of interesting domains. But as is now widely discussed, the way that those systems arrive at their outputs is often opaque, even to the experts who design and deploy them. Is it morally problematic to make use of opaque automated methods when making high-stakes decisions, like whether to issue a loan to an applicant, or whether to approve a parole request? Many scholars answer in the affirmative. However, there is no widely accepted explanation for why transparent systems are morally preferable to opaque systems. We argue that the use of automated decision-making systems sometimes violates duties of consideration that are owed by decision-makers to decision-subjects, duties that are both epistemic and practical in character. Violations of that kind generate a weighty consideration against the use of opaque decision systems. In the course of defending our approach, we show that it is able to address three major challenges sometimes leveled against attempts to defend the moral import of transparency in automated decision-making.",10.1007/s11098-023-02013-6,http://dx.doi.org/10.1007/s11098-023-02013-6,Philosophical Studies,"Grant, David Gray;Behrends, Jeff;Basl, John",2023,32,"@article{2-35209,
  title = {What we owe to decision-subjects: beyond transparency and explanation in automated decision-making},
  author = {Grant, David Gray and Behrends, Jeff and Basl, John},
  year = {2023},
  doi = {10.1007/s11098-023-02013-6},
  journal = {Philosophical Studies}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy, Law / Policy / Governance",Individual,"Advising, Explaining","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-3522,acm,"Explanations, Fairness, and Appropriate Reliance in Human-AI Decision-Making","In this work, we study the effects of feature-based explanations on distributive fairness of AI-assisted decisions, specifically focusing on the task of predicting occupations from short textual bios. We also investigate how any effects are mediated by humans’ fairness perceptions and their reliance on AI recommendations. Our findings show that explanations influence fairness perceptions, which, in turn, relate to humans’ tendency to adhere to AI recommendations. However, we see that such explanations do not enable humans to discern correct and incorrect AI recommendations. Instead, we show that they may affect reliance irrespective of the correctness of AI recommendations. Depending on which features an explanation highlights, this can foster or hinder distributive fairness: when explanations highlight features that are task-irrelevant and evidently associated with the sensitive attribute, this prompts overrides that counter AI recommendations that align with gender stereotypes. Meanwhile, if explanations appear task-relevant, this induces reliance behavior that reinforces stereotype-aligned errors. These results imply that feature-based explanations are not a reliable mechanism to improve distributive fairness.",10.1145/3613904.3642621,https://doi.org/10.1145/3613904.3642621,CHI Conference on Human Factors in Computing Systems,"Schoeffer, Jakob; De-Arteaga, Maria; Kühl, Niklas",2024,94,"@inproceedings{2-3522,
  title = {Explanations, Fairness, and Appropriate Reliance in Human-AI Decision-Making},
  author = {Schoeffer, Jakob and De-Arteaga, Maria and K{\""u}hl, Niklas},
  year = {2024},
  doi = {10.1145/3613904.3642621},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Education / Teaching / Research,Institutional,"Explaining, Advising",Decision-maker,"Change trust, Shape ethical norms, Alter decision outcomes",no such info,"feature-based explanations, recommendations, gendered condition, task-relevant condition",NA,Interactive interface,Yes,Yes
2-3523,acm,Bias in machine learning software: why? how? what to do?,"Increasingly, software is making autonomous decisions in case of criminal sentencing, approving credit cards, hiring employees, and so on. Some of these decisions show bias and adversely affect certain social groups (e.g. those defined by sex, race, age, marital status). Many prior works on bias mitigation take the following form: change the data or learners in multiple ways, then see if any of that improves fairness. Perhaps a better approach is to postulate root causes of bias and then applying some resolution strategy. This paper postulates that the root causes of bias are the prior decisions that affect- (a) what data was selected and (b) the labels assigned to those examples. Our Fair-SMOTE algorithm removes biased labels; and rebalances internal distributions such that based on sensitive attribute, examples are equal in both positive and negative classes. On testing, it was seen that this method was just as effective at reducing bias as prior approaches. Further, models generated via Fair-SMOTE achieve higher performance (measured in terms of recall and F1) than other state-of-the-art fairness improvement algorithms. To the best of our knowledge, measured in terms of number of analyzed learners and datasets, this study is one of the largest studies on bias mitigation yet presented in the literature.",10.1145/3468264.3468537,https://doi.org/10.1145/3468264.3468537,ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE),"Chakraborty, Joymallya; Majumder, Suvodeep; Menzies, Tim",2021,327,"@inproceedings{2-3523,
  title = {Bias in Machine Learning Software: Why? How? What to Do?},
  author = {Chakraborty, Joymallya and Majumder, Suvodeep and Menzies, Tim},
  year = {2021},
  doi = {10.1145/3468264.3468537},
  booktitle = {Proceedings of the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE)}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance",Institutional,Executing,Developer,NA,NA,NA,NA,NA,Yes,No
2-3527,acm,Human-Algorithmic Interaction Using a Large Language Model-Augmented Artificial Intelligence Clinical Decision Support System,"Integration of artificial intelligence (AI) into clinical decision support systems (CDSS) poses a socio-technological challenge that is impacted by usability, trust, and human-computer interaction (HCI). AI-CDSS interventions have shown limited benefit in clinical outcomes, which may be due to insufficient understanding of how health-care providers interact with AI systems. Large language models (LLMs) have the potential to enhance AI-CDSS, but haven’t been studied in either simulated or real-world clinical scenarios. We present findings from a randomized controlled trial deploying AI-CDSS for the management of upper gastrointestinal bleeding (UGIB) with and without an LLM interface within realistic clinical simulations for physician and medical student participants. We find evidence that LLM augmentation improves ease-of-use, that LLM-generated responses with citations improve trust, and HCI varies based on clinical expertise. Qualitative themes from interviews suggest the perception of LLM-augmented AI-CDSS as a team-member used to confirm initial clinical intuitions and help evaluate borderline decisions.",10.1145/3613904.3642024,https://doi.org/10.1145/3613904.3642024,CHI Conference on Human Factors in Computing Systems,"Rajashekar, Niroop Channa; Shin, Yeo Eun; Pu, Yuan; Chung, Sunny; You, Kisung; Giuffre, Mauro; Chan, Colleen E; Saarinen, Theo; Hsiao, Allen; Sekhon, Jasjeet; Wong, Ambrose H; Evans, Leigh V; Kizilcec, Rene F.; Laine, Loren; Mccall, Terika; Shung, Dennis",2024,79,"@inproceedings{2-3527,
  title = {Human-Algorithmic Interaction Using a Large Language Model-Augmented Artificial Intelligence Clinical Decision Support System},
  author = {Rajashekar, Niroop Channa and Shin, Yeo Eun and Pu, Yuan and Chung, Sunny and You, Kisung and Giuffre, Mauro and Chan, Colleen E and Saarinen, Theo and Hsiao, Allen and Sekhon, Jasjeet and Wong, Ambrose H and Evans, Leigh V and Kizilcec, Rene F. and Laine, Loren and Mccall, Terika and Shung, Dennis},
  year = {2024},
  doi = {10.1145/3613904.3642024},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Explaining","Decision-maker, Decision-subject, Knowledge provider","Change trust, Change cognitive demands, Alter decision outcomes, Change affective-perceptual, Shape ethical norms","Update AI competence, Change AI responses",LLM-generated responses with citations,NA,"Textual, Conversational/Natural Language",Yes,Yes
2-3528,acm,I can do better than your AI: expertise and explanations,"Intelligent assistants, such as navigation, recommender, and expert systems, are most helpful in situations where users lack domain knowledge. Despite this, recent research in cognitive psychology has revealed that lower-skilled individuals may maintain a sense of illusory superiority, which might suggest that users with the highest need for advice may be the least likely to defer judgment. Explanation interfaces - a method for persuading users to take a system's advice - are thought by many to be the solution for instilling trust, but do their effects hold for self-assured users? To address this knowledge gap, we conducted a quantitative study (N=529) wherein participants played a binary decision-making game with help from an intelligent assistant. Participants were profiled in terms of both actual (measured) expertise and reported familiarity with the task concept. The presence of explanations, level of automation, and number of errors made by the intelligent assistant were manipulated while observing changes in user acceptance of advice. An analysis of cognitive metrics lead to three findings for research in intelligent assistants: 1) higher reported familiarity with the task simultaneously predicted more reported trust but less adherence, 2) explanations only swayed people who reported very low task familiarity, and 3) showing explanations to people who reported more task familiarity led to automation bias.",10.1145/3301275.3302308,https://doi.org/10.1145/3301275.3302308,ACM International Conference on Intelligent User Interfaces (IUI),"Schaffer, James; O'Donovan, John; Michaelis, James; Raglin, Adrienne; Höllerer, Tobias",2019,184,"@inproceedings{2-3528,
  title = {I can do better than your AI: expertise and explanations},
  author = {Schaffer, James and O'Donovan, John and Michaelis, James and Raglin, Adrienne and H{\""o}llerer, Tobias},
  year = {2019},
  doi = {10.1145/3301275.3302308},
  booktitle = {Proceedings of the ACM International Conference on Intelligent User Interfaces (IUI)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service",Individual,"Advising, Explaining",Decision-maker,"Change trust, Alter decision outcomes, Change cognitive demands",no such info,"explanations, recommendation errors, automation","actual (measured) expertise, reported familiarity with the task concept","Semi-Autonomous System, Autonomous System",Yes,Yes
2-3529,acm,Subgoal-Based Explanations for Unreliable Intelligent Decision Support Systems,"Intelligent decision support (IDS) systems leverage artificial intelligence techniques to generate recommendations that guide human users through the decision making phases of a task. However, a key challenge is that IDS systems are not perfect, and in complex real-world scenarios may produce suboptimal output or fail to work altogether. The field of explainable AI (XAI) has sought to develop techniques that improve the interpretability of black-box systems. While most XAI work has focused on single-classification tasks, the subfield of explainable AI planning (XAIP) has sought to develop techniques that make sequential decision making AI systems explainable to domain experts. Critically, prior work in applying XAIP techniques to IDS systems has assumed that the plan being proposed by the planner is always optimal, and therefore the action or plan being recommended as decision support to the user is always optimal. In this work, we examine novice user interactions with a non-robust IDS system – one that occasionally recommends suboptimal actions, and one that may become unavailable after users have become accustomed to its guidance. We introduce a new explanation type, subgoal-based explanations, for plan-based IDS systems, that supplements traditional IDS output with information about the subgoal toward which the recommended action would contribute. We demonstrate that subgoal-based explanations lead to improved user task performance in the presence of IDS recommendations, improve user ability to distinguish optimal and suboptimal IDS recommendations, and are preferred by users. Additionally, we demonstrate that subgoal-based explanations enable more robust user performance in the case of IDS failure, showing the significant benefit of training users for an underlying task with subgoal-based explanations.",10.1145/3581641.3584055,https://doi.org/10.1145/3581641.3584055,ACM International Conference on Intelligent User Interfaces (IUI),"Das, Devleena; Kim, Been; Chernova, Sonia",2023,0,"@inproceedings{2-3529,
  title = {Subgoal-Based Explanations for Unreliable Intelligent Decision Support Systems},
  author = {Das, Devleena and Kim, Been and Chernova, Sonia},
  year = {2023},
  doi = {10.1145/3581641.3584055},
  booktitle = {ACM International Conference on Intelligent User Interfaces (IUI)}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Media / Communication / Entertainment",Individual,"Advising, Explaining",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-3530,acm,Will You Accept the AI Recommendation? Predicting Human Behavior in AI-Assisted Decision Making,"Internet users make numerous decisions online on a daily basis. With the rapid advances in AI recently, AI-assisted decision making—in which an AI model provides decision recommendations and confidence, while the humans make the final decisions—has emerged as a new paradigm of human-AI collaboration. In this paper, we aim at obtaining a quantitative understanding of whether and when would human decision makers adopt the AI model’s recommendations. We define a space of human behavior models by decomposing the human decision maker’s cognitive process in each decision-making task into two components: the utility component (i.e., evaluate the utility of different actions) and the selection component (i.e., select an action to take), and we perform a systematic search in the model space to identify the model that fits real-world human behavior data the best. Our results highlight that in AI-assisted decision making, human decision makers’ utility evaluation and action selection are influenced by their own judgement and confidence on the decision-making task. Further, human decision makers exhibit a tendency to distort the decision confidence in utility evaluations. Finally, we also analyze the differences in humans’ adoption behavior of AI recommendations as the stakes of the decisions vary.",10.1145/3485447.3512240,https://doi.org/10.1145/3485447.3512240,ACM Web Conference (formerly The World Wide Web Conference),"Wang, Xinru; Lu, Zhuoran; Yin, Ming",2022,0,"@inproceedings{2-3530,
  title = {Will You Accept the AI Recommendation? Predicting Human Behavior in AI-Assisted Decision Making},
  author = {Wang, Xinru and Lu, Zhuoran and Yin, Ming},
  year = {2022},
  doi = {10.1145/3485447.3512240},
  booktitle = {ACM Web Conference (formerly The World Wide Web Conference)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Operational,Advising,Decision-maker,"Change trust, Alter decision outcomes, Change affective-perceptual",no such info,"recommendations, confidence score",NA,Textual,Yes,Yes
2-3533,acm,Increasing the Speed and Accuracy of Data Labeling Through an AI Assisted Interface,"Labeling data is an important step in the supervised machine learning lifecycle. It is a laborious human activity comprised of repeated decision making: the human labeler decides which of several potential labels to apply to each example. Prior work has shown that providing AI assistance can improve the accuracy of binary decision tasks. However, the role of AI assistance in more complex data-labeling scenarios with a larger set of labels has not yet been explored. We designed an AI labeling assistant that uses a semi-supervised learning algorithm to predict the most probable labels for each example. We leverage these predictions to provide assistance in two ways: (i) providing a label recommendation and (ii) reducing the labeler’s decision space by focusing their attention on only the most probable labels. We conducted a user study (n=54) to evaluate an AI-assisted interface for data labeling in this context. Our results highlight that the AI assistance improves both labeler accuracy and speed, especially when the labeler finds the correct label in the reduced label space. We discuss findings related to the presentation of AI assistance and design implications for intelligent labeling interfaces.",10.1145/3397481.3450698,https://doi.org/10.1145/3397481.3450698,International Conference on Intelligent User Interfaces (IUI),"Desmond, Michael; Muller, Michael; Ashktorab, Zahra; Dugan, Casey; Duesterwald, Evelyn; Brimijoin, Kristina; Finegan-Dollak, Catherine; Brachman, Michelle; Sharma, Aabhas; Joshi, Narendra Nath; Pan, Qian",2021,8,"@inproceedings{2-3533,
  title = {Increasing the Speed and Accuracy of Data Labeling Through an AI Assisted Interface},
  author = {Desmond, Michael and Muller, Michael and Ashktorab, Zahra and Dugan, Casey and Duesterwald, Evelyn and Brimijoin, Kristina and Finegan-Dollak, Catherine and Brachman, Michelle and Sharma, Aabhas and Joshi, Narendra Nath and Pan, Qian},
  year = {2021},
  doi = {10.1145/3397481.3450698},
  booktitle = {International Conference on Intelligent User Interfaces (IUI)}
}",System/Artifact contributions,Education / Teaching / Research,Operational,"Forecasting, Advising, Collaborating","Decision-maker, Knowledge provider","Alter decision outcomes, Change cognitive demands",Update AI competence,"recommendations, the most probable labels",NA,Interactive interface,Yes,Yes
2-35345,springernature,Human–computer collaboration for skin cancer recognition,"The rapid increase in telemedicine coupled with recent advances in diagnostic artificial intelligence( AI) create the imperative to consider the opportunities and risks of inserting AI-based support into new paradigms of care. Here we build on recent achievements in the accuracy of image-based AI for skin cancer diagnosis to address the effects of varied representations of AI-based support across different levels of clinical expertise and multiple clinical workflows. We find that good quality AI-based support of clinical decision-making improves diagnostic accuracy over that of either AI or physicians alone, and that the least experienced clinicians gain the most from AI-based support. We further find that AI-based multiclass probabilities outperformed content-based image retrieval( CBIR) representations of AI in the mobile technology environment, and AI-based support had utility in simulations of second opinions and of telemedicine triage. In addition to demonstrating the potential benefits associated with good quality AI in the hands of non-expert clinicians, we find that faulty AI can mislead the entire spectrum of clinicians, including experts. Lastly, we show that insights derived from AI class-activation maps can inform improvements in human diagnosis. Together, our approach and findings offer a framework for future studies across the spectrum of image-based diagnostics to improve human–computer collaboration in clinical practice. A systematic evaluation of the value of AI-based decision support in skin tumor diagnosis demonstrates the superiority of human–computer collaboration over each individual approach and supports the potential of automated approaches in diagnostic medicine.",10.1038/s41591-020-0942-0,http://dx.doi.org/10.1038/s41591-020-0942-0,Nature Medicine,"Tschandl, Philipp;Rinner, Christoph;Apalla, Zoe;Argenziano, Giuseppe;Codella, Noel;Halpern, Allan;Janda, Monika;Lallas, Aimilios;Longo, Caterina;Malvehy, Josep;Paoli, John;Puig, Susana;Rosendahl, Cliff;Soyer, H. Peter;Zalaudek, Iris;Kittler, Harald",2020,1027,"@article{2-35345,
  title={Human--computer collaboration for skin cancer recognition},
  author={Tschandl, Philipp and Rinner, Christoph and Apalla, Zoe and Argenziano, Giuseppe and Codella, Noel and Halpern, Allan and Janda, Monika and Lallas, Aimilios and Longo, Caterina and Malvehy, Josep and Paoli, John and Puig, Susana and Rosendahl, Cliff and Soyer, H. Peter and Zalaudek, Iris and Kittler, Harald},
  year={2020},
  doi={10.1038/s41591-020-0942-0},
  journal={Nature Medicine}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Knowledge provider, Decision-maker, Decision-subject","Change cognitive demands, Change trust, Alter decision outcomes, Change affective-perceptual",no such info,"good quality support, confidence score, predicted labels","high confidence, low confidence","Textual, Visual",Yes,Yes
2-3535,acm,Trust Development and Repair in AI-Assisted Decision-Making during Complementary Expertise,"Leveraging Artificial Intelligence to support human decision-makers requires harnessing the unique strengths of both entities, where human expertise often complements AI capabilities. However, human decision-makers must accurately discern when to trust the AI. In situations with complementary Human-AI expertise, identifying AI inaccuracies becomes challenging for humans, hindering their ability to rely on the AI only when warranted. Even when AI performance improves post-errors, this inability to assess accuracy can hinder trust recovery. Through two experimental tasks, we investigate trust development, erosion, and recovery during AI-assisted decision-making, examining explicit Trust Repair Strategies (TRSs) – Apology, Denial, Promise, and Model Update. Our participants classified familiar and unfamiliar stimuli with an AI with varying accuracy. We find that participants leveraged AI accuracy in familiar tasks as a heuristic to dynamically calibrate their trust during unfamiliar tasks. Further, once trust in the AI was eroded, trust restored through Model Update surpassed initial trust values, followed by Apology, Promise, and the baseline (no repair), with Denial being least effective. We empirically demonstrate how trust calibration occurs during complementary expertise, highlighting factors influencing the different effectiveness of TRSs despite identical AI accuracy, and offering implications for effectively restoring trust in Human-AI collaborations.",10.1145/3630106.3658924,https://doi.org/10.1145/3630106.3658924,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Pareek, Saumya; Velloso, Eduardo; Goncalves, Jorge",2024,1,"@inproceedings{2-3535,
  title = {Trust Development and Repair in AI-Assisted Decision-Making during Complementary Expertise},
  author = {Pareek, Saumya and Velloso, Eduardo and Goncalves, Jorge},
  year = {2024},
  doi = {10.1145/3630106.3658924},
  booktitle = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service",Individual,"Collaborating, Advising",Decision-maker,Change trust,no such info,"system accuracy, recommendations",domain knowledge,Textual,Yes,Yes
2-35369,springernature,Evaluating the use of large language models to provide clinical recommendations in the Emergency Department,"The release of GPT-4 and other large language models( LLMs) has the potential to transform healthcare. However, existing research evaluating LLM performance on real-world clinical notes is limited. Here, we conduct a highly-powered study to determine whether LLMs can provide clinical recommendations for three tasks( admission status, radiological investigation( s) request status, and antibiotic prescription status) using clinical notes from the Emergency Department. We randomly selected 10, 000 Emergency Department visits to evaluate the accuracy of zero-shot, GPT-3. 5-turboand GPT-4-turbo-generated clinical recommendations across four different prompting strategies. We found that both GPT-4-turbo and GPT-3. 5-turbo performed poorly compared to a resident physician, with accuracy scores 8% and 24%, respectively, lower than physician on average. Both LLMs tended to be overly cautious in its recommendations, with high sensitivity at the cost of specificity. Our findings demonstrate that, while early evaluations of the clinical use of LLMs are promising, LLM performance must be significantly improved before their deployment as decision support systems for clinical recommendations and other complex tasks. The emergence of large language models has the potential to transform healthcare. Here, the authors show that, when providing clinical recommendations, these models perform poorly compared to physicians and are overly cautious in their decisions.",10.1038/s41467-024-52415-1,http://dx.doi.org/10.1038/s41467-024-52415-1,Nature Communications,"Williams, Christopher Y. K.;Miao, Brenda Y.;Kornblith, Aaron E.;Butte, Atul J.",2024,71,"@article{2-35369,
  title={Evaluating the use of large language models to provide clinical recommendations in the Emergency Department},
  author={Williams, Christopher Y. K. and Miao, Brenda Y. and Kornblith, Aaron E. and Butte, Atul J.},
  year={2024},
  journal={Nature Communications},
  doi={10.1038/s41467-024-52415-1}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3537,acm,Contestable Camera Cars: A Speculative Design Exploration of Public AI That Is Open and Responsive to Dispute,"Local governments increasingly use artificial intelligence (AI) for automated decision-making. Contestability, making systems responsive to dispute, is a way to ensure they respect human rights to autonomy and dignity. We investigate the design of public urban AI systems for contestability through the example of camera cars: human-driven vehicles equipped with image sensors. Applying a provisional framework for contestable AI, we use speculative design to create a concept video of a contestable camera car. Using this concept video, we then conduct semi-structured interviews with 17 civil servants who work with AI employed by a large northwestern European city. The resulting data is analyzed using reflexive thematic analysis to identify the main challenges facing the implementation of contestability in public AI. We describe how civic participation faces issues of representation, public AI systems should integrate with existing democratic practices, and cities must expand capacities for responsible AI development and operation.",10.1145/3544548.3580984,https://doi.org/10.1145/3544548.3580984,ACM CHI Conference on Human Factors in Computing Systems,"Alfrink, Kars; Keller, Ianus; Doorn, Neelke; Kortuem, Gerd",2023,0,"@inproceedings{2-3537,
  title = {Contestable Camera Cars: A Speculative Design Exploration of Public AI That Is Open and Responsive to Dispute},
  author = {Alfrink, Kars and Keller, Ianus and Doorn, Neelke and Kortuem, Gerd},
  year = {2023},
  doi = {10.1145/3544548.3580984},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Law / Policy / Governance,no such info,"Analyzing, Monitoring","Stakeholder, Guardian, Decision-maker","Change cognitive demands, Change affective-perceptual, Restrict human agency, Change trust","Update AI competence, Change AI responses, Shape AI for accountability",NA,democratic control,Visual,Yes,Yes
2-3540,acm,Human-Centered Tools for Coping with Imperfect Algorithms During Medical Decision-Making,"Machine learning (ML) is increasingly being used in image retrieval systems for medical decision making. One application of ML is to retrieve visually similar medical images from past patients (e.g. tissue from biopsies) to reference when making a medical decision with a new patient. However, no algorithm can perfectly capture an expert's ideal notion of similarity for every case: an image that is algorithmically determined to be similar may not be medically relevant to a doctor's specific diagnostic needs. In this paper, we identified the needs of pathologists when searching for similar images retrieved using a deep learning algorithm, and developed tools that empower users to cope with the search algorithm on-the-fly, communicating what types of similarity are most important at different moments in time. In two evaluations with pathologists, we found that these tools increased the diagnostic utility of images found and increased user trust in the algorithm. The tools were preferred over a traditional interface, without a loss in diagnostic accuracy. We also observed that users adopted new strategies when using refinement tools, re-purposing them to test and understand the underlying algorithm and to disambiguate ML errors from their own errors. Taken together, these findings inform future human-ML collaborative systems for expert decision-making.",10.1145/3290605.3300234,https://doi.org/10.1145/3290605.3300234,Conference on Human Factors in Computing Systems,"Cai, Carrie J.; Reif, Emily; Hegde, Narayan; Hipp, Jason; Kim, Been; Smilkov, Daniel; Wattenberg, Martin; Viegas, Fernanda; Corrado, Greg S.; Stumpe, Martin C.; Terry, Michael",2019,602,"@inproceedings{2-3540,
  title={Human-Centered Tools for Coping with Imperfect Algorithms During Medical Decision-Making},
  author={Cai, Carrie J. and Reif, Emily and Hegde, Narayan and Hipp, Jason and Kim, Been and Smilkov, Daniel and Wattenberg, Martin and Viegas, Fernanda and Corrado, Greg S. and Stumpe, Martin C. and Terry, Michael},
  year={2019},
  doi={10.1145/3290605.3300234},
  booktitle={Conference on Human Factors in Computing Systems}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Decision-maker, Knowledge provider","Alter decision outcomes, Change trust, Change cognitive demands, Shape ethical norms, Change affective-perceptual","Update AI competence, Change AI responses",visual analysis,personalized settings,"Visual, Interactive interface",Yes,Yes
2-3545,acm,The Role of Accuracy in Algorithmic Process Fairness Across Multiple Domains,"Machine learning is often used to aid in human decision-making, sometimes for life-altering decisions like when determining whether or not to grant bail to a defendant or a loan to an applicant. Because of their importance, it is critical to ensure that the processes used to reach these decisions are considered fair. A common approach is to enforce some fairness constraint over the outcomes of a decision maker, but there is no single, generally-accepted definition of fairness. With notable exceptions, most of the literature on algorithmic fairness takes for granted that there will be an inherent trade-off between accuracy and algorithmic fairness. Additionally, most work focuses only on one or two domains, whereas machine learning techniques are used in an increasing number of distinct decision-making contexts with differing pertinent features. In this work, we consider six different decision-making domains: bail, child protective services, hospital resources, insurance rates, loans, and unemployment aid. We focus on the fairness of the process directly, rather than the outcomes. We also take a descriptive approach, using survey data to elicit the factors that lead a decision-making process to be perceived as fair. Specifically, we ask 2157 Amazon Mechanical Turk workers to rate the features used for algorithmic decision-making in one of the six domains as either fair or unfair, as well as to rate how much they agree or disagree with the assignments of eight previously (and one newly) proposed properties to the features. For example, a worker could be asked to rate the feature of ""criminal history"" as fair or unfair to use in bail decisions, and then rate how much they agree or disagree that ""criminal history"" is a reliable feature. We show that, in every domain, disagreements in fairness judgements can be largely explained by the assignments of properties (like reliability) to features (like criminal history). We also show that fairness judgements can be well predicted across domains by training the predictor using the property assignments from one domain's data and predicting in another. These findings imply that the properties act as moral determinants for fairness judgements, and that respondents reason similarly about the implications of the properties in all the decision-making domains that we consider. Although our results are mostly consistent across domains, we find some important differences within specific demographic groups in the hospital and insurance domains, indicating that at least some differences in fairness judgements are introduced by demographic differences. However, a single property usually holds the majority of the predictive power. With some exceptions, predictors learning from only the ""increases accuracy"" property perform better (in all domains) than predictors learning from any combination of the other seven properties, implying that the primary factor affecting respondents' perceptions of the fairness of using a feature for prediction is whether or not a feature increases the accuracy of the decision being made.",10.1145/3465456.3467620,https://doi.org/10.1145/3465456.3467620,ACM Conference on Economics and Computation (ACM EC),"Albach, Michele; Wright, James R.",2021,39,"@inproceedings{2-3545,
  author    = {Albach, Michele and Wright, James R.},
  title     = {The Role of Accuracy in Algorithmic Process Fairness Across Multiple Domains},
  booktitle = {ACM Conference on Economics and Computation (ACM EC)},
  year      = {2021},
  doi       = {10.1145/3465456.3467620}
}",Empirical contributions,"Healthcare / Medicine / Surgery, Law / Policy / Governance, Finance / Business / Economy, Generic / Abstract / Domain-agnostic",Institutional,"Forecasting, Advising","Guardian, Decision-maker, Decision-subject, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-35455,springernature,An interpretable mortality prediction model for COVID-19 patients,"The sudden increase in COVID-19 cases is putting high pressure on healthcare services worldwide. At this stage, fast, accurate and early clinical assessment of the disease severity is vital. To support decision making and logistical planning in healthcare systems, this study leverages a database of blood samples from 485 infected patients in the region of Wuhan, China, to identify crucial predictive biomarkers of disease mortality. For this purpose, machine learning tools selected three biomarkers that predict the mortality of individual patients more than 10 days in advance with more than 90% accuracy: lactic dehydrogenase( LDH) , lymphocyte and high-sensitivity C-reactive protein( hs-CRP). In particular, relatively high levels of LDH alone seem to play a crucial role in distinguishing the vast majority of cases that require immediate medical attention. This finding is consistent with current medical knowledge that high LDH levels are associated with tissue breakdown occurring in various diseases, including pulmonary disorders such as pneumonia. Overall, this Article suggests a simple and operable decision rule to quickly predict patients at the highest risk, allowing them to be prioritized and potentially reducing the mortality rate. Early and accurate clinical assessment of disease severity in COVID-19 patients is essential for planning the allocation of scarce hospital resources. An explainable machine learning tool trained on blood sample data from 485 patients from Wuhan selected three biomarkers for predicting mortality of individual patients with high accuracy.",10.1038/s42256-020-0180-7,http://dx.doi.org/10.1038/s42256-020-0180-7,Nature Machine Intelligence,"Yan, Li;Zhang, Hai-Tao;Goncalves, Jorge;Xiao, Yang;Wang, Maolin;Guo, Yuqi;Sun, Chuan;Tang, Xiuchuan;Jing, Liang;Zhang, Mingyang;Huang, Xiang;Xiao, Ying;Cao, Haosen;Chen, Yanyan;Ren, Tongxin;Wang, Fang;Xiao, Yaru;Huang, Sufang;Tan, Xi;Huang, Niannian;Jiao, Bo;Cheng, Cheng;Zhang, Yong;Luo, Ailin;Mombaerts, Laurent;Jin, Junyang;Cao, Zhiguo;Li, Shusheng;Xu, Hui;Yuan, Ye",2020,12,"@article{2-35455,
  title = {An interpretable mortality prediction model for COVID-19 patients},
  author = {Yan, Li and Zhang, Hai-Tao and Goncalves, Jorge and Xiao, Yang and Wang, Maolin and Guo, Yuqi and Sun, Chuan and Tang, Xiuchuan and Jing, Liang and Zhang, Mingyang and Huang, Xiang and Xiao, Ying and Cao, Haosen and Chen, Yanyan and Ren, Tongxin and Wang, Fang and Xiao, Yaru and Huang, Sufang and Tan, Xi and Huang, Niannian and Jiao, Bo and Cheng, Cheng and Zhang, Yong and Luo, Ailin and Mombaerts, Laurent and Jin, Junyang and Cao, Zhiguo and Li, Shusheng and Xu, Hui and Yuan, Ye},
  year = {2020},
  doi = {10.1038/s42256-020-0180-7},
  journal = {Nature Machine Intelligence}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-35486,springernature,Machine learning-based clinical decision support system for treatment recommendation and overall survival prediction of hepatocellular carcinoma: a multi-center study,"The treatment decisions for patients with hepatocellular carcinoma are determined by a wide range of factors, and there is a significant difference between the recommendations of widely used staging systems and the actual initial treatment choices. Herein, we propose a machine learning-based clinical decision support system suitable for use in multi-center settings. We collected data from nine institutions in South Korea for training and validation datasets. The internal and external datasets included 935 and 1750 patients, respectively. We developed a model with 20 clinical variables consisting of two stages: the first stage which recommends initial treatment using an ensemble voting machine, and the second stage, which predicts post-treatment survival using a random survival forest algorithm. We derived the first and second treatment options from the results with the highest and the second-highest probabilities given by the ensemble model and predicted their post-treatment survival. When only the first treatment option was accepted, the mean accuracy of treatment recommendation in the internal and external datasets was 67. 27% and 55. 34%, respectively. The accuracy increased to 87. 27% and 86. 06%, respectively, when the second option was included as the correct answer. Harrell’s C index, integrated time-dependent AUC curve, and integrated Brier score of survival prediction in the internal and external datasets were 0. 8381 and 0. 7767, 91. 89 and 86. 48, 0. 12, and 0. 14, respectively. The proposed system can assist physicians by providing data-driven predictions for reference from other larger institutions or other physicians within the same institution when making treatment decisions.",10.1038/s41746-023-00976-8,http://dx.doi.org/10.1038/s41746-023-00976-8,Nature Partner Journals Digital Medicine,"Lee, Kyung Hwa;Choi, Gwang Hyeon;Yun, Jihye;Choi, Jonggi;Goh, Myung Ji;Sinn, Dong Hyun;Jin, Young Joo;Kim, Minseok Albert;Yu, Su Jong;Jang, Sangmi;Lee, Soon Kyu;Jang, Jeong Won;Lee, Jae Seung;Kim, Do Young;Cho, Young Youn;Kim, Hyung Joon;Kim, Sehwa;Kim, Ji Hoon;Kim, Namkug;Kim, Kang Mo",2024,27,"@article{2-35486,
  title={Machine learning-based clinical decision support system for treatment recommendation and overall survival prediction of hepatocellular carcinoma: a multi-center study},
  author={Lee, Kyung Hwa and Choi, Gwang Hyeon and Yun, Jihye and Choi, Jonggi and Goh, Myung Ji and Sinn, Dong Hyun and Jin, Young Joo and Kim, Minseok Albert and Yu, Su Jong and Jang, Sangmi and Lee, Soon Kyu and Jang, Jeong Won and Lee, Jae Seung and Kim, Do Young and Cho, Young Youn and Kim, Hyung Joon and Kim, Sehwa and Kim, Ji Hoon and Kim, Namkug and Kim, Kang Mo},
  year={2024},
  doi={10.1038/s41746-023-00976-8},
  journal={Nature Partner Journals Digital Medicine}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-35503,springernature,Accelerating evidence-informed decision-making for the Sustainable Development Goals using machine learning,"The United Nations Sustainable Development Goal 2( SDG 2) is to achieve zero hunger by 2030. We have designed Persephone, a machine learning model, to support a diverse volunteer network of 77 researchers from 23 countries engaged in creating interdisciplinary evidence syntheses in support of SDG 2. Such evidence syntheses, whatever the specific topic, assess original studies to determine the effectiveness of interventions. By gathering and summarizing current evidence and providing objective recommendations they can be valuable aids to decision-makers. However, they are time-consuming; estimates range from 18 months to three years to produce a single review. Persephone analysed 500, 000 unstructured text summaries from prominent sources of agricultural research, determining with 90% accuracy the subset of studies that would eventually be selected by expert researchers. We demonstrate that machine learning models can be invaluable in placing evidence into the hands of policymakers. Evidence syntheses produced from the scientific literature are important tools for policymakers. Producing such evidence syntheses can be highly timeand labour-consuming but machine learning models can help as already demonstrated in the health and medical sciences. This Perspective describes a machine learning-based framework specifically designed to support evidence syntheses in the area of agricultural research, for tackling the UN Sustainable Development Goal 2: zero hunger by 2030.",10.1038/s42256-020-00235-5,http://dx.doi.org/10.1038/s42256-020-00235-5,Nature Machine Intelligence,"Porciello, Jaron;Ivanina, Maryia;Islam, Maidul;Einarson, Stefan;Hirsh, Haym",2020,5,"@article{2-35503,
  title={Accelerating evidence-informed decision-making for the Sustainable Development Goals using machine learning},
  author={Porciello, Jaron and Ivanina, Maryia and Islam, Maidul and Einarson, Stefan and Hirsh, Haym},
  year={2020},
  doi={10.1038/s42256-020-00235-5},
  journal={Nature Machine Intelligence}
}",System/Artifact contributions,Environment / Resources / Energy,Institutional,"Forecasting, Advising, Analyzing","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-3551,acm,"""The human body is a black box"": supporting clinical decision-making with deep learning","Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.",10.1145/3351095.3372827,https://doi.org/10.1145/3351095.3372827,"ACM Conference on Fairness, Accountability, and Transparency (FAccT)","Sendak, Mark; Elish, Madeleine Clare; Gao, Michael; Futoma, Joseph; Ratliff, William; Nichols, Marshall; Bedoya, Armando; Balu, Suresh; O'Brien, Cara",2020,266,"@inproceedings{2-3551,
  title = {``The human body is a black box'': supporting clinical decision-making with deep learning},
  author = {Sendak, Mark and Elish, Madeleine Clare and Gao, Michael and Futoma, Joseph and Ratliff, William and Nichols, Marshall and Bedoya, Armando and Balu, Suresh and O'Brien, Cara},
  year = {2020},
  booktitle = {ACM Conference on Fairness, Accountability, and Transparency (FAccT)},
  doi = {10.1145/3351095.3372827}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Stakeholder, Knowledge provider","Change cognitive demands, Alter decision outcomes, Restrict human agency","Update AI competence, Change AI responses, Shape AI for accountability","preliminary diagnoses, risk score",NA,Interactive interface,Yes,Yes
2-35511,springernature,Prospective validation of dermoscopy-based open-source artificial intelligence for melanoma diagnosis( PROVE-AI study),"The use of artificial intelligence( AI) has the potential to improve the assessment of lesions suspicious of melanoma, but few clinical studies have been conducted. We validated the accuracy of an open-source, non-commercial AI algorithm for melanoma diagnosis and assessed its potential impact on dermatologist decision-making. We conducted a prospective, observational clinical study to assess the diagnostic accuracy of the AI algorithm( ADAE) in predicting melanoma from dermoscopy skin lesion images. The primary aim was to assess the reliability of ADAE’s sensitivity at a predefined threshold of 95%. Patients who had consented for a skin biopsy to exclude melanoma were eligible. Dermatologists also estimated the probability of melanoma and indicated management choices before and after real-time exposure to ADAE scores. All lesions underwent biopsy. Four hundred thirty-five participants were enrolled and contributed 603 lesions( 95 melanomas). Participants had a mean age of 59 years, 54% were female, and 96% were White individuals. At the predetermined 95% sensitivity threshold, ADAE had a sensitivity of 96. 8%( 95% CI: 91. 1–98. 9%) and specificity of 37. 4%( 95% CI: 33. 3–41. 7%). The dermatologists’ ability to assess melanoma risk significantly improved after ADAE exposure( AUC 0. 7798 vs. 0. 8161, p = 0. 042). Post-ADAE dermatologist decisions also had equivalent or higher net benefit compared to biopsying all lesions. We validated the accuracy of an open-source melanoma AI algorithm and showed its theoretical potential for improving dermatology experts’ ability to evaluate lesions suspicious of melanoma. Larger randomized trials are needed to fully evaluate the potential of adopting this AI algorithm into clinical workflows.",10.1038/s41746-023-00872-1,http://dx.doi.org/10.1038/s41746-023-00872-1,Nature Partner Journals Digital Medicine,"Marchetti, Michael A.;Cowen, Emily A.;Kurtansky, Nicholas R.;Weber, Jochen;Dauscher, Megan;DeFazio, Jennifer;Deng, Liang;Dusza, Stephen W.;Haliasos, Helen;Halpern, Allan C.;Hosein, Sharif;Nazir, Zaeem H.;Marghoob, Ashfaq A.;Quigley, Elizabeth A.;Salvador, Trina;Rotemberg, Veronica M.",2023,58,"@article{2-35511,
  title={Prospective validation of dermoscopy-based open-source artificial intelligence for melanoma diagnosis (PROVE-AI study)},
  author={Marchetti, Michael A. and Cowen, Emily A. and Kurtansky, Nicholas R. and Weber, Jochen and Dauscher, Megan and DeFazio, Jennifer and Deng, Liang and Dusza, Stephen W. and Haliasos, Helen and Halpern, Allan C. and Hosein, Sharif and Nazir, Zaeem H. and Marghoob, Ashfaq A. and Quigley, Elizabeth A. and Salvador, Trina and Rotemberg, Veronica M.},
  year={2023},
  doi={10.1038/s41746-023-00872-1},
  journal={Nature Partner Journals Digital Medicine}
}","Algorithmic contributions, Empirical contributions",Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3552,acm,How Child Welfare Workers Reduce Racial Disparities in Algorithmic Decisions,"Machine learning tools have been deployed in various contexts to support human decision-making, in the hope that human-algorithm collaboration can improve decision quality. However, the question of whether such collaborations reduce or exacerbate biases in decision-making remains underexplored. In this work, we conducted a mixed-methods study, analyzing child welfare call screen workers’ decision-making over a span of four years, and interviewing them on how they incorporate algorithmic predictions into their decision-making process. Our data analysis shows that, compared to the algorithm alone, workers reduced the disparity in screen-in rate between Black and white children from 20% to 9%. Our qualitative data show that workers achieved this by making holistic risk assessments and adjusting for the algorithm’s limitations. Our analyses also show more nuanced results about how human-algorithm collaboration affects prediction accuracy, and how to measure these effects. These results shed light on potential mechanisms for improving human-algorithm collaboration in high-risk decision-making contexts.",10.1145/3491102.3501831,https://doi.org/10.1145/3491102.3501831,ACM CHI Conference on Human Factors in Computing Systems,"Cheng, Hao-Fei; Stapleton, Logan; Kawakami, Anna; Sivaraman, Venkatesh; Cheng, Yanghuidi; Qing, Diana; Perer, Adam; Holstein, Kenneth; Wu, Zhiwei Steven; Zhu, Haiyi",2022,100,"@inproceedings{2-3552,
  title = {How Child Welfare Workers Reduce Racial Disparities in Algorithmic Decisions},
  author = {Cheng, Hao-Fei and Stapleton, Logan and Kawakami, Anna and Sivaraman, Venkatesh and Cheng, Yanghuidi and Qing, Diana and Perer, Adam and Holstein, Kenneth and Wu, Zhiwei Steven and Zhu, Haiyi},
  year = {2022},
  booktitle = {Proceedings of the ACM CHI Conference on Human Factors in Computing Systems},
  doi = {10.1145/3491102.3501831}
}",Empirical contributions,Law / Policy / Governance,Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Stakeholder, Guardian","Alter decision outcomes, Shape ethical norms","Change AI responses, Shape AI for accountability","risk score, recommendations, scoring errors in referral decisions due to system-specific context, racial disparities driven by biased CYF and county data","screening recommendation for the referral, disregarding the AI recommendations bc of AI’s incomplete consideration of relevant information, disagreement on prediction outcomes and accuracy measures with AI","Semi-Autonomous System, Autonomous System",Yes,Yes
2-3553,acm,Designing AI for Trust and Collaboration in Time-Constrained Medical Decisions: A Sociotechnical Lens,"Major depressive disorder is a debilitating disease affecting 264 million people worldwide. While many antidepressant medications are available, few clinical guidelines support choosing among them. Decision support tools (DSTs) embodying machine learning models may help improve the treatment selection process, but often fail in clinical practice due to poor system integration. We use an iterative, co-design process to investigate clinicians’ perceptions of using DSTs in antidepressant treatment decisions. We identify ways in which DSTs need to engage with the healthcare sociotechnical system, including clinical processes, patient preferences, resource constraints, and domain knowledge. Our results suggest that clinical DSTs should be designed as multi-user systems that support patient-provider collaboration and offer on-demand explanations that address discrepancies between predictions and current standards of care. Through this work, we demonstrate how current trends in explainable AI may be inappropriate for clinical environments and consider paths towards designing these tools for real-world medical systems.",10.1145/3411764.3445385,https://doi.org/10.1145/3411764.3445385,CHI Conference on Human Factors in Computing Systems,"Jacobs, Maia; He, Jeffrey; F. Pradier, Melanie; Lam, Barbara; Ahn, Andrew C.; McCoy, Thomas H.; Perlis, Roy H.; Doshi-Velez, Finale; Gajos, Krzysztof Z.",2021,62,"@inproceedings{2-3553,
  title = {Designing {AI} for Trust and Collaboration in Time-Constrained Medical Decisions: A Sociotechnical Lens},
  author = {Jacobs, Maia and He, Jeffrey and Pradier, Melanie F. and Lam, Barbara and Ahn, Andrew C. and McCoy, Thomas H. and Perlis, Roy H. and Doshi-Velez, Finale and Gajos, Krzysztof Z.},
  year = {2021},
  doi = {10.1145/3411764.3445385},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Forecasting, Advising","Decision-subject, Decision-maker","Change trust, Alter decision outcomes, Change affective-perceptual",Update AI competence,"on-demand explanations, preliminary diagnoses, prediction of alternative, personalized recommendations",personalized settings,Interactive interface,Yes,Yes
2-3554,acm,Artificial Intelligence for Modeling Complex Systems: Taming the Complexity of Expert Models to Improve Decision Making,"Major societal and environmental challenges involve complex systems that have diverse multi-scale interacting processes. Consider, for example, how droughts and water reserves affect crop production and how agriculture and industrial needs affect water quality and availability. Preventive measures, such as delaying planting dates and adopting new agricultural practices in response to changing weather patterns, can reduce the damage caused by natural processes. Understanding how these natural and human processes affect one another allows forecasting the effects of undesirable situations and study interventions to take preventive measures. For many of these processes, there are expert models that incorporate state-of-the-art theories and knowledge to quantify a system's response to a diversity of conditions. A major challenge for efficient modeling is the diversity of modeling approaches across disciplines and the wide variety of data sources available only in formats that require complex conversions. Using expert models for particular problems requires integration of models with third-party data as well as integration of models across disciplines. Modelers face significant heterogeneity that requires resolving semantic, spatiotemporal, and execution mismatches, which are largely done by hand today and may take more than 2 years of effort.We are developing a modeling framework that uses artificial intelligence (AI) techniques to reduce modeling effort while ensuring utility for decision making. Our work to date makes several innovative contributions: (1) an intelligent user interface that guides analysts to frame their modeling problem and assists them by suggesting relevant choices and automating steps along the way; (2) semantic metadata for models, including their modeling variables and constraints, that ensures model relevance and proper use for a given decision-making problem; and (3) semantic representations of datasets in terms of modeling variables that enable automated data selection and data transformations. This framework is implemented in the MINT (Model INTegration) framework, and currently includes data and models to analyze the interactions between natural and human systems involving climate, water availability, agricultural production, and markets. Our work to date demonstrates the utility of AI techniques to accelerate modeling to support decision-making and uncovers several challenging directions for future work.",10.1145/3453172,https://doi.org/10.1145/3453172,ACM Transactions on Interactive Intelligent Systems,"Gil, Yolanda; Garijo, Daniel; Khider, Deborah; Knoblock, Craig A.; Ratnakar, Varun; Osorio, Maximiliano; Vargas, Hernán; Pham, Minh; Pujara, Jay; Shbita, Basel; Vu, Binh; Chiang, Yao-Yi; Feldman, Dan; Lin, Yijun; Song, Hayley; Kumar, Vipin; Khandelwal, Ankush; Steinbach, Michael; Tayal, Kshitij; Xu, Shaoming; Pierce, Suzanne A.; Pearson, Lissa; Hardesty-Lewis, Daniel; Deelman, Ewa; Silva, Rafael Ferreira Da; Mayani, Rajiv; Kemanian, Armen R.; Shi, Yuning; Leonard, Lorne; Peckham, Scott; Stoica, Maria; Cobourn, Kelly; Zhang, Zeya; Duffy, Christopher; Shu, Lele",2021,82,"@article{2-3554,
  title={Artificial Intelligence for Modeling Complex Systems: Taming the Complexity of Expert Models to Improve Decision Making},
  author={Gil, Yolanda and Garijo, Daniel and Khider, Deborah and Knoblock, Craig A. and Ratnakar, Varun and Osorio, Maximiliano and Vargas, Hern{\'a}n and Pham, Minh and Pujara, Jay and Shbita, Basel and Vu, Binh and Chiang, Yao-Yi and Feldman, Dan and Lin, Yijun and Song, Hayley and Kumar, Vipin and Khandelwal, Ankush and Steinbach, Michael and Tayal, Kshitij and Xu, Shaoming and Pierce, Suzanne A. and Pearson, Lissa and Hardesty-Lewis, Daniel and Deelman, Ewa and Silva, Rafael Ferreira Da and Mayani, Rajiv and Kemanian, Armen R. and Shi, Yuning and Leonard, Lorne and Peckham, Scott and Stoica, Maria and Cobourn, Kelly and Zhang, Zeya and Duffy, Christopher and Shu, Lele},
  year={2021},
  journal={ACM Transactions on Interactive Intelligent Systems},
  doi={10.1145/3453172}
}",System/Artifact contributions,Environment / Resources / Energy,Operational,"Analyzing, Advising, Collaborating","Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-3555,acm,Making Human-Like Moral Decisions,"Many real-life scenarios require humans to make difficult trade-offs: do we always follow all the traffic rules or do we violate the speed limit in an emergency? In general, how should we account for and balance the ethical values, safety recommendations, and societal norms, when we are trying to achieve a certain objective? To enable effective AI-human collaboration, we must equip AI agents with a model of how humans make such trade-offs in environments where there is not only a goal to be reached, but there are also ethical constraints to be considered and to possibly align with. These ethical constraints could be both deontological rules on actions that should not be performed, or also consequentialist policies that recommend avoiding reaching certain states of the world. Our purpose is to build AI agents that can mimic human behavior in these ethically constrained decision environments, with a long term research goal to use AI to help humans in making better moral judgments and actions. To this end, we propose a computational approach where competing objectives and ethical constraints are orchestrated through a method that leverages a cognitive model of human decision making, called multi-alternative decision field theory (MDFT). Using MDFT, we build an orchestrator, called MDFT-Orchestrator (MDFT-O), that is both general and flexible. We also show experimentally that MDFT-O both generates better decisions than using a heuristic that takes a weighted average of competing policies (WA-O), but also performs better in terms of mimicking human decisions as collected through Amazon Mechanical Turk (AMT). Our methodology is therefore able to faithfully model human decision in ethically constrained decision environments.",10.1145/3514094.3534174,https://doi.org/10.1145/3514094.3534174,"AAAI/ACM Conference on AI, Ethics, and Society","Loreggia, Andrea; Mattei, Nicholas; Rahgooy, Taher; Rossi, Francesca; Srivastava, Biplav; Venable, Kristen Brent",2022,13,"@inproceedings{2-3555,
  title = {Making Human-Like Moral Decisions},
  author = {Loreggia, Andrea and Mattei, Nicholas and Rahgooy, Taher and Rossi, Francesca and Srivastava, Biplav and Venable, Kristen Brent},
  year = {2022},
  doi = {10.1145/3514094.3534174},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Individual,"Advising, Executing, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-3556,acm,Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance,"Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI’s recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?",10.1145/3411764.3445717,https://doi.org/10.1145/3411764.3445717,CHI Conference on Human Factors in Computing Systems,"Bansal, Gagan; Wu, Tongshuang; Zhou, Joyce; Fok, Raymond; Nushi, Besmira; Kamar, Ece; Ribeiro, Marco Tulio; Weld, Daniel",2021,961,"@inproceedings{2-3556,
author = {Bansal, Gagan and Wu, Tongshuang and Zhou, Joyce and Fok, Raymond and Nushi, Besmira and Kamar, Ece and Ribeiro, Marco Tulio and Weld, Daniel},
title = {Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance},
year = {2021},
doi = {10.1145/3411764.3445717},
articleno = {81},
numpages = {16}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy, Healthcare / Medicine / Surgery, Education / Teaching / Research",no such info,"Advising, Explaining",Decision-maker,"Alter decision outcomes, Change trust, Change cognitive demands",no such info,adaptive explanations,NA,"Textual, Interactive interface",Yes,Yes
2-35579,springernature,A Fast Domain-Inspired Unsupervised Method to Compute COVID-19 Severity Scores from Lung CT,"There has been a deluge of data-driven deep learning approaches to detect COVID-19 from computed tomography( CT) images over the pandemic, most of which use ad-hoc deep learning black boxes of little to no relevance to the actual process clinicians use and hence have not seen translation to real-life practical settings. Radiologists use a clinically established process of estimating the percentage of the affected area of the lung to grade the severity of infection out of a score of 0-25 from lung CT scans. Hence any computer-automated process that has aspirations of being adopted in the clinic to alleviate the workload of radiologists while being trustworthy and safe, needs to follow this clearly defined clinical process religiously. Keeping this in mind, we propose a simple yet effective methodology that uses explainable mechanistic modelling using classical image processing and pattern recognition techniques. The proposed pipeline has no learning element and hence is fast. It mimics the clinical process and hence is transparent. We collaborate with an experienced radiologist to enhance an existing benchmark COVID-19 lung CT dataset by adding the grading labels, which is another contribution of this paper, along with the methodology which has a higher potential of becoming a clinical decision support system( CDSS) due to its rapid and explainable nature. The radiologist gradations and the code is available at https://github. com/Samiran-Dey/explainable_seg.",10.1007/978-3-031-78198-8_5,http://dx.doi.org/10.1007/978-3-031-78198-8_5,Pattern Recognition,"Dey, Samiran;Kundu, Bijon;Basuchowdhuri, Partha;Saha, Sanjoy Kumar;Chakraborti, Tapabrata",2025,0,"@inproceedings{2-35579,
  title     = {A Fast Domain-Inspired Unsupervised Method to Compute COVID-19 Severity Scores from Lung CT},
  author    = {Dey, Samiran and Kundu, Bijon and Basuchowdhuri, Partha and Saha, Sanjoy Kumar and Chakraborti, Tapabrata},
  year      = {2025},
  booktitle = {Pattern Recognition},
  doi       = {10.1007/978-3-031-78198-8_5}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Explaining, Forecasting","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-3558,acm,”Because AI is 100% right and safe”: User Attitudes and Sources of AI Authority in India,"Most prior work on human-AI interaction is set in communities that indicate skepticism towards AI, but we know less about contexts where AI is viewed as aspirational. We investigated the perceptions around AI systems by drawing upon 32 interviews and 459 survey respondents in India. Not only do Indian users accept AI decisions (79.2% respondents indicate acceptance), we find a case of AI authority—AI has a legitimized power to influence human actions, without requiring adequate evidence about the capabilities of the system. AI authority manifested into four user attitudes of vulnerability: faith, forgiveness, self-blame, and gratitude, pointing to higher tolerance for system misfires, and introducing potential for irreversible individual and societal harm. We urgently call for calibrating AI authority, reconsidering success metrics and responsible AI approaches and present methodological suggestions for research and deployments in India.",10.1145/3491102.3517533,https://doi.org/10.1145/3491102.3517533,ACM CHI Conference on Human Factors in Computing Systems,"Kapania, Shivani; Siy, Oliver; Clapper, Gabe; SP, Azhagu Meena; Sambasivan, Nithya",2022,132,"@inproceedings{2-3558,
  title = {''Because AI is 100% right and safe'': User Attitudes and Sources of AI Authority in India},
  author = {Kapania, Shivani and Siy, Oliver and Clapper, Gabe and SP, Azhagu Meena and Sambasivan, Nithya},
  year = {2022},
  doi = {10.1145/3491102.3517533},
  booktitle = {Proceedings of the ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,Advising,Decision-maker,"Alter decision outcomes, Change trust, Change affective-perceptual, Change cognitive demands, Shift responsibility, Shape ethical norms",Shape AI for accountability,NA,NA,NA,Yes,Yes
2-35596,springernature,Collaborative strategies for deploying artificial intelligence to complement physician diagnoses of acute respiratory distress syndrome,"There is a growing gap between studies describing the capabilities of artificial intelligence( AI) diagnostic systems using deep learning versus efforts to investigate how or when to integrate AI systems into a real-world clinical practice to support physicians and improve diagnosis. To address this gap, we investigate four potential strategies for AI model deployment and physician collaboration to determine their potential impact on diagnostic accuracy. As a case study, we examine an AI model trained to identify findings of the acute respiratory distress syndrome( ARDS) on chest X-ray images. While this model outperforms physicians at identifying findings of ARDS, there are several reasons why fully automated ARDS detection may not be optimal nor feasible in practice. Among several collaboration strategies tested, we find that if the AI model first reviews the chest X-ray and defers to a physician if it is uncertain, this strategy achieves a higher diagnostic accuracy( 0. 869, 95% CI 0. 835–0. 903) compared to a strategy where a physician reviews a chest X-ray first and defers to an AI model if uncertain( 0. 824, 95% CI 0. 781–0. 862) , or strategies where the physician reviews the chest X-ray alone( 0. 808, 95% CI 0. 767–0. 85) or the AI model reviews the chest X-ray alone( 0. 847, 95% CI 0. 806–0. 887). If the AI model reviews a chest X-ray first, this allows the AI system to make decisions for up to 79% of cases, letting physicians focus on the most challenging subsets of chest X-rays.",10.1038/s41746-023-00797-9,http://dx.doi.org/10.1038/s41746-023-00797-9,Nature Partner Journals Digital Medicine,"Farzaneh, Negar;Ansari, Sardar;Lee, Elizabeth;Ward, Kevin R.;Sjoding, Michael W.",2023,37,"@article{2-35596,
  title = {Collaborative strategies for deploying artificial intelligence to complement physician diagnoses of acute respiratory distress syndrome},
  author = {Farzaneh, Negar and Ansari, Sardar and Lee, Elizabeth and Ward, Kevin R. and Sjoding, Michael W.},
  year = {2023},
  doi = {10.1038/s41746-023-00797-9},
  journal = {Nature Partner Journals Digital Medicine}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Collaborating, Advising, Forecasting","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-3567,acm,Optimization’s Neglected Normative Commitments,"Optimization is offered as an objective approach to resolving complex, real-world decisions involving uncertainty and conflicting interests. It drives business strategies as well as public policies and, increasingly, lies at the heart of sophisticated machine learning systems. A paradigm used to approach potentially high-stakes decisions, optimization relies on abstracting the real world to a set of decision(s), objective(s) and constraint(s). Drawing from the modeling process and a range of actual cases, this paper describes the normative choices and assumptions that are necessarily part of using optimization. It then identifies six emergent problems that may be neglected: 1) Misspecified values can yield optimizations that omit certain imperatives altogether or incorporate them incorrectly as a constraint or as part of the objective, 2) Problematic decision boundaries can lead to faulty modularity assumptions and feedback loops, 3) Failing to account for multiple agents’ divergent goals and decisions can lead to policies that serve only certain narrow interests, 4) Mislabeling and mismeasurement can introduce bias and imprecision, 5) Faulty use of relaxation and approximation methods, unaccompanied by formal characterizations and guarantees, can severely impede applicability, and 6) Treating optimization as a justification for action, without specifying the necessary contextual information, can lead to ethically dubious or faulty decisions. Suggestions are given to further understand and curb the harms that can arise when optimization is used wrongfully.",10.1145/3593013.3593976,https://doi.org/10.1145/3593013.3593976,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Laufer, Benjamin; Gilbert, Thomas; Nissenbaum, Helen",2023,17,"@inproceedings{2-3567,
  title = {Optimization’s Neglected Normative Commitments},
  author = {Laufer, Benjamin and Gilbert, Thomas and Nissenbaum, Helen},
  year = {2023},
  doi = {10.1145/3593013.3593976},
  booktitle = {ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}",Theoretical contributions,Everyday / Employment / Public Service,Organizational,"Advising, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-3568,acm,The Effects of Warmth and Competence Perceptions on Users' Choice of an AI System,"People increasingly rely on Artificial Intelligence (AI) based systems to aid decision-making in various domains and often face a choice between alternative systems. We explored the effects of users' perception of AI systems' warmth (perceived intent) and competence (perceived ability) on their choices. In a series of studies, we manipulated AI systems' warmth and competence levels. We show that, similar to the judgments of other people, there is often primacy for warmth over competence. Specifically, when faced with a choice between a high-competence system and a high-warmth system, more participants preferred the high-warmth system. Moreover, the precedence of warmth persisted even when the high-warmth system was overtly deficient in its competence compared to an alternative high competence-low warmth system. The current research proposes that it may be vital for AI systems designers to consider and communicate the system's warmth characteristics to its potential users.",10.1145/3411764.3446863,https://doi.org/10.1145/3411764.3446863,CHI Conference on Human Factors in Computing Systems,"Gilad, Zohar; Amir, Ofra; Levontin, Liat",2021,82,"@inproceedings{2-3568,
  title = {The Effects of Warmth and Competence Perceptions on Users' Choice of an AI System},
  author = {Gilad, Zohar and Amir, Ofra and Levontin, Liat},
  year = {2021},
  doi = {10.1145/3411764.3446863},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy, Media / Communication / Entertainment",Individual,Advising,Decision-maker,"Alter decision outcomes, Change affective-perceptual",Shape AI for accountability,recommendations,NA,Textual,Yes,Yes
2-3569,acm,To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making,"People supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI's suggestion even when that suggestion is wrong. Adding explanations to the AI decisions does not appear to reduce the overreliance and some studies suggest that it might even increase it. Informed by the dual-process theory of cognition, we posit that people rarely engage analytically with each individual AI recommendation and explanation, and instead develop general heuristics about whether and when to follow the AI suggestions. Building on prior research on medical decision-making, we designed three cognitive forcing interventions to compel people to engage more thoughtfully with the AI-generated explanations. We conducted an experiment (N=199), in which we compared our three cognitive forcing designs to two simple explainable AI approaches and to a no-AI baseline. The results demonstrate that cognitive forcing significantly reduced overreliance compared to the simple explainable AI approaches. However, there was a trade-off: people assigned the least favorable subjective ratings to the designs that reduced the overreliance the most. To audit our work for intervention-generated inequalities, we investigated whether our interventions benefited equally people with different levels of Need for Cognition (i.e., motivation to engage in effortful mental activities). Our results show that, on average, cognitive forcing interventions benefited participants higher in Need for Cognition more. Our research suggests that human cognitive motivation moderates the effectiveness of explainable AI solutions.",10.1145/3449287,https://doi.org/10.1145/3449287,Proceedings of the ACM on Human-Computer Interaction,"Buçinca, Zana; Malaya, Maja Barbara; Gajos, Krzysztof Z.",2021,1032,"@article{2-3569,
  title={To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making},
  author={Bu{\c{c}}inca, Zana and Malaya, Maja Barbara and Gajos, Krzysztof Z.},
  year={2021},
  doi={10.1145/3449287},
  journal={Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising",Decision-maker,"Change trust, Alter decision outcomes, Change cognitive demands",no such info,simple explainable AI conditions,cognitive forcing interventions,Interactive interface,Yes,Yes
2-3570,acm,Improving Human-AI Collaboration With Descriptions of AI Behavior,"People work with AI systems to improve their decision making, but often under- or over-rely on AI predictions and perform worse than they would have unassisted. To help people appropriately rely on AI aids, we propose showing them behavior descriptions, details of how AI systems perform on subgroups of instances. We tested the efficacy of behavior descriptions through user studies with 225 participants in three distinct domains: fake review detection, satellite image classification, and bird classification. We found that behavior descriptions can increase human-AI accuracy through two mechanisms: helping people identify AI failures and increasing people's reliance on the AI when it is more accurate. These findings highlight the importance of people's mental models in human-AI collaboration and show that informing people of high-level AI behaviors can significantly improve AI-assisted decision making.",10.1145/3579612,https://doi.org/10.1145/3579612,Proceedings of the ACM on Human-Computer Interaction,"Cabrera, Ángel Alexander; Perer, Adam; Hong, Jason I.",2023,100,"@article{2-3570,
  title={Improving Human-AI Collaboration With Descriptions of AI Behavior},
  author={Cabrera, {\'A}ngel Alexander and Perer, Adam and Hong, Jason I.},
  year={2023},
  doi={10.1145/3579612},
  journal={Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Forecasting, Collaborating, Advising",Decision-maker,"Alter decision outcomes, Change trust",no such info,behavior descriptions,NA,Interactive interface,Yes,Yes
2-3573,acm,Understanding Contestability on the Margins: Implications for the Design of Algorithmic Decision-making in Public Services,"Policymakers have established that the ability to contest decisions made by or with algorithms is core to responsible artificial intelligence (AI). However, there has been a disconnect between research on contestability of algorithms, and what the situated practice of contestation looks like in contexts across the world, especially amongst communities on the margins. We address this gap through a qualitative study of follow-up and contestation in accessing public services for land ownership in rural India and affordable housing in the urban United States. We find there are significant barriers to exercising rights and contesting decisions, which intermediaries like NGO workers or lawyers work with communities to address. We draw on the notion of accompaniment in global health to highlight the open-ended work required to support people in navigating violent social systems. We discuss the implications of our findings for key aspects of contestability, including building capacity for contestation, human review, and the role of explanations. We also discuss how sociotechnical systems of algorithmic decision-making can embody accompaniment by taking on a higher burden of preventing denials and enabling contestation.",10.1145/3613904.3641898,https://doi.org/10.1145/3613904.3641898,CHI Conference on Human Factors in Computing Systems,"Karusala, Naveena; Upadhyay, Sohini; Veeraraghavan, Rajesh; Gajos, Krzysztof Z.",2024,0,"@inproceedings{2-3573,
  title = {Understanding Contestability on the Margins: Implications for the Design of Algorithmic Decision-making in Public Services},
  author = {Karusala, Naveena and Upadhyay, Sohini and Veeraraghavan, Rajesh and Gajos, Krzysztof Z.},
  year = {2024},
  doi = {10.1145/3613904.3641898},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,"Everyday / Employment / Public Service, Law / Policy / Governance",Institutional,Executing,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-3574,acm,AI Oversight and Human Mistakes: Evidence from Centre Court,"Powered by the increasing predictive capabilities of machine learning algorithms, artificial intelligence (AI) systems have acquired the ability to improve outcomes by overruling human mistakes. However, to assess the full impact of adding AI oversight, we must understand whether its presence alters human decision-making. While a large amount of research has focused on how humans respond when being assisted by AI, very little is known about how humans respond when their decisions might be overruled by AI. Because being overruled can carry psychological costs (e.g., shame and embarrassment of being overruled) and psychological benefits (e.g., relief at having their mistakes fixed), individuals might alter their decision-making under AI oversight. We provide the first field evidence that AI oversight carries psychological costs that can impact human decision-making.",10.1145/3670865.3673481,https://doi.org/10.1145/3670865.3673481,ACM Conference on Economics and Computation (EC),"Almog, David; Gauriot, Romain; Page, Lionel; Martin, Daniel",2024,17,"@inproceedings{2-3574,
  title={AI Oversight and Human Mistakes: Evidence from Centre Court},
  author={Almog, David and Gauriot, Romain and Page, Lionel and Martin, Daniel},
  year={2024},
  doi={10.1145/3670865.3673481},
  booktitle={ACM Conference on Economics and Computation (EC)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Media / Communication / Entertainment",Operational,"Auditing, Advising, Analyzing",Decision-maker,"Alter decision outcomes, Change cognitive demands, Change affective-perceptual",no such info,AI oversight,NA,"Visual, Autonomous System",Yes,Yes
2-3575,acm,Predicting Multi-dimensional Surgical Outcomes with Multi-modal Mobile Sensing: A Case Study with Patients Undergoing Lumbar Spine Surgery,"Pre-operative prediction of post-surgical recovery for patients is vital for clinical decision-making and personalized treatments, especially with lumbar spine surgery, where patients exhibit highly heterogeneous outcomes. Existing predictive tools mainly rely on traditional Patient-Reported Outcome Measures (PROMs), which fail to capture the long-term dynamics of patient conditions before the surgery. Moreover, existing studies focus on predicting a single surgical outcome. However, recovery from spine surgery is multi-dimensional, including multiple distinctive but interrelated outcomes, such as pain interference, physical function, and quality of recovery. In recent years, the emergence of smartphones and wearable devices has presented new opportunities to capture longitudinal and dynamic information regarding patients' conditions outside the hospital. This paper proposes a novel machine learning approach, Multi-Modal Multi-Task Learning (M3TL), using smartphones and wristbands to predict multiple surgical outcomes after lumbar spine surgeries. We formulate the prediction of pain interference, physical function, and quality of recovery as a multi-task learning (MTL) problem. We leverage multi-modal data to capture the static and dynamic characteristics of patients, including (1) traditional features from PROMs and Electronic Health Records (EHR), (2) Ecological Momentary Assessment (EMA) collected from smartphones, and (3) sensing data from wristbands. Moreover, we introduce new features derived from the correlation of EMA and wearable features measured within the same time frame, effectively enhancing predictive performance by capturing the interdependencies between the two data modalities. Our model interpretation uncovers the complementary nature of the different data modalities and their distinctive contributions toward multiple surgical outcomes. Furthermore, through individualized decision analysis, our model identifies personal high risk factors to aid clinical decision making and approach personalized treatments. In a clinical study involving 122 patients undergoing lumbar spine surgery, our M3TL model outperforms a diverse set of baseline methods in predictive performance, demonstrating the value of integrating multi-modal data and learning from multiple surgical outcomes. This work contributes to advancing personalized peri-operative care with accurate pre-operative predictions of multi-dimensional outcomes.",10.1145/3659628,https://doi.org/10.1145/3659628,"Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","Xu, Ziqi; Zhang, Jingwen; Greenberg, Jacob; Frumkin, Madelyn; Javeed, Saad; Zhang, Justin K.; Benedict, Braeden; Botterbush, Kathleen; Rodebaugh, Thomas L.; Ray, Wilson Z.; Lu, Chenyang",2024,8,"@article{2-3575,
  title = {Predicting Multi-dimensional Surgical Outcomes with Multi-modal Mobile Sensing: A Case Study with Patients Undergoing Lumbar Spine Surgery},
  author = {Xu, Ziqi and Zhang, Jingwen and Greenberg, Jacob and Frumkin, Madelyn and Javeed, Saad and Zhang, Justin K. and Benedict, Braeden and Botterbush, Kathleen and Rodebaugh, Thomas L. and Ray, Wilson Z. and Lu, Chenyang},
  year = {2024},
  doi = {10.1145/3659628},
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-subject, Decision-maker",Alter decision outcomes,Update AI competence,"model interpretation, conformal prediction sets",domain knowledge,Physical / Embodiment,Yes,Yes
2-35758,springernature,Humanistic interpretation and machine learning,"This paper investigates how unsupervised machine learning methods might make hermeneutic interpretive text analysis more objective in the social sciences. Through a close examination of the uses of topic modeling—a popular unsupervised approach in the social sciences—it argues that the primary way in which unsupervised learning supports interpretation is by allowing interpreters to discover unanticipated information in larger and more diverse corpora and by improving the transparency of the interpretive process. This view highlights that unsupervised modeling does not eliminate the researchers’ judgments from the process of producing evidence for social scientific theories. The paper shows this by distinguishing between two prevalent attitudes toward topic modeling, i. e. , topic realism and topic instrumentalism. Under neither can modeling provide social scientific evidence without the researchers’ interpretive engagement with the original text materials. Thus the unsupervised text analysis cannot improve the objectivity of interpretation by alleviating the problem of underdetermination in interpretive debate. The paper argues that the sense in which unsupervised methods can improve objectivity is by providing researchers with the resources to justify to others that their interpretations are correct. This kind of objectivity seeks to reduce suspicions in collective debate that interpretations are the products of arbitrary processes influenced by the researchers’ idiosyncratic decisions or starting points. The paper discusses this view in relation to alternative approaches to formalizing interpretation and identifies several limitations on what unsupervised learning can be expected to achieve in terms of supporting interpretive work.",10.1007/s11229-020-02806-w,http://dx.doi.org/10.1007/s11229-020-02806-w,Synthese,"Pääkkönen, Juho;Ylikoski, Petri",2021,55,"@article{2-35758,
  title={Humanistic interpretation and machine learning},
  author={Pääkkönen, Juho and Ylikoski, Petri},
  year={2021},
  doi={10.1007/s11229-020-02806-w},
  journal={Synthese}
}",Theoretical contributions,Education / Teaching / Research,no such info,"Explaining, Analyzing","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-3580,acm,Explanations Can Reduce Overreliance on AI Systems During Decision-Making,"Prior work has identified a resilient phenomenon that threatens the performance of human-AI decision-making teams: overreliance, when people agree with an AI, even when it is incorrect. Surprisingly, overreliance does not reduce when the AI produces explanations for its predictions, compared to only providing predictions. Some have argued that overreliance results from cognitive biases or uncalibrated trust, attributing overreliance to an inevitability of human cognition. By contrast, our paper argues that people strategically choose whether or not to engage with an AI explanation, demonstrating empirically that there are scenarios where AI explanations reduce overreliance. To achieve this, we formalize this strategic choice in a cost-benefit framework, where the costs and benefits of engaging with the task are weighed against the costs and benefits of relying on the AI. We manipulate the costs and benefits in a maze task, where participants collaborate with a simulated AI to find the exit of a maze. Through 5 studies (N = 731), we find that costs such as task difficulty (Study 1), explanation difficulty (Study 2, 3), and benefits such as monetary compensation (Study 4) affect overreliance. Finally, Study 5 adapts the Cognitive Effort Discounting paradigm to quantify the utility of different explanations, providing further support for our framework. Our results suggest that some of the null effects found in literature could be due in part to the explanation not sufficiently reducing the costs of verifying the AI's prediction.",10.1145/3579605,https://doi.org/10.1145/3579605,Proceedings of the ACM on Human-Computer Interaction,"Vasconcelos, Helena; Jörke, Matthew; Grunde-McLaughlin, Madeleine; Gerstenberg, Tobias; Bernstein, Michael S.; Krishna, Ranjay",2023,383,"@article{2-3580,
author = {Vasconcelos, Helena and J\""{o}rke, Matthew and Grunde-McLaughlin, Madeleine and Gerstenberg, Tobias and Bernstein, Michael S. and Krishna, Ranjay},
title = {Explanations Can Reduce Overreliance on AI Systems During Decision-Making},
year = {2023},
volume = {7},
number = {CSCW1},
doi = {10.1145/3579605},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {129},
numpages = {38}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Explaining, Advising, Forecasting",Decision-maker,"Change trust, Alter decision outcomes, Change cognitive demands",no such info,"prediction of alternative, highlight explanations, textual explanations",NA,Visual,Yes,Yes
2-3581,acm,Beyond Reasonable Doubt: Improving Fairness in Budget-Constrained Decision Making using Confidence Thresholds,"Prior work on fairness in machine learning has focused on settings where all the information needed about each individual is readily available. However, in many applications, further information may be acquired at a cost. For example, when assessing a customer's creditworthiness, a bank initially has access to a limited set of information but progressively improves the assessment by acquiring additional information before making a final decision. In such settings, we posit that a fair decision maker may want to ensure that decisions for all individuals are made with similar expected error rate, even if the features acquired for the individuals are different. We show that a set of carefully chosen confidence thresholds can not only effectively redistribute an information budget according to each individual's needs, but also serve to address individual and group fairness concerns simultaneously. Finally, using two public datasets, we confirm the effectiveness of our methods and investigate the limitations.",10.1145/3461702.3462575,https://doi.org/10.1145/3461702.3462575,"AAAI/ACM Conference on AI, Ethics, and Society","Bakker, Michiel A.; Tu, Duy Patrick; Gummadi, Krishna P.; Pentland, Alex Sandy; Varshney, Kush R.; Weller, Adrian",2021,16,"@inproceedings{2-3581,
  title={Beyond Reasonable Doubt: Improving Fairness in Budget-Constrained Decision Making using Confidence Thresholds},
  author={Bakker, Michiel A. and Tu, Duy Patrick and Gummadi, Krishna P. and Pentland, Alex Sandy and Varshney, Kush R. and Weller, Adrian},
  year={2021},
  doi={10.1145/3461702.3462575},
  booktitle={AAAI/ACM Conference on AI, Ethics, and Society}
}",Methodological contributions,Finance / Business / Economy,Institutional,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-35813,springernature,Autonomised harming,"This paper sketches elements of a theory of the ethics of autonomised harming: the phenomenon of delegating decisions about whether and whom to harm to artificial intelligence( AI) in self-driving cars and autonomous weapon systems. First, the paper elucidates the challenge of integrating non-human, artificial agents, which lack rights and duties, into our moral framework which relies on precisely these notions to determine the permissibility of harming. Second, the paper examines how potential differences between human agents and non-human, artificial agents might bear on the permissibility of delegating life-and death decisions to AI systems. Third, and finally, the paper explores a series of resulting complexities. These include the challenge of weighing autonomous systems’ promise to reduce harm against the intrinsic value of rectificatory justice as well as the peculiar possibility that delegating harmful acts to AI might render ordinarily impermissible acts permissible. By illuminating what happens when we extend normative theory beyond its traditional boundaries, this discussion offers a starting point for assessing the moral permissibility of delegating consequential decisions to non-human, artificial agents.",10.1007/s11098-023-01990-y,http://dx.doi.org/10.1007/s11098-023-01990-y,Philosophical Studies,"Eggert, Linda",2023,6,"@article{2-35813,
  title = {Autonomised harming},
  author = {Eggert, Linda},
  year = {2023},
  doi = {10.1007/s11098-023-01990-y},
  journal = {Philosophical Studies}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Institutional,Executing,"Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3582,acm,Organizational Governance of Emerging Technologies: AI Adoption in Healthcare,"Private and public sector structures and norms refine how emerging technology is used in practice. In healthcare, despite a proliferation of AI adoption, the organizational governance (i.e. institutional governance) surrounding its use and integration is often poorly understood. What the Health AI Partnership (HAIP) aims to do in this research is to better define the requirements for adequate organizational governance of AI systems in healthcare settings and support health system leaders to make more informed decisions around AI adoption. To work towards this understanding, we first identify how the standards for the AI adoption in healthcare may be designed to be used easily and efficiently. Then, we map out the precise decision points involved in the practical institutional adoption of AI technology within specific health systems. Practically, we achieve this through a multi-organizational collaboration with leaders from major health systems across the United States and key informants from related fields. Working with the consultancy IDEO.org, we were able to conduct usability-testing sessions with healthcare and AI ethics professionals. Usability analysis revealed a prototype structured around mock key decision points that align with how organizational leaders approach technology adoption. Concurrently, we conducted semi-structured interviews with 89 professionals in healthcare and other relevant fields. Using a modified grounded theory approach, we were able to identify 8 key decision points and comprehensive procedures throughout the AI adoption lifecycle. This is one of the most detailed qualitative analyses to date of the current governance structures and processes involved in AI adoption by health systems in the United States. We hope these findings can inform future efforts to build capabilities to promote the safe, effective, and responsible adoption of emerging technologies in healthcare.",10.1145/3593013.3594089,https://doi.org/10.1145/3593013.3594089,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Kim, Jee Young; Boag, William; Gulamali, Freya; Hasan, Alifia; Hogg, Henry David Jeffry; Lifson, Mark; Mulligan, Deirdre; Patel, Manesh; Raji, Inioluwa Deborah; Sehgal, Ajai; Shaw, Keo; Tobey, Danny; Valladares, Alexandra; Vidal, David; Balu, Suresh; Sendak, Mark",2023,49,"@inproceedings{2-3582,
  title = {Organizational Governance of Emerging Technologies: {AI} Adoption in Healthcare},
  author = {Kim, Jee Young and Boag, William and Gulamali, Freya and Hasan, Alifia and Hogg, Henry David Jeffry and Lifson, Mark and Mulligan, Deirdre and Patel, Manesh and Raji, Inioluwa Deborah and Sehgal, Ajai and Shaw, Keo and Tobey, Danny and Valladares, Alexandra and Vidal, David and Balu, Suresh and Sendak, Mark},
  year = {2023},
  doi = {10.1145/3593013.3594089},
  booktitle = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}",Empirical contributions,Healthcare / Medicine / Surgery,Organizational,"Analyzing, Advising","Decision-maker, Decision-subject, Knowledge provider","Change trust, Change affective-perceptual, Shape ethical norms, Restrict human agency, Shift responsibility","Update AI competence, Change AI responses, Shape AI for accountability",performance of the model,NA,NA,Yes,Yes
2-3583,acm,ReLiable: Offline Reinforcement Learning for Tactical Strategies in Professional Basketball Games,"Professional basketball provides an intriguing example of a dynamic spatio-temporal game that incorporates both hidden strategy policies and situational decision making. During a game, the coaches and players are assumed to follow a general game plan, but players are also forced to make spur-of-the-moment decisions based on immediate conditions on the court. However, because it is challenging to process heterogeneous signals on the court and the space of potential actions and outcomes is massive, it is hard for players to find an optimal strategy on the fly given a short amount of time to observe conditions and take action. In this work, we present ReLiable (ReinforcemEnt Learning In bAsketBaLl gamEs). Specifically, we investigate the possibility of using reinforcement learning (RL) to guide player decisions. We train an offline deep Q-network (DQN) on historical National Basketball Association (NBA) game data from 2015-2016. The data include play-by-play and player movement sensor data. We apply our trained agent to games that it has not seen. Our method is able to propose potentially smarter tactical strategies, compared with replay gameplay data, producing expected final game scores comparable to elite NBA teams. Our approach can be useful for learning strategy policies from other game-like domains characterized by competing groups and sequential spatio-temporal event data.",10.1145/3511808.3557105,https://doi.org/10.1145/3511808.3557105,ACM International Conference on Information and Knowledge Management (CIKM),"Chen, Xiusi; Jiang, Jyun-Yu; Jin, Kun; Zhou, Yichao; Liu, Mingyan; Brantingham, P. Jeffrey; Wang, Wei",2022,22,"@inproceedings{2-3583,
  title = {ReLiable: Offline Reinforcement Learning for Tactical Strategies in Professional Basketball Games},
  author = {Chen, Xiusi and Jiang, Jyun-Yu and Jin, Kun and Zhou, Yichao and Liu, Mingyan and Brantingham, P. Jeffrey and Wang, Wei},
  year = {2022},
  booktitle = {Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM)},
  doi = {10.1145/3511808.3557105}
}",Algorithmic contributions,Media / Communication / Entertainment,Operational,"Forecasting, Advising, Executing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3584,acm,Understanding User Reliance on AI in Assisted Decision-Making,"Proper calibration of human reliance on AI is fundamental to achieving complementary performance in AI-assisted human decision-making. Most previous works focused on assessing user reliance, and more broadly trust, retrospectively, through user perceptions and task-based measures. In this work, we explore the relationship between eye gaze and reliance under varying task difficulties and AI performance levels in a spatial reasoning task. Our results show a strong positive correlation between percent gaze duration on the AI suggestion and user AI task agreement, as well as user perceived reliance. Moreover, user agency is preserved particularly when the task is easy and when AI performance is low or inconsistent. Our results also reveal nuanced differences between reliance and trust. We discuss the potential of using eye gaze to gauge human reliance on AI in real-time, enabling adaptive AI assistance for optimal human-AI team performance.",10.1145/3555572,https://doi.org/10.1145/3555572,Proceedings of the ACM on Human-Computer Interaction,"Cao, Shiye; Huang, Chien-Ming",2022,72,"@article{2-3584,
author = {Cao, Shiye and Huang, Chien-Ming},
title = {Understanding User Reliance on AI in Assisted Decision-Making},
year = {2022},
volume = {6},
number = {CSCW2},
doi = {10.1145/3555572},
journal = {Proc. ACM Hum.-Comput. Interact.},
articleno = {471},
numpages = {23}
}",Empirical contributions,"Education / Teaching / Research, Generic / Abstract / Domain-agnostic",Individual,Advising,Decision-maker,"Change trust, Alter decision outcomes, Change affective-perceptual",no such info,AI suggestions,"user agency, preserving agency when tasks are easy or AI performance is weak",Interactive interface,Yes,Yes
2-35867,springernature,Comparison of the Accuracy of Pouch Replacement Timing Decisions Using Image Generation Artificial Intelligence and Machine Learning,"This study developed a system that determines when to remove the pouch from the stoma to detect faecal leakage in non-contact stoma holders. Around January 2020, new coronary outbreaks occurred worldwide, making it difficult for hospitals and care homes to collect data from many stoma holders. Collecting data from many stoma holders in hospitals and care centers has generally been challenging. Therefore, sufficient training and correct data were obtained using artificial intelligence( AI) image generation containing more images. These training data were then used to determine the appropriate tame to change the pouch. Finally, the accuracy of the decisions was compared using two learning algorithms, the Microsoft lobe machine learning and the Google teachable machine learning modelling tools. The results showed that the percentage of correct decisions for the two learning algorithms was 100%, from the first day to approximately three days after the faceplate was fitted, but tended to be lower, ranging from 40% to 87. 5%, from one to three days before the replacement date. The Google teachable machine learning modelling tool was also less accurate than the Microsoft lobe machine learning modelling tool.",10.1007/978-3-031-35572-1_8,http://dx.doi.org/10.1007/978-3-031-35572-1_8,Human-Computer Interaction,"Mizoguchi, Michiru;Watanabe, Shun;Nakahara, Masaya;Noborio, Hiroshi",2023,2,"@inproceedings{2-35867,
  title = {Comparison of the Accuracy of Pouch Replacement Timing Decisions Using Image Generation Artificial Intelligence and Machine Learning},
  author = {Mizoguchi, Michiru and Watanabe, Shun and Nakahara, Masaya and Noborio, Hiroshi},
  year = {2023},
  doi = {10.1007/978-3-031-35572-1_8},
  booktitle = {Human-Computer Interaction}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Executing",Decision-subject,NA,NA,NA,NA,NA,Yes,No
2-3588,acm,Mastering Stock Markets with Efficient Mixture of Diversified Trading Experts,"Quantitative stock investment is a fundamental financial task that highly relies on accurate prediction of market status and profitable investment decision making. Despite recent advances in deep learning (DL) have shown stellar performance on capturing trading opportunities in the stochastic stock market, the performance of existing DL methods is unstable with sensitivity to network initialization and hyperparameter selection. One major limitation of existing works is that investment decisions are made based on one individual neural network predictor with high uncertainty, which is inconsistent with the workflow in real-world trading firms. To tackle this limitation, we propose AlphaMix, a novel three-stage mixture-of-experts (MoE) framework for quantitative investment to mimic the efficient bottom-up hierarchical trading strategy design workflow of successful trading companies. In Stage one, we introduce an efficient ensemble learning method, whose computational and memory costs are significantly lower comparing to traditional ensemble methods, to train multiple groups of trading experts with personalised market understanding and trading styles. In Stage two, we collect diversified investment suggestions through building a pool of trading experts utilizing hyperparameter level and initialization level diversity of neural networks for post hoc ensemble construction. In Stage three, we design three different mechanisms, namely as-needed router, with-replacement selection and integrated expert soup, to dynamically pick experts from the expert pool, which takes the responsibility of a portfolio manager. Through extensive experiments on US and Chinese stock markets, we demonstrate that AlphaMix significantly outperforms many state-of-the-art baselines in terms of 7 popular financial criteria.",10.1145/3580305.3599424,https://doi.org/10.1145/3580305.3599424,ACM SIGKDD Conference on Knowledge Discovery and Data Mining,"Sun, Shuo; Wang, Xinrun; Xue, Wanqi; Lou, Xiaoxuan; An, Bo",2023,24,"@inproceedings{2-3588,
  title     = {Mastering Stock Markets with Efficient Mixture of Diversified Trading Experts},
  author    = {Sun, Shuo and Wang, Xinrun and Xue, Wanqi and Lou, Xiaoxuan and An, Bo},
  year      = {2023},
  doi       = {10.1145/3580305.3599424},
  booktitle = {Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining}
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Advising, Forecasting","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-3590,acm,ThyExp: An explainable AI-assisted Decision Making Toolkit for Thyroid Nodule Diagnosis based on Ultra-sound Images,"Radiologists have an important task of diagnosing thyroid nodules present in ultra sound images. Although reporting systems exist to aid in the diagnosis process, these systems do not provide explanations about the diagnosis results. We present ThyExp – a web based toolkit for it use by medical professionals, allowing for accurate diagnosis with explanations of thyroid nodules present in ultrasound images utilising artificial intelligence models. The proposed web-based toolkit can be easily incorporated into current medical workflows, and allows medical professionals to have the confidence of a highly accurate machine learning model with explanations to provide supplementary diagnosis data. The solution provides classification results with their probability accuracy, as well as the explanations in the form of presenting the key features or characteristics that contribute to the classification results. The experiments conducted on a real-world UK NHS hospital patient dataset demonstrate the effectiveness of the proposed approach. This toolkit can improve the trust of medical professional to understand the confidence of the model in its predictions. This toolkit can improve the trust of medical professionals in understanding the models reasoning behind its predictions.",10.1145/3583780.3615131,https://doi.org/10.1145/3583780.3615131,ACM International Conference on Information and Knowledge Management (CIKM),"Morris, Jamie; Liu, Zehao; Liang, Huizhi; Nagala, Sidhartha; Hong, Xia",2023,5,"@inproceedings{2-3590,
  title={ThyExp: An explainable AI-assisted Decision Making Toolkit for Thyroid Nodule Diagnosis based on Ultra-sound Images},
  author={Morris, Jamie and Liu, Zehao and Liang, Huizhi and Nagala, Sidhartha and Hong, Xia},
  year={2023},
  doi={10.1145/3583780.3615131},
  booktitle={Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM)}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3592,acm,Improving Workflow Integration with xPath: Design and Evaluation of a Human-AI Diagnosis System in Pathology,"Recent developments in AI have provided assisting tools to support pathologists’ diagnoses. However, it remains challenging to incorporate such tools into pathologists’ practice; one main concern is AI’s insufficient workflow integration with medical decisions. We observed pathologists’ examination and discovered that the main hindering factor to integrate AI is its incompatibility with pathologists’ workflow. To bridge the gap between pathologists and AI, we developed a human-AI collaborative diagnosis tool— xPath&nbsp;—that shares a similar examination process to that of pathologists, which can improve AI’s integration into their routine examination. The viability of xPath&nbsp;is confirmed by a technical evaluation and work sessions with 12 medical professionals in pathology. This work identifies and addresses the challenge of incorporating AI models into pathology, which can offer first-hand knowledge about how HCI researchers can work with medical professionals side-by-side to bring technological advances to medical tasks towards practical applications.",10.1145/3577011,https://doi.org/10.1145/3577011,ACM Transactions on Computer-Human Interaction,"Gu, Hongyan; Liang, Yuan; Xu, Yifan; Williams, Christopher Kazu; Magaki, Shino; Khanlou, Negar; Vinters, Harry; Chen, Zesheng; Ni, Shuo; Yang, Chunxu; Yan, Wenzhong; Zhang, Xinhai Robert; Li, Yang; Haeri, Mohammad; Chen, Xiang ‘Anthony’",2023,0,"@article{2-3592,
  title = {Improving Workflow Integration with xPath: Design and Evaluation of a Human-AI Diagnosis System in Pathology},
  author = {Gu, Hongyan and Liang, Yuan and Xu, Yifan and Williams, Christopher Kazu and Magaki, Shino and Khanlou, Negar and Vinters, Harry and Chen, Zesheng and Ni, Shuo and Yang, Chunxu and Yan, Wenzhong and Zhang, Xinhai Robert and Li, Yang and Haeri, Mohammad and Chen, Xiang 'Anthony'},
  year = {2023},
  doi = {10.1145/3577011},
  journal = {ACM Transactions on Computer-Human Interaction}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Collaborating","Decision-maker, Knowledge provider","Alter decision outcomes, Change cognitive demands, Change trust","Change AI responses, Update AI competence","global explanations, local explanations, AI findings, evidence",NA,Interactive interface,Yes,Yes
2-3593,acm,Human-AI Collaboration: The Effect of AI Delegation on Human Task Performance and Task Satisfaction,"Recent work has proposed artificial intelligence (AI) models that can learn to decide whether to make a prediction for an instance of a task or to delegate it to a human by considering both parties’ capabilities. In simulations with synthetically generated or context-independent human predictions, delegation can help improve the performance of human-AI teams—compared to humans or the AI model completing the task alone. However, so far, it remains unclear how humans perform and how they perceive the task when they are aware that an AI model delegated task instances to them. In an experimental study with 196 participants, we show that task performance and task satisfaction improve through AI delegation, regardless of whether humans are aware of the delegation. Additionally, we identify humans’ increased levels of self-efficacy as the underlying mechanism for these improvements in performance and satisfaction. Our findings provide initial evidence that allowing AI models to take over more management responsibilities can be an effective form of human-AI collaboration in workplaces.",10.1145/3581641.3584052,https://doi.org/10.1145/3581641.3584052,ACM International Conference on Intelligent User Interfaces (IUI),"Hemmer, Patrick; Westphal, Monika; Schemmer, Max; Vetter, Sebastian; Vössing, Michael; Satzger, Gerhard",2023,125,"@inproceedings{2-3593,
  title = {Human-AI Collaboration: The Effect of AI Delegation on Human Task Performance and Task Satisfaction},
  author = {Hemmer, Patrick and Westphal, Monika and Schemmer, Max and Vetter, Sebastian and Vössing, Michael and Satzger, Gerhard},
  year = {2023},
  doi = {10.1145/3581641.3584052},
  booktitle = {Proceedings of the ACM International Conference on Intelligent User Interfaces (IUI)}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Collaborating, Executing",Decision-maker,"Alter decision outcomes, Change affective-perceptual",no such info,"delegation, classification, human error model",self-efficacy,Visual,Yes,Yes
2-3594,acm,Understanding Frontline Workers’ and Unhoused Individuals’ Perspectives on AI Used in Homeless Services,"Recent years have seen growing adoption of AI-based decision-support systems (ADS) in homeless services, yet we know little about stakeholder desires and concerns surrounding their use. In this work, we aim to understand impacted stakeholders’ perspectives on a deployed ADS that prioritizes scarce housing resources. We employed AI lifecycle comicboarding, an adapted version of the comicboarding method, to elicit stakeholder feedback and design ideas across various components of an AI system’s design. We elicited feedback from county workers who operate the ADS daily, service providers whose work is directly impacted by the ADS, and unhoused individuals in the region. Our participants shared concerns and design suggestions around the AI system’s overall objective, specific model design choices, dataset selection, and use in deployment. Our findings demonstrate that stakeholders, even without AI knowledge, can provide specific and critical feedback on an AI system’s design and deployment, if empowered to do so.",10.1145/3544548.3580882,https://doi.org/10.1145/3544548.3580882,ACM CHI Conference on Human Factors in Computing Systems,"Kuo, Tzu-Sheng; Shen, Hong; Geum, Jisoo; Jones, Nev; Hong, Jason I.; Zhu, Haiyi; Holstein, Kenneth",2023,92,"@inproceedings{2-3594,
  title = {Understanding Frontline Workers’ and Unhoused Individuals’ Perspectives on AI Used in Homeless Services},
  author = {Kuo, Tzu-Sheng and Shen, Hong and Geum, Jisoo and Jones, Nev and Hong, Jason I. and Zhu, Haiyi and Holstein, Kenneth},
  year = {2023},
  doi = {10.1145/3544548.3580882},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Everyday / Employment / Public Service,Operational,"Advising, Analyzing","Stakeholder, Decision-maker","Change trust, Change cognitive demands, Change affective-perceptual, Restrict human agency, Shape ethical norms",Change AI responses,NA,NA,NA,Yes,Yes
2-3595,acm,Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons,"Recent years have witnessed the growing literature in empirical evaluation of explainable AI (XAI) methods. This study contributes to this ongoing conversation by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy — improve people’s understanding of the AI model, help people recognize the model uncertainty, and support people’s calibrated trust in the model. Through three randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of AI models of varying levels of complexity, and in two kinds of decision making contexts where people perceive themselves as having different levels of domain expertise. Our results demonstrate that many AI explanations do not satisfy any of the desirable properties when used on decision making tasks that people have little domain expertise in. On decision making tasks that people are more knowledgeable, the feature contribution explanation is shown to satisfy more desiderata of AI explanations, even when the AI model is inherently complex. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making, and for advancing more rigorous empirical evaluation of XAI methods.",10.1145/3519266,https://doi.org/10.1145/3519266,ACM Transactions on Interactive Intelligent Systems,"Wang, Xinru; Yin, Ming",2022,83,"@article{2-3595,
author = {Wang, Xinru and Yin, Ming},
title = {Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons},
year = {2022},
volume = {12},
number = {4},
doi = {10.1145/3519266},
journal = {ACM Trans. Interact. Intell. Syst.},
month = nov,
articleno = {27},
numpages = {36}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance",Individual,"Advising, Explaining","Decision-maker, Knowledge provider","Alter decision outcomes, Change trust, Change cognitive demands, Change affective-perceptual",Update AI competence,"feature contribution explanation, AI confidence, feature-importance explanations, nearest-neighbors explanations, counterfactual explanations",domain knowledge,Interactive interface,Yes,Yes
2-3596,acm,Human-AI Interaction in Human Resource Management: Understanding Why Employees Resist Algorithmic Evaluation at Workplaces and How to Mitigate Burdens,"Recently, Artificial Intelligence (AI) has been used to enable efficient decision-making in managerial and organizational contexts, ranging from employment to dismissal. However, to avoid employees’ antipathy toward AI, it is important to understand what aspects of AI employees like and/or dislike. In this paper, we aim to identify how employees perceive current human resource (HR) teams and future algorithmic management. Specifically, we explored what factors negatively influence employees’ perceptions of AI making work performance evaluations. Through in-depth interviews with 21 workers, we found that 1) employees feel six types of burdens (i.e., emotional, mental, bias, manipulation, privacy, and social) toward AI's introduction to human resource management (HRM), and that 2) these burdens could be mitigated by incorporating transparency, interpretability, and human intervention to algorithmic decision-making. Based on our findings, we present design efforts to alleviate employees’ burdens. To leverage AI for HRM in fair and trustworthy ways, we call for the HCI community to design human-AI collaboration systems with various HR stakeholders.",10.1145/3411764.3445304,https://doi.org/10.1145/3411764.3445304,CHI Conference on Human Factors in Computing Systems,"Park, Hyanghee; Ahn, Daehwan; Hosanagar, Kartik; Lee, Joonhwan",2021,175,"@inproceedings{2-3596,
  title = {Human-AI Interaction in Human Resource Management: Understanding Why Employees Resist Algorithmic Evaluation at Workplaces and How to Mitigate Burdens},
  author = {Park, Hyanghee and Ahn, Daehwan and Hosanagar, Kartik and Lee, Joonhwan},
  year = {2021},
  doi = {10.1145/3411764.3445304},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Everyday / Employment / Public Service,Organizational,"Monitoring, Advising","Decision-maker, Stakeholder, Decision-subject","Change cognitive demands, Restrict human agency, Shape ethical norms, Change trust, Change affective-perceptual",Shape AI for accountability,NA,"no need for AI’s empathy, prefer human-AI collaboration over human or machine-oriented evaluation, calling for transparency in AI systems regarding and the evaluation criteria",NA,Yes,Yes
2-3598,acm,Powered by AI: Examining How AI Descriptions Influence Perceptions of Fertility Tracking Applications,"Recently, there has been a proliferation of personal health applications describing to use Artificial Intelligence (AI) to assist health consumers in making health decisions based on their data and algorithmic outputs. However, it is still unclear how such descriptions influence individuals' perceptions of such apps and their recommendations. We therefore investigate how current AI descriptions influence individuals' attitudes towards algorithmic recommendations in fertility self-tracking through a simulated study using three versions of a fertility app. We found that participants preferred AI descriptions with explanation, which they perceived as more accurate and trustworthy. Nevertheless, they were unwilling to rely on these apps for high-stakes goals because of the potential consequences of a failure. We then discuss the importance of health goals for AI acceptance, how literacy and assumptions influence perceptions of AI descriptions and explanations, and the limitations of transparency in the context of algorithmic decision-making for personal health.",10.1145/3631414,https://doi.org/10.1145/3631414,"Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","Figueiredo, Mayara Costa; Ankrah, Elizabeth; Powell, Jacquelyn E.; Epstein, Daniel A.; Chen, Yunan",2024,15,"@article{2-3598,
  author    = {Figueiredo, Mayara Costa and Ankrah, Elizabeth and Powell, Jacquelyn E. and Epstein, Daniel A. and Chen, Yunan},
  title     = {Powered by AI: Examining How AI Descriptions Influence Perceptions of Fertility Tracking Applications},
  year      = {2024},
  doi       = {10.1145/3631414},
  journal   = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
}",Empirical contributions,Healthcare / Medicine / Surgery,Individual,"Advising, Explaining","Decision-maker, Decision-subject","Change trust, Alter decision outcomes, Change affective-perceptual, Restrict human agency",Change AI responses,"AI description, recommendations, explanations",NA,Interactive interface,Yes,Yes
2-3601,acm,Advancing Automation of Design Decisions in Recommender System Pipelines,"Recommender systems have become essential in domains like streaming services, social media platforms, and e-commerce websites. However, the development of a recommender system involves a complex pipeline with preprocessing, data splitting, algorithm and model selection, and postprocessing stages. Every stage of the recommender systems pipeline requires design decisions that influence the performance of the recommender system. To ease design decisions, automated machine learning (AutoML) techniques have been adapted to the field of recommender systems, resulting in various AutoRecSys libraries. Nevertheless, these libraries limit flexibility in integrating automation techniques. In response, our research aims to enhance the usability of AutoML techniques for design decisions in recommender system pipelines. We focus on developing flexible and library-independent automation techniques for algorithm selection, model selection, and postprocessing steps. By enabling developers to make informed choices and ease the recommender system development process, we decrease the developer’s effort while improving the performance of the recommender systems. Moreover, we want to analyze the cost-to-benefit ratio of automation techniques in recommender systems, evaluating the computational overhead and the resulting improvements in predictive performance. Our objective is to leverage AutoML concepts to automate design decisions in recommender system pipelines, reduce manual effort, and enhance the overall performance and usability of recommender systems.",10.1145/3604915.3608886,https://doi.org/10.1145/3604915.3608886,ACM Conference on Recommender Systems (RecSys),"Vente, Tobias",2023,5,"@inproceedings{2-3601,
  title = {Advancing Automation of Design Decisions in Recommender System Pipelines},
  author = {Vente, Tobias},
  year = {2023},
  doi = {10.1145/3604915.3608886},
  booktitle = {ACM Conference on Recommender Systems (RecSys)}
}",System/Artifact contributions,"Software / Systems / Security, Generic / Abstract / Domain-agnostic",no such info,"Advising, Executing",Developer,NA,NA,NA,NA,NA,Yes,No
2-3605,acm,Deliberating with AI: Improving Decision-Making for the Future through Participatory AI Design and Stakeholder Deliberation,"Research exploring how to support decision-making has often used machine learning to automate or assist human decisions. We take an alternative approach for improving decision-making, using machine learning to help stakeholders surface ways to improve and make fairer decision-making processes. We created ""Deliberating with AI"", a web tool that enables people to create and evaluate ML models in order to examine strengths and shortcomings of past decision-making and deliberate on how to improve future decisions. We apply this tool to a context of people selection, having stakeholders—decision makers (faculty) and decision subjects (students)—use the tool to improve graduate school admission decisions. Through our case study, we demonstrate how the stakeholders used the web tool to create ML models that they used as boundary objects to deliberate over organization decision-making practices. We share insights from our study to inform future research on stakeholder-centered participatory AI design and technology for organizational decision-making.",10.1145/3579601,https://doi.org/10.1145/3579601,Proceedings of the ACM on Human-Computer Interaction,"Zhang, Angie; Walker, Olympia; Nguyen, Kaci; Dai, Jiajun; Chen, Anqing; Lee, Min Kyung",2023,89,"@article{2-3605,
  title = {Deliberating with AI: Improving Decision-Making for the Future through Participatory AI Design and Stakeholder Deliberation},
  author = {Zhang, Angie and Walker, Olympia and Nguyen, Kaci and Dai, Jiajun and Chen, Anqing and Lee, Min Kyung},
  year = {2023},
  doi = {10.1145/3579601},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",System/Artifact contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research",Organizational,"Analyzing, Advising","Decision-subject, Decision-maker, Stakeholder","Alter decision outcomes, Change cognitive demands, Shape ethical norms","Shape AI for accountability, Update AI competence",NA,personalized settings,Interactive interface,Yes,Yes
2-3607,acm,Are We Asking the Right Questions?: Designing for Community Stakeholders’ Interactions with AI in Policing,"Research into recidivism risk prediction in the criminal justice system has garnered significant attention from HCI, critical algorithm studies, and the emerging field of human-AI decision-making. This study focuses on algorithmic crime mapping, a prevalent yet underexplored form of algorithmic decision support (ADS) in this context. We conducted experiments and follow-up interviews with 60 participants, including community members, technical experts, and law enforcement agents (LEAs), to explore how lived experiences, technical knowledge, and domain expertise shape interactions with the ADS, impacting human-AI decision-making. Surprisingly, we found that domain experts (LEAs) often exhibited anchoring bias, readily accepting and engaging with the first crime map presented to them. Conversely, community members and technical experts were more inclined to engage with the tool, adjust controls, and generate different maps. Our findings highlight that all three stakeholders were able to provide critical feedback regarding AI design and use - community members questioned the core motivation of the tool, technical experts drew attention to the elastic nature of data science practice, and LEAs suggested redesign pathways such that the tool could complement their domain expertise.",10.1145/3613904.3642738,https://doi.org/10.1145/3613904.3642738,CHI Conference on Human Factors in Computing Systems,"Haque, MD Romael; Saxena, Devansh; Weathington, Katy; Chudzik, Joseph; Guha, Shion",2024,0,"@inproceedings{2-3607,
  author    = {Haque, MD Romael and Saxena, Devansh and Weathington, Katy and Chudzik, Joseph and Guha, Shion},
  title     = {Are We Asking the Right Questions?: Designing for Community Stakeholders’ Interactions with AI in Policing},
  booktitle = {CHI Conference on Human Factors in Computing Systems},
  year      = {2024},
  doi       = {10.1145/3613904.3642738}
}",Empirical contributions,Law / Policy / Governance,Organizational,"Analyzing, Advising","Decision-maker, Stakeholder","Change cognitive demands, Alter decision outcomes, Change trust","Change AI responses, Shape AI for accountability, Update AI competence",NA,anchoring bias,Interactive interface,Yes,Yes
2-36070,springernature,An artificial intelligence decision support system for the management of type 1 diabetes,"Type 1 diabetes( T1D) is characterized by pancreatic beta cell dysfunction and insulin depletion. Over 40% of people with T1D manage their glucose through multiple injections of long-acting basal and short-acting bolus insulin, so-called multiple daily injections( MDI) 1, 2. Errors in dosing can lead to life-threatening hypoglycaemia events( <70 mg dl −1) and hyperglycaemia( >180 mg dl −1) , increasing the risk of retinopathy, neuropathy, and nephropathy. Machine learning( artificial intelligence) approaches are being harnessed to incorporate decision support into many medical specialties. Here, we report an algorithm that provides weekly insulin dosage recommendations to adults with T1D using MDI therapy. We employ a unique virtual platform 3 to generate over 50, 000 glucose observations to train a k -nearest neighbours 4 decision support system( KNN-DSS) to identify causes of hyperglycaemia or hypoglycaemia and determine necessary insulin adjustments from a set of 12 potential recommendations. The KNN-DSS algorithm achieves an overall agreement with board-certified endocrinologists of 67. 9% when validated on real-world human data, and delivers safe recommendations, per endocrinologist review. A comparison of inter-physician-recommended adjustments to insulin pump therapy indicates full agreement of 41. 2% among endocrinologists, which is consistent with previous measures of inter-physician agreement( 41–45%) 5. In silico 3, 6 benchmarking using a platform accepted by the United States Food and Drug Administration for evaluation of artificial pancreas technologies indicates substantial improvement in glycaemic outcomes after 12 weeks of KNN-DSS use. Our data indicate that the KNN-DSS allows for early identification of dangerous insulin regimens and may be used to improve glycaemic outcomes and prevent life-threatening complications in people with T1D. An automated decision support system is reported that provides weekly insulin dosage recommendations to users with T1D and that, when analysing previously collected patient data, exhibits high agreement with recommendations from physicians.",10.1038/s42255-020-0212-y,http://dx.doi.org/10.1038/s42255-020-0212-y,Nature Metabolism,"Tyler, Nichole S.;Mosquera-Lopez, Clara M.;Wilson, Leah M.;Dodier, Robert H.;Branigan, Deborah L.;Gabo, Virginia B.;Guillot, Florian H.;Hilts, Wade W.;El Youssef, Joseph;Castle, Jessica R.;Jacobs, Peter G.",2020,174,"@article{2-36070,
  title = {An artificial intelligence decision support system for the management of type 1 diabetes},
  author = {Tyler, Nichole S. and Mosquera-Lopez, Clara M. and Wilson, Leah M. and Dodier, Robert H. and Branigan, Deborah L. and Gabo, Virginia B. and Guillot, Florian H. and Hilts, Wade W. and El Youssef, Joseph and Castle, Jessica R. and Jacobs, Peter G.},
  year = {2020},
  doi = {10.1038/s42255-020-0212-y},
  journal = {Nature Metabolism}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Individual,"Advising, Analyzing","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-3608,acm,"Rethinking ""Risk"" in Algorithmic Systems Through A Computational Narrative Analysis of Casenotes in Child-Welfare","Risk assessment algorithms are being adopted by public sector agencies to make high-stakes decisions about human lives. Algorithms model “risk” based on individual client characteristics to identify clients most in need. However, this understanding of risk is primarily based on easily quantifiable risk factors that present an incomplete and biased perspective of clients. We conducted a computational narrative analysis of child-welfare casenotes and draw attention to deeper systemic risk factors that are hard to quantify but directly impact families and street-level decision-making. We found that beyond individual risk factors, the system itself poses a significant amount of risk where parents are over-surveilled by caseworkers and lack agency in decision-making processes. We also problematize the notion of risk as a static construct by highlighting the temporality and mediating effects of different risk, protective, systemic, and procedural factors. Finally, we draw caution against using casenotes in NLP-based systems by unpacking their limitations and biases embedded within them.",10.1145/3544548.3581308,https://doi.org/10.1145/3544548.3581308,ACM CHI Conference on Human Factors in Computing Systems,"Saxena, Devansh; Moon, Erina Seh-Young; Chaurasia, Aryan; Guan, Yixin; Guha, Shion",2023,43,"@inproceedings{2-3608,
  title = {Rethinking ""Risk"" in Algorithmic Systems Through A Computational Narrative Analysis of Casenotes in Child-Welfare},
  author = {Saxena, Devansh and Moon, Erina Seh-Young and Chaurasia, Aryan and Guan, Yixin and Guha, Shion},
  year = {2023},
  doi = {10.1145/3544548.3581308},
  booktitle = {Proceedings of the ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,"Everyday / Employment / Public Service, Law / Policy / Governance",Operational,Analyzing,"Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-3609,acm,Questioning the ability of feature-based explanations to empower non-experts in robo-advised financial decision-making,"Robo-advisors are democratizing access to life-insurance by enabling fully online underwriting. In Europe, financial legislation requires that the reasons for recommending a life insurance plan be explained according to the characteristics of the client, in order to empower the client to make a “fully informed decision”. In this study conducted in France, we seek to understand whether legal requirements for feature-based explanations actually help users in their decision-making. We conduct a qualitative study to characterize the explainability needs formulated by non-expert users and by regulators expert in customer protection. We then run a large-scale quantitative study using Robex, a simplified robo-advisor built using ecological interface design that delivers recommendations with explanations in different hybrid textual and visual formats: either “dialogic”—more textual—or “graphical”—more visual. We find that providing feature-based explanations does not improve appropriate reliance or understanding compared to not providing any explanation. In addition, dialogic explanations increase users’ trust in the recommendations of the robo-advisor, sometimes to the users’ detriment. This real-world scenario illustrates how XAI can address information asymmetry in complex areas such as finance. This work has implications for other critical, AI-based recommender systems, where the General Data Protection Regulation (GDPR) may require similar provisions for feature-based explanations.",10.1145/3593013.3594053,https://doi.org/10.1145/3593013.3594053,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Bertrand, Astrid; Eagan, James R.; Maxwell, Winston",2023,30,"@inproceedings{2-3609,
  title = {Questioning the ability of feature-based explanations to empower non-experts in robo-advised financial decision-making},
  author = {Bertrand, Astrid and Eagan, James R. and Maxwell, Winston},
  year = {2023},
  booktitle = {ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)},
  doi = {10.1145/3593013.3594053}
}",Empirical contributions,Finance / Business / Economy,Individual,"Explaining, Advising","Decision-maker, Guardian","Change trust, Alter decision outcomes",Shape AI for accountability,"feature-based explanations, dialogic explanations, recommendations, visual explanations",NA,Physical / Embodiment,Yes,Yes
2-36126,springernature,Scoring the Ethics of AI Robo-Advice: Why We Need Gateways and Ratings,"Unlike the many services already transformed by artificial intelligence( AI) , the financial advice sector remains committed to a human interface. That is surprising as an AI-powered financial advisor( a robo-advisor) can offer personalised financial advice at much lower cost than traditional human advice. This is particularly important for those who need but cannot afford or access traditional financial advice. Robo-advice is easily accessible, available on-demand, and pools all relevant information in finding and implementing an optimal financial plan. In a perfectly competitive market for financial advice, robo-advice should prevail. Unfortunately, this market is imperfect with asymmetric information causing generalised advice aversion with a disproportionate lack of trust in robo-advice. Initial distrust makes advice clients reluctant to use, or switch to, robo-advice. This paper investigates the ethical concerns specific to robo-advice underpinning this lack of trust. We propose a regulatory framework addressing these concerns to ensure robo-advice can be an ethical resource for good, resolving the increasing complexity of financial decision-making. Fit for purpose regulation augments initial trust in robo-advice and supports advice clients in discriminating between high-trust and low-trust robo-advisors. Aspiring robo-advisors need to clear four licensing gateways to qualify for an AI Robo-Advice License( AIRAL). Licensed robo-advisors should then be monitored for ethical compliance. Using a balanced score card for ethical performance generates an ethics rating. This gateways-and-ratings methodology builds trust in the robo-advisory market through improved transparency, reduced information asymmetry, and lower risk of adverse selection.",10.1007/s10551-024-05753-5,http://dx.doi.org/10.1007/s10551-024-05753-5,Journal of Business Ethics,"Kofman, Paul",2024,16,"@article{2-36126,
  title={Scoring the Ethics of AI Robo-Advice: Why We Need Gateways and Ratings},
  author={Kofman, Paul},
  year={2024},
  doi={10.1007/s10551-024-05753-5},
  journal={Journal of Business Ethics}
}",Theoretical contributions,Finance / Business / Economy,Individual,"Advising, Analyzing","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-3614,acm,Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making,"Several strands of research have aimed to bridge the gap between artificial intelligence (AI) and human decision-makers in AI-assisted decision-making, where humans are the consumers of AI model predictions and the ultimate decision-makers in high-stakes applications. However, people's perception and understanding are often distorted by their cognitive biases, such as confirmation bias, anchoring bias, availability bias, to name a few. In this work, we use knowledge from the field of cognitive science to account for cognitive biases in the human-AI collaborative decision-making setting, and mitigate their negative effects on collaborative performance. To this end, we mathematically model cognitive biases and provide a general framework through which researchers and practitioners can understand the interplay between cognitive biases and human-AI accuracy. We then focus specifically on anchoring bias, a bias commonly encountered in human-AI collaboration. We implement a time-based de-anchoring strategy and conduct our first user experiment that validates its effectiveness in human-AI collaborative decision-making. With this result, we design a time allocation strategy for a resource-constrained setting that achieves optimal human-AI collaboration under some assumptions. We, then, conduct a second user experiment which shows that our time allocation strategy with explanation can effectively de-anchor the human and improve collaborative performance when the AI model has low confidence and is incorrect.",10.1145/3512930,https://doi.org/10.1145/3512930,Proceedings of the ACM on Human-Computer Interaction,"Rastogi, Charvi; Zhang, Yunfeng; Wei, Dennis; Varshney, Kush R.; Dhurandhar, Amit; Tomsett, Richard",2022,269,"@article{2-3614,
  title={Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making},
  author={Rastogi, Charvi and Zhang, Yunfeng and Wei, Dennis and Varshney, Kush R. and Dhurandhar, Amit and Tomsett, Richard},
  year={2022},
  doi={10.1145/3512930},
  journal={Proceedings of the ACM on Human-Computer Interaction}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research",Operational,"Forecasting, Explaining, Advising",Decision-maker,"Alter decision outcomes, Change cognitive demands, Change trust",no such info,"AI confidence, explanations","cognitive biases, anchoring bias",Interactive interface,Yes,Yes
2-3615,acm,Multi-Objective Model-based Reinforcement Learning for Infectious Disease Control,"Severe infectious diseases such as the novel coronavirus (COVID-19) pose a huge threat to public health. Stringent control measures, such as school closures and stay-at-home orders, while having significant effects, also bring huge economic losses. In the face of an emerging infectious disease, a crucial question for policymakers is how to make the trade-off and implement the appropriate interventions timely given the huge uncertainty. In this work, we propose a Multi-Objective Model-based Reinforcement Learning framework to facilitate data-driven decision-making and minimize the overall long-term cost. Specifically, at each decision point, a Bayesian epidemiological model is first learned as the environment model, and then the proposed model-based multi-objective planning algorithm is applied to find a set of Pareto-optimal policies. This framework, combined with the prediction bands for each policy, provides a real-time decision support tool for policymakers. The application is demonstrated with the spread of COVID-19 in China.",10.1145/3447548.3467303,https://doi.org/10.1145/3447548.3467303,ACM SIGKDD Conference on Knowledge Discovery and Data Mining,"Wan, Runzhe; Zhang, Xinyu; Song, Rui",2021,0,"@inproceedings{2-3615,
  title={Multi-Objective Model-based Reinforcement Learning for Infectious Disease Control},
  author={Wan, Runzhe and Zhang, Xinyu and Song, Rui},
  year={2021},
  doi={10.1145/3447548.3467303},
  booktitle={Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining}
}",Algorithmic contributions,"Healthcare / Medicine / Surgery, Law / Policy / Governance",Organizational,"Advising, Forecasting","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-3616,acm,Advancing Patient-Centered Shared Decision-Making with AI Systems for Older Adult Cancer Patients,"Shared decision making (SDM) plays a vital role in clinical practice guidelines, fostering enduring therapeutic communication and patient-clinician relationships. Previous research indicates that active patient participation in decision-making improves satisfaction and treatment outcomes. However, medical decision-making can be intricate and multifaceted. To help make SDM more accessible, we designed a patient-centered Artificial Intelligence (AI) SDM system for older adult cancer patients who lack high health literacy to become more involved in the clinical decision-making process and to improve comprehension toward treatment outcomes. We conducted a pilot feasibility study through 12 preliminary interviews followed by 25 usability testing interviews after the system development, with older adult cancer survivors and clinicians. Results indicated promise in the AI system’s ability to enhance SDM, providing personalized healthcare experiences and education for cancer patients. Clinician responses also provided useful suggestions for SDM’s new design and research opportunities in mitigating medical errors and improving clinical efficiency.",10.1145/3613904.3642353,https://doi.org/10.1145/3613904.3642353,CHI Conference on Human Factors in Computing Systems,"Hao, Yuexing; Liu, Zeyu; Riter, Robert N.; Kalantari, Saleh",2024,37,"@inproceedings{2-3616,
  title = {Advancing Patient-Centered Shared Decision-Making with AI Systems for Older Adult Cancer Patients},
  author = {Hao, Yuexing and Liu, Zeyu and Riter, Robert N. and Kalantari, Saleh},
  year = {2024},
  doi = {10.1145/3613904.3642353},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Decision-subject","Change trust, Change cognitive demands, Change affective-perceptual","Update AI competence, Change AI responses",NA,"personalized settings, limited understanding reduces effective participation in decision-making, limited resources",Interactive interface,Yes,Yes
2-36160,springernature,Contender: Leveraging User Opinions for Purchase Decision-Making,"User opinions posted on e-commerce websites are a valuable source to support purchase making-decision. Unfortunately, it is not generally feasible for an ordinary buyer to examine a large set of reviews on a given product for useful information on certain attributes. We present a system named Contender that can summarize product reviews aligned to the attributes of these products. Contender is implemented as an Android app for smartphones.",10.1007/978-3-030-15719-7_30,http://dx.doi.org/10.1007/978-3-030-15719-7_30,European Conference on Information Retrieval,"Melo, Tiago;Silva, Altigran S.;Moura, Edleno S.;Calado, Pável",2019,1,"@inproceedings{2-36160,
  title = {Contender: Leveraging User Opinions for Purchase Decision-Making},
  author = {Melo, Tiago and Silva, Altigran S. and Moura, Edleno S. and Calado, Pável},
  year = {2019},
  doi = {10.1007/978-3-030-15719-7_30},
  booktitle = {European Conference on Information Retrieval}
}",System/Artifact contributions,Finance / Business / Economy,Individual,Advising,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-36168,springernature,An artificial intelligence platform for the multihospital collaborative management of congenital cataracts,"Using artificial intelligence( AI) to prevent and treat diseases is an ultimate goal in computational medicine. Although AI has been developed for screening and assisted decision-making in disease prevention and management, it has not yet been validated for systematic application in the clinic. In the context of rare diseases, the main strategy has been to build specialized care centres; however, these centres are scattered and their coverage is insufficient, which leaves a large proportion of rare-disease patients with inadequate care. Here, we show that an AI agent using deep learning, and involving convolutional neural networks for diagnostics, risk stratification and treatment suggestions, accurately diagnoses and provides treatment decisions for congenital cataracts in an in silico test, in a website-based study, in a ‘finding a needle in a haystack’ test and in a multihospital clinical trial. We also show that the AI agent and individual ophthalmologists perform equally well. Moreover, we have integrated the AI agent with a cloud-based platform for multihospital collaboration, designed to improve disease management for the benefit of patients with rare diseases. An artificial intelligence agent integrated with a cloud-based platform for multihospital collaboration performs equally as well as ophthalmologists in the diagnosis of congenital cataracts in a series of online tests and a multihospital clinical trial.",10.1038/s41551-016-0024,http://dx.doi.org/10.1038/s41551-016-0024,Nature Biomedical Engineering,"Long, Erping;Lin, Haotian;Liu, Zhenzhen;Wu, Xiaohang;Wang, Liming;Jiang, Jiewei;An, Yingying;Lin, Zhuoling;Li, Xiaoyan;Chen, Jingjing;Li, Jing;Cao, Qianzhong;Wang, Dongni;Liu, Xiyang;Chen, Weirong;Liu, Yizhi",2017,391,"@article{2-36168,
  title={An artificial intelligence platform for the multihospital collaborative management of congenital cataracts},
  author={Long, Erping and Lin, Haotian and Liu, Zhenzhen and Wu, Xiaohang and Wang, Liming and Jiang, Jiewei and An, Yingying and Lin, Zhuoling and Li, Xiaoyan and Chen, Jingjing and Li, Jing and Cao, Qianzhong and Wang, Dongni and Liu, Xiyang and Chen, Weirong and Liu, Yizhi},
  year={2017},
  doi={10.1038/s41551-016-0024},
  journal={Nature Biomedical Engineering}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Executing","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-3617,acm,Towards Engineering Fair and Equitable Software Systems for Managing Low-Altitude Airspace Authorizations,"Small Unmanned Aircraft Systems (sUAS) have gained widespread adoption across a diverse range of applications. This has introduced operational complexities within shared airspaces and an increase in reported incidents, raising safety concerns. In response, the U.S. Federal Aviation Administration (FAA) is developing a UAS Traffic Management (UTM) system to control access to airspace based on an sUAS's predicted ability to safely complete its mission. However, a fully automated system capable of swiftly approving or denying flight requests can be prone to bias and must consider safety, transparency, and fairness to diverse stakeholders. In this paper, we present an initial study that explores stakeholders' perspectives on factors that should be considered in an automated system. Results indicate flight characteristics and environmental conditions were perceived as most important but pilot and drone capabilities should also be considered. Further, several respondents indicated an aversion to any AI-supported automation, highlighting the need for full transparency in automated decision-making. Results provide a societal perspective on the challenges of automating UTM flight authorization decisions and help frame the ongoing design of a solution acceptable to the broader sUAS community.",10.1145/3639475.3640103,https://doi.org/10.1145/3639475.3640103,International Conference on Software Engineering: Software Engineering in Society Track,"Gohar, Usman; Hunter, Michael C.; Marczak-Czajka, Agnieszka; Lutz, Robyn R.; Cohen, Myra B.; Cleland-Huang, Jane",2024,3,"@inproceedings{2-3617,
  title={Towards Engineering Fair and Equitable Software Systems for Managing Low-Altitude Airspace Authorizations},
  author={Gohar, Usman and Hunter, Michael C. and Marczak-Czajka, Agnieszka and Lutz, Robyn R. and Cohen, Myra B. and Cleland-Huang, Jane},
  year={2024},
  booktitle={International Conference on Software Engineering: Software Engineering in Society Track},
  doi={10.1145/3639475.3640103}
}",Empirical contributions,Transportation / Mobility / Planning,Organizational,"Forecasting, Monitoring, Executing","Stakeholder, Decision-maker, Guardian",NA,NA,drone characteristics,pilot history,Autonomous System,Yes,No
2-3619,acm,Active Fairness in Algorithmic Decision Making,"Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Recent work has proposed optimal post-processing methods that randomize classification decisions for a fraction of individuals, in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concern due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail. The present work proposes an alternativeactive framework for fair classification, where, in deployment, a decision-maker adaptively acquires information according to the needs of different groups or individuals, towards balancing disparities in classification performance. We propose two such methods, where information collection is adapted to group- and individual-level needs respectively. We show on real-world datasets that these can achieve: 1) calibration and single error parity (e.g.,equal opportunity ); and 2) parity in both false positive and false negative rates (i.e.,equal odds ). Moreover, we show that by leveraging their additional degree of freedom,active approaches can substantially outperform randomization-based classifiers previously considered optimal, while avoiding limitations such as intra-group unfairness.",10.1145/3306618.3314277,https://doi.org/10.1145/3306618.3314277,"AAAI/ACM Conference on AI, Ethics, and Society","Noriega-Campero, Alejandro; Bakker, Michiel A.; Garcia-Bulle, Bernardo; Pentland, Alex 'Sandy'",2019,120,"@inproceedings{2-3619,
  title = {Active Fairness in Algorithmic Decision Making},
  author = {Noriega-Campero, Alejandro and Bakker, Michiel A. and Garcia-Bulle, Bernardo and Pentland, Alex 'Sandy'},
  year = {2019},
  doi = {10.1145/3306618.3314277},
  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy, Healthcare / Medicine / Surgery",Institutional,"Advising, Executing","Developer, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-3621,acm,Visualizing Ubiquitously Sensed Measures of Motor Ability in Multiple Sclerosis: Reflections on Communicating Machine Learning in Practice,"Sophisticated ubiquitous sensing systems are being used to measure motor ability in clinical settings. Intended to augment clinical decision-making, the interpretability of the machine-learning measurements underneath becomes critical to their use. We explore how visualization can support the interpretability of machine-learning measures through the case of Assess MS, a system to support the clinical assessment of Multiple Sclerosis. A substantial design challenge is to make visible the algorithm's decision-making process in a way that allows clinicians to integrate the algorithm's result into their own decision process. To this end, we present a series of design iterations that probe the challenges in supporting interpretability in a real-world system. The key contribution of this article is to illustrate that simply making visible the algorithmic decision-making process is not helpful in supporting clinicians in their own decision-making process. It disregards that people and algorithms make decisions in different ways. Instead, we propose that visualisation can provide context to algorithmic decision-making, rendering observable a range of internal workings of the algorithm from data quality issues to the web of relationships generated in the machine-learning process.",10.1145/3181670,https://doi.org/10.1145/3181670,ACM Transactions on Interactive Intelligent Systems,"Morrison, Cecily; Huckvale, Kit; Corish, Bob; Banks, Richard; Grayson, Martin; Dorn, Jonas; Sellen, Abigail; Lindley, Sân",2018,16,"@article{2-3621,
  title = {Visualizing Ubiquitously Sensed Measures of Motor Ability in Multiple Sclerosis: Reflections on Communicating Machine Learning in Practice},
  author = {Morrison, Cecily and Huckvale, Kit and Corish, Bob and Banks, Richard and Grayson, Martin and Dorn, Jonas and Sellen, Abigail and Lindley, Sân},
  year = {2018},
  journal = {ACM Transactions on Interactive Intelligent Systems},
  volume = {8},
  number = {2},
  pages = {9:1--9:28},
  doi = {10.1145/3181670}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Analyzing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-3622,acm,"Strategic Decisions: Survey, Taxonomy, and Future Directions from Artificial Intelligence Perspective","Strategic Decision-Making is always challenging because it is inherently uncertain, ambiguous, risky, and complex. By contrast to tactical and operational decisions, strategic decisions are decisive, pivotal, and often irreversible, which may result in long-term and significant consequences. A strategic decision-making process usually involves many aspects of inquiry, including sensory perception, deliberative thinking, inquiry-based analysis, meta-learning, and constant interaction with the external world. Many unknowns, unpredictabilities, and environmental constraints will shape every aspect of a strategic decision. Traditionally, this task often relies on intuition, reflective thinking, visionary insights, approximate estimates, and practical wisdom. With recent advances in artificial intelligence/machine learning (AI/ML) technologies, we can leverage AI/ML to support strategic decision-making. However, there is still a substantial gap from an AI perspective due to inadequate models, despite the tremendous progress made. We argue that creating a comprehensive taxonomy of decision frames as a representation space is essential for AI because it could offer surprising insights beyond anyone's imaginary boundary today. Strategic decision-making is the art of possibility. This study develops a systematic taxonomy of decision-making frames that consists of six bases, 18 categorical, and 54 elementary frames. We formulate the model using the inquiry method based on Bloom's taxonomy approach. We aim to lay out the computational foundation that is possible to capture a comprehensive landscape view of a strategic problem. Compared with many traditional models, this novel taxonomy covers irrational, non-rational and rational frames capable of dealing with certainty, uncertainty, complexity, ambiguity, chaos, and ignorance.",10.1145/3571807,https://doi.org/10.1145/3571807,ACM Computing Surveys,"Wu, Caesar; Zhang, Rui; Kotagiri, Ramamohanarao; Bouvry, Pascal",2023,33,"@article{2-3622,
  title = {Strategic Decisions: Survey, Taxonomy, and Future Directions from Artificial Intelligence Perspective},
  author = {Wu, Caesar and Zhang, Rui and Kotagiri, Ramamohanarao and Bouvry, Pascal},
  year = {2023},
  doi = {10.1145/3571807},
  journal = {ACM Computing Surveys}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,no such info,"Advising, Explaining, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-3623,acm,Street-Level Algorithms and AI in Bureaucratic Decision-Making: A Caseworker Perspective,"Studies of algorithmic decision-making in Computer-Supported Cooperative Work (CSCW) and related fields of research increasingly recognize an analogy between AI and bureaucracies. We elaborate this link with an empirical study of AI in the context of decision-making in a street-level bureaucracy: job placement. The study examines caseworkers' perspectives on the use of AI, and contributes to an understanding of bureaucratic decision-making, with implications for integrating AI in caseworker systems. We report findings from a participatory workshop on AI with 35 caseworkers from different types of public services, followed up by interviews with five caseworkers specializing in job placement. The paper contributes an understanding of caseworkers' collaboration around documentation as a key aspect of bureaucratic decision-making practices. The collaborative aspects of casework are important to show because they are subject to process descriptions making case documentation prone for an individually focused AI with consequences for the future of how casework develops as a practice. Examining the collaborative aspects of caseworkers' documentation practices in the context of AI and (potentially) automation, our data show that caseworkers perceive AI as valuable when it can support their work towards management, (strengthen their cause, if a case requires extra resources), and towards unemployed individuals (strengthen their cause in relation to the individual's case when deciding on, and assigning a specific job placement program). We end by discussing steps to support cooperative aspects in AI decision-support systems that are increasingly implemented into the bureaucratic context of public services.",10.1145/3449114,https://doi.org/10.1145/3449114,Proceedings of the ACM on Human-Computer Interaction,"Ammitzbøll Flügge, Asbjørn; Hildebrandt, Thomas; Møller, Naja Holten",2021,110,"@article{2-3623,
  title = {Street-Level Algorithms and AI in Bureaucratic Decision-Making: A Caseworker Perspective},
  author = {Ammitzbøll Flügge, Asbjørn and Hildebrandt, Thomas and Møller, Naja Holten},
  year = {2021},
  doi = {10.1145/3449114},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,Everyday / Employment / Public Service,Organizational,Advising,"Decision-maker, Decision-subject, Knowledge provider","Alter decision outcomes, Restrict human agency",no such info,advice on whether an internship is going well,collaborative AI supporting the continuous need for negotiating common ground as part of the day-to-day bureaucratic processes in job placement,NA,Yes,Yes
2-3625,acm,"Algorithmic targeting of social policies: fairness, accuracy, and distributed governance","Targeted social policies are the main strategy for poverty alleviation across the developing world. These include targeted cash transfers (CTs), as well as targeted subsidies in health, education, housing, energy, childcare, and others. Due to the scale, diversity, and widespread relevance of targeted social policies like CTs, the algorithmic rules that decide who is eligible to benefit from them—and who is not—are among the most important algorithms operating in the world today. Here we report on a year-long engagement towards improving social targeting systems in a couple of developing countries. We demonstrate that a shift towards the use of AI methods in poverty-based targeting can substantially increase accuracy, extending the coverage of the poor by nearly a million people in two countries, without increasing expenditure. However, we also show that, absent explicit parity constraints, both status quo and AI-based systems induce disparities across population subgroups. Moreover, based on qualitative interviews with local social institutions, we find a lack of consensus on normative standards for prioritization and fairness criteria. Hence, we close by proposing a decision-support platform for distributed governance, which enables a diversity of institutions to customize the use of AI-based insights into their targeting decisions.",10.1145/3351095.3375784,https://doi.org/10.1145/3351095.3375784,"ACM Conference on Fairness, Accountability, and Transparency (FAccT)","Noriega-Campero, Alejandro; Garcia-Bulle, Bernardo; Cantu, Luis Fernando; Bakker, Michiel A.; Tejerina, Luis; Pentland, Alex",2020,6,"@inproceedings{2-3625,
  title = {Algorithmic targeting of social policies: fairness, accuracy, and distributed governance},
  author = {Noriega-Campero, Alejandro and Garcia-Bulle, Bernardo and Cantu, Luis Fernando and Bakker, Michiel A. and Tejerina, Luis and Pentland, Alex},
  year = {2020},
  booktitle = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (FAccT)},
  doi = {10.1145/3351095.3375784}
}",Empirical contributions,Law / Policy / Governance,Institutional,Advising,"Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-3626,acm,Fair Machine Guidance to Enhance Fair Decision Making in Biased People,"Teaching unbiased decision-making is crucial for addressing biased decision-making in daily life. Although both raising awareness of personal biases and providing guidance on unbiased decision-making are essential, the latter topics remains under-researched. In this study, we developed and evaluated an AI system aimed at educating individuals on making unbiased decisions using fairness-aware machine learning. In a between-subjects experimental design, 99 participants who were prone to bias performed personal assessment tasks. They were divided into two groups: a) those who received AI guidance for fair decision-making before the task and b) those who received no such guidance but were informed of their biases. The results suggest that although several participants doubted the fairness of the AI system, fair machine guidance prompted them to reassess their views regarding fairness, reflect on their biases, and modify their decision-making criteria. Our findings provide insights into the design of AI systems for guiding fair decision-making in humans.",10.1145/3613904.3642627,https://doi.org/10.1145/3613904.3642627,CHI Conference on Human Factors in Computing Systems,"Yang, Mingzhe; Arai, Hiromi; Yamashita, Naomi; Baba, Yukino",2024,0,"@inproceedings{2-3626,
  author    = {Yang, Mingzhe and Arai, Hiromi and Yamashita, Naomi and Baba, Yukino},
  title     = {Fair Machine Guidance to Enhance Fair Decision Making in Biased People},
  year      = {2024},
  doi       = {10.1145/3613904.3642627},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Institutional,Advising,Decision-maker,"Shape ethical norms, Alter decision outcomes, Change trust, Change cognitive demands, Change affective-perceptual",no such info,AI guidance,"bias, reversed-bias",Interactive interface,Yes,Yes
2-3628,acm,Understanding Choice Independence and Error Types in Human-AI Collaboration,"The ability to make appropriate delegation decisions is an important prerequisite of effective human-AI collaboration. Recent work, however, has shown that people struggle to evaluate AI systems in the presence of forecasting errors, falling well short of relying on AI systems appropriately. We use a pre-registered crowdsourcing study (N = 611) to extend this literature by two underexplored crucial features of human AI decision-making: choice independence and error type. Subjects in our study repeatedly complete two prediction tasks and choose which predictions they want to delegate to an AI system. For one task, subjects receive a decision heuristic that allows them to make informed and relatively accurate predictions. The second task is substantially harder to solve, and subjects must come up with their own decision rule. We systematically vary the AI system’s performance such that it either provides the best possible prediction for both tasks or only for one of the two. Our results demonstrate that people systematically violate choice independence by taking the AI’s performance in an unrelated second task into account. Humans who delegate predictions to a superior AI in their own expertise domain significantly reduce appropriate reliance when the model makes systematic errors in a complementary expertise domain. In contrast, humans who delegate predictions to a superior AI in a complementary expertise domain significantly increase appropriate reliance when the model systematically errs in the human expertise domain. Furthermore, we show that humans differentiate between error types and that this effect is conditional on the considered expertise domain. This is the first empirical exploration of choice independence and error types in the context of human-AI collaboration. Our results have broad and important implications for the future design, deployment, and appropriate application of AI systems.",10.1145/3613904.3641946,https://doi.org/10.1145/3613904.3641946,CHI Conference on Human Factors in Computing Systems,"Erlei, Alexander; Sharma, Abhinav; Gadiraju, Ujwal",2024,21,"@inproceedings{2-3628,
  title = {Understanding Choice Independence and Error Types in Human-AI Collaboration},
  author = {Erlei, Alexander and Sharma, Abhinav and Gadiraju, Ujwal},
  year = {2024},
  doi = {10.1145/3613904.3641946},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Environment / Resources / Energy",Operational,"Forecasting, Advising","Decision-maker, Decision-subject","Change trust, Alter decision outcomes, Change affective-perceptual",no such info,the best possible prediction,NA,Interactive interface,Yes,Yes
2-3630,acm,Dynamic Optimization of the Level of Operational Effectiveness of a CSOC Under Adverse Conditions,"The analysts at a cybersecurity operations center (CSOC) analyze the alerts that are generated by intrusion detection systems (IDSs). Under normal operating conditions, sufficient numbers of analysts are available to analyze the alert workload. For the purpose of this article, this means that the cybersecurity analysts in each shift can fully investigate each and every alert that is generated by the IDSs in a reasonable amount of time and perform their normal tasks in a shift. Normal tasks include analysis time, time to attend training programs, report writing time, personal break time, and time to update the signatures on new patterns in alerts as detected by the IDS. There are several disruptive factors that occur randomly and can adversely impact the normal operating condition of a CSOC, such as (1) higher alert generation rates from a few IDSs, (2) new alert patterns that decrease the throughput of the alert analysis process, and (3) analyst absenteeism. The impact of the preceding factors is that the alerts wait for a long duration before being analyzed, which impacts the level of operational effectiveness (LOE) of the CSOC. To return the CSOC to normal operating conditions, the manager of a CSOC can take several actions, such as increasing the alert analysis time spent by analysts in a shift by canceling a training program, spending some of his own time to assist the analysts in alert investigation, and calling upon the on-call analyst workforce to boost the service rate of alerts. However, additional resources are limited in quantity over a 14-day work cycle, and the CSOC manager must determine when and how much action to take in the face of uncertainty, which arises from both the intensity and the random occurrences of the disruptive factors. The preceding decision by the CSOC manager is nontrivial and is often made in an ad hoc manner using prior experiences. This work develops a reinforcement learning (RL) model for optimizing the LOE throughout the entire 14-day work cycle of a CSOC in the face of uncertainties due to disruptive events. Results indicate that the RL model is able to assist the CSOC manager with a decision support tool to make better decisions than current practices in determining when and how much resource to allocate when the LOE of a CSOC deviates from the normal operating condition.",10.1145/3173457,https://doi.org/10.1145/3173457,ACM Transactions on Intelligent Systems and Technology,"Shah, Ankit; Ganesan, Rajesh; Jajodia, Sushil; Cam, Hasan",2018,10,"@article{2-3630,
  title={Dynamic Optimization of the Level of Operational Effectiveness of a CSOC Under Adverse Conditions},
  author={Shah, Ankit and Ganesan, Rajesh and Jajodia, Sushil and Cam, Hasan},
  year={2018},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={,},
  number={,},
  pages={},
  doi={10.1145/3173457}
}",Algorithmic contributions,Software / Systems / Security,Operational,"Advising, Executing","Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-3632,acm,Case study: predictive fairness to reduce misdemeanor recidivism through social service interventions,"The criminal justice system is currently ill-equipped to improve outcomes of individuals who cycle in and out of the system with a series of misdemeanor offenses. Often due to constraints of caseload and poor record linkage, prior interactions with an individual may not be considered when an individual comes back into the system, let alone in a proactive manner through the application of diversion programs. The Los Angeles City Attorney's Office recently created a new Recidivism Reduction and Drug Diversion unit (R2D2) tasked with reducing recidivism in this population. Here we describe a collaboration with this new unit as a case study for the incorporation of predictive equity into machine learning based decision making in a resource-constrained setting. The program seeks to improve outcomes by developing individually-tailored social service interventions (i.e., diversions, conditional plea agreements, stayed sentencing, or other favorable case disposition based on appropriate social service linkage rather than traditional sentencing methods) for individuals likely to experience subsequent interactions with the criminal justice system, a time and resource-intensive undertaking that necessitates an ability to focus resources on individuals most likely to be involved in a future case. Seeking to achieve both efficiency (through predictive accuracy) and equity (improving outcomes in traditionally under-served communities and working to mitigate existing disparities in criminal justice outcomes), we discuss the equity outcomes we seek to achieve, describe the corresponding choice of a metric for measuring predictive fairness in this context, and explore a set of options for balancing equity and efficiency when building and selecting machine learning models in an operational public policy setting.",10.1145/3351095.3372863,https://doi.org/10.1145/3351095.3372863,"ACM Conference on Fairness, Accountability, and Transparency (FAccT)","Rodolfa, Kit T.; Salomon, Erika; Haynes, Lauren; Mendieta, Iván Higuera; Larson, Jamie; Ghani, Rayid",2020,0,"@inproceedings{2-3632,
  title = {Case study: predictive fairness to reduce misdemeanor recidivism through social service interventions},
  author = {Rodolfa, Kit T. and Salomon, Erika and Haynes, Lauren and Mendieta, Iv{\'a}n Higuera and Larson, Jamie and Ghani, Rayid},
  year = {2020},
  doi = {10.1145/3351095.3372863},
  booktitle = {ACM Conference on Fairness, Accountability, and Transparency (FAccT)}
}",Empirical contributions,Law / Policy / Governance,Organizational,"Forecasting, Advising","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-3633,acm,BISTRO: Berkeley Integrated System for Transportation Optimization,"The current trend toward urbanization and adoption of flexible and innovative mobility technologies will have complex and difficult-to-predict effects on urban transportation systems. Comprehensive methodological frameworks that account for the increasingly uncertain future state of the urban mobility landscape do not yet exist. Furthermore, few approaches have enabled the massive ingestion of urban data in planning tools capable of offering the flexibility of scenario-based design.This article introduces Berkeley Integrated System for Transportation Optimization (BISTRO), a new open source transportation planning decision support system that uses an agent-based simulation and optimization approach to anticipate and develop adaptive plans for possible technological disruptions and growth scenarios. The new framework was evaluated in the context of a machine learning competition hosted within Uber Technologies, Inc., in which over 400 engineers and data scientists participated. For the purposes of this competition, a benchmark model, based on the city of Sioux Falls, South Dakota, was adapted to the BISTRO framework. An important finding of this study was that in spite of rigorous analysis and testing done prior to the competition, the two top-scoring teams discovered an unbounded region of the search space, rendering the solutions largely uninterpretable for the purposes of decision-support. On the other hand, a follow-on study aimed to fix the objective function. It served to demonstrate BISTRO’s utility as a human-in-the-loop cyberphysical system: one that uses scenario-based optimization algorithms as a feedback mechanism to assist urban planners with iteratively refining objective function and constraints specification on intervention strategies. The portfolio of transportation intervention strategy alternatives eventually chosen achieves high-level regional planning goals developed through participatory stakeholder engagement practices.",10.1145/3384344,https://doi.org/10.1145/3384344,ACM Transactions on Intelligent Systems and Technology,"Feygin, Sidney A.; Lazarus, Jessica R.; Forscher, Edward H.; Golfier-Vetterli, Valentine; Lee, Jonathan W.; Gupta, Abhishek; Waraich, Rashid A.; Sheppard, Colin J. R.; Bayen, Alexandre M.",2020,15,"@article{2-3633,
  title={BISTRO: Berkeley Integrated System for Transportation Optimization},
  author={Feygin, Sidney A. and Lazarus, Jessica R. and Forscher, Edward H. and Golfier-Vetterli, Valentine and Lee, Jonathan W. and Gupta, Abhishek and Waraich, Rashid A. and Sheppard, Colin J. R. and Bayen, Alexandre M.},
  year={2020},
  doi={10.1145/3384344},
  journal={ACM Transactions on Intelligent Systems and Technology}
}",System/Artifact contributions,Transportation / Mobility / Planning,Organizational,"Advising, Executing","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-3634,acm,Knowing About Knowing: An Illusion of Human Competence Can Hinder Appropriate Reliance on AI Systems,"The dazzling promises of AI systems to augment humans in various tasks hinge on whether humans can appropriately rely on them. Recent research has shown that appropriate reliance is the key to achieving complementary team performance in AI-assisted decision making. This paper addresses an under-explored problem of whether the Dunning-Kruger Effect (DKE) among people can hinder their appropriate reliance on AI systems. DKE is a metacognitive bias due to which less-competent individuals overestimate their own skill and performance. Through an empirical study (N = 249), we explored the impact of DKE on human reliance on an AI system, and whether such effects can be mitigated using a tutorial intervention that reveals the fallibility of AI advice, and exploiting logic units-based explanations to improve user understanding of AI advice. We found that participants who overestimate their performance tend to exhibit under-reliance on AI systems, which hinders optimal team performance. Logic units-based explanations did not help users in either improving the calibration of their competence or facilitating appropriate reliance. While the tutorial intervention was highly effective in helping users calibrate their self-assessment and facilitating appropriate reliance among participants with overestimated self-assessment, we found that it can potentially hurt the appropriate reliance of participants with underestimated self-assessment. Our work has broad implications on the design of methods to tackle user cognitive biases while facilitating appropriate reliance on AI systems. Our findings advance the current understanding of the role of self-assessment in shaping trust and reliance in human-AI decision making. This lays out promising future directions for relevant HCI research in this community.",10.1145/3544548.3581025,https://doi.org/10.1145/3544548.3581025,ACM CHI Conference on Human Factors in Computing Systems,"He, Gaole; Kuiper, Lucie; Gadiraju, Ujwal",2023,119,"@inproceedings{2-3634,
  title = {Knowing About Knowing: An Illusion of Human Competence Can Hinder Appropriate Reliance on {AI} Systems},
  author = {He, Gaole and Kuiper, Lucie and Gadiraju, Ujwal},
  year = {2023},
  doi = {10.1145/3544548.3581025},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research",Operational,"Advising, Explaining",Decision-maker,"Change affective-perceptual, Change trust, Alter decision outcomes",no such info,"tutorial intervention, logic units-based explanations",metacognitive bias,"Textual, Interactive interface",Yes,Yes
2-3635,acm,Efficient reinforcement learning for automating human decision-making in SoC design,"The exponential growth in PVT corners due to Moore's law scaling, and the increasing demand for consumer applications and longer battery life in mobile devices, has ushered in significant cost and power-related challenges for designing and productizing mobile chips within a predictable schedule. Two main reasons for this are the reliance on human decision-making to achieve the desired performance within the target area and power budget, and significant increases in complexity of the human decision-making space. The problem is that to-date human design experience has not been replaced by design automation tools, and tasks requiring experience of past designs are still being performed manually.In this paper we investigate how machine learning may be applied to develop tools that learn from experience just like human designers, thus automating tasks that still require human intervention. The potential advantage of the machine learning approach is the ability to scale with increasing complexity and therefore hold the design-time constant with same manpower.Reinforcement Learning (RL) is a machine learning technique that allows us to mimic a human designers' ability to learn from experience and automate human decision-making, without loss in quality of the design, while making the design time independent of the complexity. In this paper we show how manual design tasks can be abstracted as RL problems. Based on the experience with applying RL to one of these problems, we show that RL can automatically achieve results similar to human designs, but in a predictable schedule. However, a major drawback is that the RL solution can require a prohibitively large number of iterations for training. If efficient training techniques can be developed for RL, it holds great promise to automate tasks requiring human experience. In this paper we present a Bayesian Optimization technique for reducing the RL training time.",10.1145/3195970.3199855,https://doi.org/10.1145/3195970.3199855,Design Automation Conference (DAC),"Sadasivam, Shankar; Chen, Zhuo; Lee, Jinwon; Jain, Rajeev",2018,8,"@inproceedings{2-3635,
  title = {Efficient reinforcement learning for automating human decision-making in {SoC} design},
  author = {Sadasivam, Shankar and Chen, Zhuo and Lee, Jinwon and Jain, Rajeev},
  year = {2018},
  doi = {10.1145/3195970.3199855},
  booktitle = {Design Automation Conference (DAC)}
}",Algorithmic contributions,Manufacturing / Industry / Automation,Operational,"Executing, Advising","Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-3636,acm,Understanding the impact of explanations on advice-taking: a user study for AI-based clinical Decision Support Systems,"The field of eXplainable Artificial Intelligence (XAI) focuses on providing explanations for AI systems’ decisions. XAI applications to AI-based Clinical Decision Support Systems (DSS) should increase trust in the DSS by allowing clinicians to investigate the reasons behind its suggestions. In this paper, we present the results of a user study on the impact of advice from a clinical DSS on healthcare providers’ judgment in two different cases: the case where the clinical DSS explains its suggestion and the case it does not. We examined the weight of advice, the behavioral intention to use the system, and the perceptions with quantitative and qualitative measures. Our results indicate a more significant impact of advice when an explanation for the DSS decision is provided. Additionally, through the open-ended questions, we provide some insights on how to improve the explanations in the diagnosis forecasts for healthcare assistants, nurses, and doctors.",10.1145/3491102.3502104,https://doi.org/10.1145/3491102.3502104,ACM CHI Conference on Human Factors in Computing Systems,"Panigutti, Cecilia; Beretta, Andrea; Giannotti, Fosca; Pedreschi, Dino",2022,171,"@inproceedings{2-3636,
  title = {Understanding the Impact of Explanations on Advice-Taking: A User Study for AI-Based Clinical Decision Support Systems},
  author = {Panigutti, Cecilia and Beretta, Andrea and Giannotti, Fosca and Pedreschi, Dino},
  year = {2022},
  doi = {10.1145/3491102.3502104},
  booktitle = {Proceedings of the ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising","Decision-maker, Decision-subject","Change trust, Alter decision outcomes, Change cognitive demands, Change affective-perceptual",no such info,recommendations,NA,"Textual, Visual, Interactive interface",Yes,Yes
2-3637,acm,Who is the Expert? Reconciling Algorithm Aversion and Algorithm Appreciation in AI-Supported Decision Making,"The increased use of algorithms to support decision making raises questions about whether people prefer algorithmic or human input when making decisions. Two streams of research on algorithm aversion and algorithm appreciation have yielded contradicting results. Our work attempts to reconcile these contradictory findings by focusing on the framings of humans and algorithms as a mechanism. In three decision making experiments, we created an algorithm appreciation result (Experiment 1) as well as an algorithm aversion result (Experiment 2) by manipulating only the description of the human agent and the algorithmic agent, and we demonstrated how different choices of framings can lead to inconsistent outcomes in previous studies (Experiment 3). We also showed that these results were mediated by the agent's perceived competence, i.e., expert power. The results provide insights into the divergence of the algorithm aversion and algorithm appreciation literature. We hope to shift the attention from these two contradicting phenomena to how we can better design the framing of algorithms. We also call the attention of the community to the theory of power sources, as it is a systemic framework that can open up new possibilities for designing algorithmic decision support systems.",10.1145/3479864,https://doi.org/10.1145/3479864,Proceedings of the ACM on Human-Computer Interaction,"Hou, Yoyo Tsung-Yu; Jung, Malte F.",2021,31,"@article{2-3637,
  title = {Who is the Expert? Reconciling Algorithm Aversion and Algorithm Appreciation in AI-Supported Decision Making},
  author = {Hou, Yoyo Tsung-Yu and Jung, Malte F.},
  year = {2021},
  doi = {10.1145/3479864},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research",Individual,Advising,Decision-maker,"Restrict human agency, Alter decision outcomes, Change trust",no such info,"AI suggestions, expert power",expert power,Interactive interface,Yes,Yes
2-36379,springernature,"Artificial intelligence–enabled electrocardiograms for identification of patients with low ejection fraction: a pragmatic, randomized clinical trial","We have conducted a pragmatic clinical trial aimed to assess whether an electrocardiogram( ECG) -based, artificial intelligence( AI) -powered clinical decision support tool enables early diagnosis of low ejection fraction( EF) , a condition that is underdiagnosed but treatable. In this trial( NCT04000087) , 120 primary care teams from 45 clinics or hospitals were cluster-randomized to either the intervention arm( access to AI results; 181 clinicians) or the control arm( usual care; 177 clinicians). ECGs were obtained as part of routine care from a total of 22, 641 adults( N = 11, 573 intervention; N = 11, 068 control) without prior heart failure. The primary outcome was a new diagnosis of low EF( ≤50%) within 90 days of the ECG. The trial met the prespecified primary endpoint, demonstrating that the intervention increased the diagnosis of low EF in the overall cohort( 1. 6% in the control arm versus 2. 1% in the intervention arm, odds ratio( OR) 1. 32( 1. 01–1. 61) , P = 0. 007) and among those who were identified as having a high likelihood of low EF( that is, positive AI-ECG, 6% of the overall cohort) ( 14. 5% in the control arm versus 19. 5% in the intervention arm, OR 1. 43( 1. 08–1. 91) , P = 0. 01). In the overall cohort, echocardiogram utilization was similar between the two arms( 18. 2% control versus 19. 2% intervention, P = 0. 17) ; for patients with positive AI-ECGs, more echocardiograms were obtained in the intervention compared to the control arm( 38. 1% control versus 49. 6% intervention, P < 0. 001). These results indicate that use of an AI algorithm based on ECGs can enable the early diagnosis of low EF in patients in the setting of routine primary care. In a pragmatic, cluster-randomized clinical trial, use of an AI algorithm for interpretation of electrocardiograms in primary care practices increased the frequency at which impaired heart function was diagnosed.",10.1038/s41591-021-01335-4,http://dx.doi.org/10.1038/s41591-021-01335-4,Nature Medicine,"Yao, Xiaoxi;Rushlow, David R.;Inselman, Jonathan W.;McCoy, Rozalina G.;Thacher, Thomas D.;Behnken, Emma M.;Bernard, Matthew E.;Rosas, Steven L.;Akfaly, Abdulla;Misra, Artika;Molling, Paul E.;Krien, Joseph S.;Foss, Randy M.;Barry, Barbara A.;Siontis, Konstantinos C.;Kapa, Suraj;Pellikka, Patricia A.;Lopez-Jimenez, Francisco;Attia, Zachi I.;Shah, Nilay D.;Friedman, Paul A.;Noseworthy, Peter A.",2021,121,"@article{2-36379,
  title = {Artificial intelligence–enabled electrocardiograms for identification of patients with low ejection fraction: a pragmatic, randomized clinical trial},
  author = {Yao, Xiaoxi and Rushlow, David R. and Inselman, Jonathan W. and McCoy, Rozalina G. and Thacher, Thomas D. and Behnken, Emma M. and Bernard, Matthew E. and Rosas, Steven L. and Akfaly, Abdulla and Misra, Artika and Molling, Paul E. and Krien, Joseph S. and Foss, Randy M. and Barry, Barbara A. and Siontis, Konstantinos C. and Kapa, Suraj and Pellikka, Patricia A. and Lopez-Jimenez, Francisco and Attia, Zachi I. and Shah, Nilay D. and Friedman, Paul A. and Noseworthy, Peter A.},
  year = {2021},
  doi = {10.1038/s41591-021-01335-4},
  journal = {Nature Medicine}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing, Forecasting","Decision-maker, Decision-subject",Alter decision outcomes,no such info,recommendations,NA,Textual,Yes,Yes
2-3640,acm,Improving Explainable Object-induced Model through Uncertainty for Automated Vehicles,"The rapid evolution of automated vehicles (AVs) has the potential to provide safer, more efficient, and comfortable travel options. However, these systems face challenges regarding reliability in complex driving scenarios. Recent explainable AV architectures neglect crucial information related to inherent uncertainties while providing explanations for actions. To overcome such challenges, our study builds upon the ""object-induced"" model approach that prioritizes the role of objects in scenes for decision-making and integrates uncertainty assessment into the decision-making process using an evidential deep learning paradigm with a Beta prior. Additionally, we explore several advanced training strategies guided by uncertainty, including uncertainty-guided data reweighting and augmentation. Leveraging the BDD-OIA dataset, our findings underscore that the model, through these enhancements, not only offers a clearer comprehension of AV decisions and their underlying reasoning but also surpasses existing baselines across a broad range of scenarios.",10.1145/3610977.3634973,https://doi.org/10.1145/3610977.3634973,ACM/IEEE International Conference on Human-Robot Interaction,"Ling, Shihong; Wan, Yue; Jia, Xiaowei; Du, Na",2024,5,"@inproceedings{2-3640,
  title = {Improving Explainable Object-induced Model through Uncertainty for Automated Vehicles},
  author = {Ling, Shihong and Wan, Yue and Jia, Xiaowei and Du, Na},
  year = {2024},
  booktitle = {ACM/IEEE International Conference on Human-Robot Interaction},
  doi = {10.1145/3610977.3634973}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Explaining, Forecasting, Executing",Developer,NA,NA,NA,NA,NA,Yes,No
2-3641,acm,An Actor-critic Reinforcement Learning Model for Optimal Bidding in Online Display Advertising,"The real-time bidding (RTB) paradigm allows the advertisers to submit a bid for each impression in online display advertising. A usual demand of the advertisers is to maximize the total value of winning impressions under constraints on some key performance indicators. Unfortunately, the existing RTB research in industrial applications can hardly achieve the optimum due to the stochastic decision scenarios and complex consumer behaviors. In this study, we address the application of RTB to mobile gaming where the in-app purchase action is of high uncertainty, making it challenging to evaluate individual impression opportunities. We first formulate the bidding process into a constrained optimization problem and then propose an actor-critic reinforcement learning (ACRL) model for obtaining the optimal policy under a dynamic decision environment. To avoid feeding too many samples with zero labels to the model, we provide a new way to quantify impression opportunities by integrating the in-app actions, such as conversion and purchase, and the characteristics of the candidate ad inventories. Moreover, the proposed ACRL learns a Gaussian distribution to simulate the audience's decision in a more real bidding scenario by taking additional contextual side information about both media and the audience. We also introduce how to deploy the learned model online to help adjust the final bid. At last, we conduct comprehensive offline experiments to demonstrate the effectiveness of ACRL and carefully set an online A/B testing experiment. The online experimental results verify the efficacy of the proposed ACRL in terms of multiple critical commercial indicators. ACRL has been deployed in the Tencent online display advertising platform and impacts billions of traffic every day. We believe proposed modifications for optimal bidding problems in RTB are practically innovative and can inspire the relative works in this field.",10.1145/3511808.3557064,https://doi.org/10.1145/3511808.3557064,ACM International Conference on Information and Knowledge Management (CIKM),"Yuan, Congde; Guo, Mengzhuo; Xiang, Chaoneng; Wang, Shuangyang; Song, Guoqing; Zhang, Qingpeng",2022,8,"@inproceedings{2-3641,
  title={An Actor-critic Reinforcement Learning Model for Optimal Bidding in Online Display Advertising},
  author={Yuan, Congde and Guo, Mengzhuo and Xiang, Chaoneng and Wang, Shuangyang and Song, Guoqing and Zhang, Qingpeng},
  year={2022},
  booktitle={Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM)},
  doi={10.1145/3511808.3557064}
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Advising, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-3642,acm,The Principles and Limits of Algorithm-in-the-Loop Decision Making,"The rise of machine learning has fundamentally altered decision making: rather than being made solely by people, many important decisions are now made through an ""algorithm-in-the-loop” process where machine learning models inform people. Yet insufficient research has considered how the interactions between people and models actually influence human decisions. Society lacks both clear normative principles regarding how people should collaborate with algorithms as well as robust empirical evidence about how people do collaborate with algorithms. Given research suggesting that people struggle to interpret machine learning models and to incorporate them into their decisions—sometimes leading these models to produce unexpected outcomes—it is essential to consider how different ways of presenting models and structuring human-algorithm interactions affect the quality and type of decisions made. This paper contributes to such research in two ways. First, we posited three principles as essential to ethical and responsible algorithm-in-the-loop decision making. Second, through a controlled experimental study on Amazon Mechanical Turk, we evaluated whether people satisfy these principles when making predictions with the aid of a risk assessment. We studied human predictions in two contexts (pretrial release and financial lending) and under several conditions for risk assessment presentation and structure. Although these conditions did influence participant behaviors and in some cases improved performance, only one desideratum was consistently satisfied. Under all conditions, our study participants 1) were unable to effectively evaluate the accuracy of their own or the risk assessment's predictions, 2) did not calibrate their reliance on the risk assessment based on the risk assessment's performance, and 3) exhibited bias in their interactions with the risk assessment. These results highlight the urgent need to expand our analyses of algorithmic decision making aids beyond evaluating the models themselves to investigating the full sociotechnical contexts in which people and algorithms interact.",10.1145/3359152,https://doi.org/10.1145/3359152,Proceedings of the ACM on Human-Computer Interaction,"Green, Ben; Chen, Yiling",2019,511,"@article{2-3642,
  title={The Principles and Limits of Algorithm-in-the-Loop Decision Making},
  author={Green, Ben and Chen, Yiling},
  year={2019},
  doi={10.1145/3359152},
  journal={Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance",Operational,"Forecasting, Advising","Decision-maker, Guardian","Change trust, Shape ethical norms, Alter decision outcomes",no such info,"risk assessment’s prediction, explanations, immediate feedback",simple feedback,Interactive interface,Yes,Yes
2-36433,springernature,A Framework for Assessing Joint Human-AI Systems Based on Uncertainty Estimation,"We investigate the role of uncertainty quantification in aiding medical decision-making. Existing evaluation metrics fail to capture the practical utility of joint human-AI decision-making systems. To address this, we introduce a novel framework to assess such systems and use it to benchmark a diverse set of confidence and uncertainty estimation methods. Our results show that certainty measures enable joint human-AI systems to outperform both standalone humans and AIs, and that for a given system there exists an optimal balance in the number of cases to refer to humans, beyond which the system’s performance degrades.",10.1007/978-3-031-72117-5_1,http://dx.doi.org/10.1007/978-3-031-72117-5_1,Medical Image Computing and Computer-Assisted Intervention Conference,"Konuk, Emir;Welch, Robert;Christiansen, Filip;Epstein, Elisabeth;Smith, Kevin",2024,1,"@inproceedings{2-36433,
  author    = {Konuk, Emir and Welch, Robert and Christiansen, Filip and Epstein, Elisabeth and Smith, Kevin},
  title     = {A Framework for Assessing Joint Human-AI Systems Based on Uncertainty Estimation},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention Conference},
  year      = {2024},
  doi       = {10.1007/978-3-031-72117-5_1}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-36435,springernature,A reinforcement learning model for AI-based decision support in skin cancer,"We investigated whether human preferences hold the potential to improve diagnostic artificial intelligence( AI) -based decision support using skin cancer diagnosis as a use case. We utilized nonuniform rewards and penalties based on expert-generated tables, balancing the benefits and harms of various diagnostic errors, which were applied using reinforcement learning. Compared with supervised learning, the reinforcement learning model improved the sensitivity for melanoma from 61. 4% to 79. 5%( 95% confidence interval( CI) : 73. 5–85. 6%) and for basal cell carcinoma from 79. 4% to 87. 1%( 95% CI: 80. 3–93. 9%). AI overconfidence was also reduced while simultaneously maintaining accuracy. Reinforcement learning increased the rate of correct diagnoses made by dermatologists by 12. 0%( 95% CI: 8. 8–15. 1%) and improved the rate of optimal management decisions from 57. 4% to 65. 3%( 95% CI: 61. 7–68. 9%). We further demonstrated that the reward-adjusted reinforcement learning model and a threshold-based model outperformed naïve supervised learning in various clinical scenarios. Our findings suggest the potential for incorporating human preferences into image-based diagnostic algorithms. A reinforcement learning model developed to adapt artificial intelligence( AI) predictions to human preferences showed better sensitivity for skin cancer diagnoses and improved management decisions compared to a supervised learning model.",10.1038/s41591-023-02475-5,http://dx.doi.org/10.1038/s41591-023-02475-5,Nature Medicine,"Barata, Catarina;Rotemberg, Veronica;Codella, Noel C. F.;Tschandl, Philipp;Rinner, Christoph;Akay, Bengu Nisa;Apalla, Zoe;Argenziano, Giuseppe;Halpern, Allan;Lallas, Aimilios;Longo, Caterina;Malvehy, Josep;Puig, Susana;Rosendahl, Cliff;Soyer, H. Peter;Zalaudek, Iris;Kittler, Harald",2023,100,"@article{2-36435,
  title = {A reinforcement learning model for AI-based decision support in skin cancer},
  author = {Barata, Catarina and Rotemberg, Veronica and Codella, Noel C. F. and Tschandl, Philipp and Rinner, Christoph and Akay, Bengu Nisa and Apalla, Zoe and Argenziano, Giuseppe and Halpern, Allan and Lallas, Aimilios and Longo, Caterina and Malvehy, Josep and Puig, Susana and Rosendahl, Cliff and Soyer, H. Peter and Zalaudek, Iris and Kittler, Harald},
  year = {2023},
  doi = {10.1038/s41591-023-02475-5},
  journal = {Nature Medicine}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Executing","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-36446,springernature,A machine learning framework supporting prospective clinical decisions applied to risk prediction in oncology,"We present a general framework for developing a machine learning( ML) tool that supports clinician assessment of patient risk using electronic health record-derived real-world data and apply the framework to a quality improvement use case in an oncology setting to identify patients at risk for a near-term( 60 day) emergency department( ED) visit who could potentially be eligible for a home-based acute care program. Framework steps include defining clinical quality improvement goals, model development and validation, bias assessment, retrospective and prospective validation, and deployment in clinical workflow. In the retrospective analysis for the use case, 8% of patient encounters were associated with a high risk( pre-defined as predicted probability ≥20%) for a near-term ED visit by the patient. Positive predictive value( PPV) and negative predictive value( NPV) for future ED events was 26% and 91%, respectively. Odds ratio( OR) of ED visit( highvs. low-risk) was 3. 5( 95% CI: 3. 4–3. 5). The model appeared to be calibrated across racial, gender, and ethnic groups. In the prospective analysis, 10% of patients were classified as high risk, 76% of whom were confirmed by clinicians as eligible for home-based acute care. PPV and NPV for future ED events was 22% and 95%, respectively. OR of ED visit( highvs. low-risk) was 5. 4( 95% CI: 2. 6–11. 0). The proposed framework for an ML-based tool that supports clinician assessment of patient risk is a stepwise development approach; we successfully applied the framework to an ED visit risk prediction use case.",10.1038/s41746-022-00660-3,http://dx.doi.org/10.1038/s41746-022-00660-3,Nature Partner Journals Digital Medicine,"Coombs, Lorinda;Orlando, Abigail;Wang, Xiaoliang;Shaw, Pooja;Rich, Alexander S.;Lakhtakia, Shreyas;Titchener, Karen;Adamson, Blythe;Miksad, Rebecca A.;Mooney, Kathi",2022,1,"@article{2-36446,
  title={A machine learning framework supporting prospective clinical decisions applied to risk prediction in oncology},
  author={Coombs, Lorinda and Orlando, Abigail and Wang, Xiaoliang and Shaw, Pooja and Rich, Alexander S. and Lakhtakia, Shreyas and Titchener, Karen and Adamson, Blythe and Miksad, Rebecca A. and Mooney, Kathi},
  year={2022},
  doi={10.1038/s41746-022-00660-3},
  journal={Nature Partner Journals Digital Medicine}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3646,acm,Algorithmic Hiring in Practice: Recruiter and HR Professional's Perspectives on AI Use in Hiring,"The use of AI-enabled hiring software raises questions about the practice of Human Resource (HR) professionals' use of the software and its consequences. We interviewed 15 recruiters and HR professionals about their experiences around two decision-making processes during hiring: sourcing and assessment. For both, AI-enabled software allowed the efficient processing of candidate data, thus providing the ability to introduce or advance candidates from broader and more diverse pools. For sourcing, it can serve as a useful learning resource to find candidates. Though, a lack of trust in data accuracy and an inadequate level of control over algorithmic candidate matches can create reluctance to embrace it. For assessment, its implementation varied across companies depending on the industry and the hiring scenario. Its inclusion may redefine HR professionals' job content as it automates or augments pieces of the existing hiring process. Finally, we discuss how candidate roles that recruiters and HR professionals support drive the use of algorithmic hiring software.",10.1145/3461702.3462531,https://doi.org/10.1145/3461702.3462531,"AAAI/ACM Conference on AI, Ethics, and Society","Li, Lan; Lassiter, Tina; Oh, Joohee; Lee, Min Kyung",2021,0,"@inproceedings{2-3646,
  title={Algorithmic Hiring in Practice: Recruiter and HR Professional's Perspectives on AI Use in Hiring},
  author={Li, Lan and Lassiter, Tina and Oh, Joohee and Lee, Min Kyung},
  year={2021},
  doi={10.1145/3461702.3462531},
  booktitle={AAAI/ACM Conference on AI, Ethics, and Society}
}",Empirical contributions,Everyday / Employment / Public Service,Organizational,Advising,"Knowledge provider, Decision-maker","Alter decision outcomes, Change trust, Change cognitive demands, Shape ethical norms","Shape AI for accountability, Update AI competence, Change AI responses",NA,NA,NA,Yes,Yes
2-3647,acm,Initial Responses to False Positives in AI-Supported Continuous Interactions: A Colonoscopy Case Study,"The use of artificial intelligence (AI) in clinical support systems is increasing. In this article, we focus on AI support for continuous interaction scenarios. A thorough understanding of end-user behaviour during these continuous human-AI interactions, in which user input is sustained over time and during which AI suggestions can appear at any time, is still missing. We present a controlled lab study involving 21 endoscopists and an AI colonoscopy support system. Using a custom-developed application and an off-the-shelf videogame controller, we record participants’ navigation behaviour and clinical assessment across 14 endoscopic videos. Each video is manually annotated to mimic an AI recommendation, being either true positive or false positive in nature. We find that time between AI recommendation and clinical assessment is significantly longer for incorrect assessments. Further, the type of medical content displayed significantly affects decision time. Finally, we discover that the participant’s clinical role plays a large part in the perception of clinical AI support systems. Our study presents a realistic assessment of the effects of imperfect and continuous AI support in a clinical scenario.",10.1145/3480247,https://doi.org/10.1145/3480247,ACM Transactions on Interactive Intelligent Systems,"van Berkel, Niels; Opie, Jeremy; Ahmad, Omer F.; Lovat, Laurence; Stoyanov, Danail; Blandford, Ann",2022,34,"@article{2-3647,
  title={Initial Responses to False Positives in AI-Supported Continuous Interactions: A Colonoscopy Case Study},
  author={van Berkel, Niels and Opie, Jeremy and Ahmad, Omer F. and Lovat, Laurence and Stoyanov, Danail and Blandford, Ann},
  year={2022},
  doi={10.1145/3480247},
  journal={ACM Transactions on Interactive Intelligent Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Advising,Decision-maker,"Change cognitive demands, Alter decision outcomes, Change affective-perceptual",no such info,"recommendations, the type of medical content",NA,Visual,Yes,Yes
2-3649,acm,Understanding Decision Subjects' Fairness Perceptions and Retention in Repeated Interactions with AI-Based Decision Systems,"The wide application of AI-based decision systems in many high-stake domains has raised concerns regarding fairness of these systems. As these systems will lead to real-life consequences to people who are subject to their decisions, understanding what these decision subjects perceive as a fair or unfair system is of vital importance. In this paper, we extend prior work in this direction by taking a perspective of repeated interactions—We ask that when decision subjects interact with an AI-based decision system repeatedly and can strategically respond to the system by determining whether to stay in the system, what factors will affect the decision subjects' fairness perceptions and retention in the system and how. To answer these questions, we conducted two randomized human-subject experiments in the context of an AI-based loan lending system. Our results suggest that in repeated interactions with the AI-based decision system, overall, decision subjects' fairness perceptions and retention in the system are significantly affected by whether the system is in favor of the group that subjects themselves belong to, rather than whether the system treats different groups in an unbiased way. However, decision subjects with different qualification levels have different reactions to the AI system's biased treatment across groups or the AI system's tendency to favor/disfavor their own group. Finally, we also find that while subjects' retention in the AI-based decision system is largely driven by their own prospects of receiving the favorable decision from the system, their fairness perceptions of the system is influenced by the system's treatment to people in other groups in a complex way.",10.1145/3514094.3534201,https://doi.org/10.1145/3514094.3534201,"AAAI/ACM Conference on AI, Ethics, and Society","Gemalmaz, Meric Altug; Yin, Ming",2022,15,"@inproceedings{2-3649,
  title={Understanding Decision Subjects' Fairness Perceptions and Retention in Repeated Interactions with AI-Based Decision Systems},
  author={Gemalmaz, Meric Altug and Yin, Ming},
  year={2022},
  booktitle={AAAI/ACM Conference on AI, Ethics, and Society},
  doi={10.1145/3514094.3534201}
}",Empirical contributions,"Finance / Business / Economy, Generic / Abstract / Domain-agnostic",Operational,"Forecasting, Advising","Decision-subject, Decision-maker","Shape ethical norms, Change cognitive demands, Change affective-perceptual",no such info,"procedural fairness, unfairness, recommendations",fairness perception ratings,"Textual, Visual, Interactive interface",Yes,Yes
2-3650,acm,Contrastive Counterfactual Fairness in Algorithmic Decision-Making,"The widespread use of artificial intelligence algorithms and their role in decision-making with consequential decisions for human subjects has resulted in a growing interest in designing AI algorithms accounting for fairness considerations. There have been attempts to account for fairness of AI algorithms without compromising their accuracy to improve poorly designed algorithms that disregard sensitive attributes (e.g., age, race, and gender) at the peril of introducing or increasing bias against specific groups. Although many studies have examined the optimal trade-off between fairness and accuracy, it remains a challenge to understand the sources of unfairness in decision-making and mitigate it effectively. To tackle this problem, researchers have proposed fair causal learning approaches which assist us in modeling cause and effect knowledge structures, discovering bias sources, and refining AI algorithms to make them more transparent and explainable. In this study, we formalize probabilistic interpretations of both contrastive and counterfactual causality as essential features in order to encourage users' trust and to expand the applicability of such automated systems. We use this formalism to define a novel fairness criterion that we call contrastive counterfactual fairness. This paper introduces, to the best of our knowledge, the first probabilistic fairness-aware data augmentation approach that is based on contrastive counterfactual causality. We tested our approach on two well-known fairness-related datasets, UCI Adult and German Credit, and concluded that our proposed method has a promising ability to capture and mitigate unfairness in AI deployment. This model-agnostic approach can be used with any AI model because it is applied in pre-processing.",10.1145/3514094.3534143,https://doi.org/10.1145/3514094.3534143,"AAAI/ACM Conference on AI, Ethics, and Society","Mutlu, Ece Çiğdem; Yousefi, Niloofar; Ozmen Garibay, Ozlem",2022,18,"@inproceedings{2-3650,
  title = {Contrastive Counterfactual Fairness in Algorithmic Decision-Making},
  author = {Mutlu, Ece Çiğdem and Yousefi, Niloofar and Ozmen Garibay, Ozlem},
  year = {2022},
  doi = {10.1145/3514094.3534143},
  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Institutional,Executing,"Decision-maker, Developer, Guardian",NA,NA,NA,NA,NA,Yes,No
2-3655,acm,Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks,"This paper addresses an under-explored problem of AI-assisted decision-making: when objective performance information of the machine learning model underlying a decision aid is absent or scarce, how do people decide their reliance on the model? Through three randomized experiments, we explore the heuristics people may use to adjust their reliance on machine learning models when performance feedback is limited. We find that the level of agreement between people and a model on decision-making tasks that people have high confidence in significantly affects reliance on the model if people receive no information about the model’s performance, but this impact will change after aggregate-level model performance information becomes available. Furthermore, the influence of high confidence human-model agreement on people’s reliance on a model is moderated by people’s confidence in cases where they disagree with the model. We discuss potential risks of these heuristics, and provide design implications on promoting appropriate reliance on AI.",10.1145/3411764.3445562,https://doi.org/10.1145/3411764.3445562,CHI Conference on Human Factors in Computing Systems,"Lu, Zhuoran; Yin, Ming",2021,187,"@inproceedings{2-3655,
  title = {Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks},
  author = {Lu, Zhuoran and Yin, Ming},
  year = {2021},
  doi = {10.1145/3411764.3445562},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service",Individual,"Advising, Forecasting","Decision-maker, Decision-subject","Change affective-perceptual, Change cognitive demands, Change trust, Alter decision outcomes",no such info,"information about the model’s performance, prediction of alternative, feedback screen with information about the ML model’s overall accuracy","NA, human confidence when disagreeing with the AI moderates how agreement on high-confidence tasks affects reliance",Interactive interface,Yes,Yes
2-3656,acm,Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making,"This paper contributes to the growing literature in empirical evaluation of explainable AI (XAI) methods by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Specifically, based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy—improve people’s understanding of the AI model, help people recognize the model uncertainty, and support people’s calibrated trust in the model. Through randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of decision making tasks where people perceive themselves as having different levels of domain expertise in (i.e., recidivism prediction and forest cover prediction). Our results show that the effects of AI explanations are largely different on decision making tasks where people have varying levels of domain expertise in, and many AI explanations do not satisfy any of the desirable properties for tasks that people have little domain expertise in. Further, for decision making tasks that people are more knowledgeable, feature contribution explanation is shown to satisfy more desiderata of AI explanations, while the explanation that is considered to resemble how human explain decisions (i.e., counterfactual explanation) does not seem to improve calibrated trust. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making.",10.1145/3397481.3450650,https://doi.org/10.1145/3397481.3450650,International Conference on Intelligent User Interfaces (IUI),"Wang, Xinru; Yin, Ming",2021,464,"@inproceedings{2-3656,
  title={Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making},
  author={Wang, Xinru and Yin, Ming},
  year={2021},
  doi={10.1145/3397481.3450650},
  booktitle={International Conference on Intelligent User Interfaces (IUI)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance, Environment / Resources / Energy",Organizational,"Explaining, Advising",Decision-maker,"Change cognitive demands, Change trust, Alter decision outcomes",no such info,"visual explanations, feature-based explanations, example-based explanations, counterfactual explanations",NA,"Visual, Textual, Interactive interface",Yes,Yes
2-3657,acm,"""We Would Never Write That Down"": Classifications of Unemployed and Data Challenges for AI","This paper draws attention to new complexities of deploying artificial intelligence (AI) to sensitive contexts, such as welfare allocation. AI is increasingly used in public administration with the promise of improving decision-making through predictive modelling. To accurately predict, it needs all the agreed criteria used as part of decisions, formal and informal. This paper empirically explores the informal classifications used by caseworkers to make unemployed welfare seekers 'fit' into the formal categories applied in a Danish job centre. Our findings show that these classifications are documentable, and hence traceable to AI. However, to the caseworkers, they are at odds with the stable explanations assumed by any bureaucratic recording system as they involve negotiated and situated judgments of people's character. Thus, for moral reasons, caseworkers find them ill-suited for formal representation and predictive purposes and choose not to write them down. As a result, although classification work is crucial to the job centre's activities, AI is denuded of the real-world (and real work) character of decision-making in this context. This is an important finding for CSCW as it is not only about whether AI can 'do' decision-making in particular contexts, as previous research has argued. This paper shows that problems may also be caused by people's unwillingness to provide data to systems. It is the purpose of this paper to present the empirical results of this research, followed by a discussion of implications for AI-supported practice and research.",10.1145/3449176,https://doi.org/10.1145/3449176,Proceedings of the ACM on Human-Computer Interaction,"Petersen, Anette C. M.; Christensen, Lars Rune; Harper, Richard; Hildebrandt, Thomas",2021,48,"@article{2-3657,
  title = {``We Would Never Write That Down'': Classifications of Unemployed and Data Challenges for AI},
  author = {Petersen, Anette C. M. and Christensen, Lars Rune and Harper, Richard and Hildebrandt, Thomas},
  year = {2021},
  doi = {10.1145/3449176},
  journal = {Proceedings of the ACM on Human-Computer Interaction}
}",Empirical contributions,"Everyday / Employment / Public Service, Law / Policy / Governance",Organizational,"Forecasting, Executing, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3658,acm,How does Value Similarity affect Human Reliance in AI-Assisted Ethical Decision Making?,"This paper explores the impact of value similarity between humans and AI on human reliance in the context of AI-assisted ethical decision-making. Using kidney allocation as a case study, we conducted a randomized human-subject experiment where workers were presented with ethical dilemmas in various conditions, including no AI recommendations, recommendations from a similar AI, and recommendations from a dissimilar AI. We found that recommendations provided by a dissimilar AI had a higher overall effect on human decisions than recommendations from a similar AI. However, when humans and AI disagreed, participants were more likely to change their decisions when provided with recommendations from a similar AI. The effect was not due to humans’ perceptions of the AI being similar, but rather due to the AI displaying similar ethical values through its recommendations. We also conduct a preliminary analysis on the relationship between value similarity and trust, and potential shifts in ethical preferences at the population-level.",10.1145/3600211.3604709,https://doi.org/10.1145/3600211.3604709,"AAAI/ACM Conference on AI, Ethics, and Society","Narayanan, Saumik; Yu, Guanghui; Ho, Chien-Ju; Yin, Ming",2023,15,"@inproceedings{2-3658,
  title = {How does Value Similarity affect Human Reliance in AI-Assisted Ethical Decision Making?},
  author = {Narayanan, Saumik and Yu, Guanghui and Ho, Chien-Ju and Yin, Ming},
  year = {2023},
  doi = {10.1145/3600211.3604709},
  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,Advising,Decision-maker,"Change trust, Alter decision outcomes, Shape ethical norms",Update AI competence,recommendations,"human value incorporation, human-impact-informed algorithm design",Textual,Yes,Yes
2-36582,springernature,DriveLM: Driving with Graph Visual Question Answering,"We study how vision-language models( VLMs) trained on web-scale data can be integrated into end-to-end driving systems to boost generalization and enable interactivity with human users. While recent approaches adapt VLMs to driving via single-round visual question answering( VQA) , human drivers reason about decisions in multiple steps. Starting from the localization of key objects, humans estimate object interactions before taking actions. The key insight is that with our proposed task, Graph VQA, where we model graph-structured reasoning through perception, prediction and planning question-answer pairs, we obtain a suitable proxy task to mimic the human reasoning process. We instantiate datasets( DriveLM-Data) built upon nuScenes and CARLA, and propose a VLM-based baseline approach( DriveLM-Agent) for jointly performing Graph VQA and end-to-end driving. The experiments demonstrate that Graph VQA provides a simple, principled framework for reasoning about a driving scene, and DriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agent baseline performs end-to-end autonomous driving competitively in comparison to state-of-the-art driving-specific architectures. Notably, its benefits are pronounced when it is evaluated zero-shot on unseen sensor configurations. Our question-wise ablation study shows that the performance gain comes from the rich annotation of prediction and planning QA pairs in the graph structure. All data, models and an official evaluation server are available at https://github. com/OpenDriveLab/DriveLM.",10.1007/978-3-031-72943-0_15,http://dx.doi.org/10.1007/978-3-031-72943-0_15,European Conference on Computer Vision,"Sima, Chonghao;Renz, Katrin;Chitta, Kashyap;Chen, Li;Zhang, Hanxue;Xie, Chengen;Beißwenger, Jens;Luo, Ping;Geiger, Andreas;Li, Hongyang",2025,51,"@inproceedings{2-36582,
  title={DriveLM: Driving with Graph Visual Question Answering},
  author={Sima, Chonghao and Renz, Katrin and Chitta, Kashyap and Chen, Li and Zhang, Hanxue and Xie, Chengen and Beißwenger, Jens and Luo, Ping and Geiger, Andreas and Li, Hongyang},
  booktitle={European Conference on Computer Vision},
  year={2025},
  doi={10.1007/978-3-031-72943-0_15}
}","Algorithmic contributions, Dataset/Benchmark contributions",Transportation / Mobility / Planning,Individual,"Advising, Executing",Knowledge provider,NA,NA,NA,NA,NA,Yes,No
2-3659,acm,A Trustworthy and Responsible Decision-Making Framework for Resource Management in Food-Energy-Water Nexus: A Control-Theoretical Approach,"This paper introduces a hybrid framework for trustworthy and responsible natural resource management, aimed at building bottom-up trust to enhance cooperation among decision makers in the Food, Energy, and Water sectors. Cooperation is highly critical for the adoption and application of resource management alternatives (solutions), including those generated by AI-based recommender systems, in communities due to significant impact of these sectors on the environment and the economic productivity of affected communities. While algorithms can recommend solutions, effectively communicating and gaining community acceptance of these solutions is crucial. Our research stands out by emphasizing the collaboration between humans and machines, which is essential for addressing broader challenges related to climate change and the need for expert trade-off handling in the management of natural resources. To support future decision-making, we propose a successful control-theory model based on previous decision-making and actor behavior. We utilize control theory to depict how community decisions can be affected by how much individuals trust and accept proposed solutions on irrigation water rights and crop operations in an iterative and interactive decision support environment. This model interacts with stakeholders to collect their feedback on the acceptability of solutions, while also examining the influence of consensus levels, trust sensitivities, and the number of decision-making rounds on the acceptance of proposed solutions. Furthermore, we investigate a system of multiple decision-making and explore the impact of learning actors who adjust their trust sensitivities based on solution acceptance and the number of decision-making rounds. Additionally, our approach can be employed to evaluate and refine potential policy modifications. Although we assess potential outcomes using hypothetical actions by individuals, it is essential to emphasize our primary objective of developing a tool that accurately captures real human behavior and fosters improved collaboration in community decision-making. Ultimately, our aim is to enhance the harmony between AI-based recommender systems and human values, promoting a deeper understanding and integration between the two.",10.1145/3660640,https://doi.org/10.1145/3660640,ACM Transactions on Intelligent Systems and Technology,"Uslu, Suleyman; Kaur, Davinder; Rivera, Samuel J.; Durresi, Arjan; Babbar-Sebens, Meghna; Tilt, Jenna H.",2024,5,"@article{2-3659,
  title = {A Trustworthy and Responsible Decision-Making Framework for Resource Management in Food-Energy-Water Nexus: A Control-Theoretical Approach},
  author = {Uslu, Suleyman and Kaur, Davinder and Rivera, Samuel J. and Durresi, Arjan and Babbar-Sebens, Meghna and Tilt, Jenna H.},
  year = {2024},
  doi = {10.1145/3660640},
  journal = {ACM Transactions on Intelligent Systems and Technology}
}",Theoretical contributions,Environment / Resources / Energy,Institutional,"Analyzing, Advising","Decision-maker, Knowledge provider, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-3660,acm,Reviewable Automated Decision-Making: A Framework for Accountable Algorithmic Systems,"This paper introduces reviewability as a framework for improving the accountability of automated and algorithmic decisionmaking (ADM) involving machine learning. We draw on an understanding of ADM as a socio-technical process involving both human and technical elements, beginning before a decision is made and extending beyond the decision itself. While explanations and other model-centric mechanisms may assist some accountability concerns, they often provide insufficient information of these broader ADM processes for regulatory oversight and assessments of legal compliance. Reviewability involves breaking down the ADM process into technical and organisational elements to provide a systematic framework for determining the contextually appropriate record-keeping mechanisms to facilitate meaningful review - both of individual decisions and of the process as a whole. We argue that a reviewability framework, drawing on administrative law's approach to reviewing human decision-making, offers a practical way forward towards more a more holistic and legally-relevant form of accountability for ADM.",10.1145/3442188.3445921,https://doi.org/10.1145/3442188.3445921,"ACM Conference on Fairness, Accountability, and Transparency (FAccT)","Cobbe, Jennifer; Lee, Michelle Seng Ah; Singh, Jatinder",2021,122,"@inproceedings{2-3660,
  title = {Reviewable Automated Decision-Making: A Framework for Accountable Algorithmic Systems},
  author = {Cobbe, Jennifer and Lee, Michelle Seng Ah and Singh, Jatinder},
  year = {2021},
  doi = {10.1145/3442188.3445921},
  booktitle = {ACM Conference on Fairness, Accountability, and Transparency (FAccT)}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance",Operational,"Executing, Auditing","Guardian, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-3663,acm,Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making,"Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.",10.1145/3351095.3372852,https://doi.org/10.1145/3351095.3372852,"ACM Conference on Fairness, Accountability, and Transparency (FAccT)","Zhang, Yunfeng; Liao, Q. Vera; Bellamy, Rachel K. E.",2020,0,"@inproceedings{2-3663,
  title = {Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision Making},
  author = {Zhang, Yunfeng and Liao, Q. Vera and Bellamy, Rachel K. E.},
  year = {2020},
  doi = {10.1145/3351095.3372852},
  booktitle = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (FAccT)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Operational,"Advising, Explaining","Decision-maker, Decision-subject","Change trust, Change affective-perceptual, Change cognitive demands, Alter decision outcomes, Restrict human agency",no such info,"confidence score, local explanations",domain knowledge,"Textual, Visual",Yes,Yes
2-3665,acm,Exploring the Effects of Machine Learning Literacy Interventions on Laypeople’s Reliance on Machine Learning Models,"Today, machine learning (ML) technologies have penetrated almost every aspect of people’s lives, yet public understandings of these technologies are often limited. This highlights the urgent need of designing effective methods to increase people’s machine learning literacy, as the lack of relevant knowledge may result in people’s inappropriate usage of machine learning technologies. In this paper, we focus on an ML-assisted decision-making setting and conduct a human-subject randomized experiment to explore how providing different types of user tutorials as the machine learning literacy interventions can influence laypeople’s reliance on ML models, on both in-distribution and out-of-distribution examples. We vary the existence, interactivity and scope of the user tutorial across different treatments in our experiment. Our results show that user tutorials, when presented in appropriate forms, can help some people rely on ML models more appropriately. For example, for those individuals who have relatively high ability in solving the decision-making task themselves, receiving a user tutorial that is interactive and addresses the specific ML model to be used allows them to reduce their over-reliance on the ML model when they could outperform the model. In contrast, low-performing individuals’ reliance on the ML model is not affected by the presence or the type of user tutorial. Finally, we also find that people perceive the interactive tutorial to be more understandable and slightly more useful. We conclude by discussing the design implications of our study.",10.1145/3490099.3511121,https://doi.org/10.1145/3490099.3511121,International Conference on Intelligent User Interfaces (IUI),"Chiang, Chun-Wei; Yin, Ming",2022,1,"@inproceedings{2-3665,
  title = {Exploring the Effects of Machine Learning Literacy Interventions on Laypeople’s Reliance on Machine Learning Models},
  author = {Chiang, Chun-Wei and Yin, Ming},
  year = {2022},
  doi = {10.1145/3490099.3511121},
  booktitle = {International Conference on Intelligent User Interfaces (IUI)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research",Individual,Advising,"Decision-maker, Knowledge provider","Change trust, Alter decision outcomes, Change affective-perceptual",no such info,prediction of alternative,NA,Textual,Yes,Yes
2-3666,acm,Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis,"Today’s AI systems for medical decision support often succeed on benchmark datasets in research papers but fail in real-world deployment. This work focuses on the decision making of sepsis, an acute life-threatening systematic infection that requires an early diagnosis with high uncertainty from the clinician. Our aim is to explore the design requirements for AI systems that can support clinical experts in making better decisions for the early diagnosis of sepsis. The study begins with a formative study investigating why clinical experts abandon an existing AI-powered Sepsis predictive module in their electrical health record (EHR) system. We argue that a human-centered AI system needs to support human experts in the intermediate stages of a medical decision-making process (e.g., generating hypotheses or gathering data), instead of focusing only on the final decision. Therefore, we build SepsisLab based on a state-of-the-art AI algorithm and extend it to predict the future projection of sepsis development, visualize the prediction uncertainty, and propose actionable suggestions (i.e., which additional laboratory tests can be collected) to reduce such uncertainty. Through heuristic evaluation with six clinicians using our prototype system, we demonstrate that SepsisLab enables a promising human-AI collaboration paradigm for the future of AI-assisted sepsis diagnosis and other high-stakes medical decision making.",10.1145/3613904.3642343,https://doi.org/10.1145/3613904.3642343,CHI Conference on Human Factors in Computing Systems,"Zhang, Shao; Yu, Jianing; Xu, Xuhai; Yin, Changchang; Lu, Yuxuan; Yao, Bingsheng; Tory, Melanie; Padilla, Lace M.; Caterino, Jeffrey; Zhang, Ping; Wang, Dakuo",2024,12,"@inproceedings{2-3666,
  title = {Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis},
  author = {Zhang, Shao and Yu, Jianing and Xu, Xuhai and Yin, Changchang and Lu, Yuxuan and Yao, Bingsheng and Tory, Melanie and Padilla, Lace M. and Caterino, Jeffrey and Zhang, Ping and Wang, Dakuo},
  year = {2024},
  booktitle = {CHI Conference on Human Factors in Computing Systems},
  doi = {10.1145/3613904.3642343}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-subject, Decision-maker","Alter decision outcomes, Change cognitive demands, Change trust",no such info,"risk score, prediction of alternative, recommendations, counterfactual predictions",NA,Interactive interface,Yes,Yes
2-36678,springernature,Automatic discovery of interpretable planning strategies,"When making decisions, people often overlook critical information or are overly swayed by irrelevant information. A common approach to mitigate these biases is to provide decision-makers, especially professionals such as medical doctors, with decision aids, such as decision trees and flowcharts. Designing effective decision aids is a difficult problem. We propose that recently developed reinforcement learning methods for discovering clever heuristics for good decision-making can be partially leveraged to assist human experts in this design process. One of the biggest remaining obstacles to leveraging the aforementioned methods for improving human decision-making is that the policies they learn are opaque to people. To solve this problem, we introduce AI-Interpret: a general method for transforming idiosyncratic policies into simple and interpretable descriptions. Our algorithm combines recent advances in imitation learning and program induction with a new clustering method for identifying a large subset of demonstrations that can be accurately described by a simple, high-performing decision rule. We evaluate our new AI-Interpret algorithm and employ it to translate information-acquisition policies discovered through metalevel reinforcement learning. The results of three large behavioral experiments showed that providing the decision rules generated by AI-Interpret as flowcharts significantly improved people’s planning strategies and decisions across three different classes of sequential decision problems. Moreover, our fourth experiment revealed that this approach is significantly more effective at improving human decision-making than training people by giving them performance feedback. Finally, a series of ablation studies confirmed that our AI-Interpret algorithm was critical to the discovery of interpretable decision rules and that it is ready to be applied to other reinforcement learning problems. We conclude that the methods and findings presented in this article are an important step towards leveraging automatic strategy discovery to improve human decision-making. The code for our algorithm and the experiments is available at https://github. com/RationalityEnhancement/InterpretableStrategyDiscovery.",10.1007/s10994-021-05963-2,http://dx.doi.org/10.1007/s10994-021-05963-2,Machine Learning,"Skirzyński, Julian;Becker, Frederic;Lieder, Falk",2021,39,"@article{2-36678,
  title = {Automatic discovery of interpretable planning strategies},
  author = {Skirzyński, Julian and Becker, Frederic and Lieder, Falk},
  year = {2021},
  doi = {10.1007/s10994-021-05963-2},
  journal = {Machine Learning}
}","Algorithmic contributions, Methodological contributions",Generic / Abstract / Domain-agnostic,Individual,"Advising, Executing, Explaining","Decision-maker, Developer",Alter decision outcomes,no such info,easily‐comprehensible decision rules,NA,Visual,Yes,Yes
2-3668,acm,Trust in AI-assisted Decision Making: Perspectives from Those Behind the System and Those for Whom the Decision is Made,"Trust between humans and AI in the context of decision-making has acquired an important role in public policy, research and industry. In this context, Human-AI Trust has often been tackled from the lens of cognitive science and psychology, but lacks insights from the stakeholders involved. In this paper, we conducted semi-structured interviews with 7 AI practitioners and 7 decision subjects from various decision domains. We found that 1) interviewees identified the prerequisites for the existence of trust and distinguish trust from trustworthiness, reliance, and compliance; 2) trust in AI-integrated systems is strongly influenced by other human actors, more than the system’s features; 3) the role of Human-AI trust factors is stakeholder-dependent. These results provide clues for the design of Human-AI interactions in which trust plays a major role, as well as outline new research directions in Human-AI Trust.",10.1145/3613904.3642018,https://doi.org/10.1145/3613904.3642018,CHI Conference on Human Factors in Computing Systems,"Vereschak, Oleksandra; Alizadeh, Fatemeh; Bailly, Gilles; Caramiaux, Baptiste",2024,32,"@inproceedings{2-3668,
  title={Trust in AI-assisted Decision Making: Perspectives from Those Behind the System and Those for Whom the Decision is Made},
  author={Vereschak, Oleksandra and Alizadeh, Fatemeh and Bailly, Gilles and Caramiaux, Baptiste},
  year={2024},
  doi={10.1145/3613904.3642018},
  booktitle={CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Analyzing, Advising","Decision-subject, Decision-maker","Change trust, Change cognitive demands, Restrict human agency",Shape AI for accountability,"recommendations, explanations","perceived risk, positive ex-pectations",NA,Yes,Yes
2-3669,acm,"Reassuring, Misleading, Debunking: Comparing Effects of XAI Methods on Human Decisions","Trust calibration is essential in AI-assisted decision-making. If human users understand the rationale on which an AI model has made a prediction, they can decide whether they consider this prediction reasonable. Especially in high-risk tasks such as mushroom hunting (where a wrong decision may be fatal), it is important that users make correct choices to trust or overrule the AI. Various explainable AI (XAI) methods are currently being discussed as potentially useful for facilitating understanding and subsequently calibrating user trust. So far, however, it remains unclear which approaches are most effective. In this article, the effects of XAI methods on human AI-assisted decision-making in the high-risk task of mushroom picking were tested. For that endeavor, the effects of (i) Grad-CAM attributions, (ii) nearest-neighbor examples, and (iii) network-dissection concepts were compared in a between-subjects experiment with (N=501) participants representing end-users of the system. In general, nearest-neighbor examples improved decision correctness the most. However, varying effects for different task items became apparent. All explanations seemed to be particularly effective when they revealed reasons to (i) doubt a specific AI classification when the AI was wrong and (ii) trust a specific AI classification when the AI was correct. Our results suggest that well-established methods, such as Grad-CAM attribution maps, might not be as beneficial to end users as expected and that XAI techniques for use in real-world scenarios must be chosen carefully.",10.1145/3665647,https://doi.org/10.1145/3665647,ACM Transactions on Interactive Intelligent Systems,"Humer, Christina; Hinterreiter, Andreas; Leichtmann, Benedikt; Mara, Martina; Streit, Marc",2024,12,"@article{2-3669,
  title = {Reassuring, Misleading, Debunking: Comparing Effects of XAI Methods on Human Decisions},
  author = {Humer, Christina and Hinterreiter, Andreas and Leichtmann, Benedikt and Mara, Martina and Streit, Marc},
  year = {2024},
  doi = {10.1145/3665647},
  journal = {ACM Transactions on Interactive Intelligent Systems}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Environment / Resources / Energy",Individual,"Explaining, Advising",Decision-maker,"Alter decision outcomes, Change trust",no such info,"Grad-CAM attributions, nearest-neighbor examples, network-dissection concepts",NA,Interactive interface,Yes,Yes
2-3670,acm,To Trust or Not To Trust: How a Conversational Interface Affects Trust in a Decision Support System,"Trust is an important component of human-AI relationships and plays a major role in shaping the reliance of users on online algorithmic decision support systems. With recent advances in natural language processing, text and voice-based conversational interfaces have provided users with new ways of interacting with such systems. Despite the growing applications of conversational user interfaces (CUIs), little is currently understood about the suitability of such interfaces for decision support and how CUIs inspire trust among humans engaging with decision support systems. In this work, we aim to address this gap and answer the following question: to what extent can a conversational interface build user trust in decision support systems in comparison to a conventional graphical user interface? To this end, we built a text-based conversational interface, and a conventional web-based graphical user interface. These served as the means for users to interact with an online decision support system to help them find housing, given a fixed set of constraints. To understand how the accuracy of the decision support system moderates user behavior and trust across the two interfaces, we considered an accurate and inaccurate system. We carried out a 2 × 2 between-subjects study (N = 240) on the Prolific crowdsourcing platform. Our findings show that the conversational interface was significantly more effective in building user trust and satisfaction in the online housing recommendation system when compared to the conventional web interface. Our results highlight the potential impact of conversational interfaces for trust development in decision support systems.",10.1145/3485447.3512248,https://doi.org/10.1145/3485447.3512248,ACM Web Conference (formerly The World Wide Web Conference),"Gupta, Akshit; Basu, Debadeep; Ghantasala, Ramya; Qiu, Sihang; Gadiraju, Ujwal",2022,68,"@inproceedings{2-3670,
  title={To Trust or Not To Trust: How a Conversational Interface Affects Trust in a Decision Support System},
  author={Gupta, Akshit and Basu, Debadeep and Ghantasala, Ramya and Qiu, Sihang and Gadiraju, Ujwal},
  year={2022},
  doi={10.1145/3485447.3512248},
  booktitle={ACM Web Conference (formerly The World Wide Web Conference)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Individual,"Advising, Collaborating","Decision-maker, Decision-subject, Stakeholder","Change trust, Alter decision outcomes, Change affective-perceptual",no such info,recommendations,personalized settings,Textual,Yes,Yes
2-36706,springernature,Radiologists’ Usage of Diagnostic AI Systems,"While diagnostic AI systems are implemented in medical practice, it is still unclear how physicians embed them in diagnostic decision making. This study examines how radiologists come to use diagnostic AI systems in different ways and what role AI assessments play in this process if they confirm or disconfirm radiologists’ own judgment. The study draws on rich qualitative data from a revelatory case study of an AI system for stroke diagnosis at a University Hospital to elaborate how three sensemaking processes revolve around confirming and disconfirming AI assessments. Through context-specific sensedemanding, sensegiving, and sensebreaking, radiologists develop distinct usage patterns of AI systems. The study reveals that diagnostic self-efficacy influences which of the three sensemaking processes radiologists engage in. In deriving six propositions, the account of sensemaking and usage of diagnostic AI systems in medical practice paves the way for future research.",10.1007/s12599-022-00750-2,http://dx.doi.org/10.1007/s12599-022-00750-2,Business & Information Systems Engineering,"Jussupow, Ekaterina;Spohrer, Kai;Heinzl, Armin",2022,28,"@article{2-36706,
  title={Radiologists' Usage of Diagnostic AI Systems},
  author={Jussupow, Ekaterina and Spohrer, Kai and Heinzl, Armin},
  year={2022},
  journal={Business \& Information Systems Engineering},
  doi={10.1007/s12599-022-00750-2}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing, Explaining","Decision-maker, Decision-subject","Change affective-perceptual, Change cognitive demands, Alter decision outcomes, Change trust",no such info,"ASPECT score, visual analysis",NA,"Visual, Textual",Yes,Yes
2-3671,acm,"Humans, AI, and Context: Understanding End-Users’ Trust in a Real-World Computer Vision Application","Trust is an important factor in people’s interactions with AI systems. However, there is a lack of empirical studies examining how real end-users trust or distrust the AI system they interact with. Most research investigates one aspect of trust in lab settings with hypothetical end-users. In this paper, we provide a holistic and nuanced understanding of trust in AI through a qualitative case study of a real-world computer vision application. We report findings from interviews with 20 end-users of a popular, AI-based bird identification app where we inquired about their trust in the app from many angles. We find participants perceived the app as trustworthy and trusted it, but selectively accepted app outputs after engaging in verification behaviors, and decided against app adoption in certain high-stakes scenarios. We also find domain knowledge and context are important factors for trust-related assessment and decision-making. We discuss the implications of our findings and provide recommendations for future research on trust in AI.",10.1145/3593013.3593978,https://doi.org/10.1145/3593013.3593978,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Kim, Sunnie S. Y.; Watkins, Elizabeth Anne; Russakovsky, Olga; Fong, Ruth; Monroy-Hernández, Andrés",2023,60,"@inproceedings{2-3671,
  title = {Humans, AI, and Context: Understanding End-Users’ Trust in a Real-World Computer Vision Application},
  author = {Kim, Sunnie S. Y. and Watkins, Elizabeth Anne and Russakovsky, Olga and Fong, Ruth and Monroy-Hern{\'a}ndez, Andr{\'e}s},
  year = {2023},
  doi = {10.1145/3593013.3593978},
  booktitle = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Forecasting, Analyzing, Advising",Decision-maker,"Alter decision outcomes, Change trust, Change cognitive demands, Change affective-perceptual",Change AI responses,NA,"domain knowledge, context",Interactive interface,Yes,Yes
2-3675,acm,Counterfactual Explanations for Prediction and Diagnosis in XAI,"We compared two sorts of explanations for decisions made by an AI system: counterfactual explanations about how an outcome could have been different in the past, and prefactual explanations about how it could be different in the future. We examined the effects of these alternative explanation strategies on the accuracy of users' judgments about the AI app's predictions about an outcome (inferred from information about the causes), compared to the accuracy of their judgments about the app's diagnoses of a cause (inferred from information about the outcome). The tasks were based on a simulated SmartAgriculture decision support system for grass growth outcomes on dairy farms in Experiment 1, and for an analogous alien planet domain in Experiment 2. The two experiments, with 243 participants, also tested users' confidence in their decisions, and their satisfaction with the explanations. Users made more accurate diagnoses of the presence of causes based on information about their outcome, compared to predictions of an outcome given information about the presence of causes. Their predictions and diagnoses were helped equally by counterfactual explanations and prefactual ones.",10.1145/3514094.3534144,https://doi.org/10.1145/3514094.3534144,"AAAI/ACM Conference on AI, Ethics, and Society","Dai, Xinyue; Keane, Mark T.; Shalloo, Laurence; Ruelle, Elodie; Byrne, Ruth M.J.",2022,57,"@inproceedings{2-3675,
  title     = {Counterfactual Explanations for Prediction and Diagnosis in XAI},
  author    = {Dai, Xinyue and Keane, Mark T. and Shalloo, Laurence and Ruelle, Elodie and Byrne, Ruth M.J.},
  year      = {2022},
  doi       = {10.1145/3514094.3534144},
  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Environment / Resources / Energy",Operational,"Explaining, Advising",Decision-maker,Change affective-perceptual,no such info,"counterfactual explanations, prefactual explanations",NA,Textual,Yes,Yes
2-3676,acm,A Case Study Investigating a User-Centred and Expert Informed 'Companion Guide' for a Complex Sensor-based Platform,"We present a case study that informs the creation of a 'companion guide' providing transparency to potential non-expert users of a ubiquitous machine learning (ML) platform during the initial onboarding. Ubiquitous platforms (e.g., smart home systems, including smart meters and conversational agents) are increasingly commonplace and increasingly apply complex ML methods. Understanding how non-ML experts comprehend these platforms is important in supporting participants in making an informed choice about if and how they adopt these platforms. To aid this decision-making process, we created a companion guide for a home health platform through an iterative user-centred-design process, seeking additional input from platform experts at all stages of the process to ensure the accuracy of explanations. This user-centred and expert informed design process highlights the need to present the platform's entire ecosystem at an appropriate level for those with differing backgrounds to understand, in order to support informed consent and decision making.",10.1145/3534625,https://doi.org/10.1145/3534625,"Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","Eardley, Rachel; Mackinnon, Sue; Tonkin, Emma L.; Soubutts, Ewan; Ayobi, Amid; Linington, Jess; Tourte, Gregory J. L.; Gross, Zoe Banks; Bailey, David J.; Knights, Russell; Gooberman-Hill, Rachael; Craddock, Ian; O'Kane, Aisling Ann",2022,13,"@article{2-3676,
  title = {A Case Study Investigating a User-Centred and Expert Informed 'Companion Guide' for a Complex Sensor-based Platform},
  author = {Eardley, Rachel and Mackinnon, Sue and Tonkin, Emma L. and Soubutts, Ewan and Ayobi, Amid and Linington, Jess and Tourte, Gregory J. L. and Gross, Zoe Banks and Bailey, David J. and Knights, Russell and Gooberman-Hill, Rachael and Craddock, Ian and O'Kane, Aisling Ann},
  year = {2022},
  doi = {10.1145/3534625},
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies}
}",Empirical contributions,Software / Systems / Security,Individual,"Forecasting, Analyzing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-36779,springernature,Learning to Complement and to Defer to Multiple Users,"With the development of Human-AI Collaboration in Classification( HAI-CC) , integrating users and AI predictions becomes challenging due to the complex decision-making process. This process has three options: 1) AI autonomously classifies, 2) learning to complement, where AI collaborates with users, and 3) learning to defer, where AI defers to users. Despite their interconnected nature, these options have been studied in isolation rather than as components of a unified system. In this paper, we address this weakness with the novel HAI-CC methodology, called Le arning to Co mplement and to D efer to Multiple U sers( LECODU). LECODU not only combines learning to complement and learning to defer strategies, but it also incorporates an estimation of the optimal number of users to engage in the decision process. The training of LECODU maximises classification accuracy and minimises collaboration costs associated with user involvement. Comprehensive evaluations across real-world and synthesized datasets demonstrate LECODU’s superior performance compared to state-of-the-art HAI-CC methods. Remarkably, even when relying on unreliable users with high rates of label noise, LECODU exhibits significant improvement over both human decision-makers alone and AI alone( Supported by the Engineering and Physical Sciences Research Council( EPSRC) through grant EP/Y018036/1). Code is available at https://github. com/zhengzhang37/LECODU. git.",10.1007/978-3-031-72992-8_9,http://dx.doi.org/10.1007/978-3-031-72992-8_9,European Conference on Computer Vision,"Zhang, Zheng;Ai, Wenjie;Wells, Kevin;Rosewarne, David;Do, Thanh-Toan;Carneiro, Gustavo",2025,29,"@inproceedings{2-36779,
  title     = {Learning to Complement and to Defer to Multiple Users},
  author    = {Zhang, Zheng and Ai, Wenjie and Wells, Kevin and Rosewarne, David and Do, Thanh-Toan and Carneiro, Gustavo},
  year      = {2025},
  booktitle = {European Conference on Computer Vision},
  doi       = {10.1007/978-3-031-72992-8_9}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,"Forecasting, Collaborating","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3679,acm,Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task,"When designing an AI-assisted decision-making system, there is often a tradeoff between precision and recall in the AI’s recommendations. We argue that careful exploitation of this tradeoff can harness the complementary strengths in the human-AI collaboration to significantly improve team performance. We investigate a real-world video anonymization task for which recall is paramount and more costly to improve. We analyze the performance of 78 professional annotators working with a) no AI assistance, b) a high-precision ""restrained"" AI, and c) a high-recall ""zealous"" AI in over 3,466 person-hours of annotation work. In comparison, the zealous AI helps human teammates achieve significantly shorter task completion time and higher recall. In a follow-up study, we remove AI assistance for everyone and find negative training effects on annotators trained with the restrained AI. These findings and our analysis point to important implications for the design of AI assistance in recall-demanding scenarios.",10.1145/3544548.3581282,https://doi.org/10.1145/3544548.3581282,ACM CHI Conference on Human Factors in Computing Systems,"Xu, Chengyuan; Lien, Kuo-Chin; Höllerer, Tobias",2023,17,"@inproceedings{2-3679,
  author    = {Xu, Chengyuan and Lien, Kuo-Chin and Höllerer, Tobias},
  title     = {Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task},
  year      = {2023},
  doi       = {10.1145/3544548.3581282},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,"Education / Teaching / Research, Generic / Abstract / Domain-agnostic",Operational,"Advising, Collaborating","Decision-maker, Knowledge provider","Change cognitive demands, Change trust, Alter decision outcomes, Change affective-perceptual","Change AI responses, Update AI competence",recommendations,"corrective feedback, voting control","Visual, Interactive interface",Yes,Yes
2-3680,acm,Do People Engage Cognitively with AI? Impact of AI Assistance on Incidental Learning,"When people receive advice while making difficult decisions, they often make better decisions in the moment and also increase their knowledge in the process. However, such incidental learning can only occur when people cognitively engage with the information they receive and process this information thoughtfully. How do people process the information and advice they receive from AI, and do they engage with it deeply enough to enable learning? To answer these questions, we conducted three experiments in which individuals were asked to make nutritional decisions and received simulated AI recommendations and explanations. In the first experiment, we found that when people were presented with both a recommendation and an explanation before making their choice, they made better decisions than they did when they received no such help, but they did not learn. In the second experiment, participants first made their own choice, and only then saw a recommendation and an explanation from AI; this condition also resulted in improved decisions, but no learning. However, in our third experiment, participants were presented with just an AI explanation but no recommendation and had to arrive at their own decision. This condition led to both more accurate decisions and learning gains. We hypothesize that learning gains in this condition were due to deeper engagement with explanations needed to arrive at the decisions. This work provides some of the most direct evidence to date that it may not be sufficient to include explanations together with AI-generated recommendation to ensure that people engage carefully with the AI-provided information. This work also presents one technique that enables incidental learning and, by implication, can help people process AI recommendations and explanations more carefully.",10.1145/3490099.3511138,https://doi.org/10.1145/3490099.3511138,International Conference on Intelligent User Interfaces (IUI),"Gajos, Krzysztof Z.; Mamykina, Lena",2022,192,"@inproceedings{2-3680,
  title = {Do People Engage Cognitively with AI? Impact of AI Assistance on Incidental Learning},
  author = {Gajos, Krzysztof Z. and Mamykina, Lena},
  year = {2022},
  doi = {10.1145/3490099.3511138},
  booktitle = {International Conference on Intelligent User Interfaces (IUI)}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,"Explaining, Advising",Decision-maker,"Change cognitive demands, Change trust, Alter decision outcomes",no such info,"recommendations, textual explanations",NA,"Textual, Visual, Interactive interface",Yes,Yes
2-3681,acm,Selective Explanations: Leveraging Human Input to Align Explainable AI,"While a vast collection of explainable AI (XAI) algorithms has been developed in recent years, they have been criticized for significant gaps with how humans produce and consume explanations. As a result, current XAI techniques are often found to be hard to use and lack effectiveness. In this work, we attempt to close these gaps by making AI explanations selective —a fundamental property of human explanations—by selectively presenting a subset of model reasoning based on what aligns with the recipient's preferences. We propose a general framework for generating selective explanations by leveraging human input on a small dataset. This framework opens up a rich design space that accounts for different selectivity goals, types of input, and more. As a showcase, we use a decision-support task to explore selective explanations based on what the decision-maker would consider relevant to the decision task. We conducted two experimental studies to examine three paradigms based on our proposed framework: in Study 1, we ask the participants to provide critique-based or open-ended input to generate selective explanations (self-input). In Study 2, we show the participants selective explanations based on input from a panel of similar users (annotator input). Our experiments demonstrate the promise of selective explanations in reducing over-reliance on AI and improving collaborative decision making and subjective perceptions of the AI system, but also paint a nuanced picture that attributes some of these positive effects to the opportunity to provide one's own input to augment AI explanations. Overall, our work proposes a novel XAI framework inspired by human communication behaviors and demonstrates its potential to encourage future work to make AI explanations more human-compatible.",10.1145/3610206,https://doi.org/10.1145/3610206,Proceedings of the ACM on Human-Computer Interaction,"Lai, Vivian; Zhang, Yiming; Chen, Chacha; Liao, Q. Vera; Tan, Chenhao",2023,283,"@article{2-3681,
  title={Selective Explanations: Leveraging Human Input to Align Explainable AI},
  author={Lai, Vivian and Zhang, Yiming and Chen, Chacha and Liao, Q. Vera and Tan, Chenhao},
  year={2023},
  doi={10.1145/3610206},
  journal={Proceedings of the ACM on Human-Computer Interaction}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Individual,"Explaining, Advising","Decision-maker, Knowledge provider","Alter decision outcomes, Change cognitive demands, Change trust, Change affective-perceptual","Change AI responses, Update AI competence","original explanations, selective explanations with open-ended input (Open-ended), selective explanations with model explanation critiques (Critique-based)","annotation, evaluation",Interactive interface,Yes,Yes
2-36814,springernature,An ethical trajectory planning algorithm for autonomous vehicles,"With the rise of artificial intelligence and automation, moral decisions that were formerly the preserve of humans are being put into the hands of algorithms. In autonomous driving, a variety of such decisions with ethical implications are made by algorithms for behaviour and trajectory planning. Therefore, here we present an ethical trajectory planning algorithm with a framework that aims at a fair distribution of risk among road users. Our implementation incorporates a combination of five ethical principles: minimization of the overall risk, priority for the worst-off, equal treatment of people, responsibility and maximum acceptable risk. To the best of our knowledge, this is the first ethical algorithm for trajectory planning of autonomous vehicles in line with the 20 recommendations from the European Union Commission expert group and with general applicability to various traffic situations. We showcase the ethical behaviour of our algorithm in selected scenarios and provide an empirical analysis of the ethical principles in 2, 000 scenarios. The code used in this research is available as open-source software. In situations where some risk of injury is unavoidable for self-driving vehicles, how risk is distributed becomes an ethical question. Geisslinger and colleagues have developed a planning algorithm that takes five ethical principles into account and aims to comply with the emerging EU regulatory recommendations.",10.1038/s42256-022-00607-z,http://dx.doi.org/10.1038/s42256-022-00607-z,Nature Machine Intelligence,"Geisslinger, Maximilian;Poszler, Franziska;Lienkamp, Markus",2023,1,"@article{2-36814,
  title={An ethical trajectory planning algorithm for autonomous vehicles},
  author={Geisslinger, Maximilian and Poszler, Franziska and Lienkamp, Markus},
  year={2023},
  doi={10.1038/s42256-022-00607-z},
  journal={Nature Machine Intelligence}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Forecasting, Explaining, Executing",Guardian,NA,NA,NA,NA,NA,Yes,No
2-3682,acm,Capable but Amoral? Comparing AI and Human Expert Collaboration in Ethical Decision Making,"While artificial intelligence (AI) is increasingly applied for decision-making processes, ethical decisions pose challenges for AI applications. Given that humans cannot always agree on the right thing to do, how would ethical decision-making by AI systems be perceived and how would responsibility be ascribed in human-AI collaboration? In this study, we investigate how the expert type (human vs. AI) and level of expert autonomy (adviser vs. decider) influence trust, perceived responsibility, and reliance. We find that participants consider humans to be more morally trustworthy but less capable than their AI equivalent. This shows in participants’ reliance on AI: AI recommendations and decisions are accepted more often than the human expert’s. However, AI team experts are perceived to be less responsible than humans, while programmers and sellers of AI systems are deemed partially responsible instead.",10.1145/3491102.3517732,https://doi.org/10.1145/3491102.3517732,ACM CHI Conference on Human Factors in Computing Systems,"Tolmeijer, Suzanne; Christen, Markus; Kandul, Serhiy; Kneer, Markus; Bernstein, Abraham",2022,0,"@inproceedings{2-3682,
  title = {Capable but Amoral? Comparing AI and Human Expert Collaboration in Ethical Decision Making},
  author = {Tolmeijer, Suzanne and Christen, Markus and Kandul, Serhiy and Kneer, Markus and Bernstein, Abraham},
  year = {2022},
  doi = {10.1145/3491102.3517732},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance, Environment / Resources / Energy",no such info,"Advising, Executing, Collaborating","Developer, Knowledge provider, Decision-subject, Decision-maker","Alter decision outcomes, Change trust, Shape ethical norms, Shift responsibility",no such info,recommendations,domain knowledge,Interactive interface,Yes,Yes
2-3683,acm,Dealing with Uncertainty: Understanding the Impact of Prognostic Versus Diagnostic Tasks on Trust and Reliance in Human-AI Decision Making,"While existing literature has explored and revealed several insights pertaining to the role of human factors (e.g., prior experience, domain knowledge) and attributes of AI systems (e.g., accuracy, trustworthiness), there is a limited understanding around how the important task characteristics of complexity and uncertainty shape human decision-making and human-AI team performance. In this work, we aim to address this research and empirical gap by systematically exploring how task complexity and uncertainty influence human-AI decision-making. Task complexity refers to the load of information associated with a task, while task uncertainty refers to the level of unpredictability associated with the outcome of a task. We conducted a between-subjects user study (N = 258) in the context of a trip-planning task to investigate the impact of task complexity and uncertainty on human trust and reliance on AI systems. Our results revealed that task complexity and uncertainty have a significant impact on user reliance on AI systems. When presented with complex and uncertain tasks, users tended to rely more on AI systems while demonstrating lower levels of appropriate reliance compared to tasks that were less complex and uncertain. In contrast, we found that user trust in the AI systems was not influenced by task complexity and uncertainty. Our findings can help inform the future design of empirical studies exploring human-AI decision-making. Insights from this work can inform the design of AI systems and interventions that are better aligned with the challenges posed by complex and uncertain tasks. Finally, the lens of diagnostic versus prognostic tasks can inspire the operationalization of uncertainty in human-AI decision-making studies.",10.1145/3613904.3641905,https://doi.org/10.1145/3613904.3641905,CHI Conference on Human Factors in Computing Systems,"Salimzadeh, Sara; He, Gaole; Gadiraju, Ujwal",2024,0,"@inproceedings{2-3683,
  title = {Dealing with Uncertainty: Understanding the Impact of Prognostic Versus Diagnostic Tasks on Trust and Reliance in Human-AI Decision Making},
  author = {Salimzadeh, Sara and He, Gaole and Gadiraju, Ujwal},
  year = {2024},
  doi = {10.1145/3613904.3641905},
  booktitle = {CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Transportation / Mobility / Planning",Individual,"Analyzing, Forecasting, Advising",Decision-maker,"Change trust, Alter decision outcomes, Change cognitive demands",no such info,"recommendations, decision suggestions",NA,Interactive interface,Yes,Yes
2-3684,acm,On the Importance of User Backgrounds and Impressions: Lessons Learned from Interactive AI Applications,"While EXplainable Artificial Intelligence (XAI) approaches aim to improve human-AI collaborative decision-making by improving model transparency and mental model formations, experiential factors associated with human users can cause challenges in ways system designers do not anticipate. In this article, we first showcase a user study on how anchoring bias can potentially affect mental model formations when users initially interact with an intelligent system and the role of explanations in addressing this bias. Using a video activity recognition tool in cooking domain, we asked participants to verify whether a set of kitchen policies are being followed, with each policy focusing on a weakness or a strength. We controlled the order of the policies and the presence of explanations to test our hypotheses. Our main finding shows that those who observed system strengths early on were more prone to automation bias and made significantly more errors due to positive first impressions of the system, while they built a more accurate mental model of the system competencies. However, those who encountered weaknesses earlier made significantly fewer errors, since they tended to rely more on themselves, while they also underestimated model competencies due to having a more negative first impression of the model. Motivated by these findings and similar existing work, we formalize and present a conceptual model of user’s past experiences that examine the relations between user’s backgrounds, experiences, and human factors in XAI systems based on usage time. Our work presents strong findings and implications, aiming to raise the awareness of AI designers toward biases associated with user impressions and backgrounds.",10.1145/3531066,https://doi.org/10.1145/3531066,ACM Transactions on Interactive Intelligent Systems,"Nourani, Mahsan; Roy, Chiradeep; Block, Jeremy E.; Honeycutt, Donald R.; Rahman, Tahrima; Ragan, Eric D.; Gogate, Vibhav",2022,1,"@article{2-3684,
  title = {On the Importance of User Backgrounds and Impressions: Lessons Learned from Interactive AI Applications},
  author = {Nourani, Mahsan and Roy, Chiradeep and Block, Jeremy E. and Honeycutt, Donald R. and Rahman, Tahrima and Ragan, Eric D. and Gogate, Vibhav},
  year = {2022},
  doi = {10.1145/3531066},
  journal = {ACM Transactions on Interactive Intelligent Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Explaining, Collaborating, Advising","Decision-maker, Developer","Alter decision outcomes, Change cognitive demands, Change affective-perceptual",no such info,"ground labels, post hoc explanations",anchoring bias,Interactive interface,Yes,Yes
2-3688,acm,A Dynamic Decision-Making Framework Promoting Long-Term Fairness,"With AI-based decisions playing an increasingly consequential role in our society, for example, in our financial and criminal justice systems, there is a great deal of interest in designing algorithms conforming to application-specific notions of fairness. In this work, we ask a complementary question: can AI-based decisions be designed to dynamically influence the evolution of fairness in our society over the long term? To explore this question, we propose a framework for sequential decision-making aimed at dynamically influencing long-term societal fairness, illustrated via the problem of selecting applicants from a pool consisting of two groups, one of which is under-represented. We consider a dynamic model for the composition of the applicant pool, in which admission of more applicants from a group in a given selection round positively reinforces more candidates from the group to participate in future selection rounds. Under such a model, we show the efficacy of the proposed Fair-Greedy selection policy which systematically trades the sum of the scores of the selected applicants (""greedy”) against the deviation of the proportion of selected applicants belonging to a given group from a target proportion (""fair”). In addition to experimenting on synthetic data, we adapt static real-world datasets on law school candidates and credit lending to simulate the dynamics of the composition of the applicant pool. We prove that the applicant pool composition converges to a target proportion set by the decision-maker when score distributions across the groups are identical.",10.1145/3514094.3534127,https://doi.org/10.1145/3514094.3534127,"AAAI/ACM Conference on AI, Ethics, and Society","Puranik, Bhagyashree; Madhow, Upamanyu; Pedarsani, Ramtin",2022,15,"@inproceedings{2-3688,
  title     = {A Dynamic Decision-Making Framework Promoting Long-Term Fairness},
  author    = {Puranik, Bhagyashree and Madhow, Upamanyu and Pedarsani, Ramtin},
  year      = {2022},
  doi       = {10.1145/3514094.3534127},
  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy, Law / Policy / Governance",Institutional,"Advising, Executing","Decision-subject, Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-3690,acm,Hierarchical Multi-agent Model for Reinforced Medical Resource Allocation with Imperfect Information,"With the advent of the COVID-19 pandemic, the shortage in medical resources became increasingly more evident. Therefore, efficient strategies for medical resource allocation are urgently needed. However, conventional rule-based methods employed by public health experts have limited capability in dealing with the complex and dynamic pandemic-spreading situation. In addition, model-based optimization methods such as dynamic programming (DP) fail to work since we cannot obtain a precise model in real-world situations most of the time. Model-free reinforcement learning (RL) is a powerful tool for decision-making; however, three key challenges exist in solving this problem via RL: (1) complex situations and countless choices for decision-making in the real world; (2) imperfect information due to the latency of pandemic spreading; and (3) limitations on conducting experiments in the real world since we cannot set up pandemic outbreaks arbitrarily. In this article, we propose a hierarchical RL framework with several specially designed components. We design a decomposed action space with a corresponding training algorithm to deal with the countless choices, ensuring efficient and real-time strategies. We design a recurrent neural network–based framework to utilize the imperfect information obtained from the environment. We also design a multi-agent voting method, which modifies the decision-making process considering the randomness during model training and, thus, improves the performance. We build a pandemic-spreading simulator based on real-world data, serving as the experimental platform. We then conduct extensive experiments. The results show that our method outperforms all baselines, which reduces infections and deaths by 14.25% on average without the multi-agent voting method and up to 15.44% with it.",10.1145/3552436,https://doi.org/10.1145/3552436,ACM Transactions on Intelligent Systems and Technology,"Hao, Qianyue; Xu, Fengli; Chen, Lin; Hui, Pan; Li, Yong",2022,12,"@article{2-3690,
  title={Hierarchical Multi-agent Model for Reinforced Medical Resource Allocation with Imperfect Information},
  author={Hao, Qianyue and Xu, Fengli and Chen, Lin and Hui, Pan and Li, Yong},
  year={2022},
  doi={10.1145/3552436},
  journal={ACM Transactions on Intelligent Systems and Technology}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,"Executing, Advising","Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-3696,acm,When in Doubt! Understanding the Role of Task Characteristics on Peer Decision-Making with AI Assistance,"With the integration of AI systems into our daily lives, human-AI collaboration has become increasingly prevalent. Prior work in this realm has primarily explored the effectiveness and performance of individual human and AI systems in collaborative tasks. While much of decision-making occurs within human peers and groups in the real world, there is a limited understanding of how they collaborate with AI systems. One of the key predictors of human-AI collaboration is the characteristics of the task at hand. Understanding the influence of task characteristics on human-AI collaboration is crucial for enhancing team performance and developing effective strategies for collaboration. Addressing a research and empirical gap, we seek to explore how the features of a task impact decision-making within human-AI group settings. In a 2 × 2 between-subjects study (N = 256) we examine the effects of task complexity and uncertainty on group performance and behaviour. The participants were grouped into pairs and assigned to one of four experimental conditions characterized by varying degrees of complexity and uncertainty. We found that high task complexity and high task uncertainty can negatively impact the performance of human-AI groups, leading to decreased group accuracy and increased disagreement with the AI system. We found that higher task complexity led to a higher efficiency in decision-making, while a higher task uncertainty had a negative impact on efficiency. Our findings highlight the importance of considering task characteristics when designing human-AI collaborative systems, as well as the future design of empirical studies exploring human-AI collaboration.",10.1145/3627043.3659567,https://doi.org/10.1145/3627043.3659567,"ACM Conference on User Modeling, Adaptation and Personalization (UMAP)","Salimzadeh, Sara; Gadiraju, Ujwal",2024,10,"@inproceedings{2-3696,
  title = {When in Doubt! Understanding the Role of Task Characteristics on Peer Decision-Making with AI Assistance},
  author = {Salimzadeh, Sara and Gadiraju, Ujwal},
  year = {2024},
  doi = {10.1145/3627043.3659567},
  booktitle = {Proceedings of the ACM Conference on User Modeling, Adaptation and Personalization (UMAP)}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Analyzing, Collaborating, Advising",Decision-maker,"Alter decision outcomes, Change affective-perceptual",Update AI competence,"recommendations, incorrect advice",collaborative consensus-building,"Textual, Interactive interface",Yes,Yes
2-3697,acm,Are Two Heads Better Than One in AI-Assisted Decision Making? Comparing the Behavior and Performance of Groups and Individuals in Human-AI Collaborative Recidivism Risk Assessment,"With the prevalence of AI assistance in decision making, a more relevant question to ask than the classical question of “are two heads better than one?’’ is how groups’ behavior and performance in AI-assisted decision making compare with those of individuals’. In this paper, we conduct a case study to compare groups and individuals in human-AI collaborative recidivism risk assessment along six aspects, including decision accuracy and confidence, appropriateness of reliance on AI, understanding of AI, decision-making fairness, and willingness to take accountability. Our results highlight that compared to individuals, groups rely on AI models more regardless of their correctness, but they are more confident when they overturn incorrect AI recommendations. We also find that groups make fairer decisions than individuals according to the accuracy equality criterion, and groups are willing to give AI more credit when they make correct decisions. We conclude by discussing the implications of our work.",10.1145/3544548.3581015,https://doi.org/10.1145/3544548.3581015,ACM CHI Conference on Human Factors in Computing Systems,"Chiang, Chun-Wei; Lu, Zhuoran; Li, Zhuoyan; Yin, Ming",2023,1,"@inproceedings{2-3697,
  title = {Are Two Heads Better Than One in AI-Assisted Decision Making? Comparing the Behavior and Performance of Groups and Individuals in Human-AI Collaborative Recidivism Risk Assessment},
  author = {Chiang, Chun-Wei and Lu, Zhuoran and Li, Zhuoyan and Yin, Ming},
  year = {2023},
  doi = {10.1145/3544548.3581015},
  booktitle = {Proceedings of the ACM CHI Conference on Human Factors in Computing Systems}
}",Empirical contributions,Law / Policy / Governance,Institutional,"Advising, Forecasting, Collaborating","Decision-maker, Decision-subject","Shape ethical norms, Alter decision outcomes, Change affective-perceptual, Shift responsibility",no such info,"recommendations, prediction of alternative",NA,Interactive interface,Yes,Yes
2-372,aaai,Dissenting Explanations: Leveraging Disagreement to Reduce Model Overreliance,"While modern explanation methods have been shown to be inconsistent and contradictory, the explainability of black-box models nevertheless remains desirable. When the role of explanations extends from understanding models to aiding decision making, the semantics of explanations is not always fully understood – to what extent do explanations ``explain” a decision and to what extent do they merely advocate for a decision? Can we help humans gain insights from explanations accompanying correct predictions and not over-rely on incorrect predictions advocated for by explanations? With this perspective in mind, we introduce the notion of dissenting explanations: conflicting predictions with accompanying explanations. We first explore the advantage of dissenting explanations in the setting of model multiplicity, where multiple models with similar performance may have different predictions. Through a human study on the task of identifying deceptive reviews, we demonstrate that dissenting explanations reduce overreliance on model predictions, without reducing overall accuracy. Motivated by the utility of dissenting explanations we present both global and local methods for their generation.",10.1609/aaai.v38i19.30151,https://ojs.aaai.org/index.php/AAAI/article/view/30151,AAAI Conference on Artificial Intelligence,Omer Reingold;Judy Hanwen Shen;Aditi Talati,2024,9,"@inproceedings{2-372,
  title     = {Dissenting Explanations: Leveraging Disagreement to Reduce Model Overreliance},
  author    = {Omer Reingold and Judy Hanwen Shen and Aditi Talati},
  year      = {2024},
  doi       = {10.1609/aaai.v38i19.30151},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Operational,"Explaining, Advising",Decision-maker,"Change trust, Alter decision outcomes",no such info,"dissenting explanations, prediction of alternative",NA,Textual,Yes,Yes
2-3726,acm,The Shortest Path to Ethics in AI: An Integrated Assignment Where Human Concerns Guide Technical Decisions,"How can we teach AI students to use human concerns to guide their technical decisions? We created an AI assignment with a human context, asking students to find the safest path rather than the shortest path. This integrated assignment evaluated 120 students’ understanding of the limitations and assumptions of standard graph search algorithms, and required students to consider human impacts to propose appropriate modifications. Since the assignment focused on algorithm selection and modification, it provided the instructor with a different perspective on student understanding (compared with questions on algorithm execution). Specifically, many students: tried to solve a bottleneck problem with algorithms designed for accumulation problems, did not distinguish between calculations that could be done during the incremental construction of a path versus ones that required knowledge of the full path, and, when proposing modifications to a standard algorithm, did not present the full technical details necessary to implement their ideas. We created rubrics to analyze students’ responses. Our rubrics cover three dimensions: technical AI knowledge, consideration of human factors, and the integration of technical decisions as they align with the human context. These rubrics demonstrate how students’ skills can vary along each dimension, and also provide a template for scoring integrated assignments for other CS topics. Overall, this work demonstrates how to integrate human concerns with technical content in a way that deepens technical rigor and supports instructor pedagogical content knowledge.",10.1145/3501385.3543978,https://doi.org/10.1145/3501385.3543978,ACM Conference on International Computing Education Research (ICER),"Brown, Noelle; South, Koriann; Wiese, Eliane S.",2022,10,"@inproceedings{2-3726,
  title     = {The Shortest Path to Ethics in AI: An Integrated Assignment Where Human Concerns Guide Technical Decisions},
  author    = {Brown, Noelle and South, Koriann and Wiese, Eliane S.},
  year      = {2022},
  doi       = {10.1145/3501385.3543978},
  booktitle = {ACM Conference on International Computing Education Research (ICER)}
}",Empirical contributions,Education / Teaching / Research,no such info,"Explaining, Analyzing",Decision-maker,no such info,no such info,NA,"human value incorporation, human-impact-informed algorithm design","Textual, Interactive interface",Yes,Yes
2-3731,acm,Costs and Benefits of Fair Representation Learning,"Machine learning algorithms are increasingly used to make or support important decisions about people's lives. This has led to interest in the problem of fair classification, which involves learning to make decisions that are non-discriminatory with respect to a sensitive variable such as race or gender. Several methods have been proposed to solve this problem, including fair representation learning, which cleans the input data used by the algorithm to remove information about the sensitive variable. We show that using fair representation learning as an intermediate step in fair classification incurs a cost compared to directly solving the problem, which we refer to as the cost of mistrust. We show that fair representation learning in fact addresses a different problem, which is of interest when the data user is not trusted to access the sensitive variable. We quantify the benefits of fair representation learning, by showing that any subsequent use of the cleaned data will not be too unfair. The benefits we identify result from restricting the decisions of adversarial data users, while the costs are due to applying those same restrictions to other data users.",10.1145/3306618.3317964,https://doi.org/10.1145/3306618.3317964,"AAAI/ACM Conference on AI, Ethics, and Society","McNamara, Daniel; Ong, Cheng Soon; Williamson, Robert C.",2019,77,"@inproceedings{2-3731,
  title     = {Costs and Benefits of Fair Representation Learning},
  author    = {McNamara, Daniel and Ong, Cheng Soon and Williamson, Robert C.},
  year      = {2019},
  doi       = {10.1145/3306618.3317964},
  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Operational,"Advising, Analyzing, Forecasting","Decision-maker, Guardian, Developer",NA,NA,NA,NA,NA,Yes,No
2-3744,acm,PanelNet: A Novel Deep Neural Network for Predicting Collective Diagnostic Ratings by a Panel of Radiologists for Pulmonary Nodules,"Reducing misdiagnosis rate is a central concern in modern medicine. In clinical practice, group-based collective diagnosis is frequently exercised to curb the misdiagnosis rate. However, little effort has been dedicated to emulating the collective intelligence behind the group-based decision making practice in computer-aided diagnosis research to this day. To fill the overlooked gap, this study introduces a novel deep neural network, titled PanelNet, that is able to computationally model and reproduce the aforesaid collective diagnosis capability demonstrated by a group of medical experts. To experimentally explore the validity of the new solution, we apply the proposed PanelNet to one of the key tasks in radiology—assessing malignant ratings of pulmonary nodules. For each nodule and a given panel, PanelNet is able to predict statistical distribution of malignant ratings collectively judged by the panel of radiologists. Extensive experimental results consistently demonstrate PanelNet outperforms multiple state-of-the-art computer-aided diagnosis methods applicable to the collective diagnostic task. To our best knowledge, no other collective computer-aided diagnosis method grounded on modern machine learning technologies has been previously proposed. By its design, PanelNet can also be easily applied to model collective diagnosis processes employed for other diseases.",10.1145/3394171.3413735,https://doi.org/10.1145/3394171.3413735,ACM International Conference on Multimedia,"Zhang, Chunyan; Xu, Songhua; Li, Zongfang",2020,1,"@inproceedings{2-3744,
  title = {PanelNet: A Novel Deep Neural Network for Predicting Collective Diagnostic Ratings by a Panel of Radiologists for Pulmonary Nodules},
  author = {Zhang, Chunyan and Xu, Songhua and Li, Zongfang},
  year = {2020},
  doi = {10.1145/3394171.3413735},
  booktitle = {Proceedings of the ACM International Conference on Multimedia}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Analyzing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-3748,acm,Retention is All You Need,"Skilled employees are the most important pillars of an organization. Despite this, most organizations face high attrition and turnover rates. While several machine learning models have been developed to analyze attrition and its causal factors, the interpretations of those models remain opaque. In this paper, we propose the HR-DSS approach, which stands for Human Resource (HR) Decision Support System, and uses explainable AI for employee attrition problems. The system is designed to assist HR departments in interpreting the predictions provided by machine learning models. In our experiments, we employ eight machine learning models to provide predictions. We further process the results achieved by the best-performing model by the SHAP explainability process and use the SHAP values to generate natural language explanations which can be valuable for HR. Furthermore, using ""What-if-analysis"", we aim to observe plausible causes for attrition of an individual employee. The results show that by adjusting the specific dominant features of each individual, employee attrition can turn into employee retention through informative business decisions.",10.1145/3583780.3615497,https://doi.org/10.1145/3583780.3615497,ACM International Conference on Information and Knowledge Management (CIKM),"Mohiuddin, Karishma; Alam, Mirza Ariful; Alam, Mirza Mohtashim; Welke, Pascal; Martin, Michael; Lehmann, Jens; Vahdati, Sahar",2023,18,"@inproceedings{2-3748,
  title={Retention is All You Need},
  author={Mohiuddin, Karishma and Alam, Mirza Ariful and Alam, Mirza Mohtashim and Welke, Pascal and Martin, Michael and Lehmann, Jens and Vahdati, Sahar},
  year={2023},
  doi={10.1145/3583780.3615497},
  booktitle={Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM)}
}",System/Artifact contributions,Everyday / Employment / Public Service,Organizational,"Explaining, Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-3765,acm,Transparency in the Wild: Navigating Transparency in a Deployed AI System to Broaden Need-Finding Approaches,"Transparency is a critical component when building artificial intelligence (AI) decision-support tools, especially for contexts in which AI outputs impact people or policy. Effectively identifying and addressing user transparency needs in practice remains a challenge. While a number of guidelines and processes for identifying transparency needs have emerged, existing methods tend to approach need-finding with a limited focus that centers around a narrow set of stakeholders and transparency techniques. To broaden this perspective, we employ numerous need-finding methods to investigate transparency mechanisms in a widely deployed AI-decision support tool developed by a wildlife conservation non-profit. Throughout our 5-month case study, we conducted need-finding through semi-structured interviews with end-users, analysis of the tool’s community forum, experiments with their ML model, and analysis of training documents created by end-users. We also held regular meetings with the tool’s product and machine learning teams. By approaching transparency need-finding from a broad lens, we uncover insights into end-users’ transparency needs as well as unexpected uses and challenges with current transparency mechanisms. Our study is one of the first to incorporate such diverse perspectives to reveal an unbiased and rich view of transparency needs. Lastly, we offer the FAccT community recommendations on broadening transparency need-finding approaches, contributing to the evolving field of transparency research.",10.1145/3630106.3658985,https://doi.org/10.1145/3630106.3658985,"ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)","Turri, Violet; Morrison, Katelyn; Robinson, Katherine-Marie; Abidi, Collin; Perer, Adam; Forlizzi, Jodi; Dzombak, Rachel",2024,14,"@inproceedings{2-3765,
  title = {Transparency in the Wild: Navigating Transparency in a Deployed AI System to Broaden Need-Finding Approaches},
  author = {Turri, Violet and Morrison, Katelyn and Robinson, Katherine-Marie and Abidi, Collin and Perer, Adam and Forlizzi, Jodi and Dzombak, Rachel},
  year = {2024},
  booktitle = {Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)},
  doi = {10.1145/3630106.3658985}
}",Empirical contributions,Environment / Resources / Energy,Operational,"Advising, Analyzing","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-3772,acm,Looper: An End-to-End ML Platform for Product Decisions,"Modern software systems and products increasingly rely on machine learning models to make data-driven decisions based on interactions with users, infrastructure and other systems. For broader adoption, this practice must (i) accommodate product engineers without ML backgrounds, (ii) support finegrain product-metric evaluation and (iii) optimize for product goals. To address shortcomings of prior platforms, we introduce general principles for and the architecture of an ML platform, Looper, with simple APIs for decision-making and feedback collection. Looper covers the end-to-end ML lifecycle from collecting training data and model training to deployment and inference, and extends support to personalization, causal evaluation with heterogenous treatment effects, and Bayesian tuning for product goals. During the 2021 production deployment, Looper simultaneously hosted 440-1,000 ML models that made 4-6 million real-time decisions per second. We sum up experiences of platform adopters and describe their learning curve.",10.1145/3534678.3539059,https://doi.org/10.1145/3534678.3539059,ACM SIGKDD Conference on Knowledge Discovery and Data Mining,"Markov, Igor L.; Wang, Hanson; Kasturi, Nitya S.; Singh, Shaun; Garrard, Mia R.; Huang, Yin; Yuen, Sze Wai Celeste; Tran, Sarah; Wang, Zehui; Glotov, Igor; Gupta, Tanvi; Chen, Peng; Huang, Boshuang; Xie, Xiaowen; Belkin, Michael; Uryasev, Sal; Howie, Sam; Bakshy, Eytan; Zhou, Norm",2022,0,"@inproceedings{2-3772,
  title = {Looper: An End-to-End ML Platform for Product Decisions},
  author = {Markov, Igor L. and Wang, Hanson and Kasturi, Nitya S. and Singh, Shaun and Garrard, Mia R. and Huang, Yin and Yuen, Sze Wai Celeste and Tran, Sarah and Wang, Zehui and Glotov, Igor and Gupta, Tanvi and Chen, Peng and Huang, Boshuang and Xie, Xiaowen and Belkin, Michael and Uryasev, Sal and Howie, Sam and Bakshy, Eytan and Zhou, Norm},
  year = {2022},
  doi = {10.1145/3534678.3539059},
  booktitle = {Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining}
}",System/Artifact contributions,"Generic / Abstract / Domain-agnostic, Software / Systems / Security",Operational,Executing,Developer,NA,NA,NA,NA,NA,Yes,No
2-38192,taylor-and-francis,“Inventor’s bias” at work: when low-performing algorithms seem fair,"article introduces the “Inventor’s Bias Effect,” the propensity for inventors to be over-optimistic about the positive features and uses of the products they create. We explore this phenomenon in the context of decision-making algorithms by conducting two online studies (N = 1001) where subjects were asked to either create or evaluate an AI-based tool that can automate human resource decisions in an organization. Study 1 revealed that individuals in the role of inventor perceived a low-performing algorithm they created as fairer relative to the ratings of other stakeholders (CEOs, employees, and the general public). The tendency for these “inventors” to personally identify with the products they created mediated this effect. Study 2 showed that inventors’ perceptions of fairness of the algorithms they created translated into an increased desire for the organization to continue using their product, even though it was inaccurate for a third of all decisions. This research demonstrates how stakeholders’ relations to algorithms may encourage biased decision making and highlights the need for caution in organizational and political decision-making processes.",10.1080/10447318.2023.2224954,https://www.tandfonline.com/doi/full/10.1080/10447318.2023.2224954,International Journal of Human–Computer Interaction,Maya J. Cratsley ; Nathanael J. Fast,2024,4,"@article{2-38192,
  title = {Inventor’s bias at work: when low-performing algorithms seem fair},
  author = {Maya J. Cratsley and Nathanael J. Fast},
  year = {2024},
  doi = {10.1080/10447318.2023.2224954},
  journal = {International Journal of Human–Computer Interaction}
}",Empirical contributions,Everyday / Employment / Public Service,Institutional,Executing,"Guardian, Stakeholder","Shape ethical norms, Change cognitive demands",no such info,"procedural fairness, system accuracy, identity cues",incentivized beliefs,Textual,Yes,Yes
2-38193,taylor-and-francis,(Over)trusting ai recommendations: how system and person variables affect dimensions of complacency,"AI systems can lead to complacency and decision errors. However, human and system variables may affect complacency and it is important to understand their interplay for HCI. In our experiment, 90 participants were confronted with traffic route problems guided by AI recommendations and thereby assigned to either a transparent system providing reasons for recommendations or a non-transparent system. We found transparent systems to lower the potential to alleviate workload (albeit not to neglect monitoring), but to simultaneously foster actual complacent behavior. On the contrary, we found performance expectancy to foster the potential to alleviate workload, but not complacent behavior. Interaction analyses showed that effects of performance expectancy depend on system transparency. This contributes to our understanding how system- and person-related variables interact in affecting complacency and stresses the differences between dimensions of complacency and the need for carefully considering transparency and performance expectancy in AI research and design.",10.1080/10447318.2023.2301250,https://www.tandfonline.com/doi/full/10.1080/10447318.2023.2301250,International Journal of Human–Computer Interaction,Lydia Harbarth;Eva Gößwein;Daniel Bodemer ; Lenka Schnaubert,2025,31,"@article{2-38193,
  title = {(Over)trusting AI Recommendations: How System and Person Variables Affect Dimensions of Complacency},
  author = {Lydia Harbarth and Eva Gößwein and Daniel Bodemer and Lenka Schnaubert},
  year = {2025},
  doi = {10.1080/10447318.2023.2301250},
  journal = {International Journal of Human–Computer Interaction}
}",Empirical contributions,Transportation / Mobility / Planning,Individual,Advising,Decision-maker,"Change trust, Change cognitive demands, Alter decision outcomes",Update AI competence,"reasoning, textual explanations",performance expectancy,NA,Yes,Yes
2-38195,taylor-and-francis,A b2b flexible pricing decision support system for managing the request for quotation process under e-commerce business environment,"the era of digitalisation, e-commerce retail sites have become decisive channels for reaching millions of potential customers worldwide. Digital marketing strategies are formulated by the marketing teams in order to increase the traffic on their e-commerce sites, thereby boosting the sales of the products. With the massive amount of data available from the cloud, which were conventionally made with a high degree of intuition based on decision makers’ knowledge and experience, can now be supported with the application of artificial intelligence techniques. This paper introduces a novel approach in applying the fuzzy association rule mining approach and the fuzzy logic technique, for discovering the factors influencing the pricing decision of products launched in e-commerce retail site, and in formulating flexible, dynamic pricing strategies for each product launched in an e-commerce site. A pricing decision support system for B2B e-commerce retail businesses, namely Smart-Quo, is developed and implemented in a Hong Kong-based B2B e-commerce retail company. A six-month pilot run reveals a significant improvement in terms of the efficiency and effectiveness in making pricing decisions on each product. The case study demonstrates the feasibility and potential benefits of applying artificial intelligence techniques in marketing management in today’s digital age.",10.1080/00207543.2019.1566674,https://www.tandfonline.com/doi/full/10.1080/00207543.2019.1566674,International Journal of Production Research,K.H. Leung;C.C. Luk;K.L. Choy;H.Y. Lam ; Carman K.M. Lee,2019,180,"@article{2-38195,
  title={A b2b flexible pricing decision support system for managing the request for quotation process under e-commerce business environment},
  author={Leung, K. H. and Luk, C. C. and Choy, K. L. and Lam, H. Y. and Lee, Carman K. M.},
  year={2019},
  journal={International Journal of Production Research},
  doi={10.1080/00207543.2019.1566674}
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Analyzing, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-38197,taylor-and-francis,A bayesian machine learning approach for optimizing dynamic treatment regimes,"Medical therapy often consists of multiple stages, with a treatment chosen by the physician at each stage based on the patient’s history of treatments and clinical outcomes. These decisions can be formalized as a dynamic treatment regime. This article describes a new approach for optimizing dynamic treatment regimes, which bridges the gap between Bayesian inference and existing approaches, like Q-learning. The proposed approach fits a series of Bayesian regression models, one for each stage, in reverse sequential order. Each model uses as a response variable the remaining payoff assuming optimal actions are taken at subsequent stages, and as covariates the current history and relevant actions at that stage. The key difficulty is that the optimal decision rules at subsequent stages are unknown, and even if these decision rules were known the relevant response variables may be counterfactual. However, posterior distributions can be derived from the previously fitted regression models for the optimal decision rules and the counterfactual response variables under a particular set of rules. The proposed approach averages over these posterior distributions when fitting each regression model. An efficient sampling algorithm for estimation is presented, along with simulation studies that compare the proposed approach with Q-learning. Supplementary materials for this article are available online.",10.1080/01621459.2017.1340887,https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1340887,Journal of the American Statistical Association,Thomas A. Murray;Ying Yuan ; Peter F. Thall,2018,71,"@article{2-38197,
  title = {A Bayesian Machine Learning Approach for Optimizing Dynamic Treatment Regimes},
  author = {Thomas A. Murray and Ying Yuan and Peter F. Thall},
  year = {2018},
  doi = {10.1080/01621459.2017.1340887},
  journal = {Journal of the American Statistical Association}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Executing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-38201,taylor-and-francis,A context-aware real-time human-robot collaborating reinforcement learning-based disassembly planning model under uncertainty,"we present a real-time multi-agent deep reinforcement learning model as a disassembly planning framework for human–robot collaboration. This disassembly plan optimises sequences to minimise operation time and the disassembling costs of end-of-life (EoL) products. Combining different data-driven decision-making tools, the plan aims to handle the complexities and uncertainties of disassembly tasks. Based on the physical features and geometric limitations of EoL product components, we calculate product disassembly difficulty scores. Subsequently, the deep reinforcement learning model integrates these scores into planning process. The model allocates tasks in real time according to the online conditions of the human operator, cobot, and product, enabling the model to cope with uncertainties that may change the process routine. We also present different scenarios wherein a cobot collaborates with human operators with different skill levels. To evaluate model performance, we compare it with baseline models in terms of the convergence time and incorporated disassembly features. The analysis indicates that our model converges three times faster than a baseline model applied to the same case study. Moreover, our model includes more features of the disassembly problem in its decision-making process than any other baseline model.",10.1080/00207543.2023.2252526,https://www.tandfonline.com/doi/full/10.1080/00207543.2023.2252526,International Journal of Production Research,Ashkan Amirnia ; Samira Keivanpour,2024,27,"@article{2-38201,
  title = {A context-aware real-time human-robot collaborating reinforcement learning-based disassembly planning model under uncertainty},
  author = {Ashkan Amirnia and Samira Keivanpour},
  year = {2024},
  doi = {10.1080/00207543.2023.2252526},
  journal = {International Journal of Production Research}
}",Algorithmic contributions,Manufacturing / Industry / Automation,Operational,"Analyzing, Collaborating","Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-38214,taylor-and-francis,A knowledge-guided process planning approach with reinforcement learning,"With the widespread application of computer-aided technologies such as CAD and CAM in the manufacturing industry, a growing number of process documents and design documents generate multi-source process knowledge and expert experience. However, due to the diverse and complex representation of process knowledge, there is a need for more effective methods to mine a large amount of multi-source information and to exploit the explicit and implicit relationships between the knowledge contained in process knowledge. Effective knowledge reuse in process planning still needs to be improved. This paper proposes a reinforcement learning approach that combines knowledge graphs and process decision-making activities in process planning to exploit the learning potential of process knowledge graphs. Firstly, a reinforcement learning environment for process planning is introduced to model the process planning decision-making phase as a sequential recommendation of process knowledge. Then, this paper designs in detail the state representation method that combines process decision sequences and potential relationships between processes. This paper also creates the composite reward function that combines the process planning environment. In addition, a new algorithm is proposed for learning the proposed model more efficiently. Experimental results show that the network structure has more accurate recommendation results than other methods.",10.1080/09544828.2024.2415831,https://www.tandfonline.com/doi/full/10.1080/09544828.2024.2415831,Journal of Engineering Design,Lijun Zhang;Hongjin Wu;Yelin Chen;Xuesong Wang ; Yibing Peng,2024,4,"@article{2-38214,
  title = {A knowledge-guided process planning approach with reinforcement learning},
  author = {Lijun Zhang and Hongjin Wu and Yelin Chen and Xuesong Wang and Yibing Peng},
  year = {2024},
  doi = {10.1080/09544828.2024.2415831},
  journal = {Journal of Engineering Design}
}",Algorithmic contributions,Manufacturing / Industry / Automation,Operational,"Advising, Executing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-38220,taylor-and-francis,A multi-agent reinforcement learning model for inventory transshipments under supply chain disruption,"The COVID-19 pandemic has significantly disrupted global Supply Chains (SCs), emphasizing the importance of SC resilience, which refers to the ability of SCs to return to their original or more desirable state following disruptions. This study focuses on collaboration, a key component of SC resilience, and proposes a novel collaborative structure that incorporates a fictitious agent to manage inventory transshipment decisions between retailers in a centralized manner while maintaining the retailers’ autonomy in ordering. The proposed collaborative structure offers the following advantages from SC resilience and operational perspectives: (i) it facilitates decision synchronization for enhanced collaboration among retailers, and (ii) it allows retailers to collaborate without the need for information sharing, addressing the potential issue of information sharing reluctance. Additionally, this study employs non-stationary probability to capture the deeply uncertain nature of the ripple effect and the highly volatile customer demand caused by the pandemic. A new Reinforcement Learning (RL) algorithm is developed to handle non-stationary environments and to implement the proposed collaborative structure. Experimental results demonstrate that the proposed collaborative structure using the new RL algorithm achieves superior SC resilience compared with centralized inventory management systems with transshipment and decentralized inventory management systems without transshipment using traditional RL algorithms.",10.1080/24725854.2023.2217248,https://www.tandfonline.com/doi/full/10.1080/24725854.2023.2217248,Institute of Industrial and Systems Engineers Transactions,Byeongmok Kim;Jong Gwang Kim ; Seokcheon Lee,2024,26,"@article{2-38220,
  title={A multi-agent reinforcement learning model for inventory transshipments under supply chain disruption},
  author={Kim, Byeongmok and Kim, Jong Gwang and Lee, Seokcheon},
  year={2024},
  journal={Institute of Industrial and Systems Engineers Transactions},
  doi={10.1080/24725854.2023.2217248}
}",Algorithmic contributions,Manufacturing / Industry / Automation,Operational,Collaborating,"Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-38243,taylor-and-francis,A supporting tool for enhancing user’s mental model elicitation and decision-making in user experience research,"Experience (UX) research is intended to find insights and elicit applicable requirements to guide usable designs. Card Sorting is one of the most utilized methods. It is used to uncover the user's mental model and increase the usability of existing products. However, although Card Sorting has been widely utilized, most applications are based on spreadsheets. Furthermore, existing tools are principally intended to obtain qualitative information or customized quantitative outcomes to improve the information architecture. In this paper, a supporting tool based on the Card Sorting method is presented and detailed, including a comprehensive use case showing the main features. The tool implements predictive analysis of results through advanced statistics and machine learning techniques, providing comprehensive reports that enable evaluators and UX researchers to obtain high-level knowledge and important quantitative clues to enhance decision-making. The tool has been evaluated with participants and evaluators, obtaining relevant usability results and feedback.",10.1080/10447318.2022.2041885,https://www.tandfonline.com/doi/full/10.1080/10447318.2022.2041885,International Journal of Human–Computer Interaction,Marina Martín ; José A. Macías,2023,300,"@article{2-38243,
  title = {A Supporting Tool for Enhancing User’s Mental Model Elicitation and Decision-Making in User Experience Research},
  author = {Martín, Marina and Macías, José A.},
  year = {2023},
  doi = {10.1080/10447318.2022.2041885},
  journal = {International Journal of Human–Computer Interaction}
}",System/Artifact contributions,Design / Creativity / Architecture,Individual,"Analyzing, Advising, Forecasting","Decision-maker, Knowledge provider","Alter decision outcomes, Change cognitive demands","Change AI responses, Update AI competence",recommendations,NA,Textual,Yes,Yes
2-38245,taylor-and-francis,Accountability increases resource sharing: effects of accountability on human and ai system performance,"pressures have been found to increase worker engagement and reduce adverse biases in people interacting with automated technology, but it is unclear if these effects can be observed in a more laterally controlled human-AI task. To address this question, 40 participants were asked to coordinate with an AI agent on a resource-management task, with half of the participants expecting to justify their decision strategy, which comprised our accountability condition. We then considered the effects of accountability on performance, as measured by participants’ resource sharing behaviors, their individual, and joint task scores (throughput), and their perceived workload. Participants in the accountability group shared more resources with their AI partner, took more time to make decisions, and performed worse in the task individually, but had AI partners who performed better. We found no difference between groups on how prepared they felt they were to justify their decisions, and participants reported similar levels of workload. Results suggest accountability pressures can influence exchange strategies in human-AI tasks with lateral control.",10.1080/10447318.2020.1824695,https://www.tandfonline.com/doi/full/10.1080/10447318.2020.1824695,International Journal of Human–Computer Interaction,Gabriel A. León;Erin K. Chiou ; Adam Wilkins,2021,18,"@article{2-38245,
  title={Accountability increases resource sharing: effects of accountability on human and AI system performance},
  author={Gabriel A. Le{\'o}n and Erin K. Chiou and Adam Wilkins},
  year={2021},
  journal={International Journal of Human--Computer Interaction},
  doi={10.1080/10447318.2020.1824695}
}",Empirical contributions,"Everyday / Employment / Public Service, Generic / Abstract / Domain-agnostic",Operational,Collaborating,Decision-maker,"Alter decision outcomes, Change trust, Change cognitive demands",no such info,autonomous cooperation,resource allocation to AI,"Interactive interface, Semi-Autonomous System",Yes,Yes
2-38246,taylor-and-francis,Actionable cognitive twins for decision making in manufacturing,"Cognitive Twins are the next generation Digital Twins enhanced with cognitive capabilities through a knowledge graph and artificial intelligence models that provide insights and decision-making options to the users. The knowledge graph describes the domain-specific knowledge regarding entities and interrelationships related to a manufacturing setting. It also contains information on possible decision-making options that can assist decision-makers, such as planners or logisticians. This paper proposes a knowledge graph modelling approach to construct actionable cognitive twins for capturing specific knowledge related to production planning and demand forecasting in a manufacturing plant. The knowledge graph provides semantic descriptions and contextualisation of the production lines and processes, including data identification and simulation or artificial intelligence algorithms and forecasts used to support them. Such semantics provide ground for inferencing, relating different knowledge types: creative, deductive, definitional, and inductive. To develop the knowledge graph models for describing the use case thoroughly, systems thinking approach is proposed to design and verify the ontology, develop a knowledge graph and build an actionable cognitive twin. Finally, we evaluate our approach in two use cases developed for a European original equipment manufacturer related to the automotive industry as part of the European Horizon 2020 project FACTLOG.",10.1080/00207543.2021.2002967,https://www.tandfonline.com/doi/full/10.1080/00207543.2021.2002967,International Journal of Production Research,Jože M. Rožanec;Jinzhi Lu;Jan Rupnik;Maja Škrjanc;Dunja Mladenić;Blaž Fortuna;Xiaochen Zheng ; Dimitris Kiritsis,2022,72,"@article{2-38246,
  title={Actionable cognitive twins for decision making in manufacturing},
  author={Rožanec, Jože M. and Lu, Jinzhi and Rupnik, Jan and Škrjanc, Maja and Mladenić, Dunja and Fortuna, Blaž and Zheng, Xiaochen and Kiritsis, Dimitris},
  year={2022},
  journal={International Journal of Production Research},
  doi={10.1080/00207543.2021.2002967}
}",Methodological contributions,Manufacturing / Industry / Automation,Operational,"Advising, Forecasting, Analyzing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-38250,taylor-and-francis,Ai based decision making: combining strategies to improve operational performance,"study investigates the strategic alignment between marketing and information technology (IT) strategies and provides production and operations decision makers a model for improving operational performance. Based on a comprehensive literature review, the combined strategies were used to develop a novel decision-making framework. The hypothesised relationships of an SEM model are validated with data collected from 242 managers from various industries. An artificial intelligence (AI)–based method is developed using artificial neural networks (ANN) feeding into a decision-making framework which explores the optimality of the combined strategies. The results indicate that (a) IT strategy is positively mediated by marketing strategy on performance and (b) the organisational structure moderates the mediation of marketing strategy on performance. The analysis confirms that the extracted strategies based on the proposed framework have superior performance compared to existing strategies. This paper contributes to the literature by conceptualising and empirically testing the mediation role of marketing strategy on IT strategy, performance and operational decision-making. The use of a novel three-phase decision-making framework which uses AI processes improves operational efficiency, increases insights and enhances the decision accuracy of complex problems at the strategic level in industries such as manufacturing. It could help operations executives to apply effective decisions.",10.1080/00207543.2021.1966540,https://www.tandfonline.com/doi/full/10.1080/00207543.2021.1966540,International Journal of Production Research,Abdulrahman Al-Surmi;Mahdi Bashiri ; Ioannis Koliousis,2022,2,"@article{2-38250,
  title = {AI Based Decision Making: Combining Strategies to Improve Operational Performance},
  author = {Abdulrahman Al-Surmi and Mahdi Bashiri and Ioannis Koliousis},
  year = {2022},
  doi = {10.1080/00207543.2021.1966540},
  journal = {International Journal of Production Research}
}",Methodological contributions,Finance / Business / Economy,"Operational, Organizational","Analyzing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-38251,taylor-and-francis,Ai-cdss design guidelines and practice verification,"This study presents systematic design guidelines for AI-powered clinical decision support systems (AI-CDSS) based on a comprehensive literature review and theme analysis. The proposed guidelines are divided into two parts, comprising 25 items: “How—design methods” and “What—design content and forms.” The usability of these guidelines is demonstrated through an AI-CDSS design practice for stroke diagnosis and thrombolytic risk assessment in a Chinese clinical setting. The empirical case study provides practical suggestions for similar AI-CDSS designs that require fast decision-making. Challenges concerning system transparency or explainability and the impact of bias on system output are considered key issues in designing an effective and satisfying AI-CDSS. The potential usefulness of AI-CDSS in clinical decision-making scenarios has been indicated by numerous studies, with efficient design guidelines still needed to address these challenges.",10.1080/10447318.2023.2235882,https://www.tandfonline.com/doi/full/10.1080/10447318.2023.2235882,International Journal of Human–Computer Interaction,Xin He;Xi Zheng;Huiyuan Ding;Yixuan Liu ; Hongling Zhu,2024,3,"@article{2-38251,
  title = {Ai-cdss design guidelines and practice verification},
  author = {He, Xin and Zheng, Xi and Ding, Huiyuan and Liu, Yixuan and Zhu, Hongling},
  year = {2024},
  doi = {10.1080/10447318.2023.2235882},
  journal = {International Journal of Human–Computer Interaction}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-38259,taylor-and-francis,An affordance-based model of human action selection in a human–machine interaction system with cognitive interpretations,"technology is not sufficient to automate all desired tasks. Human–machine interaction (HMI) has thus become a key control and design factor for tasks requiring human-level decision-making or information synthesis. Such processes require a formal representation of human actions (including decision-making) when modeling HMI systems; however, successful prescriptive approaches to this end have still been elusive. This article extends the affordance-based finite state automata model, conditioning human prior experience and natural memory decay of task knowledge (or skill decay). The new model draws upon both reinforcement learning and natural memory decay for decision-making on action choice. An empirical study is carried out to specify how action choice is affected or updated by reinforcement learning based on past experience, and Wickelgren’s decay function is jointly employed to predict human decision-making behavior.",10.1080/10447318.2016.1157678,https://www.tandfonline.com/doi/full/10.1080/10447318.2016.1157678,International Journal of Human–Computer Interaction,Hokyoung Ryu;Namhun Kim;Jangsun Lee ; Dongmin Shin,2016,9,"@article{2-38259,
  title={An affordance-based model of human action selection in a human--machine interaction system with cognitive interpretations},
  author={Ryu, Hokyoung and Kim, Namhun and Lee, Jangsun and Shin, Dongmin},
  year={2016},
  doi={10.1080/10447318.2016.1157678},
  journal={International Journal of Human--Computer Interaction}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,no such info,"Analyzing, Executing",Decision-maker,no such info,no such info,"systematic cues, system accuracy",incentivized beliefs,"Visual, Interactive interface",Yes,Yes
2-38262,taylor-and-francis,An intelligent open trading system for on-demand delivery facilitated by deep q network based reinforcement learning,"delivery in urban areas has been growing rapidly in recent years. Nevertheless, on-demand delivery networks lack an efficient, sustainable, and environmentally friendly operative strategy. An open trading system equipped with on-line auctions provides an opportunity for increasing the efficiency of on-demand delivery systems. Reinforcement learning techniques that automate decision-making can facilitate the implementation of such complex and dynamic systems. This paper presents an on-line auction-based request trading platform embedded within an open trading system as a new scheme for carriers and shippers to trade on-demand delivery requests. The system is developed based on a multi-agent model, composed of carriers, shippers, and the on-line platform as autonomous agents. Deep Q network enabled reinforcement learning is used in the decision-making processes for the agents to optimise their behaviour in a dynamic environment. Numerical experiments conducted on the Melbourne metropolitan network demonstrate the effectiveness of the open trading system, which can provide benefits for all stakeholders involved in the on-demand delivery market as well as the entire system. The reinforcement learning enabled platform can gain more profit when there are more learning carriers. The results indicate that the intelligent open trading system with on-line auctions is a promising city logistics solution.",10.1080/00207543.2024.2364349,https://www.tandfonline.com/doi/full/10.1080/00207543.2024.2364349,International Journal of Production Research,Chaojie Guo;Lele Zhang;Russell G. Thompson;Greg Foliente ; Xiaoshuai Peng,2025,4,"@article{2-38262,
  title={An intelligent open trading system for on-demand delivery facilitated by deep Q network based reinforcement learning},
  author={Guo, Chaojie and Zhang, Lele and Thompson, Russell G. and Foliente, Greg and Peng, Xiaoshuai},
  year={2025},
  journal={International Journal of Production Research},
  doi={10.1080/00207543.2024.2364349}
}",System/Artifact contributions,Manufacturing / Industry / Automation,Operational,Executing,"Stakeholder, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-38263,taylor-and-francis,Analyzing operator states and the impact of ai-enhanced decision support in control rooms: a human-in-the-loop specialized reinforcement learning framework for intervention strategies,"complex industrial and chemical process control rooms, effective decision-making is crucial for safety and efficiency. The experiments in this paper evaluate the impact and applications of an AI-based decision support system integrated into an improved human-machine interface, using dynamic influence diagrams, a hidden Markov model, and deep reinforcement learning. The enhanced support system aims to reduce operator workload, improve situational awareness, and provide different intervention strategies to the operator adapted to the current state of both the system and human performance. Such a system can be particularly useful in cases of information overload when many alarms and inputs are presented all within the same time window, or for junior operators during training. A comprehensive cross-data analysis was conducted, involving 47 participants and a diverse range of data sources such as smartwatch metrics, eye-tracking data, process logs, and responses from questionnaires. The results indicate interesting insights regarding the effectiveness of the approach in aiding decision-making, decreasing perceived workload, and increasing situational awareness for the scenarios considered. Additionally, the results provide insights to compare differences between styles of information gathering when using the system by individual participants. These findings are particularly relevant when predicting the overall performance of the individual participant and their capacity to successfully handle a plant upset and the alarms connected to it using process and human-machine interaction logs in real-time which resulted in a 95.8% prediction accuracy using hidden Markov model. These predictions enable the development of more effective intervention strategies.",10.1080/10447318.2024.2391605,https://www.tandfonline.com/doi/full/10.1080/10447318.2024.2391605,International Journal of Human–Computer Interaction,Ammar N. Abbas;Chidera W. Amazu;Joseph Mietkiewicz;Houda Briwa;Andres Alonso Perez;Gabriele Baldissone;Micaela Demichela;Georgios C. Chasparis;John D. Kelleher ; Maria Chiara Leva,2024,8,"@article{2-38263,
  title     = {Analyzing operator states and the impact of AI-enhanced decision support in control rooms: a human-in-the-loop specialized reinforcement learning framework for intervention strategies},
  author    = {Ammar N. Abbas and Chidera W. Amazu and Joseph Mietkiewicz and Houda Briwa and Andres Alonso Perez and Gabriele Baldissone and Micaela Demichela and Georgios C. Chasparis and John D. Kelleher and Maria Chiara Leva},
  year      = {2024},
  journal   = {International Journal of Human--Computer Interaction},
  doi       = {10.1080/10447318.2024.2391605}
}","Empirical contributions, System/Artifact contributions",Manufacturing / Industry / Automation,Operational,"Forecasting, Advising","Guardian, Decision-maker","Alter decision outcomes, Change cognitive demands, Change trust",no such info,"recommendations, prediction of alternative",NA,"Textual, Interactive interface, Visual",Yes,Yes
2-38271,taylor-and-francis,Applying machine learning to the dynamic selection of replenishment policies in fast-changing supply chain environments,"currently operate in highly competitive scenarios, where the environmental conditions evolve over time. Many factors intervene simultaneously and their hard-to-interpret interactions throughout the supply chain greatly complicate decision-making. The complexity clearly manifests itself in the field of inventory management, in which determining the optimal replenishment rule often becomes an intractable problem. This paper applies machine learning to help managers understand these complex scenarios and better manage the inventory flow. Building on a dynamic framework, we employ an inductive learning algorithm for setting the most appropriate replenishment policy over time by reacting to the environmental changes. This approach proves to be effective in a three-echelon supply chain where the scenario is defined by seven variables (cost structure, demand variability, three lead times, and two partners’ inventory policy). Considering four alternatives, the algorithm determines the best replenishment rule around 88% of the time. This leads to a noticeable reduction of operating costs against static alternatives. Interestingly, we observe that the nodes are much more sensitive to inventory decisions in the lower echelons than in the upper echelons of the supply chain.",10.1080/00207543.2018.1552369,https://www.tandfonline.com/doi/full/10.1080/00207543.2018.1552369,International Journal of Production Research,Paolo Priore;Borja Ponte;Rafael Rosillo ; David de la Fuente,2019,219,"@article{2-38271,
  title={Applying machine learning to the dynamic selection of replenishment policies in fast-changing supply chain environments},
  author={Priore, Paolo and Ponte, Borja and Rosillo, Rafael and de la Fuente, David},
  year={2019},
  journal={International Journal of Production Research},
  doi={10.1080/00207543.2018.1552369}
}",Algorithmic contributions,Manufacturing / Industry / Automation,Organizational,"Executing, Advising, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-38285,taylor-and-francis,"Artificial intelligence, types of decisions, and street-level bureaucrats: evidence from a survey experiment","on the logic of Simon’s decision-making theory, this study compares the effects of AI versus humans on discretion, client meaningfulness, and willingness-to-implement, and examines the moderating role of different types of decisions on those relationships. The findings show that AI usage has a negative effect on perceived discretion and a positive effect on willingness-to-implement. Conversely, non-programmed decisions tend to have a positive effect on both perceived discretion and willingness-to-implement. Moreover, non-programmed decisions mitigated the effect of AI usage on perceived discretion, while programmed decisions interacted with AI usage to improve client meaningfulness and strengthen willingness-to-implement.",10.1080/14719037.2022.2070243,https://www.tandfonline.com/doi/full/10.1080/14719037.2022.2070243,Public Management Review,Ge Wang;Shenghua Xie ; Xiaoqian Li,2024,43,"@article{2-38285,
  title={Artificial intelligence, types of decisions, and street-level bureaucrats: evidence from a survey experiment},
  author={Wang, Ge and Xie, Shenghua and Li, Xiaoqian},
  year={2024},
  journal={Public Management Review},
  doi={10.1080/14719037.2022.2070243}
}",Empirical contributions,Law / Policy / Governance,Operational,"Executing, Advising","Decision-maker, Decision-subject","Change affective-perceptual, Alter decision outcomes, Restrict human agency",no such info,NA,limited agency,Autonomous System,Yes,Yes
2-38290,taylor-and-francis,At the intersection of human and algorithmic decision-making in distributed learning,"article seeks to explore different combinations of human and Artificial Intelligence (AI) decision-making in the context of distributed learning. Distributed learning institutions face specific challenges such as high levels of student attrition and ensuring quality, cost-effective student support at scale using a range of technologies, such as AI. While there is an expanding body of research on AI in education (AIEd), this conceptual article proposes that combinations of human-algorithmic decision-making systems need careful and critical consideration, not only for their potential, but also for their appropriateness and ethical considerations. We operationalize a framework designed to consider robot autonomy at four key events in students’ learning journeys, namely (1) admission and registration; (2) student advising and support; (3) augmenting pedagogy; and (4) formative and summative assessment. We conclude the article by providing pointers for operationalizing options in human-algorithmic decision-making in distributed learning contexts.",10.1080/15391523.2022.2121343,https://www.tandfonline.com/doi/full/10.1080/15391523.2022.2121343,Journal of Research on Technology in Education,Paul Prinsloo;Sharon Slade ; Mohammad Khalil,2023,12,"@article{2-38290,
  title = {At the intersection of human and algorithmic decision-making in distributed learning},
  author = {Prinsloo, Paul and Slade, Sharon and Khalil, Mohammad},
  year = {2023},
  doi = {10.1080/15391523.2022.2121343},
  journal = {Journal of Research on Technology in Education}
}",Theoretical contributions,Education / Teaching / Research,Institutional,Executing,Decision-subject,NA,NA,recommendations,NA,NA,Yes,No
2-38295,taylor-and-francis,Becoming information centric: the emergence of new cognitive infrastructures in education policy,"cognitive infrastructures are emerging as digital platforms and artificial intelligence enable new forms of automated thinking that shape human decision-making. This paper (a) offers a new theoretical perspective on automated thinking in education policy and (b) illustrates how automated thinking is emerging in one specific policy context. We report on a case study of a policy analysis unit (‘The Centre’) in an Australian state education department that has been implementing a BI strategy since 2013. The Centre is now focused on using BI to support complex decision making and improve learning outcomes, and their strategy describes this focus as becoming ‘information centric’. The theoretical framework for our analysis draws on infrastructure studies and philosophy of technology, particularly Luciana Parisi’s recent work on automated thinking. We analyse technical documentation and semi-structured interview data to describe the enactment of a BI strategy in The Centre, with a focus on how new approaches to data analytics are shaping decision-making. Our analysis shows that The Centre is developing a cognitive infrastructure that is already creating new conditions for education policy making, and we conclude with a call for research designs that enable pragmatic exploration of what these infrastructures can do.",10.1080/02680939.2019.1678766,https://www.tandfonline.com/doi/full/10.1080/02680939.2019.1678766,Journal of Education Policy,Sam Sellar ; Kalervo N. Gulson,2021,0,"@article{2-38295,
  title={Becoming information centric: the emergence of new cognitive infrastructures in education policy},
  author={Sellar, Sam and Gulson, Kalervo N.},
  year={2021},
  doi={10.1080/02680939.2019.1678766},
  journal={Journal of Education Policy}
}","Empirical contributions, Theoretical contributions",Education / Teaching / Research,Organizational,"Analyzing, Advising","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-38299,taylor-and-francis,Building supply-chain resilience: an artificial intelligence-based technique and decision-making framework,"Intelligence (AI) offers a promising solution for building and promoting more resilient supply chains. However, the literature is highly dispersed regarding the application of AI in supply-chain management. The literature to date lacks a decision-making framework for identifying and applying powerful AI techniques to build supply-chain resilience (SCRes), curbing advances in research and practice on this interesting interface. In this paper, we propose an integrated Multi-criteria decision-making (MCDM) technique powered by AI-based algorithms such as Fuzzy systems, Wavelet Neural Networks (WNN) and Evaluation based on Distance from Average Solution (EDAS) to identify patterns in AI techniques for developing different SCRes strategies. The analysis was informed by data collected from 479 manufacturing companies to determine the most significant AI applications used for SCRes. The findings show that fuzzy logic programming, machine learning big data, and agent-based systems are the most promising techniques used to promote SCRes strategies. The study findings support decision-makers by providing an integrated decision-making framework to guide practitioners in AI deployment for building SCRes.",10.1080/00207543.2021.1950935,https://www.tandfonline.com/doi/full/10.1080/00207543.2021.1950935,International Journal of Production Research,Amine Belhadi;Sachin Kamble;Samuel Fosso Wamba ; Maciel M. Queiroz,2022,415,"@article{2-38299,
  title={Building supply-chain resilience: an artificial intelligence-based technique and decision-making framework},
  author={Belhadi, Amine and Kamble, Sachin and Wamba, Samuel Fosso and Queiroz, Maciel M.},
  year={2022},
  doi={10.1080/00207543.2021.1950935},
  journal={International Journal of Production Research}
}",Methodological contributions,Manufacturing / Industry / Automation,Institutional,Advising,"Decision-maker, Developer, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-38306,taylor-and-francis,Combining chatgpt and knowledge graph for explainable machine learning-driven design: a case study,"learning has been widely used in design activities, enabling more informed decision-making. However, high-performance machine learning models, often referred to as ‘black-box', result in a lack of explainability regarding predictions. The absence of explainability erodes the trust between designers and these models and hinders human-machine collaboration for desirable design decisions. Explainable AI focuses on creating explanations that are accessible and comprehensible to stakeholders, thereby improving explainability. A recent advancement in the field of explainable AI involves leveraging domain-specific knowledge via knowledge graph. Additionally, the advent of large language models like ChatGPT, acclaimed for their ability to output domain knowledge, perform complex language processing, and support seamless end-user interaction, has the potential to expand the horizons of explainable AI. Inspired by these developments, we propose the novel hybrid method that synergizes ChatGPT and knowledge graph to augment post-hoc explainability in design context. The outcome is the generation of more contextual and meaningful explanations, with the added possibility of further interaction to uncover deeper insights. The effectiveness of the proposed method is illustrated through a case study on customer segmentation.",10.1080/09544828.2024.2355758,https://www.tandfonline.com/doi/full/10.1080/09544828.2024.2355758,Journal of Engineering Design,Xin Hu;Ang Liu ; Yun Dai,2024,20,"@article{2-38306,
  title={Combining chatgpt and knowledge graph for explainable machine learning-driven design: a case study},
  author={Hu, Xin and Liu, Ang and Dai, Yun},
  year={2024},
  journal={Journal of Engineering Design},
  doi={10.1080/09544828.2024.2355758}
}",Methodological contributions,"Design / Creativity / Architecture, Generic / Abstract / Domain-agnostic",Operational,"Explaining, Advising","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-38307,taylor-and-francis,Communicating the limitations of ai: the effect of message framing and ownership on trust in artificial intelligence,"Trust plays an essential role in the interaction between humans and artificial intelligence (AI). To promote trust in AI, information about the AI’s performance should be communicated well to the users. Accordingly, this paper investigates how information about AI performance should be presented, focusing on message framing and the ownership of decisions. A 2 (ownership: no ownership vs. ownership) × 3 (message framing: no information vs. negative information vs. positive information) between-subjects experiment was conducted (N = 120). Participants were asked to choose items to help them survive in the desert, supported by an AI decision. The results showed that participants without decision ownership perceived higher trust than those with decision ownership. Also, trust was perceived to be higher when participants were not given performance information than when they were. The results indicate the importance of carefully communicating with AI. The implications of this study are discussed.",10.1080/10447318.2022.2049134,https://www.tandfonline.com/doi/full/10.1080/10447318.2022.2049134,International Journal of Human–Computer Interaction,Taenyun Kim ; Hayeon Song,2023,54,"@article{2-38307,
  title = {Communicating the limitations of AI: The effect of message framing and ownership on trust in artificial intelligence},
  author = {Taenyun Kim and Hayeon Song},
  year = {2023},
  doi = {10.1080/10447318.2022.2049134},
  journal = {International Journal of Human–Computer Interaction}
}",Empirical contributions,"Everyday / Employment / Public Service, Generic / Abstract / Domain-agnostic",Individual,"Advising, Auditing",Decision-maker,"Alter decision outcomes, Change trust",no such info,"confidence score, system accuracy, textual explanations",trust,Textual,Yes,Yes
2-38312,taylor-and-francis,Data and rights in the digital welfare state: the case of denmark,"paper examines how the logic of surveillance capitalism manifests itself within the public sector with a specific interest in how the government’s use of data about its citizens may reconfigure rights and power. In Denmark, for example, the public administration relies heavily on the processing of vast quantities of data about the individual and increasingly uses predictive analytics to identify specific areas of intervention, such as fraud or vulnerability, as part of its decision-making processes. Methodologically, the paper uses Denmark as an example of the digital welfare state, including two public sector cases of automated decision support, namely Gladsaxe municipality and the central processing of welfare benefits (Udbetaling Danmark). It further investigates Danish digitalisation strategies, particularly the governments AI strategy from 2019. The case is examined with a view to understand how technology (and automated decision support in particular) is deployed by state actors, which interests it serves, and how it may benefit or disadvantage the individual. Theoretically, the paper leans towards theories of surveillance capitalism, governance in the digital era, and data politics and rights. The paper argues that unless a more critical and human-centric approach to ‘smart governance’ is taken, the digital welfare state will advance a digital technocracy that treats its citizens as data points suited for calculation and prediction rather than as individuals with agency and rights.",10.1080/1369118X.2021.1934069,https://www.tandfonline.com/doi/full/10.1080/1369118X.2021.1934069,"Information, Communication & Society",Rikke Frank Jørgensen,2023,0,"@article{2-38312,
  title = {Data and Rights in the Digital Welfare State: The Case of Denmark},
  author = {Rikke Frank J{\o}rgensen},
  year = {2023},
  doi = {10.1080/1369118X.2021.1934069},
  journal = {Information, Communication \& Society}
}",Theoretical contributions,Law / Policy / Governance,Operational,"Advising, Auditing, Forecasting","Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-38313,taylor-and-francis,Data-driven hierarchical learning and real-time decision-making of equipment scheduling and location assignment in automatic high-density storage systems,"high-density storage systems (AHDSS) have attracted widespread attention in recent years owing to their advantages of high throughput and space utilisation. However, owing to the characteristics of large-scale, multi-disturbance, and short-period task scenarios, a system is required to make instant and efficient decisions. To this end, this paper proposes a data-driven real-time decision-making method to solve the real-time equipment scheduling and dynamic location assignment problem in AHDSS. The proposed method comprises two phases: decision scheme learning and real-time decision-making. The operation state attribute features of the AHDSS were constructed to generate training data for equipment scheduling and location assignment scheme learning. Thereafter, a hierarchical learning and decision-making mechanism based on the deep belief network (DBN) is proposed. The integrated learning of better scheduling solutions was realised by establishing three-stage models of lift selection, shuttle selection, and location priority. Additionally, the Taguchi method was adopted to determine the best performance parameters for DBNs at different learning stages. Compared with other well-known machine learning algorithms, DBNs have a higher learning accuracy. Finally, a real-world AHDSS problem is studied, and the results demonstrate that the proposed approach outperforms existing dispatching rules.",10.1080/00207543.2022.2148011,https://www.tandfonline.com/doi/full/10.1080/00207543.2022.2148011,International Journal of Production Research,Zhun Xu;Liyun Xu;Xufeng Ling ; Beikun Zhang,2023,10,"@article{2-38313,
  title={Data-driven hierarchical learning and real-time decision-making of equipment scheduling and location assignment in automatic high-density storage systems},
  author={Xu, Zhun and Xu, Liyun and Ling, Xufeng and Zhang, Beikun},
  year={2023},
  doi={10.1080/00207543.2022.2148011},
  journal={International Journal of Production Research}
}",Methodological contributions,Manufacturing / Industry / Automation,Operational,"Executing, Analyzing","Decision-subject, Guardian, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-38322,taylor-and-francis,Deep online recommendations for connected e-taxis by coupling trajectory mining and reinforcement learning,"There is is a growing interest in the optimization of vehicle fleets management in urban environments. However, limited attention has been paid to the integrated optimization of electric taxi fleets accounting for different operations as well as complex spatiotemporal demand dynamics. To this end, this study develops a real-time recommendation framework based on deep reinforcement learning (DRL) for electric taxis (E-taxis) to improve their system performance with explicit modeling of multiple vehicle actions and varying travel demand across space and over time. Spatiotemporal patterns of urban taxi travels are extracted from large-scale taxi trajectories. Spatiotemporal strategies are proposed to coordinate E-taxis’ repositioning and recharging with optimized recommendation for next destinations and charging stations. A spatiotemporal double deep Q-network (ST-DDQN) is embedded in the DRL framework to maximize the daily profit. A prototype real-time recommendation system for E-taxis is implemented for the decision-making of E-taxi drivers and sensitivity analyses are carried out. The experimental results in Shenzhen, China suggest that the proposed framework could improve the overall performance. This study will benefit the promotion of connected E-taxis and the development of clean and smart transportation.",10.1080/13658816.2023.2279969,https://www.tandfonline.com/doi/full/10.1080/13658816.2023.2279969,International Journal of Geographical Information Science,Wei Tu;Haoyu Ye;Ke Mai;Meng Zhou;Jincheng Jiang;Tianhong Zhao;Shengao Yi ; Qingquan Li,2024,37,"@article{2-38322,
  title = {Deep online recommendations for connected e-taxis by coupling trajectory mining and reinforcement learning},
  author = {Wei Tu and Haoyu Ye and Ke Mai and Meng Zhou and Jincheng Jiang and Tianhong Zhao and Shengao Yi and Qingquan Li},
  year = {2024},
  journal = {International Journal of Geographical Information Science},
  doi = {10.1080/13658816.2023.2279969}
}",System/Artifact contributions,Transportation / Mobility / Planning,Organizational,"Forecasting, Advising","Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-38327,taylor-and-francis,Delegating strategic decision-making to machines: dr. Strangelove redux?,"the use of artificial intelligence (AI) in strategic decision-making be stabilizing or destabilizing? What are the risks and trade-offs of pre-delegating military force to machines? How might non-nuclear state and non-state actors leverage AI to put pressure on nuclear states? This article analyzes the impact of strategic stability of the use of AI in the strategic decision-making process, in particular, the risks and trade-offs of pre-delegating military force (or automating escalation) to machines. It argues that AI-enabled decision support tools - by substituting the role of human critical thinking, empathy, creativity, and intuition in the strategic decision-making process - will be fundamentally destabilizing if defense planners come to view AI’s ‘support’ function as a panacea for the cognitive fallibilities of human analysis and decision-making. The article also considers the nefarious use of AIenhanced fake news, deepfakes, bots, and other forms of social media by non-state actors and state proxy actors, which might cause states to exaggerate a threat from ambiguous or manipulated information, increasing instability.",10.1080/01402390.2020.1759038,https://www.tandfonline.com/doi/full/10.1080/01402390.2020.1759038,Journal of Strategic Studies,James Johnson,2022,96,"@article{2-38327,
  title = {Delegating Strategic Decision-Making to Machines: Dr. Strangelove Redux?},
  author = {James Johnson},
  year = {2022},
  doi = {10.1080/01402390.2020.1759038},
  journal = {Journal of Strategic Studies}
}",Theoretical contributions,Defense / Military / Emergency,Institutional,"Advising, Executing","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-38328,taylor-and-francis,Design and development of automobile assembly model using federated artificial intelligence with smart contract,"smart sensors and embedded drivers, today’s automotive industry has taken a giant leap in emerging technologies like Machine learning, Artificial intelligence, and the Internet of things and started to build data-driven decision-making strategies to compete in global smart manufacturing. This paper proposes a novel design framework that uses Federated learning-Artificial intelligence (FAI) for decision-making and Smart Contract (SC) policies for process execution and control in a completely automated smart automobile manufacturing industry. The proposed design introduces a novel element called Trust Threshold Limit (TTL) that helps moderate the excess usage of embedded equipment, tools, energy, and cost functions, limiting wastages in the manufacturing processes. This research highlights the use cases of AI in decentralised Blockchain with smart contracts, the company’s trading policies, and its advantages for effectively handling market risk assessments during socio-economic crisis. The developed model supported by real-time cases incorporated cost functions, delivery time and energy evaluations. Results spotlight the use of FAI in decision accuracy for the developed smart contract-based Automobile Assembly Model (AAM), thereby qualitatively limiting the threshold level of cost, energy and other control functions in procurement assembly and manufacturing. Customisation and graphical user interface with cloud integration are some challenges of this model.",10.1080/00207543.2021.1988750,https://www.tandfonline.com/doi/full/10.1080/00207543.2021.1988750,International Journal of Production Research,Arunmozhi Manimuthu;V. G. Venkatesh;Yangyan Shi;V. Raja Sreedharan ; S. C. Lenny Koh,2022,91,"@article{2-38328,
  title={Design and development of automobile assembly model using federated artificial intelligence with smart contract},
  author={Arunmozhi Manimuthu and V. G. Venkatesh and Yangyan Shi and V. Raja Sreedharan and S. C. Lenny Koh},
  year={2022},
  journal={International Journal of Production Research},
  volume={60}, 
  pages={2674-2690}, 
  doi={10.1080/00207543.2021.1988750}
}",System/Artifact contributions,Manufacturing / Industry / Automation,Operational,"Advising, Executing","Guardian, Developer",NA,NA,NA,NA,NA,Yes,No
2-38329,taylor-and-francis,Design and evaluation of online educational content: ai-informed guidelines based on machine learning analysis of learners’ interactions traces,"study proposes an intelligent educational decision support system that empowers instructional designers to evaluate online educational contents in order to improve their design and effectiveness. The key challenge in developing it lies in automating and objectifying the evaluation process. To address this, the study pursues two main objectives. The first one is to propose a Multicriteria Approach for Learning Experience Analysis (MALEA) on which we have based the evaluation through the learners’ traces. The second objective consists of proposing the Approach for Content Success Prediction (ACSP), which can be used to evaluate educational content before its deployment. ACSP combines logistic regression and MALEA. This combination helps to guard against the possible imprecision of human judgment affecting the decision-making process. A case study is conducted and proves that the system meets the objective sought and thus is retained for online educational content evaluation. Results are promising with high values of precision, accuracy, specificity, and sensitivity. Different perspectives are finally proposed.",10.1080/10447318.2024.2434765,https://www.tandfonline.com/doi/full/10.1080/10447318.2024.2434765,International Journal of Human–Computer Interaction,Yosra Mourali;Maroi Agrebi;Ramzi Farhat;Christophe Kolski ; Mohamed Jemni,2024,2,"@article{2-38329,
  title = {Design and evaluation of online educational content: AI-informed guidelines based on machine learning analysis of learners’ interactions traces},
  author = {Yosra Mourali and Maroi Agrebi and Ramzi Farhat and Christophe Kolski and Mohamed Jemni},
  year = {2024},
  doi = {10.1080/10447318.2024.2434765},
  journal = {International Journal of Human–Computer Interaction}
}",Methodological contributions,Education / Teaching / Research,Operational,"Monitoring, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-38330,taylor-and-francis,Design principles for artificial intelligence-augmented decision making: an action design research study,"intelligence (AI) applications have proliferated, garnering significant interest among information systems (IS) scholars. AI-powered analytics, promising effective and low-cost decision augmentation, has become a ubiquitous aspect of contemporary organisations. Unlike traditional decision support systems (DSS) designed to support decisionmakers with fixed decision rules and models that often generate stable outcomes and rely on human agentic primacy, AI systems learn, adapt, and act autonomously, demanding recognition of IS agency within AI-augmented decision making (AIADM) systems. Given this fundamental shift in DSS; its influence on autonomy, responsibility, and accountability in decision making within organisations; the increasing regulatory and ethical concerns about AI use; and the corresponding risks of stochastic outputs, the extrapolation of prescriptive design knowledge from conventional DSS to AIADM is problematic. Hence, novel design principles incorporating contextual idiosyncrasies and practice-based domain knowledge are needed to overcome unprecedented challenges when adopting AIADM. To this end, we conduct an action design research (ADR) study within an e-commerce company specialising in producing and selling clothing. We develop an AIADM system to support marketing, consumer engagement, and product design decisions. Our work contributes to theory and practice with a set of actionable design principles to guide AIADM system design and deployment.",10.1080/0960085X.2024.2330402,https://www.tandfonline.com/doi/full/10.1080/0960085X.2024.2330402,European Journal of Information Systems,Savindu Herath Pathirannehelage;Yash Raj Shrestha ; Georg von Krogh,2024,1,"@article{2-38330,
  title={Design principles for artificial intelligence-augmented decision making: an action design research study},
  author={Herath Pathirannehelage, Savindu and Shrestha, Yash Raj and von Krogh, Georg},
  year={2024},
  doi={10.1080/0960085X.2024.2330402},
  journal={European Journal of Information Systems}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Organizational,"Advising, Collaborating, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-38331,taylor-and-francis,Design thinking framework for integration of transparency measures in time-critical decision support,"integration of artificial intelligence transparency in time-critical decision support is complex and requires consideration of the impact on human-machine teaming. The relationships between transparency, trust, workload, and situational awareness are key to understanding this impact on performance. We detail the development of a novel design framework for transparency integration in Decision Support Systems. We selected the design thinking approach as the baseline for our framework as this focuses on developing empathy with users and rapid design iteration. We adapted this framework by introducing the concept of empathy for both human and machine agents. In this situation, “empathy” is providing a deep understanding of the model, its purpose and the underlying data for AI. We developed a structured problem definition focused on understanding the relationships between constructs and established solution themes to guide the designer. We demonstrate this transparency integration framework on a Transfer of Care Decision Support System.",10.1080/10447318.2022.2068745,https://www.tandfonline.com/doi/full/10.1080/10447318.2022.2068745,International Journal of Human–Computer Interaction,Paul Stone;Sarah A. Jessup;Subhashini Ganapathy ; Assaf Harel,2022,8,"@article{2-38331,
  title = {Design thinking framework for integration of transparency measures in time-critical decision support},
  author = {Paul Stone and Sarah A. Jessup and Subhashini Ganapathy and Assaf Harel},
  year = {2022},
  journal = {International Journal of Human–Computer Interaction},
  doi = {10.1080/10447318.2022.2068745}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Operational,"Advising, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-38342,taylor-and-francis,Enhancing fairness perception – towards human-centred ai and personalized explanations understanding the factors influencing laypeople’s fairness perceptions of algorithmic decisions,"we like it or not, algorithmic decision-making systems (ADMSs) are all around us. These systems assist both public institutions and private organizations in making decisions that exert a significant impact on our lives. The widespread use of artificial intelligence (AI) and machine learning (ML) systems and the potential risks of using them are the subjects of intensive, ongoing research. It is imperative to ensure their fairness and transparency. The understanding that ADMSs should be subject to human supervision and examined for laypeople’s perceived fairness is clear. Laypeople’s perceptions regarding ADMSs’ fairness, their understanding of the reasons underlying the systems’ outcome (decision), and their comprehension of the linkage between the explanations and the results, influence their willingness to trust the systems, use them and accept their decisions. To determine and better understand which factors affect laypeople’s perceptions of the fairness of algorithmic decisions, we conducted an online between-subject experiment, employing a case study of a simulated AI-based recruitment decision-support system. We focused on three aspects: system characteristics (SC), personality characteristics (PC), and demographic characteristics (DC). We conducted an in-depth analysis to determine which explanation increases the perceived fairness the most. Based on the results, we suggest a framework for predicting a layperson’s perception of the fairness of the explanations. Our findings may help in understanding how to involve humans in the development and evaluation process of ADMSs, how to create personalized explanations based on the SC as well as on users’ PC and DC, and, consequently, how to enhance laypeople’s fairness perceptions regarding ADMSs.",10.1080/10447318.2022.2095705,https://www.tandfonline.com/doi/full/10.1080/10447318.2022.2095705,International Journal of Human–Computer Interaction,Avital Shulner-Tal;Tsvi Kuflik ; Doron Kliger,2023,77,"@article{2-38342,
  title = {Enhancing fairness perception – towards human-centred AI and personalized explanations understanding the factors influencing laypeople’s fairness perceptions of algorithmic decisions},
  author = {Avital Shulner-Tal and Tsvi Kuflik and Doron Kliger},
  year = {2023},
  doi = {10.1080/10447318.2022.2095705},
  journal = {International Journal of Human–Computer Interaction}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service",Operational,"Explaining, Advising",Decision-maker,"Alter decision outcomes, Change trust, Change affective-perceptual","Shape AI for accountability, Update AI competence","textual explanations, recommendations, system accuracy",personalizing,"Textual, Interactive interface",Yes,Yes
2-38346,taylor-and-francis,Estimating dynamic treatment regimes in mobile health using v-learning,"vision for precision medicine is to use individual patient characteristics to inform a personalized treatment plan that leads to the best possible healthcare for each patient. Mobile technologies have an important role to play in this vision as they offer a means to monitor a patient’s health status in real-time and subsequently to deliver interventions if, when, and in the dose that they are needed. Dynamic treatment regimes formalize individualized treatment plans as sequences of decision rules, one per stage of clinical intervention, that map current patient information to a recommended treatment. However, most existing methods for estimating optimal dynamic treatment regimes are designed for a small number of fixed decision points occurring on a coarse time-scale. We propose a new reinforcement learning method for estimating an optimal treatment regime that is applicable to data collected using mobile technologies in an outpatient setting. The proposed method accommodates an indefinite time horizon and minute-by-minute decision making that are common in mobile health applications. We show that the proposed estimators are consistent and asymptotically normal under mild conditions. The proposed methods are applied to estimate an optimal dynamic treatment regime for controlling blood glucose levels in patients with type 1 diabetes.",10.1080/01621459.2018.1537919,https://www.tandfonline.com/doi/full/10.1080/01621459.2018.1537919,Journal of the American Statistical Association,Daniel J. Luckett;Eric B. Laber;Anna R. Kahkoska;David M. Maahs;Elizabeth Mayer-Davis ; Michael R. Kosorok,2020,208,"@article{2-38346,
  title={Estimating dynamic treatment regimes in mobile health using v-learning},
  author={Luckett, Daniel J. and Laber, Eric B. and Kahkoska, Anna R. and Maahs, David M. and Mayer-Davis, Elizabeth and Kosorok, Michael R.},
  year={2020},
  journal={Journal of the American Statistical Association},
  doi={10.1080/01621459.2018.1537919}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Individual,"Executing, Advising, Monitoring","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-38351,taylor-and-francis,Explainable ai: the effect of contradictory decisions and explanations on users’ acceptance of ai systems,"explanations of an artificial intelligence (AI) system has been suggested as a means to increase users’ acceptance during the decision-making process. However, little research has been done to examine the psychological mechanism of how these explanations cause a positive or negative reaction in the user. To address this gap, we investigate the effect on user acceptance if decisions and the associated provided explanations contradict between an AI system and the user. An interdisciplinary research model was derived and validated by an experiment with 78 participants. Findings suggest that in decision situations with cognitive misfit users experience negative mood significantly more often and have a negative evaluation of the AI system’s support. Therefore, the following article provides further guidance regarding new interdisciplinary approaches for dealing with human-AI interaction during the decision-making process and sheds some light on how explainable AI can increase users’ acceptance of such systems.",10.1080/10447318.2022.2126812,https://www.tandfonline.com/doi/full/10.1080/10447318.2022.2126812,International Journal of Human–Computer Interaction,Carolin Ebermann;Matthias Selisky ; Stephan Weibelzahl,2023,0,"@article{2-38351,
  title={Explainable AI: The Effect of Contradictory Decisions and Explanations on Users’ Acceptance of AI Systems},
  author={Ebermann, Carolin and Selisky, Matthias and Weibelzahl, Stephan},
  year={2023},
  journal={International Journal of Human–Computer Interaction},
  doi={10.1080/10447318.2022.2126812}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Explaining, Advising",Decision-maker,"Change cognitive demands, Change affective-perceptual, Alter decision outcomes",no such info,"prediction of alternative, recommendations","corrective feedback, domain knowledge",Autonomous System,Yes,Yes
2-38352,taylor-and-francis,Explainable artificial intelligence improves human decision-making: results from a mushroom picking experiment at a public art festival,"Artificial Intelligence (XAI) enables Artificial Intelligence (AI) to explain its decisions. This holds the promise of making AI more understandable to users, improving interaction, and establishing an adequate level of trust. We tested this claim in the high-risk task of AI-assisted mushroom hunting, where people had to decide whether a mushroom was edible or poisonous. In a between-subjects experiment, 328 visitors of an Austrian media art festival played a tablet-based mushroom hunting game while walking through a highly immersive artificial indoor forest. As part of the game, an artificially intelligent app analyzed photos of the mushrooms they found and recommended classifications. One group saw the AI’s decisions only, while a second group additionally received attribution-based and example-based visual explanations of the AI’s recommendation. The results show that participants with visual explanations outperformed participants without explanations in correct edibility assessments and pick-up decisions. This exhibition-based experiment thus replicated the decision-making results of a previous online study. However, unlike in the previous study, the visual explanations did not significantly affect levels of trust or acceptance measures. In a direct comparison, we consequently discuss the findings in terms of generalizability. Besides the scientific contribution, we discuss the direct impact of conducting XAI experiments in immersive art- and game-based environments in exhibition contexts on visitors and local communities by triggering reflection and awareness for psychological issues of human–AI interaction.",10.1080/10447318.2023.2221605,https://www.tandfonline.com/doi/full/10.1080/10447318.2023.2221605,International Journal of Human–Computer Interaction,Benedikt Leichtmann;Andreas Hinterreiter;Christina Humer;Marc Streit ; Martina Mara,2024,9,"@article{2-38352,
  title={Explainable artificial intelligence improves human decision-making: results from a mushroom picking experiment at a public art festival},
  author={Leichtmann, Benedikt and Hinterreiter, Andreas and Humer, Christina and Streit, Marc and Mara, Martina},
  year={2024},
  journal={International Journal of Human--Computer Interaction},
  volume={40},
  number={1},
  pages={1--23},
  doi={10.1080/10447318.2023.2221605}
}",Empirical contributions,Media / Communication / Entertainment,Individual,"Explaining, Advising",Decision-maker,"Alter decision outcomes, Change affective-perceptual, Change trust",no such info,"recommendations, visual explanations",NA,Interactive interface,Yes,Yes
2-38360,taylor-and-francis,Exploring the role of artificial intelligence in building production resilience: learnings from the covid-19 pandemic,"ever-happening disruptive events interrupt the operationalisation of manufacturing organisations resulting in stalling the production flow and depleting societies with products. Advancements in cutting-edge technologies, viz. blockchain, artificial intelligence, virtual reality, digital twin, etc. have attracted the practitioners’ attention to overcome such saddled conditions. This study attempts to explore the role of artificial intelligence (AI) in building the resilience of production function at manufacturing organisations during a COVID-19 pandemic. In this regard, a decision support system comprising an integrated voting analytical hierarchy process (VAHP) and Bayesian network (BN) method is developed. Initially, through a comprehensive literature review, the critical success factors (CSFs) for implementing AI are determined. Further, using a multi-criteria decision-making (MCDM) based VAHP, CSFs are prioritised to determine the prominent ones. Finally, the machine learning based BN method is adopted to predict and understand the influential CSFs that help achieve the highest production resilience. The present research is one of the early attempts to know the essence of AI and bridge the interplay between AI and production resilience during COVID-19. This study can support academicians, practitioners, and decision-makers in assessing the AI adoption in manufacturing organisations and evaluate the impact of different CSFs of AI on production resilience.",10.1080/00207543.2022.2127961,https://www.tandfonline.com/doi/full/10.1080/00207543.2022.2127961,International Journal of Production Research,Vishwas Dohale;Milind Akarte;Angappa Gunasekaran ; Priyanka Verma,2024,44,"@article{2-38360,
  title = {Exploring the role of artificial intelligence in building production resilience: Learnings from the COVID-19 pandemic},
  author = {Vishwas Dohale and Milind Akarte and Angappa Gunasekaran and Priyanka Verma},
  year = {2024},
  doi = {10.1080/00207543.2022.2127961},
  journal = {International Journal of Production Research}
}",System/Artifact contributions,Manufacturing / Industry / Automation,Organizational,"Analyzing, Advising","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-38367,taylor-and-francis,From external provision to technological outsourcing: lessons for public sector automation from the outsourcing literature,"Automation is not new, but the possibilities for automation have been significantly expanded in recent years through advancements in artificial intelligence. Such technologies may drive some improvements, although they are not without risk and we lack a solid evidence base to suggest the implications of these changes. Framing AI supported automation as ‘technological outsourcing’, we draw on the well-established outsourcing literature to derive lessons about the possible implications of public sector automation and outline some principles that agencies can use to assist in their decision-making about whether to invest in automation of particular processes.",10.1080/14719037.2021.1972681,https://www.tandfonline.com/doi/full/10.1080/14719037.2021.1972681,Public Management Review,Helen Dickinson ; Sophie Yates,2023,47,"@article{2-38367,
  title={From external provision to technological outsourcing: lessons for public sector automation from the outsourcing literature},
  author={Dickinson, Helen and Yates, Sophie},
  year={2023},
  doi={10.1080/14719037.2021.1972681},
  journal={Public Management Review}
}",Theoretical contributions,"Everyday / Employment / Public Service, Law / Policy / Governance",Organizational,"Advising, Executing","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-38375,taylor-and-francis,How being outvoted by ai teammates impacts human-ai collaboration,"advances in artificial intelligence (AI) enable AI agents to go beyond simply supporting human activities and, instead, take more control in team decision-making. While significant literature has studied human-AI collaboration through the lens of AI as a “second opinion system,” this type of interaction is not fully representative of many human-human team collaboration scenarios, such as scenarios where each decision maker is granted equal voting rights for the team decision. In this research, we explore how imparting AI agents with equal voting rights to the human impacts human-AI decision-making and team performance. Using a human subjects experiment in which participants collaborate with two AI teammates for truss structure (aka, bridge) design, we manipulate a series of voting scenarios (e.g., AI agents outvoting the human vs. AI agents agreeing with the human) and AI performance levels (high vs. low performing). The results indicate that changes in human self-confidence are not consistent with whether the quality of the final team-voted design action is advantageous or disadvantageous relative to their own actions. The results also show that when humans are outvoted by their AI teammates, they do not show strong negative emotional reactions if the team-voted decision has an advantageous outcome. Additionally, AI performance significantly influences the human-AI team decision-making process and even one low-performing AI (i.e., an AI that is frequently incorrect) on the team can significantly deteriorate team performance. Taken together, this research provides empirical evidence on the effects of AI voting with equal decision authority on human-AI collaboration, as well as valuable insights supporting real-world applications of human-AI collaboration via voting.",10.1080/10447318.2024.2345980,https://www.tandfonline.com/doi/full/10.1080/10447318.2024.2345980,International Journal of Human–Computer Interaction,Mo Hu;Guanglu Zhang;Leah Chong;Jonathan Cagan ; Kosa Goucher-Lambert,2024,23,"@article{2-38375,
  title = {How being outvoted by AI teammates impacts human-AI collaboration},
  author = {Mo Hu and Guanglu Zhang and Leah Chong and Jonathan Cagan and Kosa Goucher-Lambert},
  year = {2024},
  doi = {10.1080/10447318.2024.2345980},
  journal = {International Journal of Human--Computer Interaction}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Design / Creativity / Architecture",Operational,"Collaborating, Advising","Decision-maker, Decision-subject, Knowledge provider","Alter decision outcomes, Change trust, Change affective-perceptual",no such info,recommendations,voting control,"Interactive interface, Autonomous System",Yes,Yes
2-38376,taylor-and-francis,How counterfactual fairness modelling in algorithms can promote ethical decision-making,"decision-makers often need to make difficult decisions. One popular way today is to improve those decisions by using information and recommendations provided by data-driven algorithms (i.e., AI advisors). Advice is especially important when decisions involve conflicts of interests, such as ethical dilemmas. A defining characteristic of ethical decision-making is that it often involves a thought process of exploring and imagining what would, could, and should happen under alternative conditions (i.e., what-if scenarios). Such imaginative “counterfactual thinking,” however, is not explored by AI advisors - unless they are pre-programmed to do so. Drawing on Fairness Theory, we identify key counterfactual scenarios programmers can incorporate in the code of AI advisors to improve fairness perceptions. We conducted an experimental study to test our predictions, and the results showed that explanations that include counterfactual scenarios were perceived as fairer by recipients. Taken together, we believe that counterfactual modelling will improve ethical decision-making by actively modelling what-if scenarios valued by recipients. We further discuss benefits of counterfactual modelling, such as inspiring decision-makers to engage in counterfactual thinking within their own decision-making process.",10.1080/10447318.2023.2247624,https://www.tandfonline.com/doi/full/10.1080/10447318.2023.2247624,International Journal of Human–Computer Interaction,Leander De Schutter ; David De Cremer,2024,13,"@article{2-38376,
  title = {How counterfactual fairness modelling in algorithms can promote ethical decision-making},
  author = {Leander De Schutter and David De Cremer},
  year = {2024},
  doi = {10.1080/10447318.2023.2247624},
  journal = {International Journal of Human--Computer Interaction}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Institutional,"Explaining, Advising",Decision-maker,"Shape ethical norms, Alter decision outcomes",no such info,fairness constraints,NA,Textual,Yes,Yes
2-38377,taylor-and-francis,How does acknowledging users’ preferences impact ai’s ability to make conflicting recommendations?,"intelligence (AI) decision support systems are crucial in modern decision-making processes. Their increasing human-like adaptability introduces challenges, especially when their recommendations, for whatever reason, need to conflict with user preferences. This study examines the communication strategies AI systems should employ when their recommendations conflict with user preferences. We explored this research question through a hypothetical future interface where ChatGPT offers travel recommendations populated on a map. An online survey-based experiment was conducted, presenting 160 participants with ChatGPT-generated travel recommendations displayed alongside Bing map visuals. We employed a mixed-method experimental design, combining both between-subjects and within-subjects approaches, to investigate the impact of conflicting recommendations and the acknowledgment of user preferences on the acceptance of these recommendations. This effect is especially pronounced when the AI system acknowledges users’ preferences yet still offers conflicting recommendations to them. Contrary to the expectation that acknowledging users’ preferences could buffer the impact of such conflicts, our observations indicate the contrary. The presence of conflict following acknowledgment of users’ preferences, significantly causes a backfire effect, leading users to reject the recommendations. These findings underscore the need for consideration of recommendation delivery strategies in AI decision support systems and offer insights for designing future user interfaces and user experience research in the realm of recommendations provided by AI decision-support systems.",10.1080/10447318.2024.2426035,https://www.tandfonline.com/doi/full/10.1080/10447318.2024.2426035,International Journal of Human–Computer Interaction,Deniz Marti;Anjila Budathoki;Yi Ding;Gale Lucas ; David Nelson,2024,3,"@article{2-38377,
  title = {How does acknowledging users' preferences impact AI's ability to make conflicting recommendations?},
  author = {Deniz Marti and Anjila Budathoki and Yi Ding and Gale Lucas and David Nelson},
  year = {2024},
  journal = {International Journal of Human–Computer Interaction},
  doi = {10.1080/10447318.2024.2426035}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Transportation / Mobility / Planning",Individual,Advising,Decision-maker,"Alter decision outcomes, Change affective-perceptual, Change trust",no such info,recommendations,personalized settings,"Visual, Interactive interface, Conversational/Natural Language",Yes,Yes
2-38378,taylor-and-francis,How does the usage of artificial intelligence affect felt administrative accountability of street-level bureaucrats? The mediating effect of perceived discretion,"intelligence has been increasingly applied in the decision-making process of public organizations, yet we know little about how it affects the felt administrative accountability of street-level bureaucrats. This study conducted an intragroup survey experiment to explore how AI usage affects felt administrative accountability with a representative sample of 190 traffic police. The research finds that AI usage negatively affects felt administrative accountability through perceived discretion. Our findings provide empirical evidence for previous studies and advance our theoretical understanding of the mechanism of how the felt administrative accountability of street-level bureaucrats is affected.",10.1080/14719037.2024.2370982,https://www.tandfonline.com/doi/full/10.1080/14719037.2024.2370982,Public Management Review,Yi Deng ; Yu Sun,2024,9,"@article{2-38378,
  title={How does the usage of artificial intelligence affect felt administrative accountability of street-level bureaucrats? The mediating effect of perceived discretion},
  author={Deng, Yi and Sun, Yu},
  year={2024},
  journal={Public Management Review},
  doi={10.1080/14719037.2024.2370982}
}",Empirical contributions,Law / Policy / Governance,Operational,Advising,Decision-maker,"Shift responsibility, Restrict human agency",no such info,NA,NA,"Semi-Autonomous System, Autonomous System",Yes,Yes
2-38383,taylor-and-francis,Human-centric artificial intelligence architecture for industry 5.0 applications,"Human-centricity is the core value behind the evolution of manufacturing towards Industry 5.0. Nevertheless, there is a lack of architecture that considers safety, trustworthiness, and human-centricity at its core. Therefore, we propose an architecture that integrates Artificial Intelligence (Active Learning, Forecasting, Explainable Artificial Intelligence), simulated reality, decision-making, and users' feedback, focussing on synergies between humans and machines. Furthermore, we align the proposed architecture with the Big Data Value Association Reference Architecture Model. Finally, we validate it on three use cases from real-world case studies.",10.1080/00207543.2022.2138611,https://www.tandfonline.com/doi/full/10.1080/00207543.2022.2138611,International Journal of Production Research,Jože M. Rožanec;Inna Novalija;Patrik Zajec;Klemen Kenda;Hooman Tavakoli Ghinani;Sungho Suh;Entso Veliou;Dimitrios Papamartzivanos;Thanassis Giannetsos;Sofia Anna Menesidou;Ruben Alonso;Nino Cauli;Antonello Meloni;Diego Reforgiato Recupero;Dimosthenis Kyriazis;Georgios Sofianidis;Spyros Theodoropoulos;Blaž Fortuna;Dunja Mladenić ; John Soldatos,2023,288,"@article{2-38383,
  title = {Human-centric artificial intelligence architecture for industry 5.0 applications},
  author = {Jože M. Rožanec and Inna Novalija and Patrik Zajec and Klemen Kenda and Hooman Tavakoli Ghinani and Sungho Suh and Entso Veliou and Dimitrios Papamartzivanos and Thanassis Giannetsos and Sofia Anna Menesidou and Ruben Alonso and Nino Cauli and Antonello Meloni and Diego Reforgiato Recupero and Dimosthenis Kyriazis and Georgios Sofianidis and Spyros Theodoropoulos and Blaž Fortuna and Dunja Mladenić and John Soldatos},
  year = {2023},
  doi = {10.1080/00207543.2022.2138611},
  journal = {International Journal of Production Research}
}",System/Artifact contributions,Manufacturing / Industry / Automation,Operational,"Forecasting, Explaining","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-38389,taylor-and-francis,Improving trust in ai with mitigating confirmation bias: effects of explanation type and debiasing strategy for decision-making with explainable ai,"advancements in artificial intelligence (AI), explainable AI (XAI) has emerged as a promising tool for enhancing the explainability of complex machine learning models. However, the explanations generated by an XAI may lead to cognitive biases among human users. To address this problem, this study aims to investigate how to mitigate users’ cognitive biases based on their individual characteristics. In the literature review, we found two factors that can be helpful in remedying biases: 1) debiasing strategies that have been reported to potentially reduce biases in users’ decision-making via additional information or change in information delivery, and 2) explanation modality types. To examine these factors’ effects, we conducted an experiment with a 4 (debiasing strategy) × 3 (explanation type) between-subject design. In the experiment, participants were exposed to an explainable interface that provides an AI’s outcomes with explanatory information, and their behavioral and attitudinal responses were collected. Specifically, we statistically examined the effects of textual and visual explanations on users’ trust and confirmation bias toward AI systems, considering the moderating effects of debiasing methods and watching time. The results demonstrated that textual explanations lead to higher trust in XAI systems compared to visual explanations. Moreover, we found that textual explanations are particularly beneficial for quick decision-makers to evaluate the outputs of AI systems. Next, the results indicated that the cognitive bias can be effectively mitigated by providing users with a priori information. These findings have theoretical and practical implications for designing AI-based decision support systems that can generate more trustworthy and equitable explanations.",10.1080/10447318.2023.2285640,https://www.tandfonline.com/doi/full/10.1080/10447318.2023.2285640,International Journal of Human–Computer Interaction,Taehyun Ha ; Sangyeon Kim,2024,0,"@article{2-38389,
  title={Improving trust in AI with mitigating confirmation bias: effects of explanation type and debiasing strategy for decision-making with explainable AI},
  author={Ha, Taehyun and Kim, Sangyeon},
  year={2024},
  doi={10.1080/10447318.2023.2285640},
  journal={International Journal of Human--Computer Interaction}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Operational,Explaining,Decision-maker,"Alter decision outcomes, Change trust",no such info,"visual explanations, prediction of alternative, textual explanations",NA,"Textual, Visual",Yes,Yes
2-38396,taylor-and-francis,Integrated inventory control and scheduling decision framework for packaging and products on a reusable transport item sharing platform,"study considers the problem of inventory and scheduling decisions on a reusable transport item (RTI) sharing platform with the collaborative recovery of used RTIs and replenishment of products in a two-tier container management centre (CMC). The products (packaged as full RTIs) are pre-positioned at the regional CMC (R-CMC), and empty RTIs are stored at the CMC hub. Moreover, the CMC replenishes the products and recycles RTIs respectively and periodically. The RTI and products are a set of complementary products, and the replenishment task requires sufficient empty RTIs in stock. Untimely and insufficient RTI returns without considering product inventory changes often result in RTI out-of-stock situations that harm the customer's lean productivity. This paper proposes a machine learning and simulation optimisation (MSO) decision framework to collaboratively assist RTI inventory and scheduling decisions in a two-tier CMC. Based on a case study, we can conclude the decision framework has better performance on the profitability and inventory control capability. Moreover, different inventory and scheduling parameter settings in the two-tier CMCs impact the platform's profitability to derive corresponding management insights, and a decision system can be built based on the above framework.",10.1080/00207543.2023.2187243,https://www.tandfonline.com/doi/full/10.1080/00207543.2023.2187243,International Journal of Production Research,Min Guo;Xiang T. R. Kong;Hing Kai Chan ; Dimple R. Thadani,2023,0,"@article{2-38396,
  title = {Integrated inventory control and scheduling decision framework for packaging and products on a reusable transport item sharing platform},
  author = {Min Guo and Xiang T. R. Kong and Hing Kai Chan and Dimple R. Thadani},
  year = {2023},
  journal = {International Journal of Production Research},
  doi = {10.1080/00207543.2023.2187243}
}",Methodological contributions,Manufacturing / Industry / Automation,Operational,"Advising, Collaborating","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-38402,taylor-and-francis,Investigating trust in human-ai collaboration for a speech-based data analytics task,"real-world problems can benefit from the collaboration between humans and artificial intelligence (AI) to achieve reliable decision-making. We investigate trust in a human-in-the-loop decision-making task, in which participants with background on psychological sciences collaborate with an explainable AI system for estimating one’s anxiety level from speech. The AI system relies on the explainable boosting machine (EBM) model which takes prosodic features as the input and estimates the anxiety level. Trust in AI is quantified via self-reported (i.e., administered via a questionnaire) and behavioral (i.e., computed as user-AI agreement) measures, which are positively correlated with each other. Results indicate that humans and AI depict differences in performance depending on the characteristics of the specific case under review. Overall, human annotators’ trust in the AI increases over time, with momentary decreases after the AI partner makes an error. Annotators further differ in terms of appropriate trust calibration in the AI system, with some annotators over-trusting and some under-trusting the system. Personality characteristics (i.e., agreeableness, conscientiousness) and overall propensity to trust machines further affect the level of trust in the AI system, with these findings approaching statistical significance. Results from this work will lead to a better understanding of human-AI collaboration and will guide the design of AI algorithms toward supporting better calibration of user trust.",10.1080/10447318.2024.2328910,https://www.tandfonline.com/doi/full/10.1080/10447318.2024.2328910,International Journal of Human–Computer Interaction,Abdullah Aman Tutul;Ehsanul Haque Nirjhar ; Theodora Chaspari,2024,13,"@article{2-38402,
  title={Investigating trust in human-ai collaboration for a speech-based data analytics task},
  author={Tutul, Abdullah Aman and Nirjhar, Ehsanul Haque and Chaspari, Theodora},
  year={2024},
  journal={International Journal of Human--Computer Interaction},
  doi={10.1080/10447318.2024.2328910}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,"Explaining, Collaborating","Decision-maker, Knowledge provider",Change trust,Update AI competence,visual explanations,"domain knowledge, human-impact-informed algorithm design","Interactive interface, Visual, Textual",Yes,Yes
2-38406,taylor-and-francis,Large-scale dynamic surgical scheduling under uncertainty by hierarchical reinforcement learning,"Dynamic surgical scheduling within a workday is a complicated decision-making process. The critical challenge is that the actual duration of surgery and the arrival process of emergency patients are uncertain and unknown in advance. In this work, we propose a two-level dynamic scheduling framework based on hierarchical reinforcement learning to solve dynamic surgical scheduling problems considering both elective and emergency patients. Specifically, with the realisation of uncertainty, the upper-level agent (UA) dynamically decides whether to trigger rescheduling to optimise the workday total cost. The lower-level agent (LA) aims at obtaining subscheduling solutions when rescheduling is triggered. The subproblem at the LA can be formulated as a mixed integer programming model, which can be generalised to unrelated parallel machine scheduling with machine eligibility restrictions and sequence- and machine-dependent setup times. Such problems can be solved in small-scale cases and suffers the combinatorial explosion in large scale cases. To address this issue, we propose a heuristic method that is built upon deep reinforcement learning to obtain high-quality solutions. We conduct extensive simulation experiments with real data to test the effective of our framework. The results for different scenarios show that our proposed framework outperforms existing methods in terms of overall performance and has strong generalisation ability.",10.1080/00207543.2024.2361449,https://www.tandfonline.com/doi/full/10.1080/00207543.2024.2361449,International Journal of Production Research,Lixiang Zhao;Han Zhu;Min Zhang;Jiafu Tang ; Yu Wang,2024,1,"@article{2-38406,
  title={Large-scale dynamic surgical scheduling under uncertainty by hierarchical reinforcement learning},
  author={Zhao, Lixiang and Zhu, Han and Zhang, Min and Tang, Jiafu and Wang, Yu},
  year={2024},
  journal={International Journal of Production Research},
  volume={},
  number={},
  pages={},
  doi={10.1080/00207543.2024.2361449}
}","Algorithmic contributions, Methodological contributions",Healthcare / Medicine / Surgery,Operational,Executing,"Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-38409,taylor-and-francis,Learning to trust skynet: interfacing with artificial intelligence in cyberspace,"The use of AI to automate defense and intelligence tasks is increasing. And yet, little is known about how algorithmic analyses, data capture, and decisions will be perceived by elite decision-makers. This article presents the results of two experiments that explore manifestations of AI systems in the cyber conflict decision-making loop. Though findings suggest that technical expertise positively impacts respondents’ ability to gauge the potential utility and credibility of an input (indicating that training can, in fact, overcome bias), the perception of human agency in the loop even in the presence of AI inputs mitigates this effect and makes decision-makers more willing to operate on less information. This finding is worrying given the extensive challenges involved in effectively building human oversight and opportunity for intervention into any effective employment of AI for national security purposes. The article considers these obstacles and potential solutions in the context of data gathered.",10.1080/13523260.2023.2180882,https://www.tandfonline.com/doi/full/10.1080/13523260.2023.2180882,Contemporary Security Policy,Christopher Whyte,2023,14,"@article{2-38409,
  title={Learning to trust skynet: interfacing with artificial intelligence in cyberspace},
  author={Whyte, Christopher},
  year={2023},
  journal={Contemporary Security Policy},
  doi={10.1080/13523260.2023.2180882}
}",Empirical contributions,"Software / Systems / Security, Defense / Military / Emergency",Organizational,"Advising, Auditing","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-38432,taylor-and-francis,More than a digital system: how ai is changing the role of bureaucrats in different organizational contexts,"paper highlights the effects of AI implementation on public sector innovation. This is explored by asking how AI-driven technologies in public decision-making in different organizational contexts impacts innovation in the role definition of bureaucrats. We focus on organizational as well as agency- and individual-level factors in two cases: The Dutch Childcare Allowance case and the US Integrated Data Automated System. We observe administrative process innovation in both cases where organizational structures and tasks of bureaucrats are transformed, and in the US case we also find conceptual innovation in that welfare fraud is addressed by replacing bureaucrats all together.",10.1080/14719037.2022.2095001,https://www.tandfonline.com/doi/full/10.1080/14719037.2022.2095001,Public Management Review,Sarah N. Giest ; Bram Klievink,2024,115,"@article{2-38432,
  title = {More than a Digital System: How AI is Changing the Role of Bureaucrats in Different Organizational Contexts},
  author = {Sarah N. Giest and Bram Klievink},
  year = {2024},
  doi = {10.1080/14719037.2022.2095001},
  journal = {Public Management Review}
}",Empirical contributions,"Law / Policy / Governance, Everyday / Employment / Public Service",Organizational,"Executing, Auditing","Guardian, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-38438,taylor-and-francis,On learning and testing of counterfactual fairness through data preprocessing,"learning has become more important in real-life decision-making but people are concerned about the ethical problems it may bring when used improperly. Recent work brings the discussion of machine learning fairness into the causal framework and elaborates on the concept of Counterfactual Fairness. In this article, we develop the Fair Learning through dAta Preprocessing (FLAP) algorithm to learn counterfactually fair decisions from biased training data and formalize the conditions where different data preprocessing procedures should be used to guarantee counterfactual fairness. We also show that Counterfactual Fairness is equivalent to the conditional independence of the decisions and the sensitive attributes given the processed nonsensitive attributes, which enables us to detect discrimination in the original decision using the processed data. The performance of our algorithm is illustrated using simulated data and real-world applications. Supplementary materials for this article are available online.",10.1080/01621459.2023.2186885,https://www.tandfonline.com/doi/full/10.1080/01621459.2023.2186885,Journal of the American Statistical Association,Haoyu Chen;Wenbin Lu;Rui Song ; Pulak Ghosh,2024,7,"@article{2-38438,
  title={On learning and testing of counterfactual fairness through data preprocessing},
  author={Chen, Haoyu and Lu, Wenbin and Song, Rui and Ghosh, Pulak},
  year={2024},
  journal={Journal of the American Statistical Association},
  doi={10.1080/01621459.2023.2186885}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Institutional,"Auditing, Advising","Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-38448,taylor-and-francis,"Practice with less ai makes perfect: partially automated ai during training leads to better worker motivation, engagement, and skill acquisition","increased prevalence of human-AI collaboration is reshaping the manufacturing sector, fundamentally changing the nature of human work and training needs. While high automation improves performance when functioning correctly, it can lead to problematic human performance (e.g., defect detection accuracy, response time) when operators are required to intervene and assume manual control of decision-making responsibilities. As AI capability reaches higher levels of automation and human–AI collaboration becomes ubiquitous, addressing these performance issues is crucial. Proper worker training, focusing on skill-based, cognitive, and affective outcomes, and nurturing motivation and engagement, can be a mitigation strategy. However, most training research in manufacturing has prioritized the effectiveness of a technology for training, rather than how training design influences motivation and engagement, key to training success and longevity. The current study explored how training workers using an AI system affected their motivation, engagement, and skill acquisition. Specifically, we manipulated the level of automation of decision selection of an AI used for the training of 102 participants for a quality control task. Findings indicated that fully automated decision selection negatively impacted perceived autonomy, self-determined motivation, behavioral task engagement, and skill acquisition during training. Conversely, partially automated AI-enhanced motivation and engagement, enabling participants to better adapt to AI failure by developing necessary skills. The results suggest that involving workers in decision-making during training, using AI as a decision aid rather than a decision selector, yields more positive outcomes. This approach ensures that the human aspect of manufacturing work is not overlooked, maintaining a balance between technological advancement and human skill development, motivation, and engagement. These findings can be applied to enhance real-world manufacturing practices by designing training programs that better develop operators’ technical, methodological, and personal skills, though companies may face challenges in allocating substantial resources for training redevelopment and continuously adapting these programs to keep pace with evolving technology.",10.1080/10447318.2024.2319914,https://www.tandfonline.com/doi/full/10.1080/10447318.2024.2319914,International Journal of Human–Computer Interaction,Mario Passalacqua;Robert Pellerin;Esma Yahia;Florian Magnani;Frédéric Rosin;Laurent Joblot ; Pierre-Majorique Léger,2025,40,"@article{2-38448,
  title = {Practice with less AI makes perfect: Partially automated AI during training leads to better worker motivation, engagement, and skill acquisition},
  author = {Mario Passalacqua and Robert Pellerin and Esma Yahia and Florian Magnani and Frédéric Rosin and Laurent Joblot and Pierre-Majorique Léger},
  year = {2025},
  doi = {10.1080/10447318.2024.2319914},
  journal = {International Journal of Human–Computer Interaction}
}",Empirical contributions,Manufacturing / Industry / Automation,Individual,"Advising, Executing",Decision-maker,"Restrict human agency, Change cognitive demands, Alter decision outcomes",no such info,delegation,NA,"Autonomous System, Semi-Autonomous System, Interactive interface, Physical / Embodiment",Yes,Yes
2-38451,taylor-and-francis,Predicting households’ residential mobility trajectories with geographically localized interpretable model-agnostic explanation (glime),"mobility analytics using artificial intelligence (AI) has gained significant attention with advancements in computational power and the availability of high-resolution spatial data. However, the application of deep learning in social sciences and human geography remains limited, primarily due to concerns with model explainability. In this study, we employ an explainable GeoAI approach called geographically localized interpretable model-agnostic explanation (GLIME) to explore human mobility patterns over large spatial and temporal extents. Specifically, we develop a two-layered long short-term memory (LSTM) model capable of predicting individual-level residential mobility patterns across the United States from 2012 to 2019. We leverage GLIME to provide geographical perspectives and interpret deep neural networks at the state level. The results reveal that GLIME enables spatially explicit interpretations of local impacts attributed to different variables. Our findings underscore the significance of considering path dependency in residential mobility dynamics. While the prediction of complex human spatial decision-making processes still presents challenges, this research demonstrates the utility of deep neural networks and explainable GeoAI to support human dynamics understanding. It sets the stage for further finely tuned investigations in the future, promising deep insights into intricate mobility phenomena.",10.1080/13658816.2023.2264921,https://www.tandfonline.com/doi/full/10.1080/13658816.2023.2264921,International Journal of Geographical Information Science,Chanwoo Jin;Sohyun Park;Hui Jeong Ha;Jinhyung Lee;Junghwan Kim;Johan Hutchenreuther ; Atsushi Nara,2023,10,"@article{2-38451,
  title={Predicting households’ residential mobility trajectories with geographically localized interpretable model-agnostic explanation (GLIME)},
  author={Jin, Chanwoo and Park, Sohyun and Ha, Hui Jeong and Lee, Jinhyung and Kim, Junghwan and Hutchenreuther, Johan and Nara, Atsushi},
  year={2023},
  journal={International Journal of Geographical Information Science},
  volume={},
  number={},
  pages={},
  doi={10.1080/13658816.2023.2264921}
}",Methodological contributions,"Transportation / Mobility / Planning, Everyday / Employment / Public Service",Individual,"Explaining, Forecasting, Auditing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-38458,taylor-and-francis,Radar: automated task planning for proactive decision support,"Decision Support aims at improving the decision making experience of human decision-makers by enhancing the quality of the decisions and the ease of making them. Given that AI techniques are efficient in searching over a potentially large solution space (of decision) and finding good solutions, it can be used for human-in-the-loop scenarios such as disaster response that demand naturalistic decision making. A human decision-maker, in such scenarios, may experience high-cognitive overload leading to a loss of situational awareness. In this paper, we propose the use of automated task-planning techniques coupled with design principles laid out in the Human-Computer Interaction (HCI) community for developing a proactive decision support system. To this extent, we highlight the capabilities of such a system RADAR and briefly, describe how automated planning techniques help us in providing the varying degrees of assistance. To evaluate the effectiveness of the different capabilities, we conduct ablation studies with human subjects on a synthetic environment for making an interactive plan of study. We found that planning techniques like plan validation and suggestions help to reduce planning time (objective metrics) and improves user satisfaction (subjective metrics) compared to expert human planners without any support.",10.1080/07370024.2020.1726751,https://www.tandfonline.com/doi/full/10.1080/07370024.2020.1726751,Human–Computer Interaction,Sachin Grover;Sailik Sengupta;Tathagata Chakraborti;Aditya Prasad Mishra ; Subbarao Kambhampati,2020,45,"@article{2-38458,
  title={Radar: automated task planning for proactive decision support},
  author={Grover, Sachin and Sengupta, Sailik and Chakraborti, Tathagata and Mishra, Aditya Prasad and Kambhampati, Subbarao},
  year={2020},
  doi={10.1080/07370024.2020.1726751},
  journal={Human--Computer Interaction}
}",System/Artifact contributions,"Generic / Abstract / Domain-agnostic, Defense / Military / Emergency",Operational,"Advising, Analyzing","Decision-subject, Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-38460,taylor-and-francis,Real-time agv scheduling optimisation method with deep reinforcement learning for energy-efficiency in the container terminal yard,"increasing vessel size and automation level have shifted the productivity bottleneck of automated container terminals from the terminal side to the yard side. Operating an automated container terminal (ACT) yard with a big number of automated guided vehicles (AGV) is challenging due to the complexity and dynamics of the system, severely affecting the operational efficiency and energy use efficiency. In this paper, a hybrid multi-AGV scheduling algorithm is proposed to minimise the energy consumption and the total makespan of AGVs in an ACT yard. This framework first models the AGV scheduling process as a Markov decision process (MDP). Furthermore, a novel scheduling algorithm called MDAS is proposed based on multi-agent deep deterministic policy gradient (MADDPG) to facilitate online real-time scheduling decision-making. Finally, simulation experiments show that the proposed method can effectively enhance the operational efficiency and energy use performance of AGVs in ACT yards of various scales by comparing with benchmarking methods.",10.1080/00207543.2024.2325583,https://www.tandfonline.com/doi/full/10.1080/00207543.2024.2325583,International Journal of Production Research,Lin Gong;Zijie Huang;Xi Xiang ; Xin Liu,2024,0,"@article{2-38460,
  title={Real-time AGV scheduling optimisation method with deep reinforcement learning for energy-efficiency in the container terminal yard},
  author={Gong, Lin and Huang, Zijie and Xiang, Xi and Liu, Xin},
  year={2024},
  journal={International Journal of Production Research},
  doi={10.1080/00207543.2024.2325583}
}",Algorithmic contributions,"Transportation / Mobility / Planning, Environment / Resources / Energy, Manufacturing / Industry / Automation",Operational,Executing,Guardian,NA,NA,NA,NA,NA,Yes,No
2-38461,taylor-and-francis,Real-time stochastic flexible flow shop scheduling in a credit factory with model-based reinforcement learning,"has been a significant increase in consumer credit worldwide in recent years. The scheduling of jobs in credit factories is essential for speeding up the loan application process, which can improve the efficiency of credit factories. In this study, we propose a reinforcement learning approach for addressing the scheduling problem in credit factories, which is a stochastic flexible flow shop scheduling problem (SFFSP). First, we propose a mathematical model for the credit factory stochastic flexible flow shop scheduling problem, which abstracts the decision-making process as a semi-Markov process. Then, a reinforcement learning reward mechanism is designed based on the proposed mathematical model. After that, a self-attention neural network is used to extract state information from global and local multidimensional data, enabling each decision to consider the state of the entire process and make a decision that aligns with the global goal. Meanwhile, Monte Carlo Tree Search (MCTS) is utilised to enhance the training effect and sample utilisation of reinforcement learning. Finally, we conduct extensive experiments and demonstrate that our method achieves better performance for SFFSP in credit factories compared to other approaches.",10.1080/00207543.2024.2361441,https://www.tandfonline.com/doi/full/10.1080/00207543.2024.2361441,International Journal of Production Research,Liao Chen;Hongjia Liu;Ning Jia;Nianlu Ren;Runbang Cui ; Wei Wei,2025,8,"@article{2-38461,
  title = {Real-time stochastic flexible flow shop scheduling in a credit factory with model-based reinforcement learning},
  author = {Liao Chen and Hongjia Liu and Ning Jia and Nianlu Ren and Runbang Cui and Wei Wei},
  year = {2025},
  journal = {International Journal of Production Research},
  volume = {},
  number = {},
  pages = {},
  doi = {10.1080/00207543.2024.2361441}
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Advising, Executing","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-38464,taylor-and-francis,Recurrent u-net based dynamic paddy rice mapping in south korea with enhanced data compatibility to support agricultural decision making,"integration of remote sensing and state-of-the-art deep learning models has enabled the generation of highly accurate semantic segmentation maps to serve the agricultural sector, for which continuous land monitoring is required. However, despite their wide presence in the research field, only a few such products are used in on-site decision-making processes. This is due to their incompatibility with existing datasets that are at the core of current operating processes. In this study, paddy rice mapping in South Korea was examined to determine whether it produces qualified products that can complement on-site surveys and simultaneously be compatible with existing domestic datasets. Cases of early predictions for timely rice supply control were examined using a recurrent U-Net architecture with diverse applications: chronological batch training (CBT), time-inversed padding (TIP), and super-resolution (SR). In addition, the paddy area was confirmed using diverse datasets by standardizing its spatial extent in the definition of each data manual and calibrating the levee error, which was considered a major source of incompatibility. The robustness of the recurrent U-Net in early predictions dramatically increased upon CBT and TIP, recording an F1 score of over 0.75 on July 10, when the on-site survey was performed; meanwhile, the best performance score was 0.81 at the end of the growing period. SR enhanced the spatial details of rice mapping near the levee area, which had an estimated width of 60 cm; however, the area was more similar to that in existing datasets when it was calibrated with the predicted probability of the levee ratio rather than SR. The calibration was scalable from the patch to city level, with the paddy area at both levels recording high R2 for the farm map and statistics (0.99 for both the farm map and statistics at the city level, and 0.93 and 0.95, respectively, at the patch level). This study shows that remote-sensing-based paddy rice mapping can produce not only accurate but also timely and compatible predictions by integrating deep learning applications. The results show that the predictions are compatible with domestic datasets as much as they are with each other; therefore, remote-sensing approaches are expected to be more actively and practically integrated into agricultural decision-making processes.",10.1080/15481603.2023.2206539,https://www.tandfonline.com/doi/full/10.1080/15481603.2023.2206539,GIScience & Remote Sensing,Hyun-Woo Jo;Eunbeen Park;Vasileios Sitokonstantinou;Joon Kim;Sujong Lee;Alkiviadis Koukos ; Woo-Kyun Lee,2023,11,"@article{2-38464,
  title = {Recurrent U-Net Based Dynamic Paddy Rice Mapping in South Korea with Enhanced Data Compatibility to Support Agricultural Decision Making},
  author = {Hyun-Woo Jo and Eunbeen Park and Vasileios Sitokonstantinou and Joon Kim and Sujong Lee and Alkiviadis Koukos and Woo-Kyun Lee},
  year = {2023},
  doi = {10.1080/15481603.2023.2206539},
  journal = {GIScience \& Remote Sensing}
}",System/Artifact contributions,Environment / Resources / Energy,Operational,"Forecasting, Analyzing","Decision-maker, Stakeholder, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-38469,taylor-and-francis,Reinforcement learning-based dynamic production-logistics-integrated tasks allocation in smart factories,"Industry 4.0, the production planning and execution of smart factories (SFs) full of continuously delivered small-batch orders become dynamic and complicated. Traditional centralised manufacture planning is difficult to handle unexpected disturbances. With the aid of new information technologies, resources in SFs become smart and connected to make autonomous decisions. This paper tries to release intelligence of smart connected resources to allocate production tasks and logistics tasks in SFs coordinately and autonomously. The architecture is modelled as an autonomous decision-making manufacturing system with IIoT support, which aims to synchronously allocate manufacturing tasks by the bidding of resources in SFs. Then, a dynamic production-logistics-integrated tasks allocation model is built. The orders makespan and resources utilisation are considered as the objective function, and production resources and logistics resources are integrated to autonomously communicate and interact with each other to bid for dynamic production-logistics integrated operations. To figure out, a reinforcement learning (RL) algorithm is studied, which makes operations decisions for each job step by step based on in-situ data during manufacturing process. Finally, a demonstrative case showed that compared to centralised scheduling system, the RL-based model performs better in handling production-logistics-integrated tasks allocation problem in SFs full of dynamic and small-batch individualised orders.",10.1080/00207543.2022.2142314,https://www.tandfonline.com/doi/full/10.1080/00207543.2022.2142314,International Journal of Production Research,Jingyuan Lei;Jizhuang Hui;Fengtian Chang;Salim Dassari ; Kai Ding,2023,24,"@article{2-38469,
  title={Reinforcement learning-based dynamic production-logistics-integrated tasks allocation in smart factories},
  author={Lei, Jingyuan and Hui, Jizhuang and Chang, Fengtian and Dassari, Salim and Ding, Kai},
  year={2023},
  journal={International Journal of Production Research},
  doi={10.1080/00207543.2022.2142314}
}",Algorithmic contributions,Manufacturing / Industry / Automation,Operational,"Executing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-38472,taylor-and-francis,Research on the application framework of generative ai in emergency response decision support systems for emergencies,"the rapid development of artificial intelligence technology, generative AI has shown broad application prospects and potential in various fields. Frequent emergencies have put forward higher requirements for traditional Emergency Response Decision Support Systems (ERDSS). This paper proposes a theoretical framework of ERDSS based on generative AI (ERDSS-GAI), which deeply integrates generative AI with three stages of emergency response decision-making. The framework aims to leverage the advantages of generative AI in massive data processing, knowledge mining, strategy optimization, and other aspects, thereby enhancing the intelligence level and emergency response capability of ERDSS. The key components and implementation path of ERDSS-GAI are systematically explained from a theoretical perspective, and its application value is analyzed through the case of rainstorm and flood disaster in Shenzhen. This research demonstrates that generative AI can improve the scientific and refined level of emergency response decision-making, providing a theoretical framework and practical insights for its widespread adoption in emergency management.",10.1080/10447318.2024.2423335,https://www.tandfonline.com/doi/full/10.1080/10447318.2024.2423335,International Journal of Human–Computer Interaction,Siqing Shan ; Yinong Li,2024,14,"@article{2-38472,
  title = {Research on the application framework of generative AI in emergency response decision support systems for emergencies},
  author = {Siqing Shan and Yinong Li},
  year = {2024},
  journal = {International Journal of Human–Computer Interaction},
  doi = {10.1080/10447318.2024.2423335}
}",Theoretical contributions,Defense / Military / Emergency,Organizational,"Advising, Executing","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-38492,taylor-and-francis,Team up with ai or human? Investigating candidates’ self-categorization as fluidity and ingroup-serving attribution when judged by a human–ai hybrid jury,"artificial intelligence (AI) judges are increasingly pervasive in decision-making, it is important to investigate candidates’ reactions to decisions made by human–AI hybrid juries. This study investigates candidates’ attribution of credit for success and blame for failure to the three agents in question: a human judge, an algorithmic judge, and the candidate oneself. An experiment with 3 (jury type: human-dominated, algorithm-dominated, vs. equally dominated) × 2 (decision outcome: positive vs. negative) between-subjects factorial design was conducted, with 346 valid responses. Our findings demonstrate a partial ingroup-serving attribution dependent on the outcome favorability and a significant effect of relative power status within the human–AI hybrid jury on grouping and attribution. This study reflects the fluidity of identity and self-categorization of human users when facing AI and other humans. We propose that people take a utility-oriented glance at AI in multi-agent decision-making situations.",10.1080/10447318.2024.2408511,https://www.tandfonline.com/doi/full/10.1080/10447318.2024.2408511,International Journal of Human–Computer Interaction,Shuyi Pan ; Yi Mou,2024,1,"@article{2-38492,
  title={Team up with AI or human? Investigating candidates' self-categorization as fluidity and ingroup-serving attribution when judged by a human--AI hybrid jury},
  author={Pan, Shuyi and Mou, Yi},
  year={2024},
  journal={International Journal of Human--Computer Interaction},
  doi={10.1080/10447318.2024.2408511}
}",Empirical contributions,Everyday / Employment / Public Service,Operational,"Monitoring, Collaborating","Guardian, Decision-maker, Decision-subject","Alter decision outcomes, Shift responsibility",no such info,power,"domain knowledge, power","Textual, Visual, Autonomous System, Semi-Autonomous System",Yes,Yes
2-38494,taylor-and-francis,The assistant project: ai for high level decisions in manufacturing,"paper outlines the main idea and approach of the H2020 ASSISTANT (LeArning and robuSt deciSIon SupporT systems for agile mANufacTuring environments) project. ASSISTANT is aimed at the investigation of AI-based tools for adaptive manufacturing environments, and focuses on the development of a set of digital twins for integration with, management of, and decision support for production planning and control. The ASSISTANT tools are based on the approach of extending generative design, an established methodology for product design, to a broader set of manufacturing decision making processes; and to make use of machine learning, optimisation, and simulation techniques to produce executable models capable of ethical reasoning and data-driven decision making for manufacturing systems. Combining human control and accountable AI, the ASSISTANT toolsets span a wide range of manufacturing processes and time scales, including process planning, production planning, scheduling, and real-time control. They are designed to be adaptable and applicable in a both general and specific manufacturing environments.",10.1080/00207543.2022.2069525,https://www.tandfonline.com/doi/full/10.1080/00207543.2022.2069525,International Journal of Production Research,G. Castañé;A. Dolgui;N. Kousi;B. Meyers;S. Thevenin;E. Vyhmeister ; P-O. Östberg,2023,103,"@article{2-38494,
  title        = {The assistant project: AI for high level decisions in manufacturing},
  author       = {Castañé, G. and Dolgui, A. and Kousi, N. and Meyers, B. and Thevenin, S. and Vyhmeister, E. and Östberg, P-O.},
  year         = {2023},
  journal      = {International Journal of Production Research},
  volume       = {0},
  number       = {0},
  pages        = {1-20},
  doi          = {10.1080/00207543.2022.2069525}
}",System/Artifact contributions,Manufacturing / Industry / Automation,"Institutional, Operational","Advising, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-38496,taylor-and-francis,The digital siren’s call: accepting unethical ai advice,"growing ubiquity of artificial intelligence (AI) in business raises concerns about how humans perceive AI-generated advice, especially in ethical contexts. This study, using a preregistered experiment, explores whether people are more likely to accept unethical business advice from AI compared to human advisors, and examines how individual differences in moral reasoning, decision-making style, and trust in AI influence this acceptance. Participants were presented with business decisions based on advice from either an AI or a human, with advice varying in ethicality. Results showed participants were more likely to accept unethical advice from AI than from human advisors. Lower moral reasoning, an intuitive decision-making style, and higher trust in AI were associated with greater acceptance of unethical AI advice. These findings emphasize the risks of overreliance on AI in decision-making and the importance of promoting ethical AI use in business.",10.1080/10447318.2024.2400396,https://www.tandfonline.com/doi/full/10.1080/10447318.2024.2400396,International Journal of Human–Computer Interaction,Cheng Xu;Hao Xu;Yanqi Sun ; Wanfang Xiong,2024,2,"@article{2-38496,
  title={The digital siren’s call: accepting unethical AI advice},
  author={Cheng Xu and Hao Xu and Yanqi Sun and Wanfang Xiong},
  year={2024},
  journal={International Journal of Human–Computer Interaction},
  doi={10.1080/10447318.2024.2400396}
}",Empirical contributions,Finance / Business / Economy,Operational,Advising,"Decision-maker, Knowledge provider","Shift responsibility, Alter decision outcomes, Change trust",no such info,recommendations,"trust, ethical constraints",Textual,Yes,Yes
2-38509,taylor-and-francis,Towards ai driven environmental sustainability: an application of automated logistics in container port terminals,"intelligence and data analytics capabilities have enabled the introduction of automation, such as robotics and Automated Guided Vehicles (AGVs), across different sectors of the production spectrum which successively has profound implications for operational efficiency and productivity. However, the environmental sustainability implications of such innovations have not been yet extensively addressed in the extant literature. This study evaluates the use of AGVs in container terminals by investigating the environmental sustainability gains that arise from the adoption of artificial intelligence and automation for shoreside operations at freight ports. Through a comprehensive literature review, we reveal this research gap across the use of artificial intelligence and decision support systems, as well as optimisation models. A real-world container terminal is used, as a case study in a simulation environment, on Europe’s fastest-growing container port (Piraeus), to quantify the environmental benefits related to routing scenarios via different types of AGVs. Our study contributes to the cross-section of operations management and artificial intelligence literature by articulating design principles to inform effective digital technology interventions at non-automated port terminals, both at operational and management levels.",10.1080/00207543.2021.1914355,https://www.tandfonline.com/doi/full/10.1080/00207543.2021.1914355,International Journal of Production Research,Naoum Tsolakis;Dimitris Zissis;Spiros Papaefthimiou ; Nikolaos Korfiatis,2022,319,"@article{2-38509,
  title={Towards AI Driven Environmental Sustainability: An Application of Automated Logistics in Container Port Terminals},
  author={Tsolakis, Naoum and Zissis, Dimitris and Papaefthimiou, Spiros and Korfiatis, Nikolaos},
  year={2022},
  journal={International Journal of Production Research},
  volume={60},  % Assuming volume number 60, please replace with the correct number if known
  number={1},   % Assuming issue number 1, please replace with the correct number if known
  pages={1--20},  % Placeholder pages, please replace with the correct page range
  doi={10.1080/00207543.2021.1914355}
}","Empirical contributions, Methodological contributions",Manufacturing / Industry / Automation,Operational,Executing,"Developer, Guardian, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-38513,taylor-and-francis,Understanding the role and impact of generative artificial intelligence (ai) hallucination within consumers’ tourism decision-making processes,"which launched only a year ago, is the fastest-growing website in the world today. When generative AI software such as ChatGPT generates ideas for people, they often generate false ideas. This occurrence has been called ‘AI Hallucination’. It can include generating false text output that is extremely believable to completely gibberish. This source of potential misinformation has significant potential implications for the travel and tourism industry. Using survey responses from 900 consumers, this empirical study contributes to theorizing and examination of how consumers’ awareness of AI Hallucination potential combines with existing concepts from the Technology Acceptance Model (TAM) and Theory of Planned Behaviour (TPB) when it comes to the decision to use generative AI platforms such as ChatGPT for tourism planning. This research also examines if the consumers are actually able to discern AI Hallucination and why they select to use AI technologies over other tourism information sources, such as aggregated peer review websites like TripAdvisor, government tourism websites, or social media influencers. The results indicate that many consumers chose error-filled AI tourism itineraries over other options because they trust the AI to be more impartial and customized than the other sources.",10.1080/13683500.2023.2300032,https://www.tandfonline.com/doi/full/10.1080/13683500.2023.2300032,Current Issues in Tourism,Jeff Christensen;Jared M. Hansen ; Paul Wilson,2025,0,"@article{2-38513,
  title = {Understanding the role and impact of generative artificial intelligence (AI) hallucination within consumers' tourism decision-making processes},
  author = {Christensen, Jeff and Hansen, Jared M. and Wilson, Paul},
  year = {2025},
  doi = {10.1080/13683500.2023.2300032},
  journal = {Current Issues in Tourism}
}",Empirical contributions,Media / Communication / Entertainment,Individual,"Analyzing, Advising","Decision-maker, Decision-subject","Change trust, Alter decision outcomes",no such info,error,NA,Textual,Yes,Yes
2-38516,taylor-and-francis,"Users’ experiences of algorithm-mediated public services: folk theories, trust, and strategies in the global south","the increasing prevalence of algorithm-mediated public services, there continues to be a limited understanding of citizens’ perspectives on this matter, particularly in the Global South. This study explores citizens’ experiences as users and affected stakeholders of algorithm-supported decision-making. From a qualitative perspective, we conducted and analyzed face-to-face interviews (N = 27) in Santiago, Chile. From the standpoint of folk theories as behavior guides, we identified that people tend to associate AI and algorithms with expanding the State’s monitoring, organizing, and decision-making capacity. At the same time, they express a prevailing sense of trust, but with certain boundaries. This trust is influenced by factors, such as a belief in AI’s future promise, a need for human mediation, and limitations related to structural inequalities. These findings underscore the responsibility placed on technology developers and public policymakers, emphasizing the importance of adopting an intersectional and position-based approach to AI design.",10.1080/10447318.2024.2356910,https://www.tandfonline.com/doi/full/10.1080/10447318.2024.2356910,International Journal of Human–Computer Interaction,Claudia López;Alexandra Davidoff;Francisca Luco;Mónica Humeres ; Teresa Correa,2024,0,"@article{2-38516,
  title = {Users’ Experiences of Algorithm-Mediated Public Services: Folk Theories, Trust, and Strategies in the Global South},
  author = {López, Claudia and Davidoff, Alexandra and Luco, Francisca and Humeres, Mónica and Correa, Teresa},
  year = {2024},
  doi = {10.1080/10447318.2024.2356910},
  journal = {International Journal of Human–Computer Interaction}
}",Empirical contributions,"Everyday / Employment / Public Service, Law / Policy / Governance",Institutional,"Analyzing, Monitoring, Collaborating","Decision-maker, Stakeholder, Decision-subject","Change trust, Restrict human agency",no such info,NA,NA,Autonomous System,Yes,Yes
2-38517,taylor-and-francis,"Using agent features to influence user trust, decision making and task outcome during human-agent collaboration","performance of collaborative tasks requires consideration of the interactions between intelligent agents and their human counterparts. The functionality and success of these agents lie in their ability to maintain user trust; with too much or too little trust leading to over-reliance and under-utilisation, respectively. This problem highlights the need for an appropriate trust calibration methodology with an ability to vary user trust and decision making in-task. An online experiment was run to investigate whether stimulus difficulty and the implementation of agent features by a collaborative recommender system interact to influence user perception, trust and decision making. Agent features are changes to the Human-Agent interface and interaction style, and include presentation of a disclaimer message, a request for more information from the user and no additional feature. Signal detection theory is utilised to interpret decision making, with this applied to assess decision making on the task, as well as with the collaborative agent. The results demonstrate that decision change occurs more for hard stimuli, with participants choosing to change their initial decision across all features to follow the agent recommendation. Furthermore, agent features can be utilised to mediate user decision making and trust in-task, though the direction and extent of this influence is dependent on the implemented feature and difficulty of the task. The results emphasise the complexity of user trust in Human-Agent collaboration, highlighting the importance of considering task context in the wider perspective of trust calibration.",10.1080/10447318.2022.2150691,https://www.tandfonline.com/doi/full/10.1080/10447318.2022.2150691,International Journal of Human–Computer Interaction,Sarita Herse;Jonathan Vitale ; Mary-Anne Williams,2023,17,"@article{2-38517,
  title = {Using agent features to influence user trust, decision making and task outcome during human-agent collaboration},
  author = {Herse, Sarita and Vitale, Jonathan and Williams, Mary-Anne},
  year = {2023},
  journal = {International Journal of Human--Computer Interaction},
  doi = {10.1080/10447318.2022.2150691}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Advising, Executing, Collaborating",Decision-maker,"Alter decision outcomes, Change trust, Change affective-perceptual",no such info,"disclaimer, request, task difficulty",language proficiency,"Physical / Embodiment, Autonomous System",Yes,Yes
2-38518,taylor-and-francis,Using machine learning to detect noncredible cognitive test performance,"Advanced algorithmic methods may improve the assessment of performance validity during neuropsychological testing. This study investigated whether unsupervised machine learning (ML) could serve as one such method. Method: Participants were 359 adult outpatients who underwent a neuropsychological evaluation for various referral reasons. Data relating to participants’ performance validity test scores, medical and psychiatric history, referral reason, litigation status, and disability status were examined in an unsupervised ML model. The model was programmed to synthesize the data into an unspecified number of clusters, which were then compared to predetermined ratings of whether patients had valid or invalid test performance. Ratings were established according to multiple empirical performance validity test scores. To further understand the model, we examined which data were most helpful in its clustering decision-making process. Results: Similar to the clinical determination of patients’ performance on neuropsychological testing, the model identified a two-cluster profile consisting of valid and invalid data. The model demonstrated excellent predictive accuracy (area under the curve of .92 [95% CI .88, .97]) when referenced against participants’ predetermined validity status. Performance validity test scores were the most influential in the differentiation of clusters, but medical history, referral reason, and disability status were also contributory. Conclusions: These findings serve as a proof of concept that unsupervised ML can accurately assess performance validity using various data obtained during a neuropsychological evaluation. The manner in which unsupervised ML evaluates such data may circumvent some of the limitations with traditional validity assessment approaches. Importantly, unsupervised ML is adaptable to emerging digital technologies within neuropsychology that can be used to further improve the assessment of performance validity.",10.1080/13854046.2024.2440085,https://www.tandfonline.com/doi/full/10.1080/13854046.2024.2440085,The Clinical Neuropsychologist,John-Christopher A. Finley;Anthony D. Robinson;Jason R. Soble ; Violeta J. Rodriguez,2024,1,"@article{2-38518,
  title = {Using machine learning to detect noncredible cognitive test performance},
  author = {Finley, John-Christopher A. and Robinson, Anthony D. and Soble, Jason R. and Rodriguez, Violeta J.},
  year = {2024},
  journal = {The Clinical Neuropsychologist},
  doi = {10.1080/13854046.2024.2440085}
}","Empirical contributions, Methodological contributions",Healthcare / Medicine / Surgery,Operational,"Monitoring, Forecasting","Decision-maker, Guardian, Decision-subject",Alter decision outcomes,no such info,recommendations,NA,"Textual, Autonomous System",Yes,Yes
2-38520,taylor-and-francis,Using the causal inference framework to support individualized drug treatment decisions based on observational healthcare data,"When healthcare professionals have the choice between several drug treatments for their patients, they often experience considerable decision uncertainty because many decisions simply have no single “best” choice. The challenges are manifold and include that guideline recommendations focus on randomized controlled trials whose populations do not necessarily correspond to specific patients in everyday treatment. Further reasons may be insufficient evidence on outcomes, lack of direct comparison of distinct options, and the need to individually balance benefits and risks. All these situations will occur in routine care, its outcomes will be mirrored in routine data, and could thus be used to guide decisions. We propose a concept to facilitate decision-making by exploiting this wealth of information. Our working example for illustration assumes that the response to a particular (drug) treatment can substantially differ between individual patients depending on their characteristics (heterogeneous treatment effects, HTE), and that decisions will be more precise if they are based on real-world evidence of HTE considering this information. However, such methods must account for confounding by indication and effect measure modification, eg, by adequately using machine learning methods or parametric regressions to estimate individual responses to pharmacological treatments. The better a model assesses the underlying HTE, the more accurate are predicted probabilities of treatment response. After probabilities for treatment-related benefit and harm have been calculated, decision rules can be applied and patient preferences can be considered to provide individual recommendations. Emulated trials in observational data are a straightforward technique to predict the effects of such decision rules when applied in routine care. Prediction-based decision rules from routine data have the potential to efficiently supplement clinical guidelines and support healthcare professionals in creating personalized treatment plans using decision support tools.",10.2147/CLEP.S274466,https://www.tandfonline.com/doi/full/10.2147/CLEP.S274466,Clinical Epidemiology,Andreas D Meid;Carmen Ruff;Lucas Wirbka;Felicitas Stoll;Hanna M Seidling;Andreas Groll ; Walter E Haefeli,2020,42,"@article{2-38520,
  title={Using the causal inference framework to support individualized drug treatment decisions based on observational healthcare data},
  author={Meid, Andreas D and Ruff, Carmen and Wirbka, Lucas and Stoll, Felicitas and Seidling, Hanna M and Groll, Andreas and Haefeli, Walter E},
  year={2020},
  doi={10.2147/CLEP.S274466},
  journal={Clinical Epidemiology}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-38528,taylor-and-francis,What impacts matriculation decisions? Identifying students’ university choice factors on a global scale with artificial intelligence,"study provides an empirical approach to utilizing an Artificial Intelligence (AI)-based system for identifying students’ university choice factors that impact their matriculation decision. We created an AI-based chatbot that gathered both qualitative and quantitative data from nearly 1200 participants worldwide. The entire human-AI interaction process was managed autonomously by the AI without researcher intervention. We analysed all data collected by the AI and identified relevant matriculation decision factors and themes. The AI collaboration demonstrated remarkable efficacy in streamlining the research workflow by consistently adhering to predefined criteria, eliminating variations and human-induced biases, establishing rapport with participants, and amplifying not just the efficiency and scalability of data collection but also the reliability and generalizability of the research findings. Acquiring such insights into students’ university choice factors from such a diversified and large sample may potentially empower policymakers to make informed decisions that synch higher education policies with students’ preferences, expectations, and needs, ultimately aiding institutions to improve recruitment and retention strategies leading to better overall performance and outcomes for both students and the institution itself. From an economic perspective, gaining this insight can foster closer alignment between higher education and the job market. By understanding students’ aspirations and the influences driving their decisions, institutions can tailor programmes that not only enhance graduates’ job prospects but also proactively contribute to economic growth and the development of a highly skilled and committed workforce.",10.1080/03075079.2024.2319870,https://www.tandfonline.com/doi/full/10.1080/03075079.2024.2319870,Studies in Higher Education,Ilker Cingillioglu,2024,1,"@article{2-38528,
  title = {What impacts matriculation decisions? Identifying students' university choice factors on a global scale with artificial intelligence},
  author = {Ilker Cingillioglu},
  year = {2024},
  doi = {10.1080/03075079.2024.2319870},
  journal = {Studies in Higher Education}
}",Empirical contributions,Education / Teaching / Research,"Individual, Organizational","Collaborating, Analyzing","Decision-maker, Guardian, Decision-subject, Knowledge provider",NA,NA,NA,NA,"Conversational/Natural Language, Physical / Embodiment, Textual",Yes,No
2-38531,taylor-and-francis,When ai is perceived to be fairer than a human: understanding perceptions of algorithmic decisions in a job application context,"study investigates people’s perceptions of AI decision-making as compared to human decision-making within the job application context. It takes into account both favorable and unfavorable outcomes, employing a 2 × 2 experimental design (decision-making agent: AI algorithm vs. human; outcome: favorable vs. unfavorable). Upon evaluating a job seeker’s suitability for a position, participants viewed algorithmic decisions as fairer, more competent, more trustworthy, and more useful than those made by humans. Interestingly, when a candidate was deemed unsuitable for hiring, people reacted more negatively to the verdict given by a human than to the same judgment offered by AI. Moreover, participants credited algorithmic decisions with greater sensitivity to both quantitative and qualitative qualifications, thus indicating algorithmic appreciation. Our findings shed light on the psychological basis of perceptions surrounding Algorithmic Decision-Making (ADM) and the responses to the decisions rendered by ADM systems.",10.1080/10447318.2023.2266244,https://www.tandfonline.com/doi/full/10.1080/10447318.2023.2266244,International Journal of Human–Computer Interaction,Hyesun Choung;John S. Seberger ; Prabu David,2024,0,"@article{2-38531,
  title={When AI is Perceived to be Fairer than a Human: Understanding Perceptions of Algorithmic Decisions in a Job Application Context},
  author={Choung, Hyesun and Seberger, John S. and David, Prabu},
  year={2024},
  journal={International Journal of Human–Computer Interaction},
  doi={10.1080/10447318.2023.2266244}
}",Empirical contributions,Everyday / Employment / Public Service,Operational,"Executing, Advising","Decision-maker, Decision-subject, Knowledge provider","Change trust, Alter decision outcomes",no such info,recommendations,NA,Textual,Yes,Yes
2-38532,taylor-and-francis,When are artificial intelligence versus human agents faulted for wrongdoing? Moral attributions after individual and joint decisions,"intelligence (AI) agents make decisions that affect individuals and society which can produce outcomes traditionally considered moral violations if performed by humans. Do people attribute the same moral permissibility and fault to AIs and humans when each produces the same moral violation outcome? Additionally, how do people attribute morality when the AI and human are jointly making the decision which produces that violation? We investigate these questions with an experiment that manipulates written descriptions of four real-world scenarios where, originally, a violation outcome was produced by an AI. Our decision-making structures include individual decision-making – either AIs or humans – and joint decision-making – either humans monitoring AIs or AIs recommending to humans. We find that the decision-making structure has little effect on morally faulting AIs, but that humans who monitor AIs are faulted less than solo humans and humans receiving recommendations. Furthermore, people attribute more permission and less fault to AIs compared to humans for the violation in both joint decision-making structures. The blame for joint AI-human wrongdoing suggests the potential for strategic scapegoating of AIs for human moral failings and the need for future research on AI-human teams.",10.1080/1369118X.2019.1568515,https://www.tandfonline.com/doi/full/10.1080/1369118X.2019.1568515,"Information, Communication & Society",Daniel B. Shank;Alyssa DeSanti ; Timothy Maninger,2019,33,"@article{2-38532,
  title = {When are artificial intelligence versus human agents faulted for wrongdoing? Moral attributions after individual and joint decisions},
  author = {Daniel B. Shank and Alyssa DeSanti and Timothy Maninger},
  year = {2019},
  doi = {10.1080/1369118X.2019.1568515},
  journal = {Information, Communication \& Society}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Advising, Executing, Collaborating","Guardian, Decision-maker","Shift responsibility, Alter decision outcomes",Shape AI for accountability,recommendations,NA,Textual,Yes,Yes
2-38533,taylor-and-francis,Who guards the guards with ai-driven robots? The ethicalness and cognitive neutralization of police violence following ai-robot advice,"We investigate whether the perceived ethicalness of police actions changes when police follow an AI-robot’s advice. We assess whether perceived ethicalness of police violence is higher when police follow robot advice to arrest a passer-by, compared to no robot advice to arrest the passer-by. Using neutralization theory, we test how blame-shifting occurs. When police violently arrest an innocent passer-by, the violence is neutralized when the decision was made following the AI-robot. Perceived ethicalness of police violence is higher when the passer-by is a terrorist, and police violence against a passer-by is neutralized through ‘denial of victim’ and ‘denial of injury’.",10.1080/14719037.2023.2269203,https://www.tandfonline.com/doi/full/10.1080/14719037.2023.2269203,Public Management Review,Lisa Hohensinn;Jurgen Willems;Meikel Soliman;Dieter Vanderelst ; Jonathan Stoll,2024,4,"@article{2-38533,
  title={Who guards the guards with AI-driven robots? The ethicalness and cognitive neutralization of police violence following AI-robot advice},
  author={Hohensinn, Lisa and Willems, Jurgen and Soliman, Meikel and Vanderelst, Dieter and Stoll, Jonathan},
  year={2024},
  doi={10.1080/14719037.2023.2269203},
  journal={Public Management Review}
}",Empirical contributions,Law / Policy / Governance,Operational,"Advising, Executing","Decision-maker, Decision-subject",no such info,no such info,recommendations,NA,Textual,Yes,Yes
2-38534,taylor-and-francis,Who made that decision and why? Users’ perceptions of human versus ai decision-making and the power of explainable-ai,"the advent of artificial intelligence (AI) based systems, a new era has begun. Decisions that were once made by humans are now increasingly being made by these advanced systems, with the inevitable consequence of our growing reliance on AI in many aspects of our lives. At the same time, the opaque nature of AI-based systems and the possibility of unintentional or hidden discriminatory practices and biases raises profound questions not only about the mechanics of AI, but also about how users perceive the fairness of these systems. We hypothesize that providing various explanations for AI decision-making processes and output may enhance users’ fairness perceptions and make them trust the system and adopt its decisions. Hence, we devised an online between-subject experiment that explores users’ fairness and comprehension perceptions of AI systems with respect to the explanations provided by the system, employing a case study of a managerial decision in the human resources (HR) domain. We manipulated (i) the decision-maker (AI or human); (ii) the input (candidate characteristics); (iii) the output (recommendation valence), and (iv) the explanation style. We examined the effect of the various manipulations (and individuals’ demographic and personality characteristics) using multivariate ordinal regression. We also performed a multi-level analysis of experiment components to examine the effects of the decision-maker type, explanation style, and their combination. The results suggest three main conclusions. The first conclusion is that there is a gap in users’ fairness and comprehension perception of AI-based decision making systems compared to human decision making. The second conclusion is that knowing that an AI-based system provided the decisions negatively affects users’ fairness and comprehension perceptions, compared to knowing that humans made the decision. Finally, the third conclusion is that providing case-based, certification-based, or sensitivity-based explanations can narrow this gap and may even eliminate it. Additionally, we found that users’ fairness and comprehension perceptions are influenced by a variety of factors such as the input, output, and explanation provided by the system, as well as by individuals’ age, education, computer skills, and personality. Our findings may help to understand when and how to use explanations to improve users’ perceptions regarding AI-based decision-making.",10.1080/10447318.2024.2348843,https://www.tandfonline.com/doi/full/10.1080/10447318.2024.2348843,International Journal of Human–Computer Interaction,Avital Shulner-Tal;Tsvi Kuflik;Doron Kliger ; Azzurra Mancini,2024,20,"@article{2-38534,
  title = {Who Made That Decision and Why? Users’ Perceptions of Human Versus AI Decision-Making and the Power of Explainable-AI},
  author = {Avital Shulner-Tal and Tsvi Kuflik and Doron Kliger and Azzurra Mancini},
  year = {2024},
  doi = {10.1080/10447318.2024.2348843},
  journal = {International Journal of Human–Computer Interaction}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service",Operational,"Explaining, Advising, Auditing","Decision-maker, Knowledge provider","Change cognitive demands, Shape ethical norms, Change trust",no such info,example-based explanations,fairness perception ratings,Textual,Yes,Yes
2-38535,taylor-and-francis,Why do family members reject ai in health care? Competing effects of emotions,"intelligence (AI) enables continuous monitoring of patients’ health, thus improving the quality of their health care. However, prior studies suggest that individuals resist such innovative technology. In contrast to prior studies that investigate individuals’ decisions for themselves, we focus on family members’ rejection of AI monitoring, as family members play a significant role in health care decisions. Our research investigates competing effects of emotions toward the rejection of AI monitoring for health care. Based on two scenario-based experiments, our study reveals that emotions play a decisive role in family members’ decision making on behalf of their parents. We find that anxiety about health care monitoring and anxiety about health outcomes reduce the rejection of AI monitoring, whereas surveillance anxiety and delegation anxiety increase rejection. We also find that for individual-level risks, perceived controllability moderates the relationship between surveillance anxiety and the rejection of AI monitoring. We contribute to the theory of Information System rejection by identifying the competing roles of emotions in AI monitoring decision making. We extend the literature on decision making for others by suggesting the influential role of anxiety. We also contribute to healthcare research in Information System by identifying the important role of controllability, a design factor, in AI monitoring rejection.",10.1080/07421222.2022.2096550,https://www.tandfonline.com/doi/full/10.1080/07421222.2022.2096550,Journal of Management Information Systems,Eun Hee Park;Karl Werder;Lan Cao ; Balasubramaniam Ramesh,2022,9,"@article{2-38535,
  title={Why do family members reject AI in health care? Competing effects of emotions},
  author={Park, Eun Hee and Werder, Karl and Cao, Lan and Ramesh, Balasubramaniam},
  year={2022},
  doi={10.1080/07421222.2022.2096550},
  journal={Journal of Management Information Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Individual,Monitoring,"Decision-maker, Decision-subject","Alter decision outcomes, Change trust, Restrict human agency, Change affective-perceptual",no such info,NA,system settings,Textual,Yes,Yes
2-38536,wiley,‘Just like i thought’: street-level bureaucrats trust ai recommendations if they confirm their professional judgment,"Artificial Intelligence is increasingly used to support and improve street-level decision-making, but empirical evidence on how street-level bureaucrats' work is affected by AI technologies is scarce. We investigate how AI recommendations affect street-level bureaucrats' decision-making and if explainable AI increases trust in such recommendations. We experimentally tested a realistic mock predictive policing system in a sample of Dutch police officers using a 2 × 2 factorial design. We found that police officers trust and follow AI recommendations that are congruent with their intuitive professional judgment. We found no effect of explanations on trust in AI recommendations. We conclude that police officers do not blindly trust AI technologies, but follow AI recommendations that confirm what they already thought. This highlights the potential of street-level discretion in correcting faulty AI recommendations on the one hand, but, on the other hand, poses serious limits to the hope that fair AI systems can correct human biases.",10.1111/puar.13602,https://onlinelibrary.wiley.com/doi/10.1111/puar.13602,Public Administration Review,Friso Selten;Marcel Robeer;Stephan Grimmelikhuijsen,2023,115,"@article{2-38536,
  title = {‘Just like I thought’: Street-Level Bureaucrats Trust AI Recommendations if They Confirm Their Professional Judgment},
  author = {Friso Selten and Marcel Robeer and Stephan Grimmelikhuijsen},
  year = {2023},
  journal = {Public Administration Review},
  doi = {10.1111/puar.13602}
}",Empirical contributions,Law / Policy / Governance,Operational,"Explaining, Advising, Forecasting",Decision-maker,"Alter decision outcomes, Change trust",Change AI responses,"recommendations, textual explanations",domain knowledge,"Textual, Visual, Interactive interface",Yes,Yes
2-38537,wiley,A comparative vignette study: evaluating the potential role of a generative ai model in enhancing clinical decision-making in nursing,"Aim This study explores the potential of a generative artificial intelligence tool (ChatGPT) as clinical support for nurses. Specifically, we aim to assess whether ChatGPT can demonstrate clinical decision-making equivalent to that of expert nurses and novice nursing students. This will be evaluated by comparing ChatGPT responses to clinical scenarios to those of nurses on different levels of experience. Design This is a cross-sectional study. Methods Emergency room registered nurses (i.e. experts; n = 30) and nursing students (i.e. novices; n = 38) were recruited during March–April 2023. Clinical decision-making was measured using three validated clinical scenarios involving an initial assessment and reevaluation. Clinical decision-making aspects assessed were the accuracy of initial assessments, the appropriateness of recommended tests and resource use and the capacity to reevaluate decisions. Performance was also compared by timing response generations and word counts. Expert nurses and novice students completed online questionnaires (via Qualtrics), while ChatGPT responses were obtained from OpenAI. Results Concerning aspects of clinical decision-making and compared to novices and experts: (1) ChatGPT exhibited indecisiveness in initial assessments; (2) ChatGPT tended to suggest unnecessary diagnostic tests; (3) When new information required re-evaluation, ChatGPT responses demonstrated inaccurate understanding and inappropriate modifications. In terms of performance, the mean number of words utilized in ChatGPT answers was 27–41 times greater than that utilized by both experts and novices; and responses were provided approximately 4 times faster than those of novices and twice faster than expert nurses. ChatGPT responses maintained logical structure and clarity. Conclusions A generative AI tool demonstrated indecisiveness and a tendency towards over-triage compared to human clinicians. Impact The study shows that it is important to approach the implementation of ChatGPT as a nurse's digital assistant with caution. More study is needed to optimize the model's training and algorithms to provide accurate healthcare support that aids clinical decision-making. Reporting method This study adhered to relevant EQUATOR guidelines for reporting observational studies. Patient or public contribution Patients were not directly involved in the conduct of this study.",10.1111/jan.16101,https://onlinelibrary.wiley.com/doi/10.1111/jan.16101,Journal of Advanced Nursing,Mor Saban;Ilana Dubovi,2024,48,"@article{2-38537,
  title={A comparative vignette study: evaluating the potential role of a generative ai model in enhancing clinical decision-making in nursing},
  author={Mor Saban and Ilana Dubovi},
  year={2024},
  doi={10.1111/jan.16101},
  journal={Journal of Advanced Nursing}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing, Collaborating","Decision-maker, Decision-subject, Knowledge provider","Alter decision outcomes, Change cognitive demands",no such info,additional examination and test result details,NA,Autonomous System,Yes,Yes
2-38541,wiley,Graph neural network and reinforcement learning for multi-agent cooperative control of connected autonomous vehicles,"A connected autonomous vehicle (CAV) network can be defined as a set of connected vehicles including CAVs that operate on a specific spatial scope that may be a road network, corridor, or segment. The spatial scope constitutes an environment where traffic information is shared and instructions are issued for controlling the CAVs movements. Within such a spatial scope, high-level cooperation among CAVs fostered by joint planning and control of their movements can greatly enhance the safety and mobility performance of their operations. Unfortunately, the highly combinatory and volatile nature of CAV networks due to the dynamic number of agents (vehicles) and the fast-growing joint action space associated with multi-agent driving tasks pose difficultly in achieving cooperative control. The problem is NP-hard and cannot be efficiently resolved using rule-based control techniques. Also, there is a great deal of information in the literature regarding sensing technologies and control logic in CAV operations but relatively little information on the integration of information from collaborative sensing and connectivity sources. Therefore, we present a novel deep reinforcement learning-based algorithm that combines graphic convolution neural network with deep Q-network to form an innovative graphic convolution Q network that serves as the information fusion module and decision processor. In this study, the spatial scope we consider for the CAV network is a multi-lane road corridor. We demonstrate the proposed control algorithm using the application context of freeway lane-changing at the approaches to an exit ramp. For purposes of comparison, the proposed model is evaluated vis-à-vis traditional rule-based and long short-term memory-based fusion models. The results suggest that the proposed model is capable of aggregating information received from sensing and connectivity sources and prescribing efficient operative lane-change decisions for multiple CAVs, in a manner that enhances safety and mobility. That way, the operational intentions of individual CAVs can be fulfilled even in partially observed and highly dynamic mixed traffic streams. The paper presents experimental evidence to demonstrate that the proposed algorithm can significantly enhance CAV operations. The proposed algorithm can be deployed at roadside units or cloud platforms or other centralized control facilities.",10.1111/mice.12702,https://onlinelibrary.wiley.com/doi/10.1111/mice.12702,Computer-Aided Civil and Infrastructure Engineering,Sikai Chen;Jiqian Dong;Paul (Young Joun) Ha;Yujie Li;Samuel Labi,2021,273,"@article{2-38541,
  title={Graph neural network and reinforcement learning for multi-agent cooperative control of connected autonomous vehicles},
  author={Chen, Sikai and Dong, Jiqian and Ha, Paul (Young Joun) and Li, Yujie and Labi, Samuel},
  year={2021},
  journal={Computer-Aided Civil and Infrastructure Engineering},
  doi={10.1111/mice.12702}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,"Developer, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-3858,acs,Artificial Intelligence-Based Portable Bioelectronics Platform for SARS-CoV-2 Diagnosis with Multi-nucleotide Probe Assay for Clinical Decisions,"In the context of the recent pandemic, the necessity of inexpensive and easily accessible rapid-test kits is well understood and need not be stressed further. In light of this, we report a multi-nucleotide probe-based diagnosis of SARS-CoV-2 using a bioelectronics platform, comprising low-cost chemiresistive biochips, a portable electronic readout, and an Android application for data acquisition with machine-learning-based decision making. The platform performs the desired diagnosis from standard nasopharyngeal and/or oral swabs (both on extracted and non-extracted RNA samples) without amplifying the viral load. Being a reverse transcription polymerase chain reaction-free hybridization assay, the proposed approach offers inexpensive, fast (time-to-result: ≤ 30 min), and early diagnosis, as opposed to most of the existing SARS-CoV-2 diagnosis protocols recommended by the WHO. For the extracted RNA samples, the assay accounts for 87 and 95.2% test accuracies, using a heuristic approach and a machine-learning-based classification method, respectively. In case of the non-extracted RNA samples, 95.6% decision accuracy is achieved using the heuristic approach, with the machine-learning-based best-fit model producing 100% accuracy. Furthermore, the availability of the handheld readout and the Android application-based simple user interface facilitates easy accessibility and portable applications. Besides, by eliminating viral RNA extraction from samples as a pre-requisite for specific detection, the proposed approach presents itself as an ideal candidate for point-of-care SARS-CoV-2 diagnosis.",10.1021/acs.analchem.1c01650,https://pubs.acs.org/doi/abs/10.1021/acs.analchem.1c01650,Analytical Chemistry,Suryasnata Tripathy;Patta Supraja;Swati Mohanty;Vallepu Mohan Sai;Tushant Agrawal;Ch. Gajendranath Chowdary;Madhuri Taranikanti;Rajiv Bandaru;Aswin Kumar Mudunuru;Lakshmi Jyothi Tadi;Swathi Suravaram;Imran Ahmed Siddiqui;Srinivas Maddur;Rohith Kumar Guntuka;Ranjana Singh;Vikrant Singh;Shiv Govind Singh,2021,16,"@article{2-3858,
  title={Artificial Intelligence-Based Portable Bioelectronics Platform for SARS-CoV-2 Diagnosis with Multi-nucleotide Probe Assay for Clinical Decisions},
  author={Tripathy, Suryasnata and Supraja, Patta and Mohanty, Swati and Sai, Vallepu Mohan and Agrawal, Tushant and Chowdary, Ch. Gajendranath and Taranikanti, Madhuri and Bandaru, Rajiv and Mudunuru, Aswin Kumar and Tadi, Lakshmi Jyothi and Suravaram, Swathi and Siddiqui, Imran Ahmed and Maddur, Srinivas and Guntuka, Rohith Kumar and Singh, Ranjana and Singh, Vikrant and Singh, Shiv Govind},
  year={2021},
  doi={10.1021/acs.analchem.1c01650},
  journal={Analytical Chemistry}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Decision-subject, Guardian, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-38580,wiley,Clinical decision-making in benzodiazepine deprescribing by healthcare providers vs. Ai-assisted approach,"Aims The aim of this study was to compare the clinical decision-making for benzodiazepine deprescribing between a healthcare provider (HCP) and an artificial intelligence (AI) chatbot GPT4 (ChatGPT-4). Methods We analysed real-world data from a Croatian cohort of community-dwelling benzodiazepine patients (n = 154) within the EuroAgeism H2020 ESR 7 project. HCPs evaluated the data using pre-established deprescribing criteria to assess benzodiazepine discontinuation potential. The research team devised and tested AI prompts to ensure consistency with HCP judgements. An independent researcher employed ChatGPT-4 with predetermined prompts to simulate clinical decisions for each patient case. Data derived from human-HCP and ChatGPT-4 decisions were compared for agreement rates and Cohen's kappa. Results Both HPC and ChatGPT identified patients for benzodiazepine deprescribing (96.1% and 89.6%, respectively), showing an agreement rate of 95% (κ = .200, P = .012). Agreement on four deprescribing criteria ranged from 74.7% to 91.3% (lack of indication κ = .352, P < .001; prolonged use κ = .088, P = .280; safety concerns κ = .123, P = .006; incorrect dosage κ = .264, P = .001). Important limitations of GPT-4 responses were identified, including 22.1% ambiguous outputs, generic answers and inaccuracies, posing inappropriate decision-making risks. Conclusions While AI-HCP agreement is substantial, sole AI reliance poses a risk for unsuitable clinical decision-making. This study's findings reveal both strengths and areas for enhancement of ChatGPT-4 in the deprescribing recommendations within a real-world sample. Our study underscores the need for additional research on chatbot functionality in patient therapy decision-making, further fostering the advancement of AI for optimal performance.",10.1111/bcp.15963,https://onlinelibrary.wiley.com/doi/10.1111/bcp.15963,British Journal of Clinical Pharmacology,Iva Bužančić;Dora Belec;Margita Držaić;Ingrid Kummer;Jovana Brkić;Daniela Fialová;Maja Ortner Hadžiabdić,2023,22,"@article{2-38580,
  title = {Clinical decision-making in benzodiazepine deprescribing by healthcare providers vs. AI-assisted approach},
  author = {Iva Bužančić and Dora Belec and Margita Držaić and Ingrid Kummer and Jovana Brkić and Daniela Fialová and Maja Ortner Hadžiabdić},
  year = {2023},
  journal = {British Journal of Clinical Pharmacology},
  doi = {10.1111/bcp.15963}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-38589,wiley,Explaining why the computer says no: algorithmic transparency affects the perceived trustworthiness of automated decision-making,"Algorithms based on Artificial Intelligence technologies are slowly transforming street-level bureaucracies, yet a lack of algorithmic transparency may jeopardize citizen trust. Based on procedural fairness theory, this article hypothesizes that two core elements of algorithmic transparency (accessibility and explainability) are crucial to strengthening the perceived trustworthiness of street-level decision-making. This is tested in one experimental scenario with low discretion (a denied visa application) and one scenario with high discretion (a suspicion of welfare fraud). The results show that: (1) explainability has a more pronounced effect on trust than the accessibility of the algorithm; (2) the effect of algorithmic transparency not only pertains to trust in the algorithm itself but also—partially—to trust in the human decision-maker; (3) the effects of algorithmic transparency are not robust across decision context. These findings imply that transparency-as-accessibility is insufficient to foster citizen trust. Algorithmic explainability must be addressed to maintain and foster trustworthiness algorithmic decision-making.",10.1111/puar.13483,https://onlinelibrary.wiley.com/doi/10.1111/puar.13483,Public Administration Review,Stephan Grimmelikhuijsen,2022,212,"@article{2-38589,
  title={Explaining why the computer says no: algorithmic transparency affects the perceived trustworthiness of automated decision-making},
  author={Grimmelikhuijsen, Stephan},
  year={2022},
  doi={10.1111/puar.13483},
  journal={Public Administration Review}
}",Empirical contributions,Law / Policy / Governance,Institutional,"Explaining, Executing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-38602,wiley,Comparison of artificial intelligence to the veterinary radiologist's diagnosis of canine cardiogenic pulmonary edema,"Application of artificial intelligence (AI) to improve clinical diagnosis is a burgeoning field in human and veterinary medicine. The objective of this prospective, diagnostic accuracy study was to determine the accuracy, sensitivity, and specificity of an AI-based software for diagnosing canine cardiogenic pulmonary edema from thoracic radiographs, using an American College of Veterinary Radiology-certified veterinary radiologist's interpretation as the reference standard. Five hundred consecutive canine thoracic radiographs made after-hours by a veterinary Emergency Department were retrieved. A total of 481 of 500 cases were technically analyzable. Based on the radiologist's assessment, 46 (10.4%) of these 481 dogs were diagnosed with cardiogenic pulmonary edema (CPE+). Of these cases, the AI software designated 42 of 46 as CPE+ and four of 46 as cardiogenic pulmonary edema negative (CPE−). Accuracy, sensitivity, and specificity of the AI-based software compared to radiologist diagnosis were 92.3%, 91.3%, and 92.4%, respectively (positive predictive value, 56%; negative predictive value, 99%). Findings supported using AI software screening for thoracic radiographs of dogs with suspected cardiogenic pulmonary edema to assist with short-term decision-making when a radiologist is unavailable.",10.1111/vru.13062,https://onlinelibrary.wiley.com/doi/10.1111/vru.13062,Veterinary Radiology & Ultrasound,Eunbee Kim;Anthony J. Fischetti;Pratheev Sreetharan;Joel G. Weltman;Philip R. Fox,2022,48,"@article{2-38602,
  title = {Comparison of artificial intelligence to the veterinary radiologist's diagnosis of canine cardiogenic pulmonary edema},
  author = {Eunbee Kim and Anthony J. Fischetti and Pratheev Sreetharan and Joel G. Weltman and Philip R. Fox},
  year = {2022},
  doi = {10.1111/vru.13062},
  journal = {Veterinary Radiology \& Ultrasound}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-38608,wiley,Artificial intelligence and the doctor–patient relationship expanding the paradigm of shared decision making,"Artificial intelligence (AI) based clinical decision support systems (CDSS) are becoming ever more widespread in healthcare and could play an important role in diagnostic and treatment processes. For this reason, AI-based CDSS has an impact on the doctor–patient relationship, shaping their decisions with its suggestions. We may be on the verge of a paradigm shift, where the doctor–patient relationship is no longer a dual relationship, but a triad. This paper analyses the role of AI-based CDSS for shared decision-making to better comprehend its promises and associated ethical issues. Moreover, it investigates how certain AI implementations may instead foster the inappropriate paradigm of paternalism. Understanding how AI relates to doctors and influences doctor–patient communication is essential to promote more ethical medical practice. Both doctors' and patients' autonomy need to be considered in the light of AI.",10.1111/bioe.13158,https://onlinelibrary.wiley.com/doi/10.1111/bioe.13158,Bioethics,Giorgia Lorenzini;Laura Arbelaez Ossa;David Martin Shaw;Bernice Simone Elger,2023,124,"@article{2-38608,
  title = {Artificial intelligence and the doctor--patient relationship expanding the paradigm of shared decision making},
  author = {Giorgia Lorenzini and Laura Arbelaez Ossa and David Martin Shaw and Bernice Simone Elger},
  year = {2023},
  doi = {10.1111/bioe.13158},
  journal = {Bioethics}
}",Theoretical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-38612,wiley,Ai automation and retailer regret in supply chains,"Artificial intelligence (AI) has significantly changed the supply chain process. In this study, we study the effects associated with AI automation of the retailer's order decision in a decentralized supply chain comprising one supplier and one regretful retailer. In the absence of AI automation, the retailer has a regret bias in that it behaves as though considering the deviation between the realized demand and order quantity, when making an ex ante inventory decision. We find that if profit margins of the supply chain are high, regret bias drives the retailer to decline the supplier's contract, whereas, if profit margins are low, regret drives retailers to order more from the supplier. As a result, although the automation of retailer decision leads to a higher expected profit for a retailer that operates in a centralized vacuum, it nevertheless can be a negative force for a decentralized supply chain with either high or low profit margins. Perhaps more interestingly, as a retailer's decision becomes automatic, it is not destined to earn a higher expected profit. In the extreme, a lose-lose outcome can prevail in which automation potentially leaves both the retailer and supplier worse off.",10.1111/poms.13498,https://onlinelibrary.wiley.com/doi/10.1111/poms.13498,Production and Operations Management Society (POMS) Journal,Meng Li;Tao Li,2021,68,"@article{2-38612,
  title = {AI Automation and Retailer Regret in Supply Chains},
  author = {Meng Li and Tao Li},
  year = {2021},
  doi = {10.1111/poms.13498},
  journal = {Production and Operations Management Society (POMS) Journal}
}",Theoretical contributions,Manufacturing / Industry / Automation,Organizational,Executing,"Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-38613,wiley,Preferences in ai algorithms: the need for relevant risk attitudes in automated decisions under uncertainties,"Artificial intelligence (AI) has the potential to improve life and reduce risks by providing large amounts of information embedded in big databases and by suggesting or implementing automated decisions under uncertainties. Yet, in the design of a prescriptive AI algorithm, some problems may occur, first and clearly, if the AI information is wrong or incomplete. But the main point of this article is that under uncertainties, the decision algorithm, rational or not, includes, in one way or another, a risk attitude in addition to deterministic preferences. That risk attitude implemented in the software is chosen by the analysts, the organization that they serve, the experts who inform them, and more generally by the process of identifying possible options. The problem is that it may or may not represent, as it should, the preferences of the actual decision maker (the risk manager) and of the people subjected to his/her decisions. This article briefly describes the sometimes-serious problem of that discrepancy between the preferences of the risk managers who use an AI output, and the risk attitude embedded in the AI system. The recommendation is to make these AI factors as accessible and transparent as possible and to allow for preference adjustments in the model if needed. The formulation of two simplified examples is described, that of a medical doctor and his/her patient when using an AI system to decide of a treatment option, and that of a skipper in a sailing race such as the America's Cup, receiving AI-processed sensor signals about the sailing conditions on different possible courses.",10.1111/risa.14268,https://onlinelibrary.wiley.com/doi/10.1111/risa.14268,Risk Analysis: An International Journal,Elisabeth Paté-Cornell,2024,1094,"@article{2-38613,
  title={Preferences in AI Algorithms: The Need for Relevant Risk Attitudes in Automated Decisions Under Uncertainties},
  author={Elisabeth Paté-Cornell},
  year={2024},
  doi={10.1111/risa.14268},
  journal={Risk Analysis: An International Journal}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery, Transportation / Mobility / Planning",Operational,"Collaborating, Executing, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-38618,wiley,"Valuing good health care: how medical doctors, scientists and patients relate ethical challenges with artificial intelligence decision-making support tools in prostate cancer diagnostics to good health care","Artificial intelligence (AI) is increasingly used in health care to improve diagnostics and treatment. Decision-making tools intended to help professionals in diagnostic processes are developed in a variety of medical fields. Despite the imagined benefits, AI in health care is contested. Scholars point to ethical and social issues related to the development, implementation, and use of AI in diagnostics. Here, we investigate how three relevant groups construct ethical challenges with AI decision-making tools in prostate cancer (PCa) diagnostics: scientists developing AI decision support tools for interpreting MRI scans for PCa, medical doctors working with PCa and PCa patients. This qualitative study is based on participant observation and interviews with the abovementioned actors. The analysis focuses on how each group draws on their understanding of ‘good health care’ when discussing ethical challenges, and how they mobilise different registers of valuing in this process. Our theoretical approach is inspired by scholarship on evaluation and justification. We demonstrate how ethical challenges in this area are conceptualised, weighted and negotiated among these participants as processes of valuing good health care and compare their perspectives.",10.1111/1467-9566.13818,https://onlinelibrary.wiley.com/doi/10.1111/1467-9566.13818,Sociology of Health & Illness,Maria Bårdsen Hesjedal;Emilie Hybertsen Lysø;Marit Solbjør;John-Arne Skolbekken,2024,5,"@article{2-38618,
  title = {Valuing good health care: how medical doctors, scientists and patients relate ethical challenges with artificial intelligence decision-making support tools in prostate cancer diagnostics to good health care},
  author = {Maria B{\aa}rdsen Hesjedal and Emilie Hybertsen Lys{\o} and Marit Solbj{\o}r and John-Arne Skolbekken},
  year = {2024},
  doi = {10.1111/1467-9566.13818},
  journal = {Sociology of Health \& Illness}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Auditing, Advising","Decision-subject, Decision-maker, Knowledge provider",Alter decision outcomes,"Update AI competence, Shape AI for accountability","supportive and accountable design, a high accuracy under test and validation, AI ethics, morality and empathy",accountability and final responsibility,NA,Yes,Yes
2-38622,wiley,Artificial intelligence in clinical decision-making: rethinking personal moral responsibility,"Artificially intelligent systems (AISs) are being created by software developing companies (SDCs) to influence clinical decision-making. Historically, clinicians have led healthcare decision-making, and the introduction of AISs makes SDCs novel actors in the clinical decision-making space. Although these AISs are intended to influence a clinician's decision-making, SDCs have been clear that clinicians are in fact the final decision-makers in clinical care, and that AISs can only inform their decisions. As such, the default position is that clinicians should hold responsibility for the outcomes of the use of AISs. This is not the case when an AIS has influenced a clinician's judgement and their subsequent decision. In this paper, we argue that this is an imbalanced and unjust position, and that careful thought needs to go into how personal moral responsibility for the use of AISs in clinical decision-making should be attributed. This paper employs and examines the difference between prospective and retrospective responsibility and considers foreseeability as key in determining how personal moral responsibility can be justly attributed. This leads us to the view that moral responsibility for the outcomes of using AISs in healthcare ought to be shared by the clinical users and SDCs.",10.1111/bioe.13222,https://onlinelibrary.wiley.com/doi/10.1111/bioe.13222,Bioethics,Helen Smith;Giles Birchley;Jonathan Ives,2023,20,"@article{2-38622,
  title = {Artificial intelligence in clinical decision-making: rethinking personal moral responsibility},
  author = {Smith, Helen and Birchley, Giles and Ives, Jonathan},
  year = {2023},
  journal = {Bioethics},
  doi = {10.1111/bioe.13222}
}",Theoretical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-subject, Decision-maker",no such info,no such info,NA,moral responsibility,Semi-Autonomous System,Yes,Yes
2-38680,wiley,Embracing the future—is artificial intelligence already better? A comparative study of artificial intelligence performance in diagnostic accuracy and decision-making,"Background and purpose The integration of artificial intelligence (AI) in healthcare has the potential to revolutionize patient care and clinical decision-making. This study aimed to explore the reliability of large language models in neurology by comparing the performance of an AI chatbot with neurologists in diagnostic accuracy and decision-making. Methods A cross-sectional observational study was conducted. A pool of clinical cases from the American Academy of Neurology's Question of the Day application was used as the basis for the study. The AI chatbot used was ChatGPT, based on GPT-3.5. The results were then compared to neurology peers who also answered the questions—a mean of 1500 neurologists/neurology residents. Results The study included 188 questions across 22 different categories. The AI chatbot demonstrated a mean success rate of 71.3% in providing correct answers, with varying levels of proficiency across different neurology categories. Compared to neurology peers, the AI chatbot performed at a similar level, with a mean success rate of 69.2% amongst peers. Additionally, the AI chatbot achieved a correct diagnosis in 85.0% of cases and it provided an adequate justification for its correct responses in 96.1%. Conclusions The study highlights the potential of AI, particularly large language models, in assisting with clinical reasoning and decision-making in neurology and emphasizes the importance of AI as a complementary tool to human expertise. Future advancements and refinements are needed to enhance the AI chatbot's performance and broaden its application across various medical specialties.",10.1111/ene.16195,https://onlinelibrary.wiley.com/doi/10.1111/ene.16195,European Journal of Neurology,Ângelo Fonseca;Axel Ferreira;Luís Ribeiro;Sandra Moreira;Cristina Duque,2024,27,"@article{2-38680,
  title = {Embracing the future—is artificial intelligence already better? A comparative study of artificial intelligence performance in diagnostic accuracy and decision-making},
  author = {{\^A}ngelo Fonseca and Axel Ferreira and Lu{\'\i}s Ribeiro and Sandra Moreira and Cristina Duque},
  year = {2024},
  doi = {10.1111/ene.16195},
  journal = {European Journal of Neurology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing, Collaborating","Decision-maker, Knowledge provider",no such info,Change AI responses,reasoning,NA,"Conversational/Natural Language, Textual",Yes,Yes
2-38681,wiley,Patient and dermatologists' perspectives on augmented intelligence for melanoma screening: a prospective study,"Background Artificial intelligence (AI) shows promising potential to enhance human decision-making as synergistic augmented intelligence (AuI), but requires critical evaluation for skin cancer screening in a real-world setting. Objectives To investigate the perspectives of patients and dermatologists after skin cancer screening by human, artificial and augmented intelligence. Methods A prospective comparative cohort study conducted at the University Hospital Basel included 205 patients (at high-risk of developing melanoma, with resected or advanced disease) and 8 dermatologists. Patients underwent skin cancer screening by a dermatologist with subsequent 2D and 3D total-body photography (TBP). Any suspicious and all melanocytic skin lesions ≥3 mm were imaged with digital dermoscopes and classified by corresponding convolutional neural networks (CNNs). Excisions were performed based on dermatologist's melanoma suspicion, study-defined elevated CNN risk-scores and/or melanoma suspicion by AuI. Subsequently, all patients and dermatologists were surveyed about their experience using questionnaires, including quantification of patient's safety sense following different examinations (subjective safety score (SSS): 0–10). Results Most patients believed AI could improve diagnostic performance (95.5%, n = 192/201). In total, 83.4% preferred AuI-based skin cancer screening compared to examination by AI or dermatologist alone (3D-TBP: 61.3%; 2D-TBP: 22.1%, n = 199). Regarding SSS, AuI induced a significantly higher feeling of safety than AI (mean-SSS (mSSS): 9.5 vs. 7.7, p < 0.0001) or dermatologist screening alone (mSSS: 9.5 vs. 9.1, p = 0.001). Most dermatologists expressed high trust in AI examination results (3D-TBP: 90.2%; 2D-TBP: 96.1%, n = 205). In 68.3% of the examinations, dermatologists felt that diagnostic accuracy improved through additional AI-assessment (n = 140/205). Especially beginners (<2 years' dermoscopic experience; 61.8%, n = 94/152) felt AI facilitated their clinical work compared to experts (>5 years' dermoscopic experience; 20.9%, n = 9/43). Contrarily, in divergent risk assessments, only 1.5% of dermatologists trusted a benign CNN-classification more than personal malignancy suspicion (n = 3/205). Conclusions While patients already prefer AuI with 3D-TBP for melanoma recognition, dermatologists continue to rely largely on their own decision-making despite high confidence in AI-results. Trial Registration ClinicalTrials.gov (NCT04605822).",10.1111/jdv.19905,https://onlinelibrary.wiley.com/doi/10.1111/jdv.19905,Journal of the European Academy of Dermatology and Venereology,Elisabeth Victoria Goessinger;Johannes-Christian Niederfeilner;Sara Cerminara;Julia-Tatjana Maul;Lisa Kostner;Michael Kunz;Stephanie Huber;Emrah Koral;Lea Habermacher;Gianna Sabato;Andrea Tadic;Carmina Zimmermann;Alexander Navarini;Lara Valeska Maul,2024,31,"@article{2-38681,
  title={Patient and dermatologists' perspectives on augmented intelligence for melanoma screening: a prospective study},
  author={Goessinger, Elisabeth Victoria and Niederfeilner, Johannes-Christian and Cerminara, Sara and Maul, Julia-Tatjana and Kostner, Lisa and Kunz, Michael and Huber, Stephanie and Koral, Emrah and Habermacher, Lea and Sabato, Gianna and Tadic, Andrea and Zimmermann, Carmina and Navarini, Alexander and Maul, Lara Valeska},
  year={2024},
  doi={10.1111/jdv.19905},
  journal={Journal of the European Academy of Dermatology and Venereology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising, Collaborating","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-3872,acs,Automated and Autonomous Experiments in Electron and Scanning Probe Microscopy,"Machine learning and artificial intelligence (ML/AI) are rapidly becoming an indispensable part of physics research, with domain applications ranging from theory and materials prediction to high-throughput data analysis. In parallel, the recent successes in applying ML/AI methods for autonomous systems from robotics to self-driving cars to organic and inorganic synthesis are generating enthusiasm for the potential of these techniques to enable automated and autonomous experiments (AE) in imaging. Here, we aim to analyze the major pathways toward AE in imaging methods with sequential image formation mechanisms, focusing on scanning probe microscopy (SPM) and (scanning) transmission electron microscopy ((S)TEM). We argue that automated experiments should necessarily be discussed in a broader context of the general domain knowledge that both informs the experiment and is increased as the result of the experiment. As such, this analysis should explore the human and ML/AI roles prior to and during the experiment and consider the latencies, biases, and prior knowledge of the decision-making process. Similarly, such discussion should include the limitations of the existing imaging systems, including intrinsic latencies, non-idealities, and drifts comprising both correctable and stochastic components. We further pose that the role of the AE in microscopy is not the exclusion of human operators (as is the case for autonomous driving), but rather automation of routine operations such as microscope tuning, etc., prior to the experiment, and conversion of low latency decision making processes on the time scale spanning from image acquisition to human-level high-order experiment planning. Overall, we argue that ML/AI can dramatically alter the (S)TEM and SPM fields; however, this process is likely to be highly nontrivial and initiated by combined human-ML workflows and will bring challenges both from the microscope and ML/AI sides. At the same time, these methods will enable opportunities and paradigms for scientific discovery and nanostructure fabrication.",10.1021/acsnano.1c02104,https://pubs.acs.org/doi/abs/10.1021/acsnano.1c02104,ACS Nano,Sergei V. Kalinin;Maxim Ziatdinov;Jacob Hinkle;Stephen Jesse;Ayana Ghosh;Kyle P. Kelley;Andrew R. Lupini;Bobby G. Sumpter;Rama K. Vasudevan,2021,145,"@article{2-3872,
  title = {Automated and Autonomous Experiments in Electron and Scanning Probe Microscopy},
  author = {Sergei V. Kalinin and Maxim Ziatdinov and Jacob Hinkle and Stephen Jesse and Ayana Ghosh and Kyle P. Kelley and Andrew R. Lupini and Bobby G. Sumpter and Rama K. Vasudevan},
  year = {2021},
  doi = {10.1021/acsnano.1c02104},
  journal = {ACS Nano}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,no such info,"Collaborating, Executing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-38724,wiley,Breast radiotherapy planning: a decision-making framework using deep learning,"Background Effective breast cancer treatment planning requires balancing tumor control while minimizing radiation exposure to healthy tissues. Choosing between intensity-modulated radiation therapy (IMRT) and three-dimensional conformal radiation therapy (3D-CRT) remains pivotal, influenced by patient anatomy and dosimetric constraints. Purpose This study aims to develop a decision-making framework utilizing deep learning to predict dose distributions, aiding in the selection of optimal treatment techniques. Methods A 2D U-Net convolutional neural network (CNN) model was used to predict dose distribution maps and dose-volume histogram (DVH) metrics for breast cancer patients undergoing IMRT and 3D-CRT. The model was trained and fine-tuned using retrospective datasets from two medical centers, accounting for variations in CT systems, dosimetric protocols, and clinical practices, over 346 patients. An additional 30 consecutive patients were selected for external validation, where both 3D-CRT and IMRT plans were manually created. To show the potential of the approach, an independent medical physicist evaluated both dosimetric plans and selected the most appropriate one based on applicable clinical criteria. Confusion matrices were used to compare the decisions of the independent observer with the historical decision and the proposed decision-making framework. Results Evaluation metrics, including dice similarity coefficients (DSC) and DVH analyses, demonstrated high concordance between predicted and clinical dose distribution for both IMRT and 3D-CRT techniques, especially for organs at risk (OARs). The decision-making framework demonstrated high accuracy (90), recall (95.7), and precision (91.7) when compared to independent clinical evaluations, while the historical decision-making had lower accuracy (50), recall (47.8), and precision (78.6). Conclusions The proposed decision-making model accurately predicts dose distributions for both 3D-CRT and IMRT, ensuring reliable OAR dose estimation. This decision-making framework significantly outperforms historical decision-making, demonstrating higher accuracy, recall, and precision.",10.1002/mp.17527,https://onlinelibrary.wiley.com/doi/10.1002/mp.17527,Medical Physics,Pedro Gallego;Eva Ambroa;Jaime PérezAlija;Nuria Jornet;Cristina Anson;Natalia Tejedor;Helena Vivancos;Agust Ruiz;Marta Barceló;Alejandro Dominguez;Victor Riu;Javier Roda;Pablo Carrasco;Simone Balocco;Oliver Díaz,2024,3,"@article{2-38724,
  title = {Breast radiotherapy planning: a decision-making framework using deep learning},
  author = {Pedro Gallego and Eva Ambroa and Jaime PérezAlija and Nuria Jornet and Cristina Anson and Natalia Tejedor and Helena Vivancos and Agust Ruiz and Marta Barceló and Alejandro Dominguez and Victor Riu and Javier Roda and Pablo Carrasco and Simone Balocco and Oliver Díaz},
  year = {2024},
  doi = {10.1002/mp.17527},
  journal = {Medical Physics}
}","Algorithmic contributions, Empirical contributions",Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-3875,acs,Automated-Screening Oriented Electric Sensing of Vitamin B1 Using a Machine Learning Aided Solid-State Nanopore,"Micronutrient detection and identification at the single-molecule level are paramount for both clinical and home diagnostics. Analytical tools such as high-performance liquid chromatography and liquid chromatography-tandem mass spectrometry have been widely used but include a high instrument cost and prolonged analysis time. Here, as a model system, by merging nanopore signatures with machine learning algorithms, we propose an automated electric sensing strategy to identify vitamin B1 and its phosphorylated derivatives with good accuracy. Further, the relationship between vitamin B1 dynamics and nanopore signatures is examined. To understand the machine-decision-making process, Shapley additive explanations are made. Using a machine learning aided solid-state nanopore, we pave the way for next-generation micronutrient detection.",10.1021/acs.jpcb.4c05619,https://pubs.acs.org/doi/abs/10.1021/acs.jpcb.4c05619,The Journal of Physical Chemistry B,Sneha Mittal;Milan Kumar Jena;Biswarup Pathak,2025,0,"@article{2-3875,
  title = {Automated-Screening Oriented Electric Sensing of Vitamin B1 Using a Machine Learning Aided Solid-State Nanopore},
  author = {Mittal, Sneha and Jena, Milan Kumar and Pathak, Biswarup},
  year = {2025},
  doi = {10.1021/acs.jpcb.4c05619},
  journal = {The Journal of Physical Chemistry B}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Executing, Analyzing",Developer,NA,NA,NA,NA,NA,Yes,No
2-38754,wiley,Trust in machine learning driven clinical decision support tools among otolaryngologists,"Background Machine learning driven clinical decision support tools (ML-CDST) are on the verge of being integrated into clinical settings, including in Otolaryngology-Head & Neck Surgery. In this study, we investigated whether such CDST may influence otolaryngologists' diagnostic judgement. Methods Otolaryngologists were recruited virtually across the United States for this experiment on human–AI interaction. Participants were shown 12 different video-stroboscopic exams from patients with previously diagnosed laryngopharyngeal reflux or vocal fold paresis and asked to determine the presence of disease. They were then exposed to a random diagnosis purportedly resulting from an ML-CDST and given the opportunity to revise their diagnosis. The ML-CDST output was presented with no explanation, a general explanation, or a specific explanation of its logic. The ML-CDST impact on diagnostic judgement was assessed with McNemar's test. Results Forty-five participants were recruited. When participants reported less confidence (268 observations), they were significantly (p = 0.001) more likely to change their diagnostic judgement after exposure to ML-CDST output compared to when they reported more confidence (238 observations). Participants were more likely to change their diagnostic judgement when presented with a specific explanation of the CDST logic (p = 0.048). Conclusions Our study suggests that otolaryngologists are susceptible to accepting ML-CDST diagnostic recommendations, especially when less confident. Otolaryngologists' trust in ML-CDST output is increased when accompanied with a specific explanation of its logic. Level of Evidence 2 Laryngoscope, 134:2799–2804, 2024",10.1002/lary.31260,https://onlinelibrary.wiley.com/doi/10.1002/lary.31260,The Laryngoscope,Hannah Chen MD;Xiaoyue Ma MS;Hal Rives BS;Aisha Serpedin;Peter Yao MD;Anaïs Rameau MD;MPhil;MS;FACS,2024,133,"@article{2-38754,
  title={Trust in machine learning driven clinical decision support tools among otolaryngologists},
  author={Chen, Hannah MD and Ma, Xiaoyue MS and Rives, Hal BS and Serpedin, Aisha and Yao, Peter MD and Rameau, Ana{\""i}s MD, MPhil, MS, FACS},
  year={2024},
  doi={10.1002/lary.31260},
  journal={The Laryngoscope}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Explaining","Decision-maker, Decision-subject","Alter decision outcomes, Change trust, Change affective-perceptual",Update AI competence,"preliminary diagnoses, textual explanations",domain knowledge,Visual,Yes,Yes
2-38786,wiley,"Artificial intelligence-based clinical decision support in modern medical physics: selection, acceptance, commissioning, and quality assurance","Background Recent advances in machine and deep learning based on an increased availability of clinical data have fueled renewed interest in computerized clinical decision support systems (CDSSs). CDSSs have shown great potential to improve healthcare, increase patient safety and reduce costs. However, the use of CDSSs is not without pitfalls, as an inadequate or faulty CDSS can potentially deteriorate the quality of healthcare and put patients at risk. In addition, the adoption of a CDSS might fail because its intended users ignore the output of the CDSS due to lack of trust, relevancy or actionability. Aim In this article, we provide guidance based on literature for the different aspects involved in the adoption of a CDSS with a special focus on machine and deep learning based systems: selection, acceptance testing, commissioning, implementation and quality assurance. Results A rigorous selection process will help identify the CDSS that best fits the preferences and requirements of the local site. Acceptance testing will make sure that the selected CDSS fulfills the defined specifications and satisfies the safety requirements. The commissioning process will prepare the CDSS for safe clinical use at the local site. An effective implementation phase should result in an orderly roll out of the CDSS to the well-trained end-users whose expectations have been managed. And finally, quality assurance will make sure that the performance of the CDSS is maintained and that any issues are promptly identified and solved. Conclusion We conclude that a systematic approach to the adoption of a CDSS will help avoid pitfalls, improve patient safety and increase the chances of success.",10.1002/mp.13562,https://onlinelibrary.wiley.com/doi/10.1002/mp.13562,Medical Physics,Geetha Mahadevaiah;Prasad RV;Inigo Bermejo;David Jaffray;Andre Dekker;Leonard Wee,2020,142,"@article{2-38786,
  title = {Artificial intelligence-based clinical decision support in modern medical physics: selection, acceptance, commissioning, and quality assurance},
  author = {Geetha Mahadevaiah and Prasad RV and Inigo Bermejo and David Jaffray and Andre Dekker and Leonard Wee},
  year = {2020},
  journal = {Medical Physics},
  doi = {10.1002/mp.13562}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising, Monitoring","Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-38805,wiley,Developing a clinical decision support framework for integrating predictive models into routine nursing practices in home health care for patients with heart failure,"Background The healthcare industry increasingly values high-quality and personalized care. Patients with heart failure (HF) receiving home health care (HHC) often experience hospitalizations due to worsening symptoms and comorbidities. Therefore, close symptom monitoring and timely intervention based on risk prediction could help HHC clinicians prevent emergency department (ED) visits and hospitalizations. This study aims to (1) describe important variables associated with a higher risk of ED visits and hospitalizations in HF patients receiving HHC; (2) map data requirements of a clinical decision support (CDS) tool to the exchangeable data standard for integrating a CDS tool into the care of patients with HF; (3) outline a pipeline for developing a real-time artificial intelligence (AI)-based CDS tool. Methods We used patient data from a large HHC organization in the Northeastern US to determine the factors that can predict ED visits and hospitalizations among patients with HF in HHC (9362 patients in 12,223 care episodes). We examined vital signs, HHC visit details (e.g., the purpose of the visit), and clinical note–derived variables. The study identified critical factors that can predict ED visits and hospitalizations and used these findings to suggest a practical CDS tool for nurses. The tool's proposed design includes a system that can analyze data quickly to offer timely advice to healthcare clinicians. Results Our research showed that the length of time since a patient was admitted to HHC and how recently they have shown symptoms of HF were significant factors predicting an adverse event. Additionally, we found this information from the last few HHC visits before the occurrence of an ED visit or hospitalization were particularly important in the prediction. One hundred percent of clinical demographic profiles from the Outcome and Assessment Information Set variables were mapped to the exchangeable data standard, while natural language processing–driven variables couldn't be mapped due to their nature, as they are generated from unstructured data. The suggested CDS tool alerts nurses about newly emerging or rising risks, helping them make informed decisions. Conclusions This study discusses the creation of a time-series risk prediction model and its potential CDS applications within HHC, aiming to enhance patient outcomes, streamline resource utilization, and improve the quality of care for individuals with HF. Clinical Relevance This study provides a detailed plan for a CDS tool that uses the latest AI technology designed to aid nurses in their day-to-day HHC service. Our proposed CDS tool includes an alert system that serves as a guard rail to prevent ED visits and hospitalizations. This tool can potentially improve how nurses make decisions and improve patient outcomes by providing early warnings about ED visits and hospitalizations.",10.1111/jnu.13030,https://onlinelibrary.wiley.com/doi/10.1111/jnu.13030,Journal of Nursing Scholarship,Sena Chae;Anahita Davoudi;Jiyoun Song;Lauren Evans;Kathryn H. Bowles;Margaret V. Mcdonald;Yolanda Barrón;Se Hee Min PhD;RN;Sungho Oh PhD;Danielle Scharp MSN;RN;Zidu Xu MMed;BS;RN;Maxim Topaz,2024,15,"@article{2-38805,
  title={Developing a clinical decision support framework for integrating predictive models into routine nursing practices in home health care for patients with heart failure},
  author={Chae, Sena and Davoudi, Anahita and Song, Jiyoun and Evans, Lauren and Bowles, Kathryn H. and Mcdonald, Margaret V. and Barr{\'o}n, Yolanda and Min, Se Hee and Oh, Sungho and Scharp, Danielle and Xu, Zidu and Topaz, Maxim},
  year={2024},
  journal={Journal of Nursing Scholarship},
  doi={10.1111/jnu.13030}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-subject, Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-38807,wiley,"A machine learning-based, decision support, mobile phone application for diagnosis of common dermatological diseases","Background The integration of machine learning algorithms in decision support tools for physicians is gaining popularity. These tools can tackle the disparities in healthcare access as the technology can be implemented on smartphones. We present the first, large-scale study on patients with skin of colour, in which the feasibility of a novel mobile health application (mHealth app) was investigated in actual clinical workflows. Objective To develop a mHealth app to diagnose 40 common skin diseases and test it in clinical settings. Methods A convolutional neural network-based algorithm was trained with clinical images of 40 skin diseases. A smartphone app was generated and validated on 5014 patients, attending rural and urban outpatient dermatology departments in India. The results of this mHealth app were compared against the dermatologists’ diagnoses. Results The machine–learning model, in an in silico validation study, demonstrated an overall top-1 accuracy of 76.93 ± 0.88% and mean area-under-curve of 0.95 ± 0.02 on a set of clinical images. In the clinical study, on patients with skin of colour, the app achieved an overall top-1 accuracy of 75.07% (95% CI = 73.75–76.36), top-3 accuracy of 89.62% (95% CI = 88.67–90.52) and mean area-under-curve of 0.90 ± 0.07. Conclusion This study underscores the utility of artificial intelligence-driven smartphone applications as a point-of-care, clinical decision support tool for dermatological diagnosis for a wide spectrum of skin diseases in patients of the skin of colour.",10.1111/jdv.16967,https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967,Journal of the European Academy of Dermatology and Venereology,R. Pangti;J. Mathur;V. Chouhan;S. Kumar;L. Rajput;S. Shah;A. Gupta;A. Dixit;D. Dholakia;S. Gupta;S. Gupta;M. George;V.K. Sharma;S. Gupta,2020,88,"@article{2-38807,
  title={A machine learning-based, decision support, mobile phone application for diagnosis of common dermatological diseases},
  author={Pangti, R. and Mathur, J. and Chouhan, V. and Kumar, S. and Rajput, L. and Shah, S. and Gupta, A. and Dixit, A. and Dholakia, D. and Gupta, S. and Gupta, S. and George, M. and Sharma, V.K. and Gupta, S.},
  year={2020},
  doi={10.1111/jdv.16967},
  journal={Journal of the European Academy of Dermatology and Venereology}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-38813,wiley,Automated diagnosis and quantitative analysis of plus disease in retinopathy of prematurity based on deep convolutional neural networks,"Background The purpose of this study was to develop an automated diagnosis and quantitative analysis system for plus disease. The system provides a diagnostic decision but also performs quantitative analysis of the typical pathological features of the disease, which helps the physicians to make the best judgement and communicate the decisions. Methods The deep learning network provided segmentation of the retinal vessels and the optic disc (OD). Based on the vessel segmentation, plus disease was classified and tortuosity, width, fractal dimension and vessel density were evaluated automatically. Results The trained network achieved a sensitivity of 95.1% with 97.8% specificity for the diagnosis of plus disease. For detection of preplus or worse, the sensitivity and specificity were 92.4% and 97.4%. The quadratic weighted k was 0.9244. The tortuosities for the normal, preplus and plus groups were 3.61 ± 0.08, 5.95 ± 1.57 and 10.67 ± 0.50 (104 cm−3). The widths of the blood vessels were 63.46 ± 0.39, 67.21 ± 0.70 and 68.89 ± 0.75 μm. The fractal dimensions were 1.18 ± 0.01, 1.22 ± 0.01 and 1.26 ± 0.02. The vessel densities were 1.39 ± 0.03, 1.60 ± 0.01 and 1.64 ± 0.09 (%). All values were statistically different among the groups. After treatment for plus disease with ranibizumab injection, quantitative analysis showed significant changes in the pathological features. Conclusions Our system achieved high accuracy of diagnosis of plus disease in retinopathy of prematurity. It provided a quantitative analysis of the dynamic features of the disease progression. This automated system can assist physicians by providing a classification decision with auxiliary quantitative evaluation of the typical pathological features of the disease.",10.1111/aos.14264,https://onlinelibrary.wiley.com/doi/10.1111/aos.14264,Acta Ophthalmologica,Jianbo Mao;Yuhao Luo;Lei Liu;Jimeng Lao;Yirun Shao;Min Zhang;Caiyun Zhang;Mingzhai Sun;Lijun Shen,2019,84,"@article{2-38813,
  title={Automated diagnosis and quantitative analysis of plus disease in retinopathy of prematurity based on deep convolutional neural networks},
  author={Mao, Jianbo and Luo, Yuhao and Liu, Lei and Lao, Jimeng and Shao, Yirun and Zhang, Min and Zhang, Caiyun and Sun, Mingzhai and Shen, Lijun},
  year={2019},
  doi={10.1111/aos.14264},
  journal={Acta Ophthalmologica}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-38852,wiley,Deep learning applications for acute stroke management,"Brain imaging is essential to the clinical care of patients with stroke, a leading cause of disability and death worldwide. Whereas advanced neuroimaging techniques offer opportunities for aiding acute stroke management, several factors, including time delays, inter-clinician variability, and lack of systemic conglomeration of clinical information, hinder their maximal utility. Recent advances in deep machine learning (DL) offer new strategies for harnessing computational medical image analysis to inform decision making in acute stroke. We examine the current state of the field for DL models in stroke triage. First, we provide a brief, clinical practice-focused primer on DL. Next, we examine real-world examples of DL applications in pixel-wise labeling, volumetric lesion segmentation, stroke detection, and prediction of tissue fate postintervention. We evaluate recent deployments of deep neural networks and their ability to automatically select relevant clinical features for acute decision making, reduce inter-rater variability, and boost reliability in rapid neuroimaging assessments, and integrate neuroimaging with electronic medical record (EMR) data in order to support clinicians in routine and triage stroke management. Ultimately, we aim to provide a framework for critically evaluating existing automated approaches, thus equipping clinicians with the ability to understand and potentially apply DL approaches in order to address challenges in clinical practice. ANN NEUROL 2022;92:574–587",10.1002/ana.26435,https://onlinelibrary.wiley.com/doi/10.1002/ana.26435,Annals of Neurology,Isha R. Chavva BS;Anna L. Crawford MSc;Mercy H. Mazurek BS;Matthew M. Yuen BA;Anjali M. Prabhat BA;Sam Payabvash MD;Gordon Sze MD;Guido J. Falcone MD;ScD;MPH;Charles C. Matouk MD;Adam de Havenon MD;MSCI;Jennifer A. Kim MD;PhD;Richa Sharma MD;MPH;Steven J. Schiff MD;PhD;Matthew S. Rosen PhD;Jayashree Kalpathy-Cramer PhD;Juan E. Iglesias Gonzalez PhD;W. Taylor Kimberly MD;PhD;Kevin N. Sheth MD,2022,0,"@article{2-38852,
  title={Deep Learning Applications for Acute Stroke Management},
  author={Chavva, Isha R. and Crawford, Anna L. and Mazurek, Mercy H. and Yuen, Matthew M. and Prabhat, Anjali M. and Payabvash, Sam and Sze, Gordon and Falcone, Guido J. and Matouk, Charles C. and de Havenon, Adam and Kim, Jennifer A. and Sharma, Richa and Schiff, Steven J. and Rosen, Matthew S. and Kalpathy-Cramer, Jayashree and Iglesias Gonzalez, Juan E. and Kimberly, W. Taylor and Sheth, Kevin N.},
  year={2022},
  doi={10.1002/ana.26435},
  journal={Annals of Neurology}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising, Forecasting","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-38898,wiley,Capturing uncertainty intuition in road maintenance decision-making using an evidential neural network,"Decision-making of project-level road maintenance is the process of mapping road information into a maintenance plan. Even though benefitting from deep learning, the decision-making still faces the problem of maintenance data uncertainty. The data uncertainty derives from imperfect road information collection and arbitrary selection of maintenance plans. Such uncertainty always leads to unreasonable maintenance decision-making. This study proposes an evidential approach using information entropy (IE) and Dempster–Shafer theory (DST) to capture and handle uncertainty in the decision-making of project-level road maintenance. The approach first uses an IE-based judgment method (IE-based method) to capture and observe quantitative data uncertainty. The DST-based method is then developed to handle maintenance data uncertainty through utilizing evidential neural network and set-valued decision-making. A numerical experiment is performed on the maintenance data with 280 km of semi-rigid base highways in China. The results indicate that the IE-based method can measure the data uncertainty in the information of road sections. The DST-based method captures the cautious intuition on the selection of maintenance plans, thereby reducing the decision error rate by over 14% under specific conditions when facing data uncertainty.",10.1111/mice.13374,https://onlinelibrary.wiley.com/doi/10.1111/mice.13374,Computer-Aided Civil and Infrastructure Engineering,Tianqing Hei;Zhixin Lin;Zezhen Dong;Zheng Tong;Tao Ma,2024,1,"@article{2-38898,
  title={Capturing uncertainty intuition in road maintenance decision-making using an evidential neural network},
  author={Hei, Tianqing and Lin, Zhixin and Dong, Zezhen and Tong, Zheng and Ma, Tao},
  year={2024},
  journal={Computer-Aided Civil and Infrastructure Engineering},
  doi={10.1111/mice.13374}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,"Analyzing, Advising","Guardian, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-38915,wiley,"Toward cloud-native, machine learning base detection of crop disease with imaging spectroscopy","Developing actionable early detection and warning systems for agricultural stakeholders is crucial to reduce the annual $200B USD losses and environmental impacts associated with crop diseases. Agricultural stakeholders primarily rely on labor-intensive, expensive scouting and molecular testing to detect disease. Spectroscopic imagery (SI) can improve plant disease management by offering decision-makers accurate risk maps derived from Machine Learning (ML) models. However, training and deploying ML requires significant computation and storage capabilities. This challenge will become even greater as global-scale data from the forthcoming Surface Biology & Geology satellite becomes available. This work presents a cloud-hosted architecture to streamline plant disease detection with SI from NASA’s AVIRIS-NG platform, using grapevine leafroll-associated virus complex 3 (GLRaV-3) as a model system. Here, we showcase a pipeline for processing SI to produce plant disease detection models and demonstrate that the underlying principles of a cloud-based disease detection system easily accommodate model improvements and shifting data modalities. Our goal is to make the insights derived from SI available to agricultural stakeholders via a platform designed with their needs and values in mind. The key outcome of this work is an innovative, responsive system foundation that can empower agricultural stakeholders to make data-driven plant disease management decisions while serving as a framework for others pursuing use-inspired application development for agriculture to follow that ensures social impact and reproducibility while preserving stakeholder privacy. Plain Language Summary Agricultural decision-makers need reliable access to accurate data to make sustainable crop management choices. This is especially important for decisions related to crop disease management, which can have major financial, environmental, and societal impacts. Forthcoming hyperspectral satellite systems such as Surface Biology & Geology will provide spectroscopic imagery (SI) that can be used in combination with machine learning (ML) for agricultural decision making at the global scale. However, deploying ML models trained on SI requires significant computation and storage resources, limiting non-expert use. Additionally, agricultural stakeholders frequently have reservations and/or restrictions about how data can or cannot be shared with outside entities. Here, we overview a proof-of-concept, cloud system designed with agricultural users in mind that allows researchers to rapidly deploy ML models for plant disease detection using SI from AVIRIS-NG without retaining confidential stakeholder information. We use grapevine leafroll virus-complex 3 (GLRaV-3) in California wine grapes, a virus that causes $3 billion in damages and losses to the US grape industry annually, as a case study. We provide a framework design that outlines how this system is implemented and could be made accessible to both growers and researchers, as well as discuss system limitations and opportunities for future work. Key Points Cloud-based plant disease detection system, easily accommodates model improvements and future data sources Empower agricultural stakeholders to use hyperspectral data for decision support while preserving stakeholder data privacy Outline framework for researchers interested in designing geospatial/remote sensing applications for agricultural stakeholders to follow",10.1029/2022JG007342,https://onlinelibrary.wiley.com/doi/10.1029/2022JG007342,Journal of Geophysical Research: Biogeosciences,Gloire Rubambiza;Fernando Romero Galvan;Ryan Pavlick;Hakim Weatherspoon;Kaitlin M. Gold,2023,2,"@article{2-38915,
  title={Toward cloud-native, machine learning based detection of crop disease with imaging spectroscopy},
  author={Rubambiza, Gloire and Romero Galvan, Fernando and Pavlick, Ryan and Weatherspoon, Hakim and Gold, Kaitlin M.},
  year={2023},
  doi={10.1029/2022JG007342},
  journal={Journal of Geophysical Research: Biogeosciences}
}",System/Artifact contributions,Environment / Resources / Energy,Operational,"Analyzing, Advising","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-38930,wiley,Bayesian hybrid analytics for uncertainty analysis and real-time crop management,"Dynamic, deterministic agricultural models, and current machine learning technologies based on sensor data, enable and support decision making for on-farm management. However, their predictions are subject to various sources of uncertainty. Hybrid analytics that leverage both modelled and sensor data provide predictive information that makes the best of both approaches in a timely fashion to inform operational decision making and enable inclusive uncertainty quantification. We describe and evaluate a probabilistic Bayesian data assimilation tool that combines the state variables from the Sirius wheat (Triticum aestivum L.) development model with time-series environmental and leaf count data. Additionally, the uncertainty associated with input parameters is quantified via expert opinion. The Bayesian approach obtained point estimates through time that were accompanied by inclusive, probabilistic, 95% credible intervals. At the end of simulation, a typical model predicted a final leaf number of 6.6 leaves, Sirius alone predicted seven leaves and the mean of the observed data was 6.7 leaves. The 95% credible interval was estimated as 5.1–8.4 leaves. Importantly, the tool was able to “redirect” simulated outputs if input parameters such as minimum leaf number or base phyllochron were incorrectly specified, with the implication that on-farm decision makers would have advance warning of variation in expected harvest date. Relatively few plants with time-intensive data were sufficient to fit the model, however, more plants would be desirable to reduce the rather wide range of credible intervals. Nevertheless, the tool shows potential and could be readily implemented with low resource requirements, providing more finely tuned harvest date information, with probabilistic uncertainty quantification built-in, for on-farm decisions.",10.1002/agj2.20659,https://onlinelibrary.wiley.com/doi/10.1002/agj2.20659,Agronomy Journal,Esther D. Meenken;Christopher M. Triggs;Hamish E. Brown;Sarah Sinton;Jeremy R. Bryant;Alasdair D.L. Noble;Martin Espig;Mostafa Sharifi;David M. Wheeler,2021,8,"@article{2-38930,
        author = {Meenken, Esther D. and Triggs, Christopher M. and Brown, Hamish E. and Sinton, Sarah and Bryant, Jeremy R. and Noble, Alasdair D.L. and Espig, Martin and Sharifi, Mostafa and Wheeler, David M.},
        journal = {Agronomy Journal},
        number = {3},
        pages = {2491-2505},
        title = {Bayesian hybrid analytics for uncertainty analysis and real-time crop management},
        volume = {113},
        doi={10.1002/agj2.20659},
        year = {2021}
}",Algorithmic contributions,Environment / Resources / Energy,Operational,"Forecasting, Analyzing, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-38959,wiley,Food reinforcement architecture: a framework for impulsive and compulsive overeating and food abuse,"Few reward-based theories address key drivers of susceptibility to food cues and consumption beyond fullness. Decision-making and habit formation are governed by reinforcement-based learning processes that, when overstimulated, can drive unregulated hedonically motivated overeating. Here, a model food reinforcement architecture is proposed that uses fundamental concepts in reinforcement and decision-making to identify maladaptive eating habits that can lead to obesity. This model is unique in that it identifies metabolic drivers of reward and incorporates neuroscience, computational decision-making, and psychology to map overeating and obesity. Food reinforcement architecture identifies two paths to overeating: a propensity for hedonic targeting of food cues contributing to impulsive overeating and lack of satiation that contributes to compulsive overeating. A combination of those paths will result in a conscious and subconscious drive to overeat independent of negative consequences, leading to food abuse and/or obesity. Use of this model to identify aberrant reinforcement learning processes and decision-making systems that can serve as markers of overeating risk may provide an opportunity for early intervention in obesity.",10.1002/oby.23792,https://onlinelibrary.wiley.com/doi/10.1002/oby.23792,Obesity,Kyle S. Burger,2023,0,"@article{2-38959,
  title={Food reinforcement architecture: a framework for impulsive and compulsive overeating and food abuse},
  author={Burger, Kyle S.},
  year={2023},
  journal={Obesity},
  doi={10.1002/oby.23792}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Individual,"Analyzing, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-38971,wiley,The rule of law and automation of government decision-making,"Governments around the world are deploying automation tools in making decisions that affect rights and entitlements. The interests affected are very broad, ranging from time spent in detention to the receipt of social security benefits. This article focusses on the impact on rule of law values of automation using: (1) pre-programmed rules (for example, expert systems); and (2) predictive inferencing whereby rules are derived from historic data (such by applying supervised machine learning). The article examines the use of these systems across a range of nations. It explores the tension between the rule of law and rapid technological change and concludes with observations on how the automation of government decision-making can both enhance and detract from rule of law values.",10.1111/1468-2230.12412,https://onlinelibrary.wiley.com/doi/10.1111/1468-2230.12412,The Modern Law Review,Monika Zalnieriute;Lyria Bennett Moses;George Williams,2019,2,"@article{2-38971,
  title={The rule of law and automation of government decision-making},
  author={Zalnieriute, Monika and Bennett Moses, Lyria and Williams, George},
  year={2019},
  doi={10.1111/1468-2230.12412},
  journal={The Modern Law Review}
}",Theoretical contributions,Law / Policy / Governance,Institutional,"Executing, Advising","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-38984,wiley,Citizen conceptions of democracy and support for artificial intelligence in government and politics,"How much do citizens support artificial intelligence (AI) in government and politics at different levels of decision-making authority and to what extent is this AI support associated with citizens’ conceptions of democracy? Using original survey data from Germany, the analysis shows that people are overall sceptical toward using AI in the political realm. The findings suggest that how much citizens endorse democracy as liberal democracy as opposed to several of its disfigurations matters for AI support, but only in high-level politics. While a stronger commitment to liberal democracy is linked to lower support for AI, the findings contradict the idea that a technocratic notion of democracy lies behind greater acceptance of political AI uses. Acceptance is higher only among those holding reductionist conceptions of democracy which embody the idea that whatever works to accommodate people's views and preferences is fine. Populists, in turn, appear to be against AI in political decision making.",10.1111/1475-6765.12570,https://onlinelibrary.wiley.com/doi/10.1111/1475-6765.12570,European Journal of Political Research,PASCAL D. KÖNIG,2022,39,"@article{2-38984,
  title = {Citizen conceptions of democracy and support for artificial intelligence in government and politics},
  author = {Pascal D. K{\""o}nig},
  year = {2022},
  doi = {10.1111/1475-6765.12570},
  journal = {European Journal of Political Research}
}",Empirical contributions,Law / Policy / Governance,Institutional,"Advising, Executing","Decision-maker, Guardian, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-38985,wiley,Logistics innovation and social sustainability: how to prevent an artificial divide in human–computer interaction,"Human–computer interaction (HCI) is a cornerstone for the success of technical innovation in the logistics and supply chain sector. As a major part of social sustainability, this interaction is changing as artificial intelligence applications (Internet of Things, autonomous transport, Physical Internet) are implemented, leading to larger machine autonomy, and hence the transition from a primary executive to a supervisory role of human operators. A fundamental question concerns the level of control transferred to machines, such as autonomous vehicles and automatic materials handling devices. Problems include a lack of human trust toward automatic decision making or an inclination to override the system in case automated decisions are misperceived. This paper outlines a theoretical framework, describing different levels of acceptance and trust as a key HCI element of technology innovation, and points to the possible danger of an artificial divide at both the individual and firm level. Based upon the findings of four benchmark cases, a classification of the roles of human employees in adopting innovations is developed. Measures at operational, tactical, and strategic level are discussed to improve HCI, more in particular the capacity of individuals and firms to apply state-of-the-art techniques and to prevent an artificial divide, thereby increasing social sustainability.",10.1111/jbl.12198,https://onlinelibrary.wiley.com/doi/10.1111/jbl.12198,Journal of Business Logistics,Matthias Klumpp;Henk Zijm,2019,0,"@article{2-38985,
  title={Logistics innovation and social sustainability: how to prevent an artificial divide in human--computer interaction},
  author={Klumpp, Matthias and Zijm, Henk},
  year={2019},
  doi={10.1111/jbl.12198},
  journal={Journal of Business Logistics}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Manufacturing / Industry / Automation",Organizational,"Executing, Collaborating","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-38986,wiley,"Modeling morality in 3-d: decision-making, judgment, and inference","Humans face a fundamental challenge of how to balance selfish interests against moral considerations. Such trade-offs are implicit in moral decisions about what to do; judgments of whether an action is morally right or wrong; and inferences about the moral character of others. To date, these three dimensions of moral cognition–decision-making, judgment, and inference–have been studied largely independently, using very different experimental paradigms. However, important aspects of moral cognition occur at the intersection of multiple dimensions; for instance, moral hypocrisy can be conceived as a disconnect between moral decisions and moral judgments. Here we describe the advantages of investigating these three dimensions of moral cognition within a single computational framework. A core component of this framework is harm aversion, a moral sentiment defined as a distaste for harming others. The framework integrates economic utility models of harm aversion with Bayesian reinforcement learning models describing beliefs about others’ harm aversion. We show how this framework can provide novel insights into the mechanisms of moral decision-making, judgment, and inference.",10.1111/tops.12382,https://onlinelibrary.wiley.com/doi/10.1111/tops.12382,Topics in Cognitive Science,Hongbo Yu;Jenifer Z. Siegel;Molly J. Crockett,2018,37,"@article{2-38986,
  title={Modeling morality in 3-d: decision-making, judgment, and inference},
  author={Hongbo Yu and Jenifer Z. Siegel and Molly J. Crockett},
  year={2018},
  doi={10.1111/tops.12382},
  journal={Topics in Cognitive Science}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Individual,"Explaining, Analyzing","Decision-maker, Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-39015,wiley,Handle with care: assessing performance measures of medical ai for shared clinical decision-making,"In this article I consider two pertinent questions that practitioners must consider when they deploy an algorithmic system as support in clinical shared decision-making. The first question concerns how to interpret and assess the significance of different performance measures for clinical decision-making. The second question concerns the professional obligations that practitioners have to communicate information about the quality of an algorithm's output to patients in light of the principles of autonomy, beneficence, and justice. In the article I review the four standard performance measures used to evaluate and validate algorithms, outline their role in the discussion of algorithmic fairness, and discuss the professional responsibilities that practitioners face when communicating information about these measures to patients.",10.1111/bioe.12930,https://onlinelibrary.wiley.com/doi/10.1111/bioe.12930,Bioethics,Sune Holm,2021,15,"@article{2-39015,
  title={Handle with care: assessing performance measures of medical AI for shared clinical decision-making},
  author={Holm, Sune},
  year={2021},
  journal={Bioethics},
  doi={10.1111/bioe.12930}
}",Theoretical contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising, Explaining","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-39035,wiley,Intelligent decision-making system of air defense resource allocation via hierarchical reinforcement learning,"Intelligent decision-making in air defense operations has attracted wide attention from researchers. Facing complex battlefield environments, existing decision-making algorithms fail to make targeted decisions according to the hierarchical decision-making characteristics of air defense operational command and control. What’s worse, in the process of problem-solving, these algorithms are beset by defects such as dimensional disaster and poor real-time performance. To address these problems, a new hierarchical reinforcement learning algorithm named Hierarchy Asynchronous Advantage Actor-Critic (H-A3C) is developed. This algorithm is designed to have a hierarchical decision-making framework considering the characteristics of air defense operations and employs the hierarchical reinforcement learning method for problem-solving. With a hierarchical decision-making capability similar to that of human commanders in decision-making, the developed algorithm produces many new policies during the learning process. The features of air situation information are extracted using the bidirectional-gated recurrent unit (Bi-GRU) network, and then the agent is trained using the H-A3C algorithm. In the training process, the multihead attention mechanism and the event-based reward mechanism are introduced to facilitate the training. In the end, the proposed H-A3C algorithm is verified in a digital battlefield environment, and the results prove its advantages over existing algorithms.",10.1155/2024/7777050,https://onlinelibrary.wiley.com/doi/10.1155/2024/7777050,International Journal of Intelligent Systems,Minrui Zhao;Gang Wang;Qiang Fu;Wen Quan;Quan Wen;Xiaoqiang Wang;Tengda Li;Yu Chen;Shan Xue;Jiaozhi Han,2024,3,"@article{2-39035,
  title={Intelligent decision-making system of air defense resource allocation via hierarchical reinforcement learning},
  author={Zhao, Minrui and Wang, Gang and Fu, Qiang and Quan, Wen and Wen, Quan and Wang, Xiaoqiang and Li, Tengda and Chen, Yu and Xue, Shan and Han, Jiaozhi},
  year={2024},
  journal={International Journal of Intelligent Systems},
  doi={10.1155/2024/7777050}
}",Algorithmic contributions,Defense / Military / Emergency,Institutional,Executing,Guardian,NA,NA,NA,NA,NA,Yes,No
2-39045,wiley,"Nurses' perceptions of the design, implementation, and adoption of machine learning clinical decision support: a descriptive qualitative study","Introduction The purpose of this study was to explore nurses' perspectives on Machine Learning Clinical Decision Support (ML CDS) design, development, implementation, and adoption. Design Qualitative descriptive study. Methods Nurses (n = 17) participated in semi-structured interviews. Data were transcribed, coded, and analyzed using Thematic analysis methods as described by Braun and Clarke. Results Four major themes and 14 sub-themes highlight nurses' perspectives on autonomy in decision-making, the influence of prior experience in shaping their preferences for use of novel CDS tools, the need for clarity in why ML CDS is useful in improving practice/outcomes, and their desire to have nursing integrated in design and implementation of these tools. Conclusion This study provided insights into nurse perceptions regarding the utility and usability of ML CDS as well as the influence of previous experiences with technology and CDS, change management strategies needed at the time of implementation of ML CDS, the importance of nurse-perceived engagement in the development process, nurse information needs at the time of ML CDS deployment, and the perceived impact of ML CDS on nurse decision making autonomy. Clinical Relevance This study contributes to the body of knowledge about the use of AI and machine learning (ML) in nursing practice. Through generation of insights drawn from nurses' perspectives, these findings can inform successful design and adoption of ML Clinical Decision Support.",10.1111/jnu.13001,https://onlinelibrary.wiley.com/doi/10.1111/jnu.13001,Journal of Nursing Scholarship,Ann M. Wieben PhD;RN;NI-BC;Bader G. Alreshidi PhD;RN;ACNP-BC;Brian J. Douthit PhD;NI-BC;Marisa Sileo MSN;RN;NI-BC;Pankaj Vyas MSN;MBA;RN;Linsey Steege PhD;Andrea Gilmore-Bykovskyi PhD;RN,2024,13,"@article{2-39045,
  title = {Nurses' perceptions of the design, implementation, and adoption of machine learning clinical decision support: a descriptive qualitative study},
  author = {Wieben, Ann M. and Alreshidi, Bader G. and Douthit, Brian J. and Sileo, Marisa and Vyas, Pankaj and Steege, Linsey and Gilmore-Bykovskyi, Andrea},
  year = {2024},
  doi = {10.1111/jnu.13001},
  journal = {Journal of Nursing Scholarship}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Guardian, Decision-subject, Knowledge provider","Change trust, Change cognitive demands, Alter decision outcomes, Restrict human agency","Update AI competence, Change AI responses","risk classification, recommendations","domain knowledge, autonomy in flux",NA,Yes,Yes
2-39111,wiley,Effects of explainable artificial intelligence in neurology decision support,"Objective Artificial intelligence (AI)-based decision support systems (DSS) are utilized in medicine but underlying decision-making processes are usually unknown. Explainable AI (xAI) techniques provide insight into DSS, but little is known on how to design xAI for clinicians. Here we investigate the impact of various xAI techniques on a clinician's interaction with an AI-based DSS in decision-making tasks as compared to a general population. Methods We conducted a randomized, blinded study in which members of the Child Neurology Society and American Academy of Neurology were compared to a general population. Participants received recommendations from a DSS via a random assignment of an xAI intervention (decision tree, crowd sourced agreement, case-based reasoning, probability scores, counterfactual reasoning, feature importance, templated language, and no explanations). Primary outcomes included test performance and perceived explainability, trust, and social competence of the DSS. Secondary outcomes included compliance, understandability, and agreement per question. Results We had 81 neurology participants with 284 in the general population. Decision trees were perceived as the more explainable by the medical versus general population (P < 0.01) and as more explainable than probability scores within the medical population (P < 0.001). Increasing neurology experience and perceived explainability degraded performance (P = 0.0214). Performance was not predicted by xAI method but by perceived explainability. Interpretation xAI methods have different impacts on a medical versus general population; thus, xAI is not uniformly beneficial, and there is no one-size-fits-all approach. Further user-centered xAI research targeting clinicians and to develop personalized DSS for clinicians is needed.",10.1002/acn3.52036,https://onlinelibrary.wiley.com/doi/10.1002/acn3.52036,Annals of Clinical and Translational Neurology,Grace Y. Gombolay;Andrew Silva;Mariah Schrum;Nakul Gopalan;Jamika Hallman-Cooper;Monideep Dutt;Matthew Gombolay,2024,1,"@article{2-39111,
  title = {Effects of explainable artificial intelligence in neurology decision support},
  author = {Grace Y. Gombolay and Andrew Silva and Mariah Schrum and Nakul Gopalan and Jamika Hallman-Cooper and Monideep Dutt and Matthew Gombolay},
  year = {2024},
  doi = {10.1002/acn3.52036},
  journal = {Annals of Clinical and Translational Neurology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising","Decision-maker, Decision-subject, Knowledge provider","Change trust, Alter decision outcomes, Change affective-perceptual",no such info,"recommendations, reasoning, counterfactual explanations",domain knowledge,Textual,Yes,Yes
2-39123,wiley,Development and clinical validation of real-time artificial intelligence diagnostic companion for fetal ultrasound examination,"Objective Prenatal diagnosis of a rare disease on ultrasound relies on a physician's ability to remember an intractable amount of knowledge. We developed a real-time decision support system (DSS) that suggests, at each step of the examination, the next phenotypic feature to assess, optimizing the diagnostic pathway to the smallest number of possible diagnoses. The objective of this study was to evaluate the performance of this real-time DSS using clinical data. Methods This validation study was conducted on a database of 549 perinatal phenotypes collected from two referral centers (one in France and one in the UK). Inclusion criteria were: at least one anomaly was visible on fetal ultrasound after 11 weeks' gestation; the anomaly was confirmed postnatally; an associated rare disease was confirmed or ruled out based on postnatal/postmortem investigation, including physical examination, genetic testing and imaging; and, when confirmed, the syndrome was known by the DSS software. The cases were assessed retrospectively by the software, using either the full phenotype as a single input, or a stepwise input of phenotypic features, as prompted by the software, mimicking its use in a real-life clinical setting. Adjudication of discordant cases, in which there was disagreement between the DSS output and the postnatally confirmed (‘ascertained’) diagnosis, was performed by a panel of external experts. The proportion of ascertained diagnoses within the software's top-10 differential diagnoses output was evaluated, as well as the sensitivity and specificity of the software to select correctly as its best guess a syndromic or isolated condition. Results The dataset covered 110/408 (27%) diagnoses within the software's database, yielding a cumulative prevalence of 83%. For syndromic cases, the ascertained diagnosis was within the top-10 list in 93% and 83% of cases using the full-phenotype and stepwise input, respectively, after adjudication. The full-phenotype and stepwise approaches were associated, respectively, with a specificity of 94% and 96% and a sensitivity of 99% and 84%. The stepwise approach required an average of 13 queries to reach the final set of diagnoses. Conclusions The DSS showed high performance when applied to real-world data. This validation study suggests that such software can improve perinatal care, efficiently providing complex and otherwise overlooked knowledge to care-providers involved in ultrasound-based prenatal diagnosis. © 2023 The Authors. Ultrasound in Obstetrics & Gynecology published by John Wiley & Sons Ltd on behalf of International Society of Ultrasound in Obstetrics and Gynecology.",10.1002/uog.26242,https://onlinelibrary.wiley.com/doi/10.1002/uog.26242,Ultrasound in Obstetrics & Gynecology,J. J. Stirnemann;R. Besson;E. Spaggiari;S. Rojo;F. Loge;H. Peyro-Saint-Paul;S. Allassonniere;E. Le Pennec;C. Hutchinson;N. Sebire;Y. Ville,2023,0,"@article{2-39123,
  title={Development and clinical validation of real-time artificial intelligence diagnostic companion for fetal ultrasound examination},
  author={Stirnemann, J. J. and Besson, R. and Spaggiari, E. and Rojo, S. and Loge, F. and Peyro-Saint-Paul, H. and Allassonniere, S. and Le Pennec, E. and Hutchinson, C. and Sebire, N. and Ville, Y.},
  year={2023},
  doi={10.1002/uog.26242},
  journal={Ultrasound in Obstetrics \& Gynecology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-39138,wiley,"Automated, machine learning–based alerts increase epilepsy surgery referrals: a randomized controlled trial","Objective To determine whether automated, electronic alerts increased referrals for epilepsy surgery. Methods We conducted a prospective, randomized controlled trial of a natural language processing–based clinical decision support system embedded in the electronic health record (EHR) at 14 pediatric neurology outpatient clinic sites. Children with epilepsy and at least two prior neurology visits were screened by the system prior to their scheduled visit. Patients classified as a potential surgical candidate were randomized 2:1 for their provider to receive an alert or standard of care (no alert). The primary outcome was referral for a neurosurgical evaluation. The likelihood of referral was estimated using a Cox proportional hazards regression model. Results Between April 2017 and April 2019, at total of 4858 children were screened by the system, and 284 (5.8%) were identified as potential surgical candidates. Two hundred four patients received an alert, and 96 patients received standard care. Median follow-up time was 24 months (range: 12–36 months). Compared to the control group, patients whose provider received an alert were more likely to be referred for a presurgical evaluation (3.1% vs 9.8%; adjusted hazard ratio [HR] = 3.21, 95% confidence interval [CI]: 0.95–10.8; one-sided p = .03). Nine patients (4.4%) in the alert group underwent epilepsy surgery, compared to none (0%) in the control group (one-sided p = .03). Significance Machine learning–based automated alerts may improve the utilization of referrals for epilepsy surgery evaluations.",10.1111/epi.17629,https://onlinelibrary.wiley.com/doi/10.1111/epi.17629,Epilepsia,Benjamin D. Wissel;Hansel M. Greiner;Tracy A. Glauser;Francesco T. Mangano;Katherine D. Holland-Bouley;Nanhua Zhang;Rhonda D. Szczesniak;Daniel Santel;John P. Pestian;Judith W. Dexheimer,2023,17,"@article{2-39138,
  title={Automated, machine learning-based alerts increase epilepsy surgery referrals: a randomized controlled trial},
  author={Wissel, Benjamin D. and Greiner, Hansel M. and Glauser, Tracy A. and Mangano, Francesco T. and Holland-Bouley, Katherine D. and Zhang, Nanhua and Szczesniak, Rhonda D. and Santel, Daniel and Pestian, John P. and Dexheimer, Judith W.},
  year={2023},
  doi={10.1111/epi.17629},
  journal={Epilepsia}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-39145,wiley,Prospective validation study of an epilepsy seizure risk system for outpatient evaluation,"Objective We conducted clinical testing of an automated Bayesian machine learning algorithm (Epilepsy Seizure Assessment Tool [EpiSAT]) for outpatient seizure risk assessment using seizure counting data, and validated performance against specialized epilepsy clinician experts. Methods We conducted a prospective longitudinal study of EpiSAT performance against 24 specialized clinician experts at three tertiary referral epilepsy centers in the United States. Accuracy, interrater reliability, and intra-rater reliability of EpiSAT for correctly identifying changes in seizure risk (improvements, worsening, or no change) were evaluated using 120 seizures from four synthetic seizure diaries (seizure risk known) and 120 seizures from four real seizure diaries (seizure risk unknown). The proportion of observed agreement between EpiSAT and clinicians was evaluated to assess compatibility of EpiSAT with clinical decision patterns by epilepsy experts. Results EpiSAT exhibited substantial observed agreement (75.4%) with clinicians for assessing seizure risk. The mean accuracy of epilepsy providers for correctly assessing seizure risk was 74.7%. EpiSAT accurately identified seizure risk in 87.5% of seizure diary entries, corresponding to a significant improvement of 17.4% (P = .002). Clinicians exhibited low-to-moderate interrater reliability for seizure risk assessment (Krippendorff's α = 0.46) with good intrarater reliability across a 4- to 12-week evaluation period (Scott's π = 0.89). Significance These results validate the ability of EpiSAT to yield objective clinical recommendations on seizure risk which follow decision patterns similar to those from specialized epilepsy providers, but with improved accuracy and reproducibility. This algorithm may serve as a useful clinical decision support system for quantitative analysis of clinical seizure frequency in clinical epilepsy practice.",10.1111/epi.16397,https://onlinelibrary.wiley.com/doi/10.1111/epi.16397,Epilepsia,Sharon Chiang;Daniel M. Goldenholz;Robert Moss;Vikram R. Rao;Zulfi Haneef;William H. Theodore;Jonathan K. Kleen;Jay Gavvala;Marina Vannucci;John M. Stern,2019,28,"@article{2-39145,
        author = {Chiang, Sharon and Goldenholz, Daniel M. and Moss, Robert and Rao, Vikram R. and Haneef, Zulfi and Theodore, William H. and Kleen, Jonathan K. and Gavvala, Jay and Vannucci, Marina and Stern, John M.},
        journal = {Epilepsia},
        number = {1},
        pages = {29-38},
        doi={10.1111/epi.16397},
        title = {Prospective validation study of an epilepsy seizure risk system for outpatient evaluation},
        volume = {61},
        year = {2020}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-39170,wiley,"Utilizing people, analytics, and ai for decision making in the digitalized retail supply chain","Our research reveals the continued and evolving role of the human factor in decision making in digitalized retail supply chains. We compare managerial roles in a pre- and post-COVID era through conducting in-depth interviews of 25 executives spanning the retail supply chain ecosystem. We use grounded theory to develop four main contributions. First, we find that the involvement of managerial judgment is found to be progressively greater moving up the retail supply chain, away from the customer and the demand signal. Second, integration of analytics and judgment is now the primary method of decision making, and we identify elements needed for success. Third, we develop an essential framework for a successful integration process. Fourth, we isolate the necessary components of a successful process for analytics/artificial intelligence (AI) implementation. Our paper offers important insights into how analytics and AI are—and should be used—in judgment and decision making and opportunities for researchers to understand the changing role of the human factor in digitalized retail supply chains.",10.1111/jbl.12355,https://onlinelibrary.wiley.com/doi/10.1111/jbl.12355,Journal of Business Logistics,Rebekah I. Brau;Nada R. Sanders;John Aloysius;Donnie Williams,2023,63,"@article{2-39170,
  title = {Utilizing people, analytics, and AI for decision making in the digitalized retail supply chain},
  author = {Brau, Rebekah I. and Sanders, Nada R. and Aloysius, John and Williams, Donnie},
  year = {2023},
  doi = {10.1111/jbl.12355},
  journal = {Journal of Business Logistics}
}",Empirical contributions,"Manufacturing / Industry / Automation, Finance / Business / Economy",Organizational,"Forecasting, Advising","Decision-maker, Decision-subject, Knowledge provider","Alter decision outcomes, Change cognitive demands, Change trust, Shift responsibility","Update AI competence, Change AI responses",NA,NA,NA,Yes,Yes
2-39205,wiley,Automated artificial intelligence-based system for clinical follow-up of patients with age-related macular degeneration,"Purpose In this study, we investigate the potential of a novel artificial intelligence-based system for autonomous follow-up of patients treated for neovascular age-related macular degeneration (AMD). Methods A temporal deep learning model was trained on a data set of 84 489 optical coherence tomography scans from AMD patients to recognize disease activity, and its performance was compared with a published non-temporal model trained on the same data (Acta Ophthalmol, 2021). An autonomous follow-up system was created by augmenting the AI model with deterministic logic to suggest treatment according to the observe-and-plan regimen. To validate the AI-based system, a data set comprising clinical decisions and imaging data from 200 follow-up consultations was collected prospectively. In each case, both the autonomous AI decision and original clinical decision were compared with an expert panel consensus. Results The temporal AI model proved superior at detecting disease activity compared with the model without temporal input (area under the curve 0.900 (95% CI 0.894–0.906) and 0.857 (95% CI 0.846–0.867) respectively). The AI-based follow-up system could make an autonomous decision in 73% of the cases, 91.8% of which were in agreement with expert consensus. This was on par with the 87.7% agreement rate between decisions made in the clinic and expert consensus (p = 0.33). Conclusions The proposed autonomous follow-up system was shown to be safe and compliant with expert consensus on par with clinical practice. The system could in the future ease the pressure on public ophthalmology services from an increasing number of AMD patients.",10.1111/aos.15133,https://onlinelibrary.wiley.com/doi/10.1111/aos.15133,Acta Ophthalmologica,Ivan Potapenko;Bo Thiesson;Mads Kristensen;Javad Nouri Hajari;Tomas Ilginis;Josefine Fuchs;Steffen Hamann;Morten la Cour,2022,0,"@article{2-39205,
  title={Automated artificial intelligence-based system for clinical follow-up of patients with age-related macular degeneration},
  author={Potapenko, Ivan and Thiesson, Bo and Kristensen, Mads and Nouri Hajari, Javad and Ilginis, Tomas and Fuchs, Josefine and Hamann, Steffen and la Cour, Morten},
  year={2022},
  doi={10.1111/aos.15133},
  journal={Acta Ophthalmologica}
}","Algorithmic contributions, Empirical contributions",Healthcare / Medicine / Surgery,Operational,"Executing, Advising","Knowledge provider, Decision-subject, Decision-maker",Alter decision outcomes,Update AI competence,"textual explanations, recommendations","domain knowledge, evaluation",Autonomous System,Yes,Yes
2-39215,wiley,Attention-based deep learning system for automated diagnoses of age-related macular degeneration in optical coherence tomography images,"Purpose The progression of age-related macular degeneration (AMD) is critical to treatment decisions in clinical practice. The disease can be classified into four categories namely, drusen, inactive choroidal neovascularization (CNV), active CNV, and normal, according to severity based on optical coherence tomography (OCT) images. Interpreting numerous OCT images is still time-consuming and labor-intensive, especially for the detection of the CNV activity. To address this problem, we developed a deep learning (DL) system based on OCT images, with the assistance of an attention mechanism, to automatically diagnose AMD. Methods A public dataset (total 51,140 OCT images) and a private dataset (total 4951 OCT images) were utilized as a training dataset and a clinical validation dataset, respectively, to develop the DL model. A ResNet-34 DL model, with convolutional block attention module (CBAM) block integrated into each unit, was pretrained on the public dataset first and then finetuned on our private dataset to automatically diagnose AMD and assist clinical decision-making. GradCam, a visualization technique, was used to improve the interpretability of our model. Results The precision and recall of our model were, respectively, 84.3% and 87.3% for drusen, 81.2% and 80.0% for inactive CNV, 97.7% and 90.2% for active CNV, and 93.7% and 96.5% for normal. The area under the curve (AUC) corresponding to drusen, inactive CNV, active CNV, and normal for our model reached 0.9395, 0.9476, 0.9880, and 0.9925, respectively. The heatmaps indicated a high level of correspondence in the region of interest between our model and ophthalmologists on the diagnosis. Conclusions The implementation of finetuning and attention mechanism improve the performance of our model in a distinct dataset. Our model successfully assisted in the diagnosis of AMD and achieved a detection precision and recall equal to those of ophthalmologists. The results of our study could contribute to the precise diagnosis of and decision-making regarding AMD.",10.1002/mp.15002,https://onlinelibrary.wiley.com/doi/10.1002/mp.15002,Medical Physics,Yan Yan;Kai Jin;Zhiyuan Gao;Xiaoling Huang;Fanyi Wang;Yao Wang;Juan Ye,2021,22,"@article{2-39215,
  title={Attention-based deep learning system for automated diagnoses of age-related macular degeneration in optical coherence tomography images},
  author={Yan, Yan and Jin, Kai and Gao, Zhiyuan and Huang, Xiaoling and Wang, Fanyi and Wang, Yao and Ye, Juan},
  year={2021},
  journal={Medical Physics},
  doi={10.1002/mp.15002},
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Explaining, Forecasting","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-39241,wiley,Aapm task group report 273: recommendations on best practices for ai and machine learning for computer-aided diagnosis in medical imaging,"Rapid advances in artificial intelligence (AI) and machine learning, and specifically in deep learning (DL) techniques, have enabled broad application of these methods in health care. The promise of the DL approach has spurred further interest in computer-aided diagnosis (CAD) development and applications using both “traditional” machine learning methods and newer DL-based methods. We use the term CAD-AI to refer to this expanded clinical decision support environment that uses traditional and DL-based AI methods. Numerous studies have been published to date on the development of machine learning tools for computer-aided, or AI-assisted, clinical tasks. However, most of these machine learning models are not ready for clinical deployment. It is of paramount importance to ensure that a clinical decision support tool undergoes proper training and rigorous validation of its generalizability and robustness before adoption for patient care in the clinic. To address these important issues, the American Association of Physicists in Medicine (AAPM) Computer-Aided Image Analysis Subcommittee (CADSC) is charged, in part, to develop recommendations on practices and standards for the development and performance assessment of computer-aided decision support systems. The committee has previously published two opinion papers on the evaluation of CAD systems and issues associated with user training and quality assurance of these systems in the clinic. With machine learning techniques continuing to evolve and CAD applications expanding to new stages of the patient care process, the current task group report considers the broader issues common to the development of most, if not all, CAD-AI applications and their translation from the bench to the clinic. The goal is to bring attention to the proper training and validation of machine learning algorithms that may improve their generalizability and reliability and accelerate the adoption of CAD-AI systems for clinical decision support.",10.1002/mp.16188,https://onlinelibrary.wiley.com/doi/10.1002/mp.16188,Medical Physics,Lubomir Hadjiiski;Kenny Cha;Heang-Ping Chan;Karen Drukker;Lia Morra;Janne J. Näppi;Berkman Sahiner;Hiroyuki Yoshida;Quan Chen;Thomas M. Deserno;Hayit Greenspan;Henkjan Huisman;Zhimin Huo;Richard Mazurchuk;Nicholas Petrick;Daniele Regge;Ravi Samala;Ronald M. Summers;Kenji Suzuki;Georgia Tourassi;Daniel Vergara;Samuel G. Armato,2022,0,"@article{2-39241,
  title     = {AAPM Task Group Report 273: Recommendations on Best Practices for AI and Machine Learning for Computer-Aided Diagnosis in Medical Imaging},
  author    = {Lubomir Hadjiiski and Kenny Cha and Heang-Ping Chan and Karen Drukker and Lia Morra and Janne J. N{\""a}ppi and Berkman Sahiner and Hiroyuki Yoshida and Quan Chen and Thomas M. Deserno and Hayit Greenspan and Henkjan Huisman and Zhimin Huo and Richard Mazurchuk and Nicholas Petrick and Daniele Regge and Ravi Samala and Ronald M. Summers and Kenji Suzuki and Georgia Tourassi and Daniel Vergara and Samuel G. Armato},
  year      = {2022},
  doi       = {10.1002/mp.16188},
  journal   = {Medical Physics}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Executing","Decision-maker, Developer, Guardian",NA,NA,NA,NA,NA,Yes,No
2-39286,wiley,Vulnerable student digital well-being in ai-powered educational decision support systems (ai-edss) in higher education,"Students' physical and digital lives are increasingly entangled. It is difficult to separate students' digital well-being from their offline well-being given that artificial intelligence increasingly shapes both. Within the context of education's fiduciary and moral duty to ensure safe, appropriate and effective digital learning spaces for students, the continuing merger between artificial intelligence and learning analytics not only opens up many opportunities for more responsive teaching and learning but also raises concerns, specifically for previously disadvantaged and vulnerable students. While digital well-being is a well-established research focus, it is not clear how AI-Powered Educational Decision Support Systems (AI-EDSS) might impact on the inherent, situational and pathogenic vulnerability of students. In this conceptual paper, we map the digital well-being of previously disadvantaged and vulnerable students in four overlapping fields, namely (1) digital well-being research; (2) digital well-being research in education; (3) digital well-being research in learning analytics; and (4) digital well-being in AI-informed educational contexts. With this as the basis, we engage with six domains from the IEEE standard 7010–2020—IEEE Recommended Practice for Assessing the Impact of Autonomous and Intelligent Systems on Human Well-Being and provide pointers for safeguarding and enhancing disadvantaged and vulnerable student digital well-being in AI-EDSS. Practitioner notes What is already known about this topic Digital well-being research is a well-established focus referring to the impact of digital engagement on human well-being. Digital well-being is effectively inseparable from general well-being as it is increasingly difficult to disentangle our online and offline lives and, as such, inherently intersectional. Artificial Intelligence shows promise for enhancing human digital well-being, but there are concerns about issues such as privacy, bias, transparency, fairness and accountability. The notion of ‘vulnerable individuals’ includes individuals who were previously disadvantaged, and those with inherent, situational and/or pathogenic vulnerabilities. While current advances in AI-EDSS may support identification of digital wellness, proxies for digital wellness should be used with care. What this study contributes An overview of digital well-being research with specific reference how it may impact on vulnerable students. Illustrates specific vulnerabilities in five domains from the IEEE standard 7010–2020—IEEE Recommended Practice for Assessing the Impact of Autonomous and Intelligent Systems on Human Well-Being selected for their significance in online learning environments. Pointers for the design and implementation of fair, ethical, accountable, and transparent AI-EDSS with specific reference to vulnerable students. Implications for practice and/or policy Fairness, equity, transparency and accountability in AI-EDSS affect all students but may have a greater (positive or negative) impact on vulnerable students. A critically informed understanding of the nature of students' vulnerability—whether as inherent, situational and/or pathogenic, as well as temporal/permanent aspects—is crucial. Since AI-EDSS can exacerbate existing vulnerabilities resulting in pathogenic vulnerability, care is needed when designing AI-EDSS.",10.1111/bjet.13508,https://onlinelibrary.wiley.com/doi/10.1111/bjet.13508,British Journal of Educational Technology,Paul Prinsloo;Mohammad Khalil;Sharon Slade,2024,16,"@article{2-39286,
  title={Vulnerable student digital well-being in AI-powered educational decision support systems (AI-EDSS) in higher education},
  author={Prinsloo, Paul and Khalil, Mohammad and Slade, Sharon},
  year={2024},
  journal={British Journal of Educational Technology},
  doi={10.1111/bjet.13508}
}",Theoretical contributions,Education / Teaching / Research,Institutional,"Advising, Forecasting, Analyzing","Decision-maker, Decision-subject, Guardian, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-39316,wiley,Navigating ai convergence in human–artificial intelligence teams: a signaling theory approach,"Teams that combine human intelligence with artificial intelligence (AI) have become indispensable for solving complex tasks in various decision-making contexts in modern organizations. However, the factors that contribute to AI convergence, where human team members align their decisions with those of their AI counterparts, still remain unclear. This study integrates signaling theory with self-determination theory to investigate how specific signals—such as signal fit, optional AI advice, and signal set congruence—affect employees' AI convergence in human–AI teams. Based on four experimental studies conducted in facial recognition and hiring contexts with approximately 1100 participants, the findings highlight the significant positive impact of congruent signals from both human and AI team members on AI convergence. Moreover, providing an option for employees to solicit AI advice also enhances AI convergence; when AI signals are chosen by employees rather than forced upon them, participants are more likely to accept AI advice. This research advances knowledge on human–AI teaming by (1) expanding signaling theory into the human–AI team context; (2) developing a deeper understanding of AI convergence and its drivers in human–AI teams; (3) providing actionable insights for designing teams and tasks to optimize decision-making in high-stakes, uncertain environments; and (4) introducing facial recognition as an innovative context for human–AI teaming.",10.1002/job.2856,https://onlinelibrary.wiley.com/doi/10.1002/job.2856,Journal of Organizational Behavior,Andria Smith;Hunter Phoenix van Wagoner;Ksenia Keplinger;Can Celebi,2025,164,"@article{2-39316,
  title={Navigating AI convergence in human--artificial intelligence teams: A signaling theory approach},
  author={Smith, Andria and van Wagoner, Hunter Phoenix and Keplinger, Ksenia and Celebi, Can},
  year={2025},
  journal={Journal of Organizational Behavior},
  volume={},
  number={},
  pages={},
  doi={10.1002/job.2856}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service",Operational,"Advising, Collaborating","Decision-maker, Decision-subject","Change trust, Alter decision outcomes, Shape ethical norms, Restrict human agency",no such info,"prediction of alternative, recommendations",NA,"Textual, Conversational/Natural Language",Yes,Yes
2-39322,wiley,How artificial intelligence is reshaping the autonomy and boundary work of radiologists. A qualitative study,"The application of artificial intelligence (AI) in medical practice is spreading, especially in technologically dense fields such as radiology, which could consequently undergo profound transformations in the near future. This article aims to qualitatively explore the potential influence of AI technologies on the professional identity of radiologists. Drawing on 12 in-depth interviews with a subgroup of radiologists who participated in a larger study, this article investigated (1) whether radiologists perceived AI as a threat to their decision-making autonomy; and (2) how radiologists perceived the future of their profession compared to other health-care professions. The findings revealed that while AI did not generally affect radiologists’ decision-making autonomy, it threatened their professional and epistemic authority. Two discursive strategies were identified to explain these findings. The first strategy emphasised radiologists’ specific expertise and knowledge that extends beyond interpreting images, a task performed with high accuracy by AI machines. The second strategy underscored the fostering of radiologists’ professional prestige through developing expertise in using AI technologies, a skill that would distinguish them from other clinicians who did not pose this knowledge. This study identifies AI machines as status objects and useful tools in performing boundary work in and around the radiological profession.",10.1111/1467-9566.13702,https://onlinelibrary.wiley.com/doi/10.1111/1467-9566.13702,Sociology of Health & Illness,Linda Lombi;Eleonora Rossero,2023,30,"@article{2-39322,
  title={How artificial intelligence is reshaping the autonomy and boundary work of radiologists. A qualitative study},
  author={Lombi, Linda and Rossero, Eleonora},
  year={2023},
  doi={10.1111/1467-9566.13702},
  journal={Sociology of Health \& Illness}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Guardian, Decision-maker, Knowledge provider",NA,NA,NA,potential vulnerability,NA,Yes,No
2-39334,wiley,Choosing human over ai doctors? How comparative trust associations and knowledge relate to risk and benefit perceptions of ai in healthcare,"The development of artificial intelligence (AI) in healthcare is accelerating rapidly. Beyond the urge for technological optimization, public perceptions and preferences regarding the application of such technologies remain poorly understood. Risk and benefit perceptions of novel technologies are key drivers for successful implementation. Therefore, it is crucial to understand the factors that condition these perceptions. In this study, we draw on the risk perception and human-AI interaction literature to examine how explicit (i.e., deliberate) and implicit (i.e., automatic) comparative trust associations with AI versus physicians, and knowledge about AI, relate to likelihood perceptions of risks and benefits of AI in healthcare and preferences for the integration of AI in healthcare. We use survey data (N = 378) to specify a path model. Results reveal that the path for implicit comparative trust associations on relative preferences for AI over physicians is only significant through risk, but not through benefit perceptions. This finding is reversed for AI knowledge. Explicit comparative trust associations relate to AI preference through risk and benefit perceptions. These findings indicate that risk perceptions of AI in healthcare might be driven more strongly by affect-laden factors than benefit perceptions, which in turn might depend more on reflective cognition. Implications of our findings and directions for future research are discussed considering the conceptualization of trust as heuristic and dual-process theories of judgment and decision-making. Regarding the design and implementation of AI-based healthcare technologies, our findings suggest that a holistic integration of public viewpoints is warranted.",10.1111/risa.14216,https://onlinelibrary.wiley.com/doi/10.1111/risa.14216,Risk Analysis: An International Journal,Sophie Kerstan;Nadine Bienefeld;Gudela Grote,2023,44,"@article{2-39334,
  title = {Choosing human over AI doctors? How comparative trust associations and knowledge relate to risk and benefit perceptions of AI in healthcare},
  author = {Kerstan, Sophie and Bienefeld, Nadine and Grote, Gudela},
  year = {2023},
  doi = {10.1111/risa.14216},
  journal = {Risk Analysis: An International Journal}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-subject, Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-39353,wiley,Navigating the decision-making landscape of ai in risk finance: techno-accountability unveiled,"The integration of artificial intelligence (AI) systems has ushered in a profound transformation. This conversion is marked by revolutionary extrapolative capabilities, a shift toward data-centric decision-making processes, and the enhancement of tools for managing risks. However, the adoption of these AI innovations has sparked controversy due to their unpredictable and opaque disposition. This study employs the transactional stress model to empirically investigate how six technological stressors (techno-stressors) impact both techno-eustress (positive stress) and techno-distress (negative stress) experienced by finance professionals and experts. To collect data for this research, an e-survey was distributed to a diverse group of 251 participants from various sources. The findings, particularly the identification and development of techno-accountability as a significant factor, contribute to the risk analysis domain by improving the failure mode and effect analysis framework to better fit the rapidly evolving landscape of AI-driven innovations.",10.1111/risa.14336,https://onlinelibrary.wiley.com/doi/10.1111/risa.14336,Risk Analysis: An International Journal,Helmi Issa;Roy Dakroub;Hussein Lakkis;Jad Jaber,2024,0,"@article{2-39353,
  title = {Navigating the Decision-Making Landscape of AI in Risk Finance: Techno-Accountability Unveiled},
  author = {Helmi Issa and Roy Dakroub and Hussein Lakkis and Jad Jaber},
  year = {2024},
  doi = {10.1111/risa.14336},
  journal = {Risk Analysis: An International Journal}
}",Empirical contributions,Finance / Business / Economy,Operational,"Advising, Forecasting, Analyzing","Decision-maker, Knowledge provider","Alter decision outcomes, Change cognitive demands",Shape AI for accountability,"recommendations, risk analysis","domain knowledge, guideline setting",Textual,Yes,No
2-39372,wiley,Artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring,"The question: “What is an appropriate role for AI?” is the subject of much discussion and interest. Arguments about whether AI should be a human replacing technology or a human assisting technology frequently take centre stage. Education is no exception when it comes to questions about the role that AI should play, and as with many other professional areas, the exact role of AI in education is not easy to predict. Here, we argue that one potential role for AI in education is to provide opportunities for human intelligence augmentation, with AI supporting us in decision-making processes, rather than replacing us through automation. To provide empirical evidence to support our argument, we present a case study in the context of debate tutoring, in which we use prediction and classification models to increase the transparency of the intuitive decision-making processes of expert tutors for advanced reflections and feedback. Furthermore, we compare the accuracy of unimodal and multimodal classification models of expert human tutors' decisions about the social and emotional aspects of tutoring while evaluating trainees. Our results show that multimodal data leads to more accurate classification models in the context we studied.",10.1111/bjet.12829,https://onlinelibrary.wiley.com/doi/10.1111/bjet.12829,British Journal of Educational Technology,Mutlu Cukurova;Carmel Kent;Rosemary Luckin,2019,0,"@article{2-39372,
  title = {Artificial intelligence and multimodal data in the service of human decision-making: a case study in debate tutoring},
  author = {Mutlu Cukurova and Carmel Kent and Rosemary Luckin},
  year = {2019},
  journal = {British Journal of Educational Technology},
  doi = {10.1111/bjet.12829}
}",Empirical contributions,Education / Teaching / Research,Operational,"Advising, Analyzing","Decision-maker, Knowledge provider",Change cognitive demands,no such info,"causal explanations, multimodal signaling (gesture)",domain knowledge,"Textual, Auditory",Yes,Yes
2-39375,wiley,The emergence of explainability of intelligent systems: delivering explainable and personalized recommendations for energy efficiency,"The recent advances in artificial intelligence namely in machine learning and deep learning, have boosted the performance of intelligent systems in several ways. This gave rise to human expectations, but also created the need for a deeper understanding of how intelligent systems think and decide. The concept of explainability appeared, in the extent of explaining the internal system mechanics in human terms. Recommendation systems are intelligent systems that support human decision making, and as such, they have to be explainable to increase user trust and improve the acceptance of recommendations. In this study, we focus on a context-aware recommendation system for energy efficiency and develop a mechanism for explainable and persuasive recommendations, which are personalized to user preferences and habits. The persuasive facts either emphasize on the economical saving prospects (Econ) or on a positive ecological impact (Eco) and explanations provide the reason for recommending an energy saving action. Based on a study conducted using a Telegram bot, different scenarios have been validated with actual data and human feedback. Current results show a total increase of 19% on the recommendation acceptance ratio when both economical and ecological persuasive facts are employed. This revolutionary approach on recommendation systems, demonstrates how intelligent recommendations can effectively encourage energy saving behavior.",10.1002/int.22314,https://onlinelibrary.wiley.com/doi/10.1002/int.22314,International Journal of Intelligent Systems,Christos Sardianos;Iraklis Varlamis;Christos Chronis;George Dimitrakopoulos;Abdullah Alsalemi;Yassine Himeur;Faycal Bensaali;Abbes Amira,2020,1,"@article{2-39375,
  title = {The emergence of explainability of intelligent systems: delivering explainable and personalized recommendations for energy efficiency},
  author = {Christos Sardianos and Iraklis Varlamis and Christos Chronis and George Dimitrakopoulos and Abdullah Alsalemi and Yassine Himeur and Faycal Bensaali and Abbes Amira},
  year = {2020},
  doi = {10.1002/int.22314},
  journal = {International Journal of Intelligent Systems}
}",System/Artifact contributions,Environment / Resources / Energy,Individual,"Explaining, Advising",Decision-maker,Alter decision outcomes,Change AI responses,"recommendations, textual explanations, reasoning",personalized settings,"Textual, Conversational/Natural Language, Autonomous System",Yes,Yes
2-39379,wiley,Seismic risk prioritization of masonry building stocks using machine learning,"The seismic risk mitigation plans are vital since vulnerable structures are prone to partial or total collapse under the effect of future major earthquake events. Therefore, vulnerable structures in large building stocks should be determined using robust and accurate methods to prevent loss of lives and property. In the current state-of-the-art, the risk states (i.e., whether risky or not) of structures completely depend on the experience of the reconnaissance team of engineers, which could not result in standardized decisions. In this study, machine learning has been integrated into the decision-making algorithm to classify more precise and reliable seismic risk states of masonry buildings, categorizing them into up to four risk categories. For this purpose, a large database, including 12 features and detailed seismic risk analysis results of 4356 masonry buildings, is formed. Firstly, the input variables are preprocessed using feature engineering methods. Then, several machine learning algorithms are utilized to produce a network to estimate the risk state of masonry buildings in association with the risk states obtained from the detailed analysis results. As a result of the analysis of these algorithms, the correct prediction percentages for the testing database of the proposed method for two, three, and four risk states classification are predicted as approximately 87.5%, 86.6%, and 79.0%, respectively. This new approach makes it possible to produce risk color maps of large building stocks and reduce the number of buildings that require immediate action.",10.1002/eqe.4227,https://onlinelibrary.wiley.com/doi/10.1002/eqe.4227,Earthquake Engineering & Structural Dynamics,Onur Coskun;Rafet Aktepe;Alper Aldemir;Ali Erhan Yilmaz;Murat Durmaz;Burcu Guldur Erkal;Engin Tunali,2024,5,"@article{2-39379,
        doi={10.1002/eqe.4227},
        author = {Coskun, Onur and Aktepe, Rafet and Aldemir, Alper and Yilmaz, Ali Erhan and Durmaz, Murat and Erkal, Burcu Guldur and Tunali, Engin},
        journal = {Earthquake Engineering \& Structural Dynamics},
        number = {14},
        pages = {4432-4450},
        title = {Seismic risk prioritization of masonry building stocks using machine learning},
        volume = {53},
        year = {2024}
}","Algorithmic contributions, Methodological contributions","Manufacturing / Industry / Automation, Environment / Resources / Energy",Organizational,"Forecasting, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-39389,wiley,Explanation seeking and anomalous recommendation adherence in human-to-human versus human-to-artificial intelligence interactions,"The use of artificial intelligence (AI) in operational decision-making is growing, but individuals can display algorithm aversion, preventing adherence to AI system recommendations—even when the system outperforms human decision-makers. Understanding why such algorithm aversion occurs and how to reduce it is important to ensure AI is fully leveraged. While the ability to seek an explanation from an AI may be a promising approach to mitigate this aversion, there is conflicting evidence on their benefits. Based on several behavioral theories, including Bayesian choice, loss aversion, and sunk cost avoidance, we hypothesize that if a recommendation is perceived as an anomalous loss, it will decrease recommendation adherence; however, the effect will be mediated by explanations and differ depending on whether the advisor providing the recommendation and explanation is a human or an AI. We conducted a survey-based lab experiment set in the online rental market space and found that presenting a recommendation as a loss anomaly significantly reduces adherence compared to presenting it as a gain, however, this negative effect can be dampened if the advisor is an AI. We find explanation-seeking has a limited impact on adherence, even after considering the influence of the advisor; we discuss the managerial and theoretical implications of these findings.",10.1111/deci.12658,https://onlinelibrary.wiley.com/doi/10.1111/deci.12658,Decision Sciences Journal,Tracy Jenkin;Stephanie Kelley;Anton Ovchinnikov;Cecilia Ying,2024,2,"@article{2-39389,
  title = {Explanation seeking and anomalous recommendation adherence in human-to-human versus human-to-artificial intelligence interactions},
  author = {Tracy Jenkin and Stephanie Kelley and Anton Ovchinnikov and Cecilia Ying},
  year = {2024},
  journal = {Decision Sciences Journal},
  doi = {10.1111/deci.12658}
}",Empirical contributions,"Everyday / Employment / Public Service, Generic / Abstract / Domain-agnostic",Individual,Advising,Decision-maker,"Alter decision outcomes, Change trust",no such info,"recommendations, textual explanations, visual analysis, identity cues",NA,"Textual, Visual, Conversational/Natural Language",Yes,Yes
2-39398,wiley,Integrating decision modeling and machine learning to inform treatment stratification,"There is increasing interest in moving away from “one size fits all (OSFA)” approaches toward stratifying treatment decisions. Understanding how expected effectiveness and cost-effectiveness varies with patient covariates is a key aspect of stratified decision making. Recently proposed machine learning (ML) methods can learn heterogeneity in outcomes without pre-specifying subgroups or functional forms, enabling the construction of decision rules (‘policies’) that map individual covariates into a treatment decision. However, these methods do not yet integrate ML estimates into a decision modeling framework in order to reflect long-term policy-relevant outcomes and synthesize information from multiple sources. In this paper, we propose a method to integrate ML and decision modeling, when individual patient data is available to estimate treatment-specific survival time. We also propose a novel implementation of policy tree algorithms to define subgroups using decision model output. We demonstrate these methods using the SPRINT (Systolic Blood Pressure Intervention Trial), comparing outcomes for “standard” and “intensive” blood pressure targets. We find that including ML into a decision model can impact the estimate of incremental net health benefit (INHB) for OSFA policies. We also find evidence that stratifying treatment using subgroups defined by a tree-based algorithm can increase the estimates of the INHB.",10.1002/hec.4834,https://onlinelibrary.wiley.com/doi/10.1002/hec.4834,Health Economics,David Glynn;John Giardina;Julia Hatamyar;Ankur Pandya;Marta Soares;Noemi Kreif,2024,4,"@article{2-39398,
  title={Integrating decision modeling and machine learning to inform treatment stratification},
  author={Glynn, David and Giardina, John and Hatamyar, Julia and Pandya, Ankur and Soares, Marta and Kreif, Noemi},
  year={2024},
  journal={Health Economics},
  doi={10.1002/hec.4834}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Executing","Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-39399,wiley,Artificial intelligence for supply chain management: disruptive innovation or innovative disruption?,"This article examines the theoretical and practical implications of artificial intelligence (AI) integration in supply chain management (SCM). AI has developed dramatically in recent years, embodied by the newest generation of large language models (LLMs) that exhibit human-like capabilities in various domains. However, SCM as a discipline seems unprepared for this potential revolution, as existing perspectives do not capture the potential for disruption offered by AI tools. Moreover, AI integration in SCM is not only a technical but also a social process, influenced by human sensemaking and interpretation of AI systems. This article offers a novel theoretical lens called the AI Integration (AII) framework, which considers two key dimensions: the level of AI integration across the supply chain and the role of AI in decision-making. It also incorporates human meaning-making as an overlaying factor that shapes AI integration and disruption dynamics. The article demonstrates that different ways of integrating AI will lead to different kinds of disruptions, both in theory and in practice. It also discusses the implications of AI integration for SCM theorizing and practice, highlighting the need for cross-disciplinary collaboration and sociotechnical perspectives.",10.1111/jscm.12304,https://onlinelibrary.wiley.com/doi/10.1111/jscm.12304,Journal of Supply Chain Management,Christian Hendriksen,2023,193,"@article{2-39399,
  title = {Artificial Intelligence for Supply Chain Management: Disruptive Innovation or Innovative Disruption?},
  author = {Christian Hendriksen},
  year = {2023},
  journal = {Journal of Supply Chain Management},
  doi = {10.1111/jscm.12304},
}",Theoretical contributions,Manufacturing / Industry / Automation,Organizational,"Advising, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-39407,wiley,The role of adaptation in collective human–ai teaming,"This paper explores a framework for defining artificial intelligence (AI) that adapts to individuals within a group, and discusses the technical challenges for collaborative AI systems that must work with different human partners. Collaborative AI is not one-size-fits-all, and thus AI systems must tune their output based on each human partner's needs and abilities. For example, when communicating with a partner, an AI should consider how prepared their partner is to receive and correctly interpret the information they are receiving. Forgoing such individual considerations may adversely impact the partner's mental state and proficiency. On the other hand, successfully adapting to each person's (or team member's) behavior and abilities can yield performance benefits for the human–AI team. Under this framework, an AI teammate adapts to human partners by first learning components of the human's decision-making process and then updating its own behaviors to positively influence the ongoing collaboration. This paper explains the role of this AI adaptation formalism in dyadic human–AI interactions and examines its application through a case study in a simulated navigation domain.",10.1111/tops.12633,https://onlinelibrary.wiley.com/doi/10.1111/tops.12633,Topics in Cognitive Science,Michelle Zhao;Reid Simmons;Henny Admoni,2022,0,"@article{2-39407,
  title = {The role of adaptation in collective human–AI teaming},
  author = {Zhao, Michelle and Simmons, Reid and Admoni, Henny},
  year = {2022},
  doi = {10.1111/tops.12633},
  journal = {Topics in Cognitive Science}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,no such info,"Collaborating, Analyzing",Decision-maker,"Change trust, Change cognitive demands, Change affective-perceptual",Change AI responses,"adaptive instruction, static instruction",domain knowledge,Autonomous System,Yes,Yes
2-39421,wiley,From prediction to decision: optimizing long-term care placements among older delayed discharge patients,"This study examines long-term care (LTC) discharge planning among older delayed discharge patients. While awaiting placements in alternate care such as LTC, these patients occupy hospital beds despite not requiring an intensive level of care. This study proposes a novel discharge decision model based on the Markov decision process (MDP) framework, which incorporates predictions regarding the patients' health trajectory and the associated hospital costs. Our machine learning (ML)-based predictive analytics allow for considering heterogeneous health transitions, hence personalized decision making, leading to valuable information for reducing hospital costs. We also develop data-driven cost functions using patient characteristics to estimate the person-level costs associated with the decisions in the optimization model, that is, whether or not to discharge a patient to LTC. The data analyses and cost estimations are based on large historical data collected over 13 years in Ontario, Canada. To solve the resulting high-dimensional MDP models, we develop an index policy, where each patient's index value is calculated using their health complexity (comorbidity), sex, age, and acute length of stay in the hospital. Using extensive numerical experiments, we illustrate the superior performance of the proposed index policy against some benchmarking policies and demonstrate the significance of predictive information in optimizing discharge decisions. Our results also indicate that the value of predictive information increases with LTC bed availability and decreases with hospital capacity. We also demonstrate that with the anticipated exacerbating mismatch between supply and demand, targeted prediction-driven discharge policies, such as the proposed index policy, become even more critical.",10.1111/poms.13910,https://onlinelibrary.wiley.com/doi/10.1111/poms.13910,Production and Operations Management Society (POMS) Journal,Ya-Tang Chuang;Manaf Zargoush;Somayeh Ghazalbash;Saied Samiedaluie;Kerry Kuluski;Sara Guilcher,2022,12,"@article{2-39421,
  title = {From prediction to decision: optimizing long-term care placements among older delayed discharge patients},
  author = {Ya-Tang Chuang and Manaf Zargoush and Somayeh Ghazalbash and Saied Samiedaluie and Kerry Kuluski and Sara Guilcher},
  year = {2022},
  doi = {10.1111/poms.13910},
  journal = {Production and Operations Management Society (POMS) Journal}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising, Analyzing","Decision-maker, Decision-subject, Guardian, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-39422,wiley,Dynamics of reliance on algorithmic advice,"This study examines the dynamics of human reliance on algorithmic advice in a situation with strategic interaction. Participants played the strategic game of Rock–Paper–Scissors (RPS) under various conditions, receiving algorithmic decision support while facing human or algorithmic opponents. Results indicate that participants often underutilize algorithmic recommendations, particularly after early errors, but increasingly rely on the algorithm following successful early predictions. This behavior demonstrates a sensitivity to decision outcomes, with asymmetry: rejecting advice consistently reinforces rejecting advice again while accepting advice leads to varied reactions based on outcomes. We also investigate how personal characteristics, such as algorithm familiarity and domain experience, influence reliance on algorithmic advice. Both factors positively correlate with increased reliance, and algorithm familiarity significantly moderates the relationship between outcome feedback and reliance. Facing an algorithmic opponent increases advice rejection frequencies, and the determinants of trust and interaction dynamics differ from those with human opponents. Our findings enhance the understanding of algorithm aversion and reliance on AI, suggesting that increasing familiarity with algorithms can improve their integration into decision-making processes.",10.1002/bdm.2414,https://onlinelibrary.wiley.com/doi/10.1002/bdm.2414,Journal of Behavioral Decision Making,Andrej Gill;Robert M. Gillenkirch;Julia Ortner;Louis Velthuis,2024,187,"@article{2-39422,
  title={Dynamics of reliance on algorithmic advice},
  author={Gill, Andrej and Gillenkirch, Robert M. and Ortner, Julia and Velthuis, Louis},
  year={2024},
  journal={Journal of Behavioral Decision Making},
  volume={},
  number={},
  pages={},
  doi={10.1002/bdm.2414}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Media / Communication / Entertainment",Individual,Advising,Decision-maker,"Change trust, Alter decision outcomes",no such info,recommendations,NA,"Visual, Interactive interface",Yes,Yes
2-39452,wiley,Advanced decision-making using patient-reported outcome measures in total joint replacement,"Up to one-third of total joint replacement (TJR) procedures may be performed inappropriately in a subset of patients who remain dissatisfied with their outcomes, stressing the importance of shared decision-making. Patient-reported outcome measures capture physical, emotional, and social aspects of health and wellbeing from the patient's perspective. Powerful computer systems capable of performing highly sophisticated analysis using different types of data, including patient-derived data, such as patient-reported outcomes, may eliminate guess work, generating impactful metrics to better inform the decision-making process. We have created a shared decision-making tool which generates personalized predictions of risks and benefits from TJR based on patient-reported outcomes as well as clinical and demographic data. We present the protocol for a randomized controlled trial designed to assess the impact of this tool on decision quality, level of shared decision-making, and patient and process outcomes. We also discuss current concepts in this field and highlight opportunities leveraging patient-reported data and artificial intelligence for decision support across the care continuum.",10.1002/jor.24614,https://onlinelibrary.wiley.com/doi/10.1002/jor.24614,Journal of Orthopaedic Research,Prakash Jayakumar MD;PhD;Kevin J. Bozic MD;MBA,2020,58,"@article{2-39452,
  title = {Advanced decision-making using patient-reported outcome measures in total joint replacement},
  author = {Prakash Jayakumar and Kevin J. Bozic},
  year = {2020},
  journal = {Journal of Orthopaedic Research},
  doi = {10.1002/jor.24614}
}","System/Artifact contributions, Empirical contributions",Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-39456,wiley,A graph convolution network-deep reinforcement learning model for resilient water distribution network repair decisions,"Water distribution networks (WDNs) are critical infrastructure for communities. The dramatic expansion of the WDNs associated with urbanization makes them more vulnerable to high-consequence hazards such as earthquakes, which requires strategies to ensure their resilience. The resilience of a WDN is related to its ability to recover its service after disastrous events. Sound decisions on the repair sequence play a crucial role to ensure a resilient WDN recovery. This paper introduces the development of a graph convolutional neural network-integrated deep reinforcement learning (GCN-DRL) model to support optimal repair decisions to improve WDN resilience after earthquakes. A WDN resilience evaluation framework is first developed, which integrates the dynamic evolution of WDN performance indicators during the post-earthquake recovery process. The WDN performance indicator considers the relative importance of the service nodes and the extent of post-earthquake water needs that are satisfied. In this GCN-DRL model framework, the GCN encodes the information of the WDN. The topology and performance of service nodes (i.e., the degree of water that needs satisfaction) are inputs to the GCN; the outputs of GCN are the reward values (Q-values) corresponding to each repair action, which are fed into the DRL process to select the optimal repair sequence from a large action space to achieve highest system resilience. The GCN-DRL model is demonstrated on a testbed WDN subjected to three earthquake damage scenarios. The performance of the repair decisions by the GCN-DRL model is compared with those by four conventional decision methods. The results show that the recovery sequence by the GCN-DRL model achieved the highest system resilience index values and the fastest recovery of system performance. Besides, by using transfer learning based on a pre-trained model, the GCN-DRL model achieved high computational efficiency in determining the optimal repair sequences under new damage scenarios. This novel GCN-DRL model features robustness and universality to support optimal repair decisions to ensure resilient WDN recovery from earthquake damages.",10.1111/mice.12813,https://onlinelibrary.wiley.com/doi/10.1111/mice.12813,Computer-Aided Civil and Infrastructure Engineering,Xudong Fan;Xijin Zhang;Xiong (Bill) Yu,2022,1,"@article{2-39456,
  title={A graph convolution network-deep reinforcement learning model for resilient water distribution network repair decisions},
  author={Fan, Xudong and Zhang, Xijin and Yu, Xiong (Bill)},
  year={2022},
  journal={Computer-Aided Civil and Infrastructure Engineering},
  doi={10.1111/mice.12813}
}",Algorithmic contributions,Environment / Resources / Energy,Operational,"Executing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-39465,wiley,Strategic decision making in the digital age: expert sentiment and corporate capital allocation,"We develop a theoretical perspective on how the increasing data availability in the digital age affects firms’ strategic decisions. Specifically, we explore whether, and under which conditions, external experts’ collective assessment of product-technology domains impacts firms’ allocation of capital to their strategic business units engaged in these domains. Drawing on decision comprehensiveness theory, we propose that such expert sentiment has informational value for capital allocation decisions, and that this value is contingent on the available information’s determinacy and quantity. We find evidence for our propositions by studying 669 capital allocation decisions made by 85 pharmaceutical firms from 2005 to 2016, and by using supervised machine learning classifiers to analyse expert sentiment in almost 250,000 articles. Our study contributes to the behavioural theory of the firm by showing that, contrary to what scholars traditionally assume, decision makers broaden, rather than narrow, their information processing in comprehensive information environments.",10.1111/joms.12742,https://onlinelibrary.wiley.com/doi/10.1111/joms.12742,Journal of Management Studies,Steffen Nauhaus;Johannes Luger;Sebastian Raisch,2021,0,"@article{2-39465,
  title={Strategic decision making in the digital age: expert sentiment and corporate capital allocation},
  author={Nauhaus, Steffen and Luger, Johannes and Raisch, Sebastian},
  year={2021},
  journal={Journal of Management Studies},
  doi={10.1111/joms.12742}
}",Empirical contributions,Finance / Business / Economy,Organizational,Analyzing,"Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-39479,wiley,How to be helpful to multiple people at once,"When someone hosts a party, when governments choose an aid program, or when assistive robots decide what meal to serve to a family, decision-makers must determine how to help even when their recipients have very different preferences. Which combination of people’s desires should a decision-maker serve? To provide a potential answer, we turned to psychology: What do people think is best when multiple people have different utilities over options? We developed a quantitative model of what people consider desirable behavior, characterizing participants’ preferences by inferring which combination of “metrics” (maximax, maxsum, maximin, or inequality aversion [IA]) best explained participants’ decisions in a drink-choosing task. We found that participants’ behavior was best described by the maximin metric, describing the desire to maximize the happiness of the worst-off person, though participant behavior was also consistent with maximizing group utility (the maxsum metric) and the IA metric to a lesser extent. Participant behavior was consistent across variation in the agents involved and tended to become more maxsum-oriented when participants were told they were players in the task (Experiment 1). In later experiments, participants maintained maximin behavior across multi-step tasks rather than shortsightedly focusing on the individual steps therein (Experiment 2, Experiment 3). By repeatedly asking participants what choices they would hope for in an optimal, just decision-maker, and carefully disambiguating which quantitative metrics describe these nuanced choices, we help constrain the space of what behavior we desire in leaders, artificial intelligence systems helping decision-makers, and the assistive robots and decision-makers of the future.",10.1111/cogs.12841,https://onlinelibrary.wiley.com/doi/10.1111/cogs.12841,Cognitive Science Society,Vael Gates;Thomas L. Griffiths;Anca D. Dragan,2020,2,"@inproceedings{2-39479,
  title={How to be helpful to multiple people at once},
  author={Gates, Vael and Griffiths, Thomas L. and Dragan, Anca D.},
  year={2020},
  doi={10.1111/cogs.12841},
  booktitle={Proceedings of the Cognitive Science Society}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service",Institutional,"Analyzing, Explaining, Advising","Decision-maker, Knowledge provider","Alter decision outcomes, Shape ethical norms, Change cognitive demands",no such info,"feature-based explanations, system accuracy, model risk, procedural fairness","fairness constraints, intuition",Textual,Yes,Yes
2-39482,wiley,Australian perspectives on artificial intelligence in veterinary practice,"While artificial intelligence (AI) and recent developments in deep learning (DL) have sparked interest in medical imaging, there has been little commentary on the impact of AI on the veterinarian and veterinary imaging technologists. This survey study aimed to understand the attitudes, applications, and concerns among veterinarians and radiography professionals in Australia regarding the rapidly emerging applications of AI. An anonymous online survey was circulated to the members of three Australian veterinary professional organizations. The survey invitations were shared via email and social media with the survey open for 5 months. Among the 84 respondents, there was a high level of acceptance of lower order tasks (e.g., patient registration, triage, and dispensing) and less acceptance of high order task automation (e.g., surgery and interpretation). There was a low priority perception for the role of AI in higher order tasks (e.g., diagnosis, interpretation, and decision making) and high priority for those applications that automate complex tasks (e.g., quantitation, segmentation, reconstruction) or improve image quality (e.g., dose/noise reduction and pseudo CT for attenuation correction). Medico-legal, ethical, diversity, and privacy issues posed moderate or high concern while there appeared to be no concern regarding AI being clinically useful and improving efficiency. Mild concerns included redundancy, training bias, transparency, and validity. Australian veterinarians and veterinary professionals recognize important applications of AI for assisting with repetitive tasks, performing less complex tasks, and enhancing the quality of outputs in medical imaging. There are concerns relating to ethical aspects of algorithm development and implementation.",10.1111/vru.13234,https://onlinelibrary.wiley.com/doi/10.1111/vru.13234,Veterinary Radiology & Ultrasound,Geoff Currie BPharm;MMedRadSc(NucMed);MAppMngt(Hlth);MBA;PhD;Adrien-Maxence Hespel DVM;MS;DACVR;Ann Carstens BVSc;MS;MMedVet (Large Anim Surg);MMedVet(Diag Im);Dipl ECVDI;PhD,2023,8,"@article{2-39482,
  title = {Australian Perspectives on Artificial Intelligence in Veterinary Practice},
  author = {Geoff Currie and Adrien-Maxence Hespel and Ann Carstens},
  year = {2023},
  journal = {Veterinary Radiology \& Ultrasound},
  doi = {10.1111/vru.13234}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Executing","Decision-maker, Stakeholder","Change trust, Change affective-perceptual, Shift responsibility, Shape ethical norms",no such info,NA,domain knowledge,"Textual, Conversational/Natural Language",Yes,Yes
2-39516,wiley,How do the existing fairness metrics and unfairness mitigation algorithms contribute to ethical learning analytics?,"With the widespread use of learning analytics (LA), ethical concerns about fairness have been raised. Research shows that LA models may be biased against students of certain demographic subgroups. Although fairness has gained significant attention in the broader machine learning (ML) community in the last decade, it is only recently that attention has been paid to fairness in LA. Furthermore, the decision on which unfairness mitigation algorithm or metric to use in a particular context remains largely unknown. On this premise, we performed a comparative evaluation of some selected unfairness mitigation algorithms regarded in the fair ML community to have shown promising results. Using a 3-year program dropout data from an Australian university, we comparatively evaluated how the unfairness mitigation algorithms contribute to ethical LA by testing for some hypotheses across fairness and performance metrics. Interestingly, our results show how data bias does not always necessarily result in predictive bias. Perhaps not surprisingly, our test for fairness-utility tradeoff shows how ensuring fairness does not always lead to drop in utility. Indeed, our results show that ensuring fairness might lead to enhanced utility under specific circumstances. Our findings may to some extent, guide fairness algorithm and metric selection for a given context. Practitioner notes What is already known about this topic LA is increasingly being used to leverage actionable insights about students and drive student success. LA models have been found to make discriminatory decisions against certain student demographic subgroups—therefore, raising ethical concerns. Fairness in education is nascent. Only a few works have examined fairness in LA and consequently followed up with ensuring fair LA models. What this paper adds A juxtaposition of unfairness mitigation algorithms across the entire LA pipeline showing how they compare and how each of them contributes to fair LA. Ensuring ethical LA does not always lead to a dip in performance. Sometimes, it actually improves performance as well. Fairness in LA has only focused on some form of outcome equality, however equality of outcome may be possible only when the playing field is levelled. Implications for practice and/or policy Based on desired notion of fairness and which segment of the LA pipeline is accessible, a fairness-minded decision maker may be able to decide which algorithm to use in order to achieve their ethical goals. LA practitioners can carefully aim for more ethical LA models without trading significant utility by selecting algorithms that find the right balance between the two objectives. Fairness enhancing technologies should be cautiously used as guides—not final decision makers. Human domain experts must be kept in the loop to handle the dynamics of transcending fair LA beyond equality to equitable LA.",10.1111/bjet.13217,https://onlinelibrary.wiley.com/doi/10.1111/bjet.13217,British Journal of Educational Technology,Oscar Blessed Deho;Chen Zhan;Jiuyong Li;Jixue Liu;Lin Liu;Thuc Duy Le,2022,1,"@article{2-39516,
  title={How do the existing fairness metrics and unfairness mitigation algorithms contribute to ethical learning analytics?},
  author={Deho, Oscar Blessed and Zhan, Chen and Li, Jiuyong and Liu, Jixue and Liu, Lin and Le, Thuc Duy},
  year={2022},
  doi={10.1111/bjet.13217},
  journal={British Journal of Educational Technology}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research",Institutional,"Auditing, Advising","Decision-maker, Knowledge provider, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-39697,wiley,Who is a better decision maker? Data-driven expert ranking under unobserved quality,"The capacity to rank expert workers by their decision quality is a key managerial task of substantial significance to business operations. However, when no ground truth information is available on experts’ decisions, the evaluation of expert workers typically requires enlisting peer-experts, and this form of evaluation is prohibitively costly in many important settings. In this work, we develop a data-driven approach for producing effective rankings based on the decision quality of expert workers; our approach leverages historical data on past decisions, which are commonly available in organizational information systems. Specifically, we first formulate a new business data science problem: Ranking Expert decision makers’ unobserved decision Quality (REQ) using only historical decision data and excluding evaluation by peer experts. The REQ problem is challenging because the correct decisions in our settings are unknown (unobserved) and because some of the information used by decision makers might not be available for retrospective evaluation. To address the REQ problem, we develop a machine-learning–based approach and analytically and empirically explore conditions under which our approach is advantageous. Our empirical results over diverse settings and datasets show that our method yields robust performance: Its rankings of expert workers are consistently either superior or at least comparable to those obtained by the best alternative approach. Accordingly, our method constitutes a de facto benchmark for future research on the REQ problem.",10.1111/poms.13260,https://onlinelibrary.wiley.com/doi/10.1111/poms.13260,Production and Operations Management Society (POMS) Journal,Tomer Geva;Maytal Saar-Tsechansky,2020,19,"@article{2-39697,
  title={Who is a better decision maker? Data-driven expert ranking under unobserved quality},
  author={Geva, Tomer and Saar-Tsechansky, Maytal},
  year={2020},
  journal={Production and Operations Management Society (POMS) Journal},
  volume={29},
  number={8},
  pages={1971--1992},
  doi={10.1111/poms.13260}
}",Methodological contributions,Finance / Business / Economy,Operational,"Forecasting, Advising, Monitoring","Decision-maker, Knowledge provider, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-40,aaai,AI Risk Profiles: A Standards Proposal for Pre-deployment AI Risk Disclosures,"As AI systems’ sophistication and proliferation have increased, awareness of the risks has grown proportionally. The AI industry is increasingly emphasizing the need for transparency, with proposals ranging from standardizing use of technical disclosures, like model cards, to regulatory licensing regimes. Since the AI value chain is complicated, with actors bringing varied expertise, perspectives, and values, it is crucial that consumers of transparency disclosures be able to understand the risks of the AI system in question. In this paper we propose a risk profiling standard which can guide downstream decision-making, including triaging further risk assessment, informing procurement and deployment, and directing regulatory frameworks. The standard is built on our proposed taxonomy of AI risks, which distills the wide variety of risks proposed in the literature into a high-level categorization. We outline the myriad data sources needed to construct informative Risk Profiles and propose a template and methodology for collating risk information into a standard, yet flexible, structure. We apply this methodology to a number of prominent AI systems using publicly available information. To conclude, we discuss design decisions for the profiles and future work.",10.1609/aaai.v38i21.30348,https://ojs.aaai.org/index.php/AAAI/article/view/30348,AAAI Conference on Artificial Intelligence,Eli Sherman;Ian Eisenberg,2024,0,"@inproceedings{2-40,
  title={AI Risk Profiles: A Standards Proposal for Pre-deployment AI Risk Disclosures},
  author={Eli Sherman and Ian Eisenberg},
  year={2024},
  booktitle={AAAI Conference on Artificial Intelligence},
  doi={10.1609/aaai.v38i21.30348}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Operational,"Analyzing, Auditing, Forecasting","Decision-subject, Guardian, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-402,aaai,Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can Existing Algorithms Fulfill Clinical Requirements?,"Being able to explain the prediction to clinical end-users is a necessity to leverage the power of artificial intelligence( AI) models for clinical decision support. For medical images, a feature attribution map, or heatmap, is the most common form of explanation that highlights important features for AI models prediction. However, it is unknown how well heatmaps perform on explaining decisions on multi-modal medical images, where each image modality or channel visualizes distinct clinical information of the same underlying biomedical phenomenon. Understanding such modality-dependent features is essential for clinical users interpretation of AI decisions. To tackle this clinically important but technically ignored problem, we propose the modality-specific feature importance( MSFI) metric. It encodes clinical image and explanation interpretation patterns of modality prioritization and modality-specific feature localization. We conduct a clinical requirement-grounded, systematic evaluation using computational methods and a clinician user study. Results show that the examined 16 heatmap algorithms failed to fulfill clinical requirements to correctly indicate AI model decision process or decision quality. The evaluation and MSFI metric can guide the design and selection of explainable AI algorithms to meet clinical requirements on multi-modal explanation.",10.1609/aaai.v36i11.21452,https://ojs.aaai.org/index.php/AAAI/article/view/21452,AAAI Conference on Artificial Intelligence,Weina Jin;Xiaoxiao Li;Ghassan Hamarneh,2022,1,"@inproceedings{2-402,
  title={Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can Existing Algorithms Fulfill Clinical Requirements?},
  author={Jin, Weina and Li, Xiaoxiao and Hamarneh, Ghassan},
  year={2022},
  doi={10.1609/aaai.v36i11.21452},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising, Forecasting, Explaining","Decision-maker, Knowledge provider","Change trust, Alter decision outcomes",Update AI competence,"prediction of alternative, visual explanations",domain knowledge,"Textual, Visual, Interactive interface",Yes,Yes
2-407,aaai,EasySM: A Data-Driven Intelligent Decision Support System for Server Merge,"As an independent social and economic entity, game servers plays a dominant role in building a stable, living, and attractive virtual world in massive multi-player online role-playing games( MMORPGs). We propose and implement a novel intelligent decision support system for server merge( SM) for maintaining the game ecology at the macro level. The services provided by this system include server health diagnosis, server merge assessment, and combination strategy recommendation. Specifically, we design an effective time series prediction algorithm to diagnose the health status of one server( e. g. , user activity, online time, daily revenue) based on real game scenarios, and then select the servers with poor status from all servers. Moreover, to dig out the inherent development laws of servers from the historical merge records, we leverage a correlation measurement algorithm to find the historical merged servers that are similar to the servers to be merged and then evaluate the potential trend after merging, which can assist experts to make reasonable decisions. We deploy our system into practice for multiple MMORPGs and achieve sound online performance endorsed by the game operation team.",10.1609/aaai.v36i11.21731,https://ojs.aaai.org/index.php/AAAI/article/view/21731,AAAI Conference on Artificial Intelligence,Manhu Qu;Jie Huang;Hao Deng;Runze Wu;Xudong Shen;Jianrong Tao;Tangjie Lv,2022,1,"@inproceedings{2-407,
  title = {EasySM: A Data-Driven Intelligent Decision Support System for Server Merge},
  author = {Manhu Qu and Jie Huang and Hao Deng and Runze Wu and Xudong Shen and Jianrong Tao and Tangjie Lv},
  year = {2022},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v36i11.21731},
}",System/Artifact contributions,Media / Communication / Entertainment,Operational,Advising,"Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-427,aaai,Embedding a Long Short-Term Memory Network in a Constraint Programming Framework for Tomato Greenhouse Optimisation,"Increasing global food demand, accompanied by the limited number of expert growers, brings the need for more sustainable and efficient horticulture. The controlled environment of greenhouses enable data collection and precise control. For optimally controlling the greenhouse climate, a grower not only looks at crop production, but rather aims at maximising the profit. However this is a complex, long term optimisation task. In this paper, Constraint Programming( CP) is applied to task of optimal greenhouse economic control, by leveraging a learned greenhouse climate model through a CP embedding. In collaboration with an industrial partner, we demonstrate how to model the greenhouse climate with an LSTM model, embed this LSTM into a CP optimisation framework, and optimise the expected profit of the grower. This data-to-decision pipeline is being integrated into a decision support system for multiple greenhouses in the Netherlands.",10.1609/aaai.v37i13.26867,https://ojs.aaai.org/index.php/AAAI/article/view/26867,AAAI Conference on Artificial Intelligence,Dirk van Bokkem;Max van den Hemel;Sebastijan Dumančić;Neil Yorke-Smith,2024,5,"@inproceedings{2-427,
  title     = {Embedding a Long Short-Term Memory Network in a Constraint Programming Framework for Tomato Greenhouse Optimisation},
  author    = {Dirk van Bokkem and Max van den Hemel and Sebastijan Dumančić and Neil Yorke-Smith},
  year      = {2024},
  doi       = {10.1609/aaai.v37i13.26867},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence}
}",System/Artifact contributions,Environment / Resources / Energy,Operational,"Executing, Advising, Forecasting","Knowledge provider, Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-4279,acs,NanoBeacon.AI: AI-Enhanced Nanodiamond Biosensor for Automated Sensitivity Prediction to Oxidative Phosphorylation Inhibitors,"Spalt-like transcription factor 4 (SALL4) is an oncofetal protein that has been identified to drive cancer progression in hepatocellular carcinoma (HCC) and hematological malignancies. Furthermore, a high SALL4 expression level is correlated to poor prognosis in these cancers. However, SALL4 lacks well-structured small-molecule binding pockets, making it difficult to design targeted inhibitors. SALL4-induced expression of oxidative phosphorylation (OXPHOS) genes may serve as a therapeutically targetable vulnerability in HCC through OXPHOS inhibition. Because OXPHOS functions through a set of genes with intertumoral heterogeneous expression, identifying therapeutic sensitivity to OXPHOS inhibitors may not rely on a single clear biomarker. Here, we developed a workflow that utilized molecular beacons, nucleic-acid-based, activatable sensors with high specificity to the target mRNA, delivered by nanodiamonds, to establish an artificial intelligence (AI)-assisted platform for rapid evaluation of patient-specific drug sensitivity. Specifically, when the HCC cells were treated with the nanodiamond-medicated OXPHOS biosensor, high sensitivity and specificity of the sensor allowed for improved identification of OXPHOS expression in cells. Assisted by a trained convolutional neural network, drug sensitivity of cells toward an OXPHOS inhibitor, IACS-010759, could be accurately predicted. AI-assisted OXPHOS drug sensitivity assessment could be accomplished within 1 day, enabling rapid and efficient clinical decision support for HCC treatment. The work proposed here serves as a foundation for the patient-based subtype-specific therapeutic research platform and is well suited for precision medicine.",10.1021/acssensors.3c00126,https://pubs.acs.org/doi/abs/10.1021/acssensors.3c00126,ACS Sensors,Jingru Xu;Mengjia Zheng;Dexter Kai Hao Thng;Tan Boon Toh;Lei Zhou;Glenn Kunnath Bonney;Yock Young Dan;Pierce Kah Hoe Chow;Chenjie Xu;Edward Kai-Hua Chow,2023,12,"@article{2-4279,
  title={NanoBeacon.AI: AI-Enhanced Nanodiamond Biosensor for Automated Sensitivity Prediction to Oxidative Phosphorylation Inhibitors},
  author={Xu, Jingru and Zheng, Mengjia and Thng, Dexter Kai Hao and Toh, Tan Boon and Zhou, Lei and Bonney, Glenn Kunnath and Dan, Yock Young and Chow, Pierce Kah Hoe and Xu, Chenjie and Chow, Edward Kai-Hua},
  year={2023},
  journal={ACS Sensors},
  doi={10.1021/acssensors.3c00126}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-4367,acs,Programmable Bio-nanochip Platform: A Point-of-Care Biosensor System with the Capacity To Learn,"ConspectusThe combination of point-of-care (POC) medical microdevices and machine learning has the potential transform the practice of medicine. In this area, scalable lab-on-a-chip (LOC) devices have many advantages over standard laboratory methods, including faster analysis, reduced cost, lower power consumption, and higher levels of integration and automation. Despite significant advances in LOC technologies over the years, several remaining obstacles are preventing clinical implementation and market penetration of these novel medical microdevices. Similarly, while machine learning has seen explosive growth in recent years and promises to shift the practice of medicine toward data-intensive and evidence-based decision making, its uptake has been hindered due to the lack of integration between clinical measurements and disease determinations.In this Account, we describe recent developments in the programmable bio-nanochip (p-BNC) system, a biosensor platform with the capacity for learning. The p-BNC is a “platform to digitize biology” in which small quantities of patient sample generate immunofluorescent signal on agarose bead sensors that is optically extracted and converted to antigen concentrations. The platform comprises disposable microfluidic cartridges, a portable analyzer, automated data analysis software, and intuitive mobile health interfaces. The single-use cartridges are fully integrated, self-contained microfluidic devices containing aqueous buffers conveniently embedded for POC use. A novel fluid delivery method was developed to provide accurate and repeatable flow rates via actuation of the cartridge’s blister packs. A portable analyzer instrument was designed to integrate fluid delivery, optical detection, image analysis, and user interface, representing a universal system for acquiring, processing, and managing clinical data while overcoming many of the challenges facing the widespread clinical adoption of LOC technologies. We demonstrate the p-BNC’s flexibility through the completion of multiplex assays within the single-use disposable cartridges for three clinical applications: prostate cancer, ovarian cancer, and acute myocardial infarction.Toward the goal of creating “sensors that learn”, we have developed and describe here the Cardiac ScoreCard, a clinical decision support system for a spectrum of cardiovascular disease. The Cardiac ScoreCard approach comprises a comprehensive biomarker panel and risk factor information in a predictive model capable of assessing early risk and late-stage disease progression for heart attack and heart failure patients. These marker-driven tests have the potential to radically reduce costs, decrease wait times, and introduce new options for patients needing regular health monitoring. Further, these efforts demonstrate the clinical utility of fusing data from information-rich biomarkers and the Internet of Things (IoT) using predictive analytics to generate single-index assessments for wellness/illness status. By promoting disease prevention and personalized wellness management, tools of this nature have the potential to improve health care exponentially.",10.1021/acs.accounts.6b00112,https://pubs.acs.org/doi/abs/10.1021/acs.accounts.6b00112,Accounts of Chemical Research,Michael P. McRae;Glennon Simmons;Jorge Wong;John T. McDevitt,2016,69,"@article{2-4367,
  title={Programmable Bio-nanochip Platform: A Point-of-Care Biosensor System with the Capacity To Learn},
  author={McRae, Michael P. and Simmons, Glennon and Wong, Jorge and McDevitt, John T.},
  year={2016},
  journal={Accounts of Chemical Research},
  doi={10.1021/acs.accounts.6b00112}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-44,aaai,A Submodular Optimization Approach to Accountable Loan Approval,"In the field of finance, the underwriting process is an essential step in evaluating every loan application. During this stage, the borrowers creditworthiness and ability to repay the loan are assessed to ultimately decide whether to approve the loan application. One of the core components of underwriting is credit scoring, in which the probability of default is estimated. As such, there has been significant progress in enhancing the predictive accuracy of credit scoring models through the use of machine learning, but there still exists a need to ultimately construct an approval rule that takes into consideration additional criteria beyond the score itself. This construction process is traditionally done manually to ensure that the approval rule remains interpretable to humans. In this paper, we outline an automated system for optimizing a rule-based system for approving loan applications, which has been deployed at Hyundai Capital Services( HCS). The main challenge lay in creating a high-quality rule base that is simultaneously simple enough to be interpretable by risk analysts as well as customers, since the approval decision should be accountable. We addressed this challenge through principled submodular optimization. The deployment of our system has led to a 14% annual growth in the volume of loan services at HCS, while maintaining the target bad rate, and has resulted in the approval of customers who might have otherwise been rejected.",10.1609/aaai.v38i21.30310,https://ojs.aaai.org/index.php/AAAI/article/view/30310,AAAI Conference on Artificial Intelligence,Kyungsik Lee;Hana Yoo;Sumin Shin;Wooyoung Kim;Yeonung Baek;Hyunjin Kang;Jaehyun Kim;Kee-Eung Kim,2024,0,"@inproceedings{2-44,
  title     = {A Submodular Optimization Approach to Accountable Loan Approval},
  author    = {Kyungsik Lee and Hana Yoo and Sumin Shin and Wooyoung Kim and Yeonung Baek and Hyunjin Kang and Jaehyun Kim and Kee-Eung Kim},
  year      = {2024},
  doi       = {10.1609/aaai.v38i21.30310},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence}
}",System/Artifact contributions,Finance / Business / Economy,Operational,"Executing, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-4552,aisel,``ai for social good'' adoption through divergent perceptions: exploring a social landlord's strategizing process,"The promises of artificial intelligence (AI) are tremendous but controversial. Developing a ""good AI society"" is becoming an imperative for researchers in information systems (IS), practitioners, and policymakers. This challenge involves weighing up the benefits and risks of AI for society. Social purpose organizations are particularly struggling to adopt and implement AI. IS research provides an understanding of individual and organizational drivers of AI adoption. However, a lack of awareness on how divergent positions towards AI lead to adoption decisions remains within organizations. Our paper focuses on the day-to-day strategic decision-making process discourse on AI adoption rather than the adoption results through a case study of a social landlord, contributing towards the understanding of social influence in AI adoption processes in the context of AI for social good. It also recommends a future research agenda focusing on collective action in AI adoption theories.",NA,https://aisel.aisnet.org/ecis2024/track01_peoplefirst/track01_peoplefirst/8,European Conference on Information Systems,"Duarte, Magalie;Biot-Paquerot, Guillaume;Dominguez-P{\'e}ry, Carine;Arab, Kenza",2024,0,"@inproceedings{2-4552,
  title = {``AI for Social Good'' Adoption Through Divergent Perceptions: Exploring a Social Landlord's Strategizing Process},
  author = {Duarte, Magalie and Biot-Paquerot, Guillaume and Dominguez-P{\'e}ry, Carine and Arab, Kenza},
  year = {2024},
  booktitle = {European Conference on Information Systems}
}",Empirical contributions,Everyday / Employment / Public Service,Organizational,"Advising, Analyzing, Collaborating","Decision-maker, Guardian, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-4555,aisel,A framework for artificial knowledge creation in organizations,"Recent advances in Artificial Intelligence (AI) have increased the ability of organizations to analyze data to support decisions. However, there is little focus to date, on the potential role of AI in organizational knowledge creation. This paper develops a framework of organizational artificial knowledge creation based on a synthesis of the literature, and the implementation of a multi-agent AI in an organization. We identify five stages for developing organizational artificial knowledge: 1) Extracting and Collecting, 2) Curating, 3) Ingesting, 4) Training and Testing, 5) Analyzing and Predicting. We also identified two main practices triggered by the development of the AI multi-agent that distinguish them from traditional IS: the ability to initiate a dialogue between the different actors which can lead to the consolidation and aggregation of the organizational knowledge, and the ability to establish recursive and reflexive relation between individual knowledge and the organizational artificial knowledge.",NA,https://aisel.aisnet.org/icis2017/General/Presentations/15,International Conference on Information Systems,"HARFOUCHE, Antoine;Quinio, Bernard;Skandrani, Sana;Marciniak, Rolande",2017,34,"@inproceedings{2-4555,
  title     = {A framework for artificial knowledge creation in organizations},
  author    = {Harfouche, Antoine and Quinio, Bernard and Skandrani, Sana and Marciniak, Rolande},
  year      = {2017},
  booktitle = {International Conference on Information Systems}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Operational,"Advising, Analyzing, Forecasting, Collaborating","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-4559,aisel,A mythic belief regarding trust in artificial intelligence: uncovering the role of responsibility perception for ai use in decision makings,"This study aims to analyze a mechanism of AI responsibility based on attribution theory. It also identifies a new concept, AI locus of control (AI-LOC), reflecting an individual's belief about the degree to which AI determines decision performance. To this end, we built a website with embedded AI systems where participants longitudinally made corporate credit rating decisions. We created a dynamic panel dataset that includes participants' decisions per task and decision performance and attitudes per session. The results revealed that AI-LOC and trust in AI were developed in parallel yet differed over time. AI-LOC positively influenced AI use, but trust in AI did not. We reasoned that individuals would likely exhibit self-serving biases and take an egocentric and disengagement coping strategy regarding their decision-making with AI. This study can contribute to understanding the psychological and behavioral aspects of AI use.",NA,https://aisel.aisnet.org/hicss-57/da/xai/4,Hawaii International Conference on System Sciences,"Lee, Kyootai;Cho, Wooje;Woo, Han-Gyun",2024,0,"@inproceedings{2-4559,
  author    = {Kyootai Lee and Wooje Cho and Han-Gyun Woo},
  title     = {A Mythic Belief Regarding Trust in Artificial Intelligence: Uncovering the Role of Responsibility Perception for AI Use in Decision Makings},
  year      = {2024},
  booktitle = {Proceedings of the Hawaii International Conference on System Sciences}
}","Empirical contributions, System/Artifact contributions",Generic / Abstract / Domain-agnostic,no such info,Advising,"Decision-maker, Decision-subject","Alter decision outcomes, Change trust, Change cognitive demands, Change affective-perceptual, Restrict human agency",no such info,"recommendations, credit score",NA,"Interactive interface, Visual",Yes,Yes
2-4564,aisel,A probabilistic perspective of human-machine interaction,"Human-machine interaction (HMI) has become an essential part of the daily routine in organizations. Although the machines are designed with state-of-the-art Artificial Intelligence applications, they are limited in their ability to mimic human behavior. The human-human interaction occurs between two or more humans; when a machine replaces a human, the interaction dynamics are not the same. The results indicate that a machine that interacts with a human can increase the mental uncertainty that a human experiences. Developments in decision sciences indicate that using quantum probability theory (QPT) improves the understanding of human decision-making than merely using classical probability theory (CPT). In this paper, we examine the HMI from a QPT perspective. Applying QPT to studying HMI for decision-making shows improvement in understanding the decision process when interacting with machines because it provides insights into the mental uncertainty of a human that is not apparent in CPT.",NA,https://aisel.aisnet.org/hicss-55/st/cyber_systems/2,Hawaii International Conference on System Sciences,"Canan, Mustafa;Demir, Mustafa;Kovacic, Samual",2022,7,"@inproceedings{2-4564,
  title = {A probabilistic perspective of human-machine interaction},
  author = {Canan, Mustafa and Demir, Mustafa and Kovacic, Samual},
  year = {2022},
  booktitle = {Hawaii International Conference on System Sciences}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Individual,"Collaborating, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4566,aisel,A reinforcement learning powered digital twin to support supply chain decisions,"The complexity of making supply chain planning decisions is growing along with the Volatility, Uncertainty, Complexity and Ambiguity of supply chain environments. As a consequence, the complexity of designing adequate decision support systems is also increasing. New approaches emerged for supporting decisions, and digital twins is one of those. Concurrently, the artificial intelligence field is growing, including approaches such as reinforcement learning. This paper explores the potential of creating digital twins with reinforcement learning capabilities. It first proposes a framework for unifying digital twins and reinforcement learning into a single approach. It then illustrates how this framework is put into practice for making supply and delivery decisions within a drug supply chain use case. Finally, the results of the experiment are compared with results given by traditional approaches, showing the applicability of the proposed framework.",NA,https://aisel.aisnet.org/hicss-55/dg/supply_chain/2,Hawaii International Conference on System Sciences,"Martin, Guillaume;Oger, Rapha{\""e}l",2022,0,"@inproceedings{2-4566,
  title = {A Reinforcement Learning Powered Digital Twin to Support Supply Chain Decisions},
  author = {Martin, Guillaume and Oger, Rapha{\""e}l},
  year = {2022},
  booktitle = {Hawaii International Conference on System Sciences}
}",Algorithmic contributions,Manufacturing / Industry / Automation,Institutional,"Advising, Executing, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4569,aisel,A stressful explanation: the dual effect of explainable artificial intelligence in personal health management,"Artificial intelligence (AI) is increasingly incorporated into innovative personal health apps to improve the decision-making of its users. To facilitate the understanding and to increase usage of such AI-based personal health apps, firms are progressively turning to explainable artificial intelligence (XAI) designs. However, we argue that explanations of the AI-based recommendations have not only positive but also negative consequences. Based on a socio-technical lens, we develop a model that relates XAI to technostress - both eustress and distress - and its downstream consequences. To test our model, we conducted an online experiment, in which participants interact with XAI or black-box AI. Our results show that (1) XAI causes both eu- and distress, and (2) simultaneously exerts differential influence on objective performance, satisfaction, and intention to use. Our findings contribute to information systems research and practice by uncovering the dual effect of XAI on decision processes in the health context.",NA,https://aisel.aisnet.org/hicss-57/hc/wellness_management/2,Hawaii International Conference on System Sciences,"Gr{\""u}ning, Maximilian;Wolf, Tobias;Trenz, Manuel",2024,2,"@inproceedings{2-4569,
  title     = {A Stressful Explanation: The Dual Effect of Explainable Artificial Intelligence in Personal Health Management},
  author    = {Gr{\""u}ning, Maximilian and Wolf, Tobias and Trenz, Manuel},
  year      = {2024},
  booktitle = {Hawaii International Conference on System Sciences},
}",Empirical contributions,Healthcare / Medicine / Surgery,Individual,"Explaining, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-457,aaai,Ethically Compliant Sequential Decision Making,"Enabling autonomous systems to comply with an ethical theory is critical given their accelerating deployment in domains that impact society. While many ethical theories have been studied extensively in moral philosophy, they are still challenging to implement by developers who build autonomous systems. This paper proposes a novel approach for building ethically compliant autonomous systems that optimize completing a task while following an ethical framework. First, we introduce a definition of an ethically compliant autonomous system and its properties. Next, we offer a range of ethical frameworks for divine command theory, prima facie duties, and virtue ethics. Finally, we demonstrate the accuracy and usability of our approach in a set of autonomous driving simulations and a user study of planning and robotics experts.",10.1609/aaai.v35i13.17386,https://ojs.aaai.org/index.php/AAAI/article/view/17386,AAAI Conference on Artificial Intelligence,Justin Svegliato;Samer B. Nashed;Shlomo Zilberstein,2021,11,"@inproceedings{2-457,
  title={Ethically Compliant Sequential Decision Making},
  author={Svegliato, Justin and Nashed, Samer B. and Zilberstein, Shlomo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2021},
  doi={10.1609/aaai.v35i13.17386}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Individual,"Advising, Executing",Knowledge provider,Alter decision outcomes,"Update AI competence, Shape AI for accountability","ethics, planning, recommendations",domain knowledge,Autonomous System,Yes,Yes
2-4571,aisel,Acceptance of ai for delegating emotional intelligence: results from an experiment,"Detecting emotions of other humans is challenging for us humans. It is however important in many social contexts so that many individuals seek help in this regard. As technology is evolving, more and more AI-based options emerge that promise to detect human emotions and support decision making. We focus on the full delegation of detecting emotions to AI to contribute to our understanding how such AI is perceived and why it is accepted. For this, we conduct an online scenario-based experiment in which participants have the choice to delegate emotion detection to another human in one group and to an AI tool in the other group. Our results show that the delegation rates are higher for a human, but surprisingly high for AI. The results provide insights that should be considered when designing AI-based emotion-detection tools to build trustworthy and accepted designs.",NA,https://aisel.aisnet.org/hicss-54/os/promises_and_perils_of_ai/2,Hawaii International Conference on System Sciences,"Aysolmaz, Banu;Leyer, Michael;Iren, Deniz",2021,0,"@inproceedings{2-4571,
  title={Acceptance of AI for Delegating Emotional Intelligence: Results from an Experiment},
  author={Aysolmaz, Banu and Leyer, Michael and Iren, Deniz},
  year={2021},
  booktitle={Hawaii International Conference on System Sciences}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Executing, Analyzing","Decision-subject, Decision-maker","Alter decision outcomes, Change trust, Shift responsibility",no such info,NA,"delegation, a higher attachment with the colleague than with the AI, a higher attachment with the colleague/AI leads to a higher acceptance of the offer",Autonomous System,Yes,Yes
2-4574,aisel,Accountability-based user interface design artifacts and their implications for user acceptance of ai-enabled services,"Although AI-enabled interactive decision aids (IDAs) have demonstrated to provide reliable advice, users are rather reluctant to follow this advice. One recently highly discussed reason for this reluctance is users' perceived unclear accountability of the AI-service regarding the decisions of these AI-based IDAs. Drawing on accountability theory, we designed user-interface (UI) design artifacts for AI-enabled IDAs based on the dimensions identifiability, expectation of evaluation, awareness of monitoring, and social presence and tested them through a scenario-based factorial survey method (N = 629). We show that accountability-emphasizing UI design artifacts individually raise users' accountability perceptions of the AI-enabled service, which in turn influence users' compliance to follow the advice from the AI-enabled service. These findings have important theoretical and practical implications, particularly as they inform how to increase the transparency of accountability of AI-enabled services and thus user compliance.",NA,https://aisel.aisnet.org/ecis2022_rp/5,European Conference on Information Systems,"Adam, Martin",2022,12,"@inproceedings{2-4574,
  title={Accountability-based user interface design artifacts and their implications for user acceptance of AI-enabled services},
  author={Adam, Martin},
  year={2022},
  booktitle={European Conference on Information Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Institutional,Advising,Decision-maker,"Alter decision outcomes, Shift responsibility",no such info,"accountability-based UI design artifacts, recommendations",NA,"Visual, Interactive interface",Yes,Yes
2-4575,aisel,Accuracy and explainability in artificial intelligence: unpacking the terms,"Artificial intelligence (AI) has permeated many aspects of human life from product recommendations on retailers' websites to critical decisions affecting healthcare and law enforcement. As such systems become prevalent in high risk areas, explaining their logic to demonstrate issues such as fairness acquire increasing significance. Yet the current focus of machine learning models is the accuracy of decisions rather than their explainability. This paper analyses the findings from two citizens' juries convened to investigate the perceived trade-off between AI explainability and AI accuracy. While the official juries' report shows clear preferences for accuracy over explanation in some settings, this paper presents an alternative perspective informed by the concept of ambivalence. By introducing some additional metrics and highlighting the possibilities for different forms of explanation, this research demonstrates how the findings from the citizens' juries might be otherwise, and the social consequences arising. The paper concludes with some future research directions.",NA,https://aisel.aisnet.org/icis2021/ai_business/ai_business/18,International Conference on Information Systems,"McGrath, Kathy",2021,1,"@inproceedings{2-4575,
  title={Accuracy and explainability in artificial intelligence: unpacking the terms},
  author={McGrath, Kathy},
  year={2021},
  booktitle={International Conference on Information Systems}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance",Institutional,"Advising, Explaining","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-4576,aisel,Achieving decisional fit with ai-aided group decisions: the role of intuitive decision-making style in predicting perceived fairness and decision acceptance,"As organizations integrate AI decision tools into their decision-making processes, there is a need to understand factors that promote acceptance of decisions made with AI tools. This study draws from the theory of decisional fit and design features of an AI platform to examine the relationship between decision-making styles, procedural fairness, and decision acceptance when teams collaborate with AI decision aid to reach a decision. The results confirm the mediating relationship of procedural fairness between an intuitive decision-making style and decision acceptance. These results extend theory related to decision-making styles by identifying individual differences that predict procedural fairness and decision acceptance. Moreover, it offers guidance to managers and organizations seeking to adopt and design AI decision aids.",NA,https://aisel.aisnet.org/hicss-57/cl/machines_as_teammates/3,Hawaii International Conference on System Sciences,"Askay, David;Dhillon, Anuraj;Metcalf, Lynn",2024,1,"@inproceedings{2-4576,
  title={Achieving decisional fit with AI-aided group decisions: The role of intuitive decision-making style in predicting perceived fairness and decision acceptance},
  author={Askay, David and Dhillon, Anuraj and Metcalf, Lynn},
  year={2024},
  booktitle={Hawaii International Conference on System Sciences}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service",Organizational,"Advising, Collaborating",Decision-maker,"Change trust, Alter decision outcomes, Change cognitive demands, Change affective-perceptual","Update AI competence, Change AI responses","procedural fairness, visual explanations",personalized settings,"Conversational/Natural Language, Textual",Yes,Yes
2-4577,aisel,Achieving efficient resilience through human adjustments of algorithm prescriptions --- a retail management application,"As the frequency of significant market disruptions rises, retailers are forced to respond through resilience activities which often comes at the cost of sacrificing operational efficiency. To alleviate this resilience-efficiency trade-off, retailers increasingly rely on digital technologies, particularly artificial intelligence (AI). Specifically, they are employing AI-based algorithm prescriptions to guide humans in operational decision-making, aiming to improve resilience while maintaining high efficiency. In our field study involving 341 stores of a large European retailer, we examine the effects of human-AI collaboration on this resilience-efficiency trade-off. Our results indicate that human adjustments of algorithm prescriptions regarding supply strategy factors (i.e., delivery frequency and delivery pattern) can intensify the trade-off. However, if organizational experience and product differentiation are high, adjusting algorithm prescriptions helps to reduce the conflict between resilience and efficiency. For practice, we offer important implications on how firms can leverage the potential of AI-based tools in retail stores to become both resilient and efficient.",NA,https://aisel.aisnet.org/ecis2024/track03_ai/track03_ai/15,European Conference on Information Systems,"Berendes, Katharina;Hammerschmidt, Maik;Arabmaldar, Aliasghar;Loske, Dominic;Klumpp, Matthias",2024,1,"@inproceedings{2-4577,
  title     = {Achieving efficient resilience through human adjustments of algorithm prescriptions --- a retail management application},
  author    = {Berendes, Katharina and Hammerschmidt, Maik and Arabmaldar, Aliasghar and Loske, Dominic and Klumpp, Matthias},
  year      = {2024},
  booktitle = {European Conference on Information Systems}
}",Empirical contributions,Finance / Business / Economy,Operational,Advising,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4580,aisel,Adversarial cognitive engineering (ace) and defensive cybersecurity: leveraging attacker decision-making heuristics in a cybersecurity task,"The role of cyberspace continues to expand, touching nearly every aspect in our lives. Critical information, when stolen, can be devastating to a nation's people, economy, and security. To defend against this threat, it is essential to understand the human behind the attack. A first step in developing new defenses where human attackers are involved is obtaining valid and reliable human performance and decision-making data. These data can be procured through rigorous human science research that experimentally evaluates foundational theory and measures human performance. Taking the key concepts from behavioral economics, the game-based testbed, CYPHER, was specifically designed to test the occurrence of the Sunk Cost Fallacy across multiple decisions in an abstract cyber environment. Evaluating decisions made over a series of actions to catch a fictitious cyber thief, we analyze the effects of two antecedents (uncertainty and project completion) and resource expenditure. Our results show that irrespective of condition, significantly more participants unnecessarily wasted resources, demonstrating behavior consistent with the Sunk Cost Fallacy. These data provide a baseline upon which to build artificial intelligence algorithms for automated cyber defense.",NA,https://aisel.aisnet.org/hicss-57/da/cyber_deception/6,Hawaii International Conference on System Sciences,"Johnson, Chelsea;Van Tassel, Richard W.;Shade, Temmie;Rogers, Andrew;Ferguson-Walter, Kimberly",2024,3,"@inproceedings{2-4580,
  title={Adversarial cognitive engineering (ace) and defensive cybersecurity: leveraging attacker decision-making heuristics in a cybersecurity task},
  author={Johnson, Chelsea and Van Tassel, Richard W. and Shade, Temmie and Rogers, Andrew and Ferguson-Walter, Kimberly},
  year={2024},
  booktitle={Hawaii International Conference on System Sciences}
}",Empirical contributions,Software / Systems / Security,Operational,Analyzing,"Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-4581,aisel,Advice utilization in combined human-algorithm decision-making: an analysis of preferences and behaviors,"As artificial intelligence (AI) becomes more pervasive, humans will interact with autonomous agents more frequently and in deeper ways. While there is a significant body of work addressing the interface between a single human and a single AI agent, less is known about how individuals react to AI when they are part of human-agent hybrids, namely multiple humans and potentially multiple AI. These hybrid forms are unique in that advice is often given simultaneously, i.e., a human decision maker evaluates advice from other humans and algorithms at the same time. This scenario presents a boundary condition on the extant literature, as it is unclear how a human decision maker will differentially appraise a human advisor compared to an algorithmic advisor when their advice is simultaneous. This study presents the results of three experiments asking individuals to estimate property rental prices with the support of both human and algorithmic advice. We tested whether explicitly labeling an advisor as an algorithm rather than a human impacts how individuals perceive both the algorithm and another human advisor. We also examined the role of conflicting advice during simultaneous evaluation. Based on the results of 904 participants, we found that labeling an advisor as an algorithm resulted in a significantly significant algorithmic appreciation bias, even when an equivalent human was present. Further, we found that uncertainty induced by conflicting information weakened the appreciation effect, while agreement among advisors resulted in the strongest behavioral responses.",NA,https://aisel.aisnet.org/jais/vol25/iss6/9,Journal of the Association for Information Systems,"Sachin, Panda Kumar;Schecter, Aaron",2024,7,"@article{2-4581,
  title={Advice utilization in combined human-algorithm decision-making: an analysis of preferences and behaviors},
  author={Sachin, Panda Kumar and Schecter, Aaron},
  year={2024},
  journal={Journal of the Association for Information Systems}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Everyday / Employment / Public Service",Individual,Advising,Decision-maker,"Change trust, Alter decision outcomes, Change affective-perceptual",no such info,"identity cues, recommendations",recommendations,"Textual, Interactive interface",Yes,Yes
2-4586,aisel,Ai for situational awareness in situations with high uncertainty: an explorative case study,"Often, the police experience scenarios with much uncertainty. These scenarios can be characterized by high time pressure, huge amounts of information, and potentially severe consequences. In this paper, we study whether artificial intelligence (AI) can be a fit for the information processing needs of the police helping them achieve situational awareness and make better decisions. Given the potential severity of police situations, AI can potentially reduce the risk of fatal outcomes and wrong decisions. Investigating this issue with police officers and AI experts as our informants, our findings suggest that our informants are positive to AI as a support tool, but more skeptical to whether AI can make an impact in their daily police work due to the complexity of their work. The importance of implementing AI to suitable tasks is emphasized.",NA,https://aisel.aisnet.org/ecis2022_rp/62,European Conference on Information Systems,"Skaug, Henrik Aage;Busch, Peter Andr{\'e },",2022,4,"@inproceedings{2-4586,
  title = {AI for Situational Awareness in Situations with High Uncertainty: An Explorative Case Study},
  author = {Skaug, Henrik Aage and Busch, Peter Andr{\'e}},
  year = {2022},
  booktitle = {European Conference on Information Systems}
}",Empirical contributions,"Law / Policy / Governance, Defense / Military / Emergency",Operational,"Advising, Analyzing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-4587,aisel,Ai implementation and capability development in manufacturing: an action research case,"This action research article presents a case study of a global manufacturing company deploying artificial intelligence (AI) to develop capabilities and enhance decision-making. This study explores considerations and trade-offs involved in introducing AI into daily operations, leading up to the decision to develop AI capabilities in-house or outsource them. The case study offers in-depth technical descriptions of model selection, dataset creation, model adoption, model training and evaluation while addressing organizational obstacles and decision-making processes. The study's findings highlight the importance of collaboration between technical experts, business leaders, and end-users, as well as the interaction and collaboration between AI systems and human employees in the workplace. The article contributes a practical perspective on AI implementation in manufacturing, emphasizing the need to balance in-house capability development with external acquisition. Although the case study company managed to create an in-house model, factors such as implementation, debugging, data requirements, training time, and performance led to outsourcing the capabilities. However, making this informed decision required capabilities and insights that were acquired through practical work. Consequently, although in-house development can be challenging, it can also enhance organizational capabilities and provide the necessary knowledge to make informed decisions about future development or outsourcing.",NA,https://aisel.aisnet.org/hicss-57/os/ai_and_organizing/3,Hawaii International Conference on System Sciences,"Eklof, Jon;Snis, Ulrika;Hamelryck, Thomas;Grima, Alexander;Ronning, Ola",2024,33,"@inproceedings{2-4587,
  author    = {Eklof, Jon and Snis, Ulrika and Hamelryck, Thomas and Grima, Alexander and Ronning, Ola},
  title     = {AI Implementation and Capability Development in Manufacturing: An Action Research Case},
  year      = {2024},
  booktitle = {Hawaii International Conference on System Sciences}
}",Empirical contributions,Manufacturing / Industry / Automation,Organizational,"Collaborating, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4590,aisel,Ai suffrage: a four-country survey on the acceptance of an automated voting system,"Governments have begun to employ technological systems that use massive amounts of data and artificial intelligence (AI) in the domains of law enforcement, public health, or social welfare. In some areas, shifts in public opinion increasingly favor technology-aided public decision-making. This development presents an opportunity to explore novel approaches to how technology could be used to reinvigorate democratic governance and how the public perceives such changes. The study therefore posits a hypothetical AI voting system that mediates political decision-making between citizens and the state. We conducted a four-country online survey (N=6043) in Greece, Singapore, Switzerland, and the US to find out what factors affect the public's acceptance of such a system. The data show that Singaporeans are most likely and Greeks least likely to accept the system. Considerations of the technology's utility have a large effect on acceptance rates across cultures whereas attitudes towards political norms and political performance have partial effects.",NA,https://aisel.aisnet.org/hicss-55/dg/ai/2,Hawaii International Conference on System Sciences,"Suter, Viktor;Meckel, Miriam;Shahrezaye, Morteza;Steinacker, L{\'e}a",2022,4,"@article{2-4590,
  title={AI Suffrage: A four-country survey on the acceptance of an automated voting system},
  author={Suter, Viktor and Meckel, Miriam and Shahrezaye, Morteza and Steinacker, L{\'e}a},
  year={2022},
  jounral={Hawaii International Conference on System Sciences}
}",Empirical contributions,Law / Policy / Governance,Institutional,"Executing, Advising","Decision-maker, Stakeholder",NA,NA,NA,NA,Textual,Yes,No
2-4597,aisel,Ai-enhanced hybrid decision management,"The Decision Model and Notation (DMN) modeling language allows the precise specification of business decisions and business rules. DMN is readily understandable by business users involved in decision management. However, as the models get complex, the cognitive abilities of humans threaten manual maintainability and comprehensibility. Proper design of the decision logic thus requires comprehensive automated analysis of e.g., all possible cases the decision shall cover; correlations between inputs and outputs; and the importance of inputs for deriving the output. In the paper, the authors explore the mutual benefits of combining human-driven DMN decision modeling with the computational power of Artificial Intelligence for DMN model analysis and improved comprehension. The authors propose a model-driven approach that uses DMN models to generate Machine Learning (ML) training data and show, how the trained ML models can inform human decision modelers by means of superimposing the feature importance within the original DMN models. An evaluation with multiple real DMN models from an insurance company evaluates the feasibility and the utility of the approach.",NA,https://aisel.aisnet.org/bise/vol65/iss2/6,Business & Information Systems Engineering,"Bork, Dominik;Ali, Syed J.;Dinev, Georgi M.",2023,19,"@article{2-4597,
  title={Ai-enhanced hybrid decision management},
  author={Bork, Dominik and Ali, Syed J. and Dinev, Georgi M.},
  year={2023},
  journal={Business \& Information Systems Engineering}
}",Methodological contributions,Finance / Business / Economy,Operational,"Advising, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4600,aisel,Alphavc: a reinforcement learning-based venture capital investment strategy,"Venture capital investments play a powerful role in fueling the emergence and growth of early-stage startups. However, only a small fraction of venture-backed startups can survive and exit successfully. Prior data-driven prediction-based or recommendation-based solutions are incapable of providing effective and actionable strategies on proper investment timing and amounts for startups across different investment rounds. In this paper, we develop a novel reinforcement learning-based method, AlphaVC, to facilitate venture capitalists' decision-making. Our policy-based reinforcement learning agents can dynamically identify the best candidates and sequentially place the optimal investment amounts at proper rounds to maximize financial returns for a given portfolio. We retrieve company demographics and investment activity data from Crunchbase. Our methodology demonstrates its efficacy and superiority in ranking and portfolio-based performance metrics in comparison with various state-of-the-art baseline methods. Through sensitivity and ablation analyses, our research highlights the significance of factoring in the distal outcome and acknowledging the learning effect when making decisions at different time points. Additionally, we observe that AlphaVC concentrates on a select number of high-potential companies, but distributes investments evenly across various stages of the investment process.",NA,https://aisel.aisnet.org/hicss-57/in/impacts/3,Hawaii International Conference on System Sciences,"Zhong, Hao;Yuan, Zixuan;Zhang, Denghui;Jiang, Yi;Zhang, Shengming;Xiong, Hui",2024,1,"@inproceedings{2-4600,
  title = {Alphavc: A Reinforcement Learning-based Venture Capital Investment Strategy},
  author = {Zhong, Hao and Yuan, Zixuan and Zhang, Denghui and Jiang, Yi and Zhang, Shengming and Xiong, Hui},
  year = {2024},
  booktitle = {Hawaii International Conference on System Sciences}
}",Algorithmic contributions,Finance / Business / Economy,Organizational,"Executing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4602,aisel,An empirical study exploring difference in trust of perceived human and intelligent system partners,"Intelligent systems are increasingly relied on as partners used to make decisions in business contexts. With advances in artificial intelligence technology and system interfaces, it is increasingly difficult to distinguish these system partners from their human counterparts. Understanding the role of perceived humanness and its impact on trust in these situations is important as trust is widely recognized as critical to system adoption and effective collaboration. We conducted an exploratory study involving individuals collaborating with an intelligent system partner to make several critical decisions. Measured trust levels and survey responses were analyzed. Results suggest that greater trust is experienced when the partner is perceived to be human. Additionally, the attribution of partners possessing expert knowledge drove perceptions of humanness. Partners viewed to adhere to strict syntactical requirements, displaying quick response times, having unnatural conversational tone, and unrealistic availability contributed to perceptions of partners being machine-like.",NA,https://aisel.aisnet.org/hicss-54/cl/ai_and_future_work/2,Hawaii International Conference on System Sciences,"Elson, Joel;Derrick, Douglas;Merino, Luis",2021,4,"@inproceedings{2-4602,
  title={An empirical study exploring difference in trust of perceived human and intelligent system partners},
  author={Elson, Joel and Derrick, Douglas and Merino, Luis},
  year={2021},
  booktitle={Hawaii International Conference on System Sciences}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Advising, Collaborating, Analyzing",Decision-maker,Change trust,no such info,"recommendations, (continuous) support",perceiving AI as human,"Conversational/Natural Language, Textual, Autonomous System",Yes,Yes
2-4605,aisel,An exploratory study on fairness-aware design decision-making,"With advances in machine learning (ML) and big data analytics, data-driven predictive models play an essential role in supporting a wide range of simple and complex decision-making processes. However, historical data embedded with unfairness may unintentionally reinforce discrimination towards minority groups when using data-driven decision-support technologies. In this paper, we quantify unfairness and analyze its impact in the context of data-driven engineering design using the Adult Income dataset. First, we introduce a fairness-aware design concept. Subsequently, we introduce standard definitions and statistical measures of fairness to the engineering design research. Then, we use the outcomes from two supervised ML models, Logistic Regression and CatBoost classifiers, to conduct the Disparate Impact and fair-test analyses to quantify any unfairness present in the data and decision outcomes. Based on the results, we highlight the importance of considering fairness in product design and marketing, and the consequences, if there is a loss of fairness.",NA,https://aisel.aisnet.org/hicss-55/ks/aspects_of_ai/2,Hawaii International Conference on System Sciences,"Tanu, Sumaiya Sultana;Zhang, Lu;Gauri, Dinesh;Sha, Zhenghui",2022,1,"@inproceedings{2-4605,
  title = {An exploratory study on fairness-aware design decision-making},
  author = {Tanu, Sumaiya Sultana and Zhang, Lu and Gauri, Dinesh and Sha, Zhenghui},
  year = {2022},
  booktitle = {Hawaii International Conference on System Sciences}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy, Design / Creativity / Architecture",Operational,"Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4606,aisel,An intelligent decision support system for the empty unit load device repositioning problem in air cargo industry,"Unit load devices (ULDs) are containers and pallets used in the air cargo industry to bundle freight for efficient loading and transportation. Mainly due to imbalances in global air transportation networks, deficits and surpluses of ULDs are the result and require stock balancing through the repositioning of (empty) ULDs. Following a design science research approach, we (1) elaborate the hitherto uninvestigated problem class of empty ULD repositioning (EUR) and (2) propose an intelligent decision support system (IDSS) that incorporates a heuristic for the given problem and combines artificial intelligence (i.e., rule-based expert system technology) with business analytics. We evaluate the IDSS with real-world data and demonstrate that the proposed solution is both effective and efficient. In addition, our results provide empirical evidence regarding the positive economic and ecological impact of leveraging the potential of ULD pooling in multi-carrier networks.",NA,https://aisel.aisnet.org/hicss-51/da/decision_support_for_scm/4,Hawaii International Conference on System Sciences,"D{\""o}ppner, Daniel A.;Derckx, Patrick;Schoder, Detlef",2018,141,"@inproceedings{2-4606,
  title={An intelligent decision support system for the empty unit load device repositioning problem in air cargo industry},
  author={D{\""o}ppner, Daniel A. and Derckx, Patrick and Schoder, Detlef},
  year={2018},
  booktitle={Hawaii International Conference on System Sciences}
}",System/Artifact contributions,Transportation / Mobility / Planning,Organizational,"Analyzing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4611,aisel,Artificial intelligence and decision-making: the question of accountability,"Public sector organizations literature has addressed the influence of AI on decision-making process, looking mainly at rationalization and efficiency. However, recent adoptions of AI have been challenged because of their discriminatory nature. As a result, questions emerged on the accountability of AI supported decision-making processes in the public sector. This research sheds light on how AI transforms decision-making processes in the public sector and hence on their accountability. The paper illustrates that AI adoptions lead to the emergency of techno-legal entanglements -- assemblages -- which might impact upon AI accountability. Building on the findings of some of the most controversial and discussed cases of AI adoption in the public sector -- COMPAS in the US and UKVI in the UK -- the paper makes the case for a new approach to AI supported public sector decision-making accountability.",NA,https://aisel.aisnet.org/hicss-54/dg/emerging_topics_in_e-gov/2,Hawaii International Conference on System Sciences,"Gualdi, Francesco;Cordella, Antonio",2021,63,"@inproceedings{2-4611,
  title={Artificial intelligence and decision-making: the question of accountability},
  author={Gualdi, Francesco and Cordella, Antonio},
  year={2021},
  booktitle={Hawaii International Conference on System Sciences}
}",Empirical contributions,Everyday / Employment / Public Service,Institutional,Advising,"Decision-maker, Guardian, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-4614,aisel,Artificial intelligence as a service: trade-offs impacting service design and selection,"AI as a Service (AIaaS) is a promising path to leverage AI capabilities from the cloud. However, there is no one-size-fits-all service, but heterogenous service options since interdependent AIaaS characteristics require trade-offs. We lack knowledge on these trade-offs and how they result from conflicting characteristics. Therefore, we interviewed 39 AIaaS providers, customers, and consultants to provide rich descriptions of interdependent characteristics and uncover resulting trade-offs and their consequences. This study contributes to a better understanding of the inner functioning and interplay of AIaaS characteristics and discusses how this complex nature of service offerings impacts providers' design and customers' selection decisions.",NA,https://aisel.aisnet.org/icis2023/itadopt/itadopt/5,International Conference on Information Systems,"Brecker, Kathrin;Lins, Sebastian;Trenz, Manuel;Sunyaev, Ali",2023,12,"@inproceedings{2-4614,
  title={Artificial intelligence as a service: trade-offs impacting service design and selection},
  author={Brecker, Kathrin and Lins, Sebastian and Trenz, Manuel and Sunyaev, Ali},
  year={2023},
  booktitle={International Conference on Information Systems}
}",Empirical contributions,Software / Systems / Security,Organizational,"Advising, Executing","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-4616,aisel,Artificial intelligence for managerial information processing and decision-making in the era of information overload,"In the big data era, managers are exposed to an increasing amount of structured and unstructured information that they must process daily to make decisions. In this context, artificial intelligence (AI) functionalities can support managerial information processing (IP), which forms the basis of managers' decision-making. To date, little is known about the themes that managers face when integrating AI into their IP and decision-making. The present paper identifies these through three focus group interviews with managers from the financial industry, validates them through a survey and derives organizational implications. The results imply that organizations should (1) evaluate managerial IP tasks and matching AI systems, (2) (re)define roles for managers and AI systems, and (3) redesign management processes for sustainable human-AI interaction.",NA,https://aisel.aisnet.org/hicss-55/os/ai_and_organizing/3,Hawaii International Conference on System Sciences,"Dietzmann, Christian;Duan, Yanqing",2022,0,"@inproceedings{2-4616,
  title = {Artificial Intelligence for Managerial Information Processing and Decision-Making in the Era of Information Overload},
  author = {Dietzmann, Christian and Duan, Yanqing},
  year = {2022},
  booktitle = {Hawaii International Conference on System Sciences}
}",Empirical contributions,Finance / Business / Economy,Operational,"Analyzing, Advising",Decision-maker,"Alter decision outcomes, Shape ethical norms, Change trust, Change cognitive demands",no such info,"recommendations, filtering",NA,"Textual, Conversational/Natural Language",Yes,Yes
2-4619,aisel,Artificial intelligence to improve public budgeting,"Accurate public budgeting is essential for efficient resource allocation and societal trust. However, recent studies have shown that public budgets often project deficits but have substantial surpluses. This budgetary slack can lead taxpayers to overpay for services not rendered, delay necessary investments, or distort public perceptions of government efficiency. To avoid such unfortunate outcomes, we study how artificial intelligence (AI) can help decision-makers in the public sector with budgeting. We operationalize our research question using a two-step approach. First, we utilize open data from Swiss financial authorities to train and test an AI model. Our preliminary results validate the potential of AI to predict public accounts better than human experts. Second, we study whether decision-makers effectively utilize the AI model in an experimental scenario. The results of our experimental study indicate that human-AI collaboration could indeed support decision-makers to improve public budgeting by reducing budgetary slack.",NA,https://aisel.aisnet.org/icis2024/iot_smartcity/iot_smartcity/1,International Conference on Information Systems,"Santschi, Dominic;Grau, Marc Christopher;Fehrenbacher, Dennis;Blohm, Ivo",2024,3,"@inproceedings{2-4619,
  title={Artificial intelligence to improve public budgeting},
  author={Santschi, Dominic and Grau, Marc Christopher and Fehrenbacher, Dennis and Blohm, Ivo},
  year={2024},
  booktitle={International Conference on Information Systems}
}",Empirical contributions,Law / Policy / Governance,Organizational,"Forecasting, Advising, Collaborating","Decision-maker, Guardian, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-4620,aisel,Artificial intelligence-augmented decision making in supply chain monitoring: an action design research study,"Organizations are progressively adopting hybrid human--AI systems in decision-making processes in which human decisions are augmented by AI insights. Among the promising AI applications in supply chain monitoring (SCMo) are predictive maintenance systems that predict device failures and augment maintenance decisions, allowing for timely interventions. Despite the growing use of such systems, prescriptive knowledge encompassing technical, business, social, and organizational aspects on how to design, develop, and deploy them in real operational environments remain obscure. To address this shortcoming, our action design research study designed and deployed a predictive maintenance system that predicts the failure of SCMo devices and augments the maintenance decisions of those devices. By doing so, we outline our learnings as generalizable design principles that may guide prospective predictive maintenance systems in SCMo.",NA,https://aisel.aisnet.org/ecis2023_rp/282,European Conference on Information Systems,"Herath Pathirannehelage, Savindu;J{\'o}hannsson, J{\'o}hann G.;Shrestha, Yash Raj;von Krogh, Georg F.",2023,4,"@inproceedings{2-4620,
  title = {Artificial Intelligence-Augmented Decision Making in Supply Chain Monitoring: An Action Design Research Study},
  author = {Herath Pathirannehelage, Savindu and Jóhannsson, Jóhann G. and Shrestha, Yash Raj and von Krogh, Georg F.},
  year = {2023},
  booktitle = {European Conference on Information Systems}
}",System/Artifact contributions,Manufacturing / Industry / Automation,Organizational,"Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4627,aisel,Beauty's in the ai of the beholder: how ai anchors subjective and objective predictions,"Researchers increasingly acknowledge that algorithms can exhibit bias, but artificial intelligence (AI) is increasingly integrated into the organizational decision-making process. How does biased AI shape human choices? We consider a sequential AI-human decision that mirrors organizational decisions; an automated system provides a score and then a human decides a score using their discretion. We conduct an AMT survey and ask participants to assign one of two types of scores: a subjective, context-dependent measure (Beauty) and objective, observer-independent measure (Age). Participants are either shown the AI score, shown the AI score and its error, or not shown the AI score. We find that participants without knowledge of the AI score do not exhibit bias; however, knowing the AI scores for the subjective measure induces bias in the participants' scores due to the anchoring effect. Although participants' scores do not display bias, participants who receive information about the AI error rates devalue the AI score and reduce their error. This study makes several contributions to the information systems literature. First, this paper provides a novel way to discuss artificial intelligence bias by distinguishing between subjective and objective measures. Second, this paper highlights the potential spillover effects from algorithmic bias into human decisions. If biased artificial intelligence anchors human decisions, then it can induce bias into previously unbiased scores. Third, we examine a method to encourage participants to reduce their reliance on the artificial intelligence, reporting the error rate, and find evidence that it is effective for the objective measure.",NA,https://aisel.aisnet.org/icis2019/future_of_work/future_work/15,International Conference on Information Systems,"Rhue, Lauren",2019,14,"@inproceedings{2-4627,
  title={Beauty's in the ai of the beholder: how ai anchors subjective and objective predictions},
  author={Rhue, Lauren},
  year={2019},
  booktitle={International Conference on Information Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Advising, Forecasting",Decision-maker,"Alter decision outcomes, Shape ethical norms, Change trust",Change AI responses,"AI score, error",NA,Textual,Yes,Yes
2-4628,aisel,Better the devil you don't know. Perceived discrimination increases positive evaluation of ai recruiting decisions.,"Recruiting is one of the most controversial and sensitive use cases of AI due to the potential impact on the life of job seekers. Thus, on the one hand, regulators introduce new regulations on AI classifying recruiting as high-risk use case. On the other hand, technology research shows AI could reduce discrimination. One perspective is still unknown: How do discriminated people perceive the application of AI in recruiting? The paper investigates how discriminated people perceive the use of AI in recruiting and how transparency of AI decisions influences this perception. We conducted a lab-in-the-field experiment using an online questionnaire yielding a random sample of (N = 1335) unemployed registered with the Austrian Public Unemployment Service. We show that perceived everyday discrimination increases the positive overall evaluation of AI decisions in recruiting. Evaluating regulatory requirements and technical features should also explicitly take discriminated groups into account.",NA,https://aisel.aisnet.org/icis2024/soc_impactIS/soc_impactIS/17,International Conference on Information Systems,"Flei{\ss}, J{\""u}rgen;Kubicek, Bettina;Thalmann, Stefan",2024,2,"@article{2-4628,
  title={Better the devil you don't know. Perceived discrimination increases positive evaluation of ai recruiting decisions},
  author={Flei{\ss}, J{\""u}rgen;Kubicek, Bettina;Thalmann, Stefan},
  year={2024},
  jounral={International Conference on Information Systems}
}",Empirical contributions,Everyday / Employment / Public Service,Operational,Executing,"Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-4631,aisel,Bringing machine learning systems into clinical practice: a design science approach to explainable machine learning-based clinical decision support systems,"Clinical decision support systems (CDSSs) based on machine learning (ML) hold great promise for improving medical care. Technically, such CDSSs are already feasible but physicians have been skeptical about their application. In particular, their opacity is a major concern, as it may lead physicians to overlook erroneous outputs from ML-based CDSSs, potentially causing serious consequences for patients. Research on explainable AI (XAI) offers methods with the potential to increase the explainability of black-box ML systems. This could significantly accelerate the application of ML-based CDSSs in medicine. However, XAI research to date has mainly been technically driven and largely neglects the needs of end users. To better engage the users of ML-based CDSSs, we applied a design science approach to develop a design for explainable ML-based CDSSs that incorporates insights from XAI literature while simultaneously addressing physicians' needs. This design comprises five design principles that designers of ML-based CDSSs can apply to implement user-centered explanations, which are instantiated in a prototype of an explainable ML-based CDSS for lung nodule classification. We rooted the design principles and the derived prototype in a body of justificatory knowledge consisting of XAI literature, the concept of usability, and an online survey study involving 57 physicians. We refined the design principles and their instantiation by conducting walk-throughs with six radiologists. A final experiment with 45 radiologists demonstrated that our design resulted in physicians perceiving the ML-based CDSS as more explainable and usable in terms of the required cognitive effort than a system without explanations.",NA,https://aisel.aisnet.org/jais/vol24/iss4/8,Journal of the Association for Information Systems,"Pumplun, Luisa;Peters, Felix;Gawlitza, Joshua F.;Buxmann, Peter",2023,2,"@article{2-4631,
  title = {Bringing machine learning systems into clinical practice: a design science approach to explainable machine learning-based clinical decision support systems},
  author = {Pumplun, Luisa and Peters, Felix and Gawlitza, Joshua F. and Buxmann, Peter},
  year = {2023},
  journal = {Journal of the Association for Information Systems}
}","System/Artifact contributions, Methodological contributions",Healthcare / Medicine / Surgery,Operational,"Explaining, Advising","Decision-maker, Knowledge provider","Change cognitive demands, Change affective-perceptual",no such info,"local explanations, global explanations, model explanations",NA,"Interactive interface, Visual",Yes,Yes
2-4638,aisel,Chatbot catalysts: improving team decision-making through cognitive diversity and information elaboration,"As the integration of artificial intelligence (AI) into team decision-making continues to expand, it is both theoretically and practically pressing for researchers to understand the impact of the technology on team dynamics and performance. To investigate this relationship, we conducted an online experiment in which teams made decisions supported by chatbots and employed computational methods to analyze team interaction processes. Our results indicated that compared to those assisted by chatbots in later phases, teams receiving chatbot assistance during the initial phase of their decision-making process exhibited increased cognitive diversity (i.e., diversity in shared information) and information elaboration (i.e., exchange and integration of information). Ultimately, teams assisted by chatbots early on performed better. These results imply that introducing AI at the beginning of the process can enhance team decision-making by promoting effective information sharing among team members.",NA,https://aisel.aisnet.org/icis2023/hti/hti/18,International Conference on Information Systems,"Gurkan, Necdet;Yan, Bei",2023,0,"@inproceedings{2-4638,
  title={Chatbot catalysts: improving team decision-making through cognitive diversity and information elaboration},
  author={Gurkan, Necdet and Yan, Bei},
  year={2023},
  booktitle={International Conference on Information Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Organizational,"Analyzing, Advising, Collaborating",Decision-maker,"Alter decision outcomes, Change cognitive demands",no such info,recommendations,shared information,Conversational/Natural Language,Yes,Yes
2-4640,aisel,Choose wisely: leveraging explainable ai to support reflective decision-making,"Explainable Artificial Intelligence (XAI) can contribute to the idea of AI being an instrument for reflection when used for augmentation of human decision-making. In the educational domain, reflective decision-making is crucial as decisions have a meaningful and long-term impact. Against this background, we propose an XAI-based approach that supports users in making reflective educational decisions. Our approach introduces three main ideas: concepts as a ``shared language'' between AI and users, concept-based explanations, and concept-based interventions. We demonstrate the practical applicability of our approach for a real-world dataset with university courses. We evaluate the efficacy of our approach in a user study with 495 participants. Results suggest that our novel approach effectively supports users in making reflective decisions compared to black box recommender systems, while increasing users' exploration, self-reflection, confidence, and trust. The effectiveness of our approach is attributable to the combination of concept-based explanations and the opportunity to intervene.",NA,https://aisel.aisnet.org/icis2024/aiinbus/aiinbus/22,International Conference on Information Systems,"F{\""o}rster, Maximilian;Schr{\""o}ppel, Philipp;Schwenke, Chiara;Fink, Lior;Klier, Mathias",2024,1,"@inproceedings{2-4640,
  title={Choose wisely: leveraging explainable ai to support reflective decision-making},
  author={F{\""o}rster, Maximilian and Schr{\""o}ppel, Philipp and Schwenke, Chiara and Fink, Lior and Klier, Mathias},
  year={2024},
  booktitle={International Conference on Information Systems}
}",System/Artifact contributions,Education / Teaching / Research,Individual,"Explaining, Advising",Decision-maker,"Change affective-perceptual, Change trust, Alter decision outcomes",no such info,"concept-based interventions, concept-based explanations, recommendations",NA,"Textual, Interactive interface, Conversational/Natural Language",Yes,Yes
2-4652,aisel,Considerations on human-ai collaboration in knowledge work -- recruitment experts' needs and expectations,"Organizations' decision-making processes are increasingly supported by novel AI applications. While intelligent systems appear promising for enhancing various professional tasks, domain experts' perceptions of the adoption and use of AI remain understudied. Following a human-centered design approach, this qualitative interview study (N=15) explores the potential of AI applications from recruitment experts' perspectives. The results of the study emphasize the collaborative nature of AI: recruitment experts anticipate AI to augment their expertise positioning as a complementary information source. Domain experts would then evaluate and justify this outcome according to certain recruitment situations and combined with recruitment experts' tacit knowledge. Novel AI applications are expected to align with underlying social and societal factors that guide the domain experts' work practices. The results provide qualitative understanding into domain experts' user experience and human-AI collaboration in knowledge work, offering insight into human-centered AI design and development.",NA,https://aisel.aisnet.org/hicss-57/cl/ai_and_future_work/6,Hawaii International Conference on System Sciences,"Ala-Luopa, Saara;Koivunen, Sami;Olsson, Thomas;V{\""a}{\""a}n{\""a}nen, Kaisa",2024,0,"@inproceedings{2-4652,
  title = {Considerations on human-AI collaboration in knowledge work-- Recruitment experts' needs and expectations},
  author = {Ala-Luopa, Saara and Koivunen, Sami and Olsson, Thomas and V{\""a}{\""a}n{\""a}nen, Kaisa},
  year = {2024},
  booktitle = {Hawaii International Conference on System Sciences}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,no such info,"Analyzing, Advising","Decision-maker, Knowledge provider","Change trust, Change affective-perceptual, Shape ethical norms, Alter decision outcomes",Shape AI for accountability,recommendations,"surrounding cultural context, candidate's previous work experience, hidden biases, evaluation",Autonomous System,Yes,Yes
2-4659,aisel,Credit risk modeling without sensitive features: an adversarial deep learning model for fairness and profit,"We propose an adversarial deep learning model for credit risk modeling. We make use of sophisticated machine learning model's ability to triangulate (i.e., infer the sensitive group affiliation by using only permissible features), which is often deemed ``troublesome'' in fair machine learning research, in a positive way to increase both borrower welfare and lender profits while improving fairness. We train and test our model on a dataset from a real-world microloan company. Our model significantly outperforms regular deep neural networks without adversaries and the most popular credit risk model XGBoost, in terms of both improving borrowers' welfare and lenders' profits. Our empirical findings also suggest that the traditional AUC metric cannot reflect a model's performance on the borrowers' welfare and lenders' profits. Our framework is ready to be customized for other microloan firms, and can be easily adapted to many other decision-making scenarios.",NA,https://aisel.aisnet.org/icis2022/ai_business/ai_business/4,International Conference on Information Systems,"Hu, Xiyang;Huang, Yan;Li, Beibei;Lu, Tian",2022,4,"@inproceedings{2-4659,
  title     = {Credit Risk Modeling Without Sensitive Features: An Adversarial Deep Learning Model for Fairness and Profit},
  author    = {Hu, Xiyang and Huang, Yan and Li, Beibei and Lu, Tian},
  year      = {2022},
  booktitle = {International Conference on Information Systems}
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-4664,aisel,Data-driven power system optimal decision making strategy under wildfire events,"Wildfire activities are increasing in the western United States in recent years, causing escalating threats to power systems. This paper developed an optimal and data-driven decision-making framework that improves power system resilience under wildfire risks. An optimal load shedding plan is formulated based on optimal power flow analysis. To avoid power system cascading failure caused by wildfire, we added additional transmission line flow constraints based on the identification of power lines with high ignition risk. Finally, a data-driven method is developed, leveraging multiple machine learning techniques, to model the complex correlations between input wildfire scenarios and the output power management strategy with significantly reduced computational complexities. The proposed data-driven decision-making framework can reduce the safety impacts on the electricity consumers, improve power system resilience under wildfire events.",NA,https://aisel.aisnet.org/hicss-55/es/resilient_networks/2,Hawaii International Conference on System Sciences,"Hong, Wanshi;Wang, Bin;Yao, Mengqi;Callaway, Duncan;Dale, Larry;Huang, Can",2022,23,"@inproceedings{2-4664,
  title={Data-driven power system optimal decision making strategy under wildfire events},
  author={Hong, Wanshi and Wang, Bin and Yao, Mengqi and Callaway, Duncan and Dale, Larry and Huang, Can},
  year={2022},
  booktitle={Hawaii International Conference on System Sciences}
}",Algorithmic contributions,Environment / Resources / Energy,Operational,"Advising, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4669,aisel,Design and evaluation of an ai-augmented screening system for venture capitalists,"The venture capital (VC) industry has been experiencing a substantive transformation in recent years due to the rise of online sources for startup data and their utilization in identifying promis-ing startup founders. The resulting significant increase in leads has proportionally elevated the workload required for screening them along the investment process. Together with a well-known German VC firm, we have designed and implemented a Machine Learning pipeline trained on a large dataset of 39,114 LinkedIn profiles with their respective screening decisions aiming to en-hance the efficiency of the process. Our method boosts screening productivity by auto-rejecting 23% of founders, missing at most 1% relevant ones, yielding a 663:1 accurate rejection ratio. Venture capitalists can adjust auto-rejection up to 57% at 10% miss rate. 84% of users prefer the AI-augmented workflow, while 16% pre-filter profiles for reduced workload. Its success convinced the VC firm to promptly implement the system in production.",NA,https://aisel.aisnet.org/ecis2024/track03_ai/track03_ai/7,European Conference on Information Systems,"Maurer, Marc;Buz, Tolga;Dremel, Christian;de Melo, Gerard",2024,3,"@inproceedings{2-4669,
  title = {Design and Evaluation of an AI-Augmented Screening System for Venture Capitalists},
  author = {Maurer, Marc and Buz, Tolga and Dremel, Christian and de Melo, Gerard},
  year = {2024},
  booktitle = {European Conference on Information Systems}
}",System/Artifact contributions,Finance / Business / Economy,Organizational,"Advising, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4670,aisel,Design foundations for ai assisted decision making: a self determination theory approach,"Progress of technology and processing power has enabled the advent of sophisticated technology including Artificial Intelligence (AI) agents. AI agents have penetrated society in many forms including conversation agents or chatbots. As these chatbots have a social component to them, is it critical to evaluate the social aspects of their design and its impact on user outcomes. This study employs Social Determination Theory to examine the effect of the three motivational needs on user interaction outcome variables of a decision-making chatbot. Specifically, this study looks at the influence of relatedness, competency, and autonomy on user satisfaction, engagement, decision efficiency, and decision accuracy. A carefully designed experiment revealed that all three needs are important for user satisfaction and engagement while competency and autonomy is associated with decision accuracy. These findings highlight the importance of considering psychological constructs during AI design. Our findings also offer useful implications for AI designers and organizations that plan on using AI assisted chatbots to improve decision-making efforts.",NA,https://aisel.aisnet.org/hicss-54/cl/ai_and_future_work/5,Hawaii International Conference on System Sciences,"De Vreede, Triparna;Raghavan, Mukhunth;De Vreede, Gert-Jan",2021,65,"@inproceedings{2-4670,
  title={Design foundations for AI assisted decision making: a self determination theory approach},
  author={De Vreede, Triparna and Raghavan, Mukhunth and De Vreede, Gert-Jan},
  year={2021},
  booktitle={Hawaii International Conference on System Sciences}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Advising, Collaborating",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4671,aisel,Design knowledge for deep-learning-enabled image-based decision support systems,"With the ever-increasing societal dependence on electricity, one of the critical tasks in power supply is maintaining the power line infrastructure. In the process of making informed, cost-effective, and timely decisions, maintenance engineers must rely on human-created, heterogeneous, structured, and also largely unstructured information. The maturing research on vision-based power line inspection driven by advancements in deep learning offers first possibilities to move towards more holistic, automated, and safe decision-making. However, (current) research focuses solely on the extraction of information rather than its implementation in decision-making processes. The paper addresses this shortcoming by designing, instantiating, and evaluating a holistic deep-learning-enabled image-based decision support system artifact for power line maintenance at a German distribution system operator in southern Germany. Following the design science research paradigm, two main components of the artifact are designed: A deep-learning-based model component responsible for automatic fault detection of power line parts as well as a user-oriented interface responsible for presenting the captured information in a way that enables more informed decisions. As a basis for both components, preliminary design requirements are derived from literature and the application field. Drawing on justificatory knowledge from deep learning as well as decision support systems, tentative design principles are derived. Based on these design principles, a prototype of the artifact is implemented that allows for rigorous evaluation of the design knowledge in multiple evaluation episodes, covering different angles. Through a technical experiment the technical novelty of the artifact's capability to capture selected faults (regarding insulators and safety pins) in unmanned aerial vehicle (UAV)-captured image data (model component) is validated. Subsequent interviews, surveys, and workshops in a natural environment confirm the usefulness of the model as well as the user interface component. The evaluation provides evidence that (1) the image processing approach manages to address the gap of power line component inspection and (2) that the proposed holistic design knowledge for image-based decision support systems enables more informed decision-making. The paper therefore contributes to research and practice in three ways. First, the technical feasibility to detect certain maintenance-intensive parts of power lines with the help of unique UAV image data is shown. Second, the distribution system operators' specific problem is solved by supporting decisions in maintenance with the proposed image-based decision support system. Third, precise design knowledge for image-based decision support systems is formulated that can inform future system designs of a similar nature.",NA,https://aisel.aisnet.org/bise/vol64/iss6/2,Business & Information Systems Engineering,"Landwehr, Julius P.;K{\""u}hl, Niklas;Walk, Jannis;Gn{\""a}dig, Mario",2023,22,"@article{2-4671,
  title={Design knowledge for deep-learning-enabled image-based decision support systems},
  author={Landwehr, Julius P. and K{\""u}hl, Niklas and Walk, Jannis and Gn{\""a}dig, Mario},
  year={2023},
  journal={Business \& Information Systems Engineering}
}",System/Artifact contributions,Environment / Resources / Energy,Organizational,"Advising, Auditing, Analyzing",Decision-maker,"Alter decision outcomes, Change cognitive demands",no such info,"visual fault explanations, visual data exploration, power",NA,"Visual, Interactive interface",Yes,Yes
2-4673,aisel,Design principles for human-centred ai,"Advancements within artificial intelligence (AI) enable organisations to reformulate strategies for exploiting data in order to refine their business models, make better decisions and maintain a competitive advantage. We recognise the technical advantages of AI. However, our view is that the technical perspective as a base for decision-making is necessary but insufficient. Several studies in human science report that essential human knowledge and competencies that affect decision making are not represented in AI systems. Based on this observation, we have developed design principles for developing decision-support systems (DSS) that combine human intelligence (HI) with AI. The design principles are: design for amplified decision-making, design for unbiased decision-making and design for human and AI learning. The design principles constitute the scientific contribution to the emergent field of Human-Centred AI. The contribution to practice consists of a DSS (a digital prototype) that supports the combination of HI and AI.",NA,https://aisel.aisnet.org/ecis2022_rp/32,European Conference on Information Systems,"Cronholm, Stefan;G{\""o}bel, Hannes",2022,54,"@inproceedings{2-4673,
  title = {Design principles for human-centred AI},
  author = {Cronholm, Stefan and G{\""o}bel, Hannes},
  year = {2022},
  booktitle = {European Conference on Information Systems}
}",Methodological contributions,"Finance / Business / Economy, Generic / Abstract / Domain-agnostic",Institutional,"Advising, Collaborating, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4676,aisel,Designing interactive explainable ai systems for lay users,"Explainability considered a critical component of trustworthy artificial intelligence (AI) systems, has been proposed to address AI systems' lack of transparency by revealing the reasons behind their decisions to lay users. However, most explainability methods developed so far provide static explanations that limit the information conveyed to lay users resulting in an insufficient understanding of how AI systems make decisions. To address this challenge and support the efforts to improve the transparency of AI systems, we conducted a design science research project to design an interactive explainable artificial intelligence (XAI) system to help lay users understand AI systems' decisions. We relied on existing knowledge in the XAI literature to propose design principles and instantiate them in an initial prototype. We then conducted an evaluation of the prototype and interviews with lay users. Our research contributes design knowledge for interactive XAI systems and provides practical guidelines for practitioners.",NA,https://aisel.aisnet.org/icis2023/dab_sc/dab_sc/5,International Conference on Information Systems,"Meza Mart{\'\i}nez, Miguel Angel;M{\""a}dche, Alexander",2023,0,"@inproceedings{2-4676,
  title={Designing interactive explainable AI systems for lay users},
  author={Meza Mart{\'\i}nez, Miguel Angel and M{\""a}dche, Alexander},
  year={2023},
  booktitle={International Conference on Information Systems}
}",System/Artifact contributions,Generic / Abstract / Domain-agnostic,Individual,"Explaining, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4677,aisel,Designing successful human-ai collaboration for creative-problem solving in architectural design,"The rapid development of artificial intelligence has enabled its use in many fields. However, for complex design tasks, the question arises of how AI can effectively support humans and lead to a symbiotic collaboration without replacing humans. During the design process of buildings in the architecture, engineering, and construction industry (AEC industry), there are many points of choice and decision where an AI-based support system can complement the designers` experience and managerial decision-making. With this in mind, the paper identifies design principles for a human-AI collaboration for architectural design. It explores the use of generative AI as creative stimuli provider (e.g. building footprints). We employ a design-oriented research approach (design science research) that paves the way to a concept for human-AI collaboration in the idea generation phase of architectural design.",NA,https://aisel.aisnet.org/icis2024/isdesign/isdesign/14,International Conference on Information Systems,"Weber, Sebastian;Klein, Hans Christian;Siemon, Dominik;Kordyaka, Bastian;Niehaves, Bj{\""o}rn",2024,5,"@inproceedings{2-4677,
  title={Designing successful human-ai collaboration for creative-problem solving in architectural design},
  author={Weber, Sebastian and Klein, Hans Christian and Siemon, Dominik and Kordyaka, Bastian and Niehaves, Bj{\""o}rn},
  year={2024},
  booktitle={International Conference on Information Systems}
}",Methodological contributions,Design / Creativity / Architecture,Individual,"Collaborating, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4681,aisel,Diagnosis of poisoning using probabilistic logic networks,"Medical toxicology is the clinical specialty that treats the toxic effects of substances, be it an overdose, a medication error, or a scorpion sting. The volume of toxicological knowledge has, as with other medical specialties, outstripped the ability of the individual clinician to master and stay current with it. The application of machine learning techniques to medical toxicology is challenging because initial treatment decisions are often based on a few pieces of textual data and rely heavily on prior knowledge. Moreover, ML techniques often do not represent knowledge in a way that is transparent for the physician, raising barriers to usability. Rule-based systems and decision tree learning are more transparent approaches, but often generalize poorly and require expert curation to implement and maintain. Here, we construct a probabilistic logic network to represent a portion of the knowledge base of a medical toxicologist. Our approach transparently mimics the knowledge representation and clinical decision-making of practicing clinicians. The software, dubbed \emph{Tak}, performs comparably to humans on straightforward cases and intermediate difficulty cases, but is outperformed by humans on challenging clinical cases. \emph{Tak} outperforms a decision tree classifier at all levels of difficulty. Probabilistic logic provides one form of explainable artificial intelligence that may be more acceptable for use in healthcare, if it can achieve acceptable levels of performance.",NA,https://aisel.aisnet.org/hicss-54/hc/body_sensor/3,Hawaii International Conference on System Sciences,"Chary, Michael;Boyer, Edward;Burns, Michele",2021,0,"@inproceedings{2-4681,
  title={Diagnosis of poisoning using probabilistic logic networks},
  author={Chary, Michael and Boyer, Edward and Burns, Michele},
  year={2021},
  booktitle={Hawaii International Conference on System Sciences}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Explaining","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-4682,aisel,Diagnostic doubt and artificial intelligence: an inductive field study of radiology work,"Technological developments in emerging AI technologies are assumed to further routinize and improve the efficiency of decision making tasks, even in professional contexts such as medical diagnosis, human resource management, and criminal justice. We have little research on how AI technologies are actually used and adopted in practice. Prior research on technology in organizations documents a gap between the expectations for new technology and its actual use in practice. We conducted a comparative field study of three sections in a Department of Radiology in a major US hospital, whereby new and existing AI tools were being used and experimented with. In contrast to expectations about AI tools, our study reveals how such tools can lead routine professional decision making tasks to become nonroutine, as they increased ambiguity and decision makers had to work to reduce it. This is particularly challenging since the costs of dealing with ambiguity -- increased time to diagnose -- were often weighed against the benefits of such ambiguity (potentially more accurate diagnoses). This study contributes to literatures related to technology, work, and organizations, as well as the role of ambiguity in professionals' knowledge work.",NA,https://aisel.aisnet.org/icis2019/future_of_work/future_work/11,International Conference on Information Systems,"Lebovitz, Sarah",2019,56,"@inproceedings{2-4682,
  title={Diagnostic doubt and artificial intelligence: an inductive field study of radiology work},
  author={Lebovitz, Sarah},
  year={2019},
  booktitle={International Conference on Information Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Decision-maker, Knowledge provider","Alter decision outcomes, Change cognitive demands, Change trust, Change affective-perceptual","Change AI responses, Update AI competence","ambiguity, uncertainty, visual analysis",domain knowledge,"Textual, Visual",Yes,Yes
2-4686,aisel,Distributed cognitive expert systems in cancer data analytics: a decision support system for oral and maxillofacial surgery,"Although researchers have uncovered potential positive impacts of digital technologies in healthcare and medical centers have been increasingly making use of technology to digitally store their data, the use of healthcare analytics in clinical practice remains limited. In particular, the application of machine learning (ML) approaches, although holding the potential of providing valuable insights, is mainly restricted to descriptive ML, due to the approximate nature of ML, the impact of inaccuracies, and the perceived potential additional efforts in clinical workflows. Taking into account these barriers to healthcare analytics adoption, in this multidisciplinary study, we obtained and jointly analyzed cancer data on 799 cases of cranio-maxillofacial and oral-maxillofacial surgery. We developed a real-time decision support system that predicts optimal treatments and communicates its prediction confidence along with patient attributes that are significant to decision making, thereby providing potentials simultaneously for improving quality of care and for increasing process efficiency for physicians.",NA,https://aisel.aisnet.org/icis2017/IT-and-Healthcare/Presentations/19,International Conference on Information Systems,"Tofangchi, Schahin;Hanelt, Andre;B{\""o}hrnsen, Florian",2017,8,"@inproceedings{2-4686,
  title={Distributed cognitive expert systems in cancer data analytics: a decision support system for oral and maxillofacial surgery},
  author={Tofangchi, Schahin and Hanelt, Andre and B{\""o}hrnsen, Florian},
  year={2017},
  booktitle={International Conference on Information Systems}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting, Explaining","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-4691,aisel,Effectiveness of ai in strategic decision making: an empirical study on identifying high-potential talents,"While the efficiency of AI in decision making has been confirmed in the literature, the effectiveness of AI decision making is less studied. To address this research void, we examine the effectiveness of AI using data collected from a leading technology company that applies both AI and human in the high-potential talent identification process. The cross-sectional comparison of these employees' after-selection performance generates mixed results. While the AI-selected employees had higher contribution scores than their human-recommended peers, the human-recommended employees showed higher growth potential (proxied by promotion speed) and organizational commitment (proxied by turnover). Further analysis indicates that the AI-selected employees exhibited suboptimal performance on all three aspects when compared to the short-listed employees that were selected through an additional round of human evaluation. Jointly, these results suggest that AI can be an effective screening tool for identifying high-potential talents, but human instinct is essential for the final selection.",NA,https://aisel.aisnet.org/icis2021/governance/governance/4,International Conference on Information Systems,"Cheng, Yihang;Tang, Xinlin;Zhang, Xi;Xiong, Hui",2021,5,"@inproceedings{2-4691,
  title={Effectiveness of AI in strategic decision making: An empirical study on identifying high-potential talents},
  author={Cheng, Yihang and Tang, Xinlin and Zhang, Xi and Xiong, Hui},
  year={2021},
  booktitle={International Conference on Information Systems}
}",Empirical contributions,Everyday / Employment / Public Service,Operational,"Advising, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-4692,aisel,Effectiveness of example-based explanations to improve human decision quality in machine learning forecasting systems,"Algorithmic forecasts outperform human forecasts by 10% on average. State-of-the-art machine learning (ML) algorithms have further expanded this discrepancy. Because a variety of other activities rely on them, sales forecasting is critical to a company's profitability. However, individuals are hesitant to use ML forecasts. To overcome this algorithm aversion, explainable artificial intelligence (XAI) can be a solution by making ML systems more comprehensible by providing explanations. However, current XAI techniques are incomprehensible for laymen, as they impose too much cognitive load. We contribute to this research gap by investigating the effectiveness in terms of forecast accuracy of two example-based explanation approaches. We conduct an online experiment based on a two-by-two between-subjects design with factual and counterfactual examples as experimental factors. A control group has access to ML predictions, but not to explanations. We report results of this study: While factual explanations significantly improved participants' decision quality, counterfactual explanations did not.",NA,https://aisel.aisnet.org/icis2022/hci_robot/hci_robot/5,International Conference on Information Systems,"Fahse, Tobias Benjamin;Blohm, Ivo;van Giffen, Benjamin",2022,7,"@inproceedings{2-4692,
  title = {Effectiveness of example-based explanations to improve human decision quality in machine learning forecasting systems},
  author = {Fahse, Tobias Benjamin and Blohm, Ivo and van Giffen, Benjamin},
  year = {2022},
  booktitle = {International Conference on Information Systems}
}",Empirical contributions,Finance / Business / Economy,Operational,"Forecasting, Explaining",Decision-maker,"Alter decision outcomes, Change cognitive demands",no such info,"example-based explanations, prediction of alternative, counterfactual explanations",NA,NA,Yes,Yes
2-4694,aisel,Enhancing service chatbot effectiveness: the effect of dyadic communication traits on consumer unplanned purchase,"Owing to the rapid development of artificial intelligence, especially, natural language processing, service chatbot has been pervasively applied in online service. However, it remains to be a challenge for online retailers to attract and convert consumers with service chatbots. Various design artifacts have been studied to promote service chatbot effectiveness, among which anthropomorphism is considered one of the most influential factors. This study draws upon interpersonal attraction theory to reveal how human-machine communication traits influence consumer unplanned purchase decisions (i.e., urge to purchase and actual purchase behavior). Specifically, we examine the rule of attractions between chatbots with specific personalities and consumers with different communication styles. A mixed-method approach was adopted to examine the research model. The results will advance our understanding of human-machine communication and provide practical insights into the design of chatbots in online service.",NA,https://aisel.aisnet.org/icis2021/hci_robot/hci_robot/9,International Conference on Information Systems,"Tan, Ran;Li, Yang;Huang, Qian",2021,1,"@inproceedings{2-4694,
  title = {Enhancing service chatbot effectiveness: the effect of dyadic communication traits on consumer unplanned purchase},
  author = {Tan, Ran and Li, Yang and Huang, Qian},
  year = {2021},
  booktitle = {International Conference on Information Systems}
}",Empirical contributions,Finance / Business / Economy,Individual,"Advising, Executing, Collaborating","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-4699,aisel,Explainable ai in healthcare: factors influencing medical practitioners' trust calibration in collaborative tasks,"Artificial intelligence is transforming clinical decision-making processes by using patient data for improved diagnosis and treatment. However, the increasing black box nature of AI systems presents comprehension challenges for users. To ensure the safe and efficient utilisation of these systems, it is essential to establish appropriate levels of trust. Accordingly, this study aims to answer the following research question: What factors influence medical practitioners' trust calibration in their interactions with AI-based clinical decision support systems (CDSSs)? Applying an exploratory approach, the data is collected through semi-structured interviews with medical and AI experts, and is examined through qualitative content analysis. The results indicate that perceived understandability, technical competence and reliability of the system, along with other userand context-related factors, impact physicians' trust calibration in AI-based CDSSs. As there is limited literature on this specific topic, our findings provide a foundation for future studies aiming to delve deeper into this field.",NA,https://aisel.aisnet.org/hicss-57/hc/process/7,Hawaii International Conference on System Sciences,"Darvish, Mahdieh;Holst, Jan-Hendrik;Bick, Markus",2024,3,"@inproceedings{2-4699,
  title     = {Explainable AI in Healthcare: Factors Influencing Medical Practitioners' Trust Calibration in Collaborative Tasks},
  author    = {Darvish, Mahdieh and Holst, Jan-Hendrik and Bick, Markus},
  year      = {2024},
  booktitle = {Hawaii International Conference on System Sciences}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising, Explaining","Decision-maker, Knowledge provider","Alter decision outcomes, Change trust, Change cognitive demands",Update AI competence,"preliminary diagnoses, explanations",NA,"Textual, Visual, Conversational/Natural Language, Autonomous System",Yes,Yes
2-4702,aisel,Explaining data-driven decisions made by ai systems: the counterfactual approach,"We examine counterfactual explanations for explaining the decisions made by model-based AI systems. The counterfactual approach we consider defines an explanation as a set of the system's data inputs that causally drives the decision (i.e., changing the inputs in the set changes the decision) and is irreducible (i.e., changing any subset of the inputs does not change the decision). We (1) demonstrate how this framework may be used to provide explanations for decisions made by general data-driven AI systems that can incorporate features with arbitrary data types and multiple predictive models, and (2) propose a heuristic procedure to find the most useful explanations depending on the context. We then contrast counterfactual explanations with methods that explain model predictions by weighting features according to their importance (e.g., Shapley additive explanations [SHAP], local interpretable model-agnostic explanations [LIME]) and present two fundamental reasons why we should carefully consider whether importance-weight explanations are well suited to explain system decisions. Specifically, we show that (1) features with a large importance weight for a model prediction may not affect the corresponding decision, and (2) importance weights are insufficient to communicate whether and how features influence decisions. We demonstrate this with several concise examples and three detailed case studies that compare the counterfactual approach with SHAP to illustrate conditions under which counterfactual explanations explain data-driven decisions better than importance weights.",NA,https://aisel.aisnet.org/misq/vol46/iss3/16,Management Information Systems Quarterly,"Fern{\'a}ndez-Lor{\'\i}a, Carlos;Provost, Foster;Han, Xintian",2022,110,"@article{2-4702,
  title={Explaining data-driven decisions made by AI systems: the counterfactual approach},
  author={Fern{\'a}ndez-Lor{\'\i}a, Carlos and Provost, Foster and Han, Xintian},
  year={2022},
  journal={Management Information Systems Quarterly}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Operational,"Explaining, Advising","Developer, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-471,aaai,ExpeL: LLM Agents Are Experiential Learners,"The recent surge in research interest in applying large language models( LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the models generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning( ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.",10.1609/aaai.v38i17.29936,https://ojs.aaai.org/index.php/AAAI/article/view/29936,AAAI Conference on Artificial Intelligence,Andrew Zhao;Daniel Huang;Quentin Xu;Matthieu Lin;Yong-Jin Liu;Gao Huang,2024,38,"@inproceedings{2-471,
  title={ExpeL: LLM Agents Are Experiential Learners},
  author={Zhao, Andrew and Huang, Daniel and Xu, Quentin and Lin, Matthieu and Liu, Yong-Jin and Huang, Gao},
  year={2024},
  doi={10.1609/aaai.v38i17.29936},
  booktitle={AAAI Conference on Artificial Intelligence}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research",no such info,"Executing, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4711,aisel,Exploring xai users' needs: a novel approach to personalize explanations using contextual bandits,"Explainable Artificial Intelligence (XAI) aims at automatically generating explanations along AI decisions to assist users in interacting with AI systems. In the realm of user-centric XAI, research efforts focus on XAI approaches that specifically address users' needs for interpretability. In the ultimate consequence, explanations must be individually tailored to each individual user's needs. Against this background, we propose a novel approach to personalize automatically generated explanations along AI decisions. In a given context with given objectives of explanations, our XAI Personalization Approach automatically adjusts the hyperparameters of an XAI method for individual users. The approach processes individual user interaction data based on contextual bandits to increase the effectiveness of resulting explanations. We demonstrate the practical applicability of our approach and evaluate its efficacy in a user study with the task of image classification. Results suggest that explanations personalized by our approach are more effective than one-size-fits-all user-centric explanations.",NA,https://aisel.aisnet.org/ecis2024/track07_busanalytics/track07_busanalytics/13,European Conference on Information Systems,"Schr{\""o}ppel, Philipp;F{\""o}rster, Maximilian",2024,4,"@inproceedings{2-4711,
  title = {Exploring XAI Users' Needs: A Novel Approach to Personalize Explanations Using Contextual Bandits},
  author = {Schr{\""o}ppel, Philipp and F{\""o}rster, Maximilian},
  year = {2024},
  booktitle = {European Conference on Information Systems}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Individual,"Explaining, Advising, Executing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4714,aisel,Factors that influence the adoption of human-ai collaboration in clinical decision-making,"Recent developments in Artificial Intelligence (AI) have fueled the emergence of human-AI collaboration, a setting where AI is a coequal partner. Especially in clinical decision-making, it has the potential to improve treatment quality by assisting overworked medical professionals. Even though research has started to investigate the utilization of AI for clinical decision-making, its potential benefits do not imply its adoption by medical professionals. While several studies have started to analyze adoption criteria from a technical perspective, research providing a human-centered perspective with a focus on AI's potential for becoming a coequal team member in the decision-making process remains limited. Therefore, in this work, we identify factors for the adoption of human-AI collaboration by conducting a series of semi-structured interviews with experts in the healthcare domain. We identify six relevant adoption factors and highlight existing tensions between them and effective human-AI collaboration.",NA,https://aisel.aisnet.org/ecis2022_rp/139,European Conference on Information Systems,"Hemmer, Patrick;Schemmer, Max;Riefle, Lara;Rosellen, Nico;V{\""o}ssing, Michael;Kuehl, Niklas",2022,39,"@inproceedings{2-4714,
  title={Factors that influence the adoption of human-ai collaboration in clinical decision-making},
  author={Hemmer, Patrick and Schemmer, Max and Riefle, Lara and Rosellen, Nico and V{\""o}ssing, Michael and Kuehl, Niklas},
  year={2022},
  booktitle={European Conference on Information Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Collaborating, Advising","Knowledge provider, Decision-maker",Alter decision outcomes,no such info,"prediction of alternative, AI knowledge",NA,"Textual, Visual, Conversational/Natural Language",Yes,Yes
2-4719,aisel,Feedback loops in machine learning: a study on the interplay of continuous updating and human discrimination,"Machine learning (ML) models often endogenously shape the data available for future updates. This is important because of their role in influencing human decisions, which then generate new data points for training. For instance, if an ML prediction results in the rejection of a loan application, the bank forgoes the opportunity to record the applicant's actual creditworthiness, thereby impacting the availability of this data point for future model updates and potentially affecting the model's performance. This paper delves into the relationship between the continuous updating of ML models and algorithmic discrimination in environments where predictions endogenously influence the creation of new training data. Using comprehensive simulations based on secondary empirical data, we examine the dynamic evolution of an ML model's fairness and economic consequences in a setting that mirrors sequential interactions, such as loan approval decisions. Our findings indicate that continuous updating can help mitigate algorithmic discrimination and enhance economic efficiency over time. Importantly, we provide evidence that human decision makers in the loop who possess the authority to override ML predictions may impede the self-correction of discriminatory models and even induce initially unbiased models to become discriminatory with time. These findings underscore the complex sociotechnological nature of algorithmic discrimination and highlight the role that humans play in addressing it when ML models undergo continuous updating. Our results have important practical implications, especially considering the impending regulations mandating human involvement in ML-supported decision-making processes.",NA,https://aisel.aisnet.org/jais/vol25/iss4/9,Journal of the Association for Information Systems,"Bauer, Kevin;Heigl, Rebecca;Hinz, Oliver;Kosfeld, Michael",2024,4,"@article{2-4719,
  title = {Feedback Loops in Machine Learning: A Study on the Interplay of Continuous Updating and Human Discrimination},
  author = {Bauer, Kevin and Heigl, Rebecca and Hinz, Oliver and Kosfeld, Michael},
  year = {2024},
  journal = {Journal of the Association for Information Systems}
}",Empirical contributions,"Finance / Business / Economy, Generic / Abstract / Domain-agnostic",Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-4722,aisel,"Follow me, everything is alright (or not): the impact of explanations on appropriate reliance on artificial intelligence","Artificial Intelligence (AI) has the potential to augment human decision making in an astonishing variety of domains. However, its opaque nature is a barrier to appropriate reliance on AI-based decision support. One possible solution stems from the research field of Explainable AI (XAI), creating automatically-generated explanations to make the inner functioning of AI understandable to humans. Our research on XAI focuses on understanding the impact of explanations alongside confidence scores on appropriate reliance on AI-based decision support systems. To this end, we conducted a randomized, between-subjects online experiment with 126 participants performing an image classification task. We find that while XAI-based explanations alongside confidence scores improve AI users' relative positive self-reliance, they simultaneously reduce users' relative positive AI-reliance. Thus, explanations alongside confidence scores can help reduce AI-overreliance but run the risk of causing AI-underreliance. Our findings help advance the understanding of explanations as facilitators of appropriate reliance on AI systems.",NA,https://aisel.aisnet.org/hicss-57/da/xai/2,Hawaii International Conference on System Sciences,"Walter, Marie Christine",2024,1,"@inproceedings{2-4722,
  title={Follow me, everything is alright (or not): the impact of explanations on appropriate reliance on artificial intelligence},
  author={Walter, Marie Christine},
  year={2024},
  booktitle={Hawaii International Conference on System Sciences}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Analyzing, Explaining, Advising",Decision-maker,"Change trust, Alter decision outcomes, Change affective-perceptual","Update AI competence, Change AI responses","confidence score, XAI suggestions",NA,"Visual, Textual",Yes,Yes
2-4730,aisel,Heuristics and errors in xai-augmented clinical decision-making: moving beyond algorithmic appreciation and aversion,"How do physicians integrate AI tools into medical decision-making? Prior research has analyzed extensively whether they exhibit AI algorithmic aversion or appreciation. Yet we argue that these behavioral outcomes arise from underlying decision-making heuristics such as pro-innovation bias, ambiguity aversion, or commitment bias. In this qualitative study, we examined 330 clinical decisions using ``think aloud'' protocols to identify heuristics employed with AI and explainable AI (XAI). We observed the presence of multiple heuristics, including a ``mere exposure effect'' and ``false confirmation bias''. These heuristics were associated with decision-making errors. The ``mere exposure effect'' occurred commonly with XAI, when physicians, feeling uncertain about their diagnoses, altered their decision to an incorrect AI diagnosis. False confirmation errors also emerged when AI confirmed an erroneous diagnosis, precluding doctors from seeking alternative information. We also discuss how cognitive interventions could redress these heuristics in decision-making to better optimize accuracy.",NA,https://aisel.aisnet.org/ecis2024/track03_ai/track03_ai/11,European Conference on Information Systems,"Rosenbacke, Rikard",2024,2,"@inproceedings{2-4730,
  title = {Heuristics and Errors in XAI-Augmented Clinical Decision-Making: Moving Beyond Algorithmic Appreciation and Aversion},
  author = {Rosenbacke, Rikard},
  year = {2024},
  booktitle = {European Conference on Information Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Explaining","Decision-maker, Knowledge provider","Alter decision outcomes, Change trust, Change cognitive demands, Restrict human agency",no such info,"textual explanations, recommendations, system accuracy",domain knowledge,Textual,Yes,Yes
2-4731,aisel,Hiring algorithms: an ethnography of fairness in practice,"While increasing attention in society is given to the role of AI in affording and threatening ethical values, such as fairness, little is known about how ethical values and AI are played out in organizations. Building on an ethnographic in-depth study of a large multinational company that recently implemented AI to enable a fair recruitment process, we show that AI brings to the fore the role of fairness in decision-making in several ways. We reveal that the development and use of AI does not necessarily improve nor degrade ethical values, but instead shapes what comes to be understood as ethical in the first place. We extend the conversations on AI by showing that it may not be enough to focus on changes in work practices, occupational boundaries, and power relations, but that research should take into account the role of AI in shaping what we consider ethical.",NA,https://aisel.aisnet.org/icis2019/future_of_work/future_work/6,International Conference on Information Systems,"van den Broek, Elmira;Sergeeva, Anastasia;Huysman, Marleen",2019,5,"@inproceedings{2-4731,
  title     = {Hiring algorithms: an ethnography of fairness in practice},
  author    = {van den Broek, Elmira and Sergeeva, Anastasia and Huysman, Marleen},
  year      = {2019},
  booktitle = {International Conference on Information Systems}
}",Empirical contributions,Everyday / Employment / Public Service,Operational,"Advising, Executing","Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-4733,aisel,How ai-based systems can induce reflections: the case of ai-augmented diagnostic work,"This paper addresses a thus-far neglected dimension in human-artificial intelligence (AI) augmentation: machine-induced reflections. By establishing a grounded theoretical-informed model of machine-induced reflection, we contribute to the ongoing discussion in information systems (IS) regarding AI and research on reflection theories. In our multistage study, physicians used a machine learning-based (ML) clinical decision support system (CDSS) to see if and how this interaction can stimulate reflective practice in the context of an X-ray diagnosis task. By analyzing verbal protocols, performance metrics, and survey data, we developed an integrative theoretical foundation to explain how ML-based systems can help stimulate reflective practice. Individuals engage in more critical or shallower modes depending on whether they perceive a conflict or agreement with these CDSS systems, which in turn leads to different levels of reflection depth. By uncovering the process of machine-induced reflections, we offer IS research a different perspective on how such AI-based systems can help individuals become more reflective, and consequently more effective, professionals. This perspective stands in stark contrast to the traditional, efficiency-focused view of ML-based decision support systems and also enriches theories on human-AI augmentation.",NA,https://aisel.aisnet.org/misq/vol47/iss4/5,Management Information Systems Quarterly,"Abdel-Karim, Benjamin M.;Pfeuffer, Nicolas;Carl, K. Valerie;Hinz, Oliver",2023,98,"@article{2-4733,
  title={How AI-Based Systems Can Induce Reflections: The Case of AI-Augmented Diagnostic Work},
  author={Abdel-Karim, Benjamin M. and Pfeuffer, Nicolas and Carl, K. Valerie and Hinz, Oliver},
  year={2023},
  journal={Management Information Systems Quarterly}
}",Theoretical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-4737,aisel,How much ai do you require? Decision factors for adopting ai technology,"Artificial intelligence (AI) based on machine learning technology disrupts how knowledge is gained. Nevertheless, ML's improved accuracy of prediction comes at the cost of low traceability due to its black-box nature. The field of explainable AI tries to counter this. However, for practical use in IT projects, these two research streams offer only partial advice for AI adoption as the trade-off between accuracy and explainability has not been adequately discussed yet. Thus, we simulate a decision process by implementing three best practice AI-based decision support systems for a high-stake maintenance decision scenario and evaluate the decision and attitude factors using the Analytical Hierarchy Process (AHP) through an expert survey. The combined results indicate that system performance is still the most important factor and that implementation effort and explainability are relatively even factors. Further, we found that systems using similarity-based matching or direct modeling for remaining useful life estimation performed best.",NA,https://aisel.aisnet.org/icis2020/implement_adopt/implement_adopt/10,International Conference on Information Systems,"Wanner, Jonas;Heinrich, Kai;Janiesch, Christian;Zschech, Patrick",2020,58,"@inproceedings{2-4737,
  title={How much AI do you require? Decision factors for adopting AI technology},
  author={Wanner, Jonas and Heinrich, Kai and Janiesch, Christian and Zschech, Patrick},
  year={2020},
  booktitle={International Conference on Information Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Organizational,"Forecasting, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-4738,aisel,How to achieve ethical persuasive design: a review and theoretical propositions for information systems,"Persuasive system design (PSD) is an umbrella term for designs in information systems (IS) that can influence people's attitude, behavior, or decision making for better or for worse. On the one hand, PSD can improve users' engagement and motivation to change their attitude, behavior, or decision making in a favorable way, which can help them achieve a desired outcome and, thus, improve their wellbeing. On the other hand, PSD misuse can lead to unethical and undesirable outcomes, such as disclosing unnecessary information or agreeing to terms that do not favor users, which, in turn, can negatively impact their wellbeing. These powerful persuasive designs can involve concepts such as gamification, gamblification, and digital nudging, which all have become prominent in recent years and have been implemented successfully across different sectors, such as education, e-health, e-governance, e-finance, and digital privacy contexts. However, such persuasive influence on individuals raises ethical questions as PSD can impair users' autonomy or persuade them towards a third party's goals and, hence, lead to unethical decision-making processes and outcomes. In human-computer interaction, recent advances in artificial intelligence have made this topic particularly significant. These novel technologies allow one to influence the decisions that users make, to gather data, and to profile and persuade users into unethical outcomes. These unethical outcomes can lead to psychological and emotional damage to users. To understand the role that ethics play in persuasive system design, we conducted an exhaustive systematic literature analysis and 20 interviews to overview ethical considerations for persuasive system design. Furthermore, we derive potential propositions for more ethical PSD and shed light on potential research gaps.",NA,https://aisel.aisnet.org/thci/vol14/iss4/5,AIS Transactions on Human-Computer Interaction,"Benner, Dennis;Sch{\""o}bel, Sofia Marlena;Janson, Andreas;Leimeister, Jan Marco",2022,33,"@article{2-4738,
  title = {How to achieve ethical persuasive design: a review and theoretical propositions for information systems},
  author = {Benner, Dennis and Sch{\""o}bel, Sofia Marlena and Janson, Andreas and Leimeister, Jan Marco},
  year = {2022},
  journal = {AIS Transactions on Human-Computer Interaction}
}","Empirical contributions, Survey contributions",Generic / Abstract / Domain-agnostic,Individual,"Advising, Analyzing, Collaborating",Decision-maker,"Change trust, Change affective-perceptual, Restrict human agency, Alter decision outcomes",Shape AI for accountability,"the number of PSD components, opt-in design, avoiding potential exploitation, existing asymmetrical power dynamics",NA,Interactive interface,Yes,Yes
2-4740,aisel,Human decision making in ai augmented systems: evidence from the initial coin offering market,"The growing consensus that human intelligence and artificial intelligence are complementary has led to Human-AI hybrid systems. As digital platforms incorporating human-AI hybrids, platform designers need to evaluate the influence of AI on human judgment, and how such hybrid systems perform. In this paper, we investigate: Are human decisions influenced by AI agents in high uncertainty environments, such as evaluating ICO projects? Under what situations are humans able to mitigate AI agents-induced errors? Our results suggest that in general, humans are influenced by AI agents. Humans tend to use AI as a filter to rule out low quality projects, while a high AI rating triggers human expert to apply their own judgment.",NA,https://aisel.aisnet.org/hicss-54/cl/ai_and_future_work/6,Hawaii International Conference on System Sciences,"Basu, Saunak;Garimella, Aravinda;Han, Wencui;Dennis, Alan",2021,10,"@inproceedings{2-4740,
  title = {Human decision making in AI augmented systems: evidence from the initial coin offering market},
  author = {Basu, Saunak and Garimella, Aravinda and Han, Wencui and Dennis, Alan},
  year = {2021},
  booktitle = {Hawaii International Conference on System Sciences}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Operational,"Executing, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-4742,aisel,"Human-in-the-loop ai reviewing: feasibility, opportunities, and risks","The promise of AI for academic work is bewitching and easy to envisage, but the risks involved are often hard to detect and usually not readily exposed. In this opinion piece, we explore the feasibility, opportunities, and risks of using large language models (LLMs) for reviewing academic submissions, while keeping the human in the loop. We experiment with GPT-4 in the role of a reviewer to demonstrate the opportunities and the risks we experience and ways to mitigate them. The reviews are structured according to a conference review form with the dual purpose of evaluating submissions for editorial decisions and providing authors with constructive feedback according to predefined criteria, which include contribution, soundness, and presentation. We demonstrate feasibility by evaluating and comparing LLM reviews with human reviews, concluding that current AI-augmented reviewing is sufficiently accurate to alleviate the burden of reviewing but not completely and not for all cases. We then enumerate the opportunities of AI-augmented reviewing and present open questions. Next, we identify the risks of AI-augmented reviewing, highlighting bias, value misalignment, and misuse. We conclude with recommendations for managing these risks.",NA,https://aisel.aisnet.org/jais/vol25/iss1/7,Journal of the Association for Information Systems,"Drori, Iddo;Te'eni, Dov",2024,52,"@article{2-4742,
  title={Human-in-the-loop AI reviewing: feasibility, opportunities, and risks},
  author={Drori, Iddo and Te'eni, Dov},
  year={2024},
  journal={Journal of the Association for Information Systems}
}","Vision contributions, Empirical contributions",Education / Teaching / Research,Operational,"Advising, Collaborating, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4743,aisel,Human-machine hybrid decision making with applications in auditing,"The decision making process in a variety of organizations faces substantial changes, largely as a result of advances in information technology and artificial intelligence (AI). A considerable number of decisions that were traditionally made by humans, are now made by machines. Consequently, many jobs that were held by experts in some fields are now occupied by data scientists who can build AI algorithms. In this paper, we address this change in work environments and suggest an innovative process for hybrid decision making between humans and machines. We focus on the auditing profession, but our method can be used in other human-intensive and critical fields such as healthcare, financial services public sectors, and humanitarian organizations.",NA,https://aisel.aisnet.org/hicss-55/cl/human-ai_collaboration/4,Hawaii International Conference on System Sciences,"Hooshangi, Sara;Sibdari, Soheil",2022,3,"@inproceedings{2-4743,
  title={Human-machine hybrid decision making with applications in auditing},
  author={Hooshangi, Sara and Sibdari, Soheil},
  year={2022},
  booktitle={Hawaii International Conference on System Sciences}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Operational,"Executing, Auditing, Collaborating","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-4747,aisel,I3dermoscopyapp: hacking melanoma thanks to iot technologies,"The paper introduces I3DermoscopyApp, a new declination of the Internet of Things (IoT) paradigm, designed to allow the early detection of melanoma. Even though artificial intelligence programs cannot outperform the diagnostic accuracy of expert dermatologists yet, they reveal to be very useful in providing second opinions to physicians with short clinical experience, thus improving significantly their diagnostic performance. \ \ Following this trend, an original integration of mobile app technology and well-known image processing algorithms allows the automatic analysis of pigmented skin lesions to help physicians apply a diagnostic method (Seven Point Check List) based on dermoscopy. The web-based platform makes the physician able to: i) store digital images captured by smartphones featured with a dermatoscope; ii) measure morphological and chromatic parameters of the skin lesion; iii) make a diagnostic decision according to the Seven Point Checklist method. \ \ A detailed description of the adopted techniques, together with the first validation results are reported.",NA,https://aisel.aisnet.org/hicss-50/hc/apps_for_health_management/4,Hawaii International Conference on System Sciences,"Di Leo, Giuseppe;Liguori, Consolatina;Paciello, Vincenzo;Pietrosanto, Antonio;Sommella, Paolo",2017,4,"@inproceedings{2-4747,
  title = {I3dermoscopyapp: Hacking Melanoma Thanks to IoT Technologies},
  author = {Di Leo, Giuseppe and Liguori, Consolatina and Paciello, Vincenzo and Pietrosanto, Antonio and Sommella, Paolo},
  year = {2017},
  booktitle = {Hawaii International Conference on System Sciences}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4748,aisel,Identification of decision rules from legislative documents using machine learning and natural language processing,"Decision logic extraction from natural language texts can be a tedious, labor-intensive task. This is especially true for legislative texts, since they do not always follow usual speech and writing patterns. This paper explores the possibility of using machine learning and natural language processing approaches to identify decision rules within legislative documents, and ultimately provides the possibility of building an extraction algorithm on top of the solution to extract and visualize decision logic automatically. Such a novel method for decision rules identification bears the potential to reduce human labor, minimize mistakes, and lessen context dependency. To accomplish this, we use pre-trained word vectorization in conjunction with a complex multi-layer convolutional neural network (CNN). The relevant data used in this project was generated from the Austrian income tax code and labeled by hand. A quantitative evaluation shows that our approach can be trained on as little as a single code of law and still obtain significant accuracy.",NA,https://aisel.aisnet.org/hicss-55/os/business_rule/4,Hawaii International Conference on System Sciences,"Michel, Maximilian;Djurica, Djordje;Mendling, Jan",2022,12,"@inproceedings{2-4748,
  title={Identification of decision rules from legislative documents using machine learning and natural language processing},
  author={Michel, Maximilian and Djurica, Djordje and Mendling, Jan},
  year={2022},
  booktitle={Hawaii International Conference on System Sciences}
}",Methodological contributions,Law / Policy / Governance,Institutional,"Analyzing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4750,aisel,Impact of artificial intelligence on human decision making on ico platforms,"As artificial intelligence is increasingly used in assisting and augmenting different tasks, it is now imperative to understand how A.I. influences human decision making. Using data collected from a leading ICO evaluation platform, we empirically investigate whether and how human domain experts' judgments are influenced by A.I. bots. Our preliminary results suggest that experts are consistently influenced by artificial intelligence regardless of whether the project is successful or not, and whether the A.I. bot's evaluation was correct or wrong. Our results also indicate that going through a Pre-ICO stage reduces information asymmetry and therefore mitigates the influence of A.I. on experts. This paper contributes to the nascent literature in the areas of human-A.I. interaction, A.I.-facilitated decision making and A.I.-induced biases.",NA,https://aisel.aisnet.org/icis2019/human_computer_interact/human_computer_interact/14,International Conference on Information Systems,"Basu, Saunak;Han, Wencui;Garimella, Aravinda",2019,6,"@inproceedings{2-4750,
  title={Impact of artificial intelligence on human decision making on ICO platforms},
  author={Basu, Saunak and Han, Wencui and Garimella, Aravinda},
  year={2019},
  booktitle={International Conference on Information Systems}
}",Empirical contributions,"Finance / Business / Economy, Generic / Abstract / Domain-agnostic",Operational,"Monitoring, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-4751,aisel,Impact of explainable ai on cognitive load: insights from an empirical study,"While the emerging research field of explainable artificial intelligence (XAI) claims to address the lack of explainability in high-performance machine learning models, in practice XAI research targets developers rather than actual end-users. Unsurprisingly, end-users are unwilling to use XAI-based decision support systems. Similarly, there is scarce interdisciplinary research on end-users' behavior during XAI explanations usage, rendering it unknown how explanations may impact cognitive load and further affect end-user performance. Therefore, we conducted an empirical study with 271 prospective physicians, measuring their cognitive load, task performance, and task time for distinct implementation-independent XAI explanation types using a COVID-19 use case. We found that these explanation types strongly influence end-users' cognitive load, task performance, and task time. Based on these findings, we classified the explanation types in a mental efficiency matrix, ranking local XAI explanation types as best, and thereby providing recommendations for future applications and implications for sociotechnical XAI research.",NA,https://aisel.aisnet.org/ecis2023_rp/269,European Conference on Information Systems,"Herm, Lukas-Valentin",2023,0,"@inproceedings{2-4751,
  title={Impact of explainable AI on cognitive load: insights from an empirical study},
  author={Herm, Lukas-Valentin},
  year={2023},
  booktitle={European Conference on Information Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising","Decision-maker, Decision-subject","Alter decision outcomes, Change cognitive demands",no such info,"local explanations, global explanations",NA,"Visual, Textual",Yes,Yes
2-4752,aisel,Impacts of ai tool clues on fake reviews detecting and user decision-making,"Human fake information detection ability is a critical and debated capacity. Practitioners have developed various AI detection tools to aid and alert online consumers about fake content. Based on the sociotechnical perspective and construal level theory, this paper aims to investigate how the types of clues provided by AI detection tools influence human fake information detection ability, measured through decision-making competence (including resistance to fake information and cognitive biases). Online vigilance is proposed as a potential underlying mechanism. Additionally, this research introduces two key boundaries: the expectation gap and the stages of detection tool usage. Theoretical and practical contributions are discussed.",NA,https://aisel.aisnet.org/icis2024/humtechinter/humtechinter/13,International Conference on Information Systems,"LIU, Yue;Sia, Choon-Ling",2024,0,"@inproceedings{2-4752,
  title={Impacts of AI tool clues on fake reviews detecting and user decision-making},
  author={Liu, Yue and Sia, Choon-Ling},
  year={2024},
  booktitle={International Conference on Information Systems}
}","Empirical contributions, Theoretical contributions",Media / Communication / Entertainment,Individual,"Analyzing, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-4756,aisel,"Incorporating stakeholder enfranchisement, risks, gains, and ai decisions in ai governance framework","The emergence of AI-enabled applications has drawn attention to the need for AI governance. This essay builds on organizational governance literature and proposes a framework for developing organizational governance structures. Following a call to incorporate all stakeholders in governance [1], the framework considers the interests of all organizational stakeholder groups. In addition, it delineates five types of AI-related organizational decisions, which have the potential to significantly impact stakeholder interests. Furthermore, the framework considers four distinct outcomes and byproducts of AI which may impact the distribution of stakeholder benefits and risks. These need to be specifically addressed by organizational AI governance structures. We contend that the details furnished by the framework pave the way for future research on AI governance, adaptation in an AI-driven organization, and AI-related legal framework development.",NA,https://aisel.aisnet.org/hicss-55/os/ai_and_organizing/5,Hawaii International Conference on System Sciences,"Sidorova, Anna;Saeed, Kashif",2022,5,"@inproceedings{2-4756,
  title = {Incorporating Stakeholder Enfranchisement, Risks, Gains, and AI Decisions in AI Governance Framework},
  author = {Sidorova, Anna and Saeed, Kashif},
  year = {2022},
  booktitle = {Hawaii International Conference on System Sciences}
}",Theoretical contributions,Law / Policy / Governance,Institutional,Executing,"Decision-maker, Decision-subject, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-4765,aisel,Intelligent technologies supporting the management of a smart city. Qualitative approach,"Intelligent technologies such as Business Intelligence systems, big data, artificial intelligence including machine learning and cognitive technologies play crucial role in the process of a smart city management. The aim of the paper is to indicate the role of intelligent solutions in the management of a contemporary city, particularly focusing on the support of decision making process. The research methodology is based on a qualitative approach where six case studies were conducted in the selected big cities in Poland in 2021 year. The respondents belonged to the group of mainly managers of IT departments in the cities. The case study analyses showed that implemented intelligent solutions in the process of a smart city management positively and significantly affect efficacy, efficiency, quality, and acceleration of the decision-making process and also support the creation of a particular city development strategy. The paper puts also an emphasis on the review of AI applications within the concept of smart city in a big worldwide metropolies.",NA,https://aisel.aisnet.org/hicss-56/cl/bi_for_organizations/2,Hawaii International Conference on System Sciences,"Sobczak, Andrzej;Ziora, Leszek",2023,4,"@inproceedings{2-4765,
  title = {Intelligent technologies supporting the management of a smart city. Qualitative approach},
  author = {Sobczak, Andrzej and Ziora, Leszek},
  year = {2023},
  booktitle = {Hawaii International Conference on System Sciences}
}",Empirical contributions,Transportation / Mobility / Planning,Organizational,"Analyzing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4768,aisel,Introducing (machine) learning ability as antecedent of trust in intelligent systems,"Artificial intelligence enables the emergence of novel intelligent decision support systems (IDSSs). Despite the potential for increased efficiency, mixed evidence on user aversion to or appreciation for such intelligent systems prevails, questioning user trust in algorithmic decision support. Recent advances in machine learning facilitate the incorporation of a promising driver of trust into the systems: the systems' ability to learn. In this study, we conduct an experiment, manipulating the type of decision support (human vs. automated) and their learning ability in the context of a clinical decision support system. Results indicate increased trust in automated decision support with the ability to learn. Our findings contribute to theory and practice, identifying (machine) learning as an antecedent of trust, thereby enhancing our understanding of user perceptions of IDSSs. Furthermore, we add to literature on algorithm aversion by showing that people readily rely on algorithmic support in the context of clinical decision making.",NA,https://aisel.aisnet.org/ecis2021_rp/23,European Conference on Information Systems,"Lohoff, Laura;R{\""u}hr, Alexander",2021,2,"@inproceedings{2-4768,
  title={Introducing (machine) learning ability as antecedent of trust in intelligent systems},
  author={Lohoff, Laura and R{\""u}hr, Alexander},
  year={2021},
  booktitle={European Conference on Information Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Executing",Decision-maker,"Alter decision outcomes, Change trust",no such info,learning,NA,Autonomous System,Yes,Yes
2-4771,aisel,Is ethics really such a big deal? The influence of perceived usefulness of ai-based surveillance technology on ethical decision-making in scenarios of public surveillance,"So far, ethical perspectives have been neglected in empirical research focusing on the acceptance of artificial intelligence (AI)-based surveillance technologies on an individual level. This paper addresses this research gap by examining the individual moral intent to accept AI-based surveillance technologies deployed in public scenarios. After a thorough literature review to identify antecedents of moral intent, we surveyed n = 112 American participants in an online survey on mTurk and analyzed the data by using a fuzzy set qualitative comparative analysis. The resulting antecedent configurations provide insights into the inherent ethical decision-making process and thus contribute to a better understanding of the causality for accepting or rejecting AI-based surveillance technologies. Our findings emphasize in particular the influence of perceived usefulness of the technology on the ethical decision-making process.",NA,https://aisel.aisnet.org/hicss-54/dg/digital_society/4,Hawaii International Conference on System Sciences,"Anton, Eduard;Kus, Kevin;Teuteberg, Frank",2021,2,"@inproceedings{2-4771,
  title = {Is ethics really such a big deal? The influence of perceived usefulness of AI-based surveillance technology on ethical decision-making in scenarios of public surveillance},
  author = {Anton, Eduard and Kus, Kevin and Teuteberg, Frank},
  year = {2021},
  booktitle = {Hawaii International Conference on System Sciences}
}",Empirical contributions,Defense / Military / Emergency,Institutional,"Advising, Monitoring","Decision-maker, Stakeholder",Shape ethical norms,no such info,classification,"privacy concern, public safety","Textual, Visual",Yes,Yes
2-4772,aisel,It depends on the timing: the ripple effect of ai on team decision-making,"Whereas artificial intelligence (AI) is increasingly used to facilitate team decision-making, little is known about how the timing of AI assistance may impact team performance. The study investigates this question with an online experiment in which teams completed a new product development task with assistance from a chatbot. Information needed for making the decision was distributed among the team members. The chatbot shared information critical to the decision in either the first half or second half of team interaction. The results suggest that teams assisted by the chatbot in the first half of the decision-making task made better decisions than those assisted by the chatbot in the second half. Analysis of team member perceptions and interaction processes suggests that having a chatbot at the beginning of team interaction may have generated a ripple effect in the team that promoted information sharing among team members.",NA,https://aisel.aisnet.org/hicss-56/cl/machines_as_teammates/2,Hawaii International Conference on System Sciences,"Yan, Bei;Gurkan, Necdet",2023,0,"@inproceedings{2-4772,
  title={It depends on the timing: the ripple effect of AI on team decision-making},
  author={Yan, Bei and Gurkan, Necdet},
  year={2023},
  booktitle={Hawaii International Conference on System Sciences}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Advising, Analyzing",Decision-maker,"Alter decision outcomes, Change cognitive demands",no such info,"first-half assistance, second-half assistance",NA,Physical / Embodiment,Yes,Yes
2-4774,aisel,Local post-hoc explanations for predictive process monitoring in manufacturing,"This study proposes an innovative explainable predictive quality analytics solution to facilitate the data-driven decision-making for process planning in manufacturing by combining process mining, machine learning, and explainable artificial intelligence (XAI) methods. For this purpose, after integrating the top-floor and shop-floor data obtained from various enterprise information systems, a deep learning model was applied to predict the process outcomes. Since this study aims to operationalize the delivered predictive insights by embedding them into decision-making processes, it is essential to generate the relevant explanations for domain experts. To this end, two complementary local post-hoc explanation approaches, Shapley values and Individual Conditional Expectation (ICE) plots are adopted, which are expected to enhance the decision-making capabilities by enabling experts to examine explanations from different perspectives. After assessing the predictive strength of the applied deep neural network with relevant binary classification evaluation measures, a discussion of the generated explanations is provided.",NA,https://aisel.aisnet.org/ecis2021_rp/35,European Conference on Information Systems,"Mehdiyev, Nijat;Fettke, Peter",2021,22,"@inproceedings{2-4774,
  title={Local post-hoc explanations for predictive process monitoring in manufacturing},
  author={Mehdiyev, Nijat and Fettke, Peter},
  year={2021},
  booktitle={European Conference on Information Systems}
}",System/Artifact contributions,Manufacturing / Industry / Automation,Operational,"Explaining, Forecasting, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-4775,aisel,Longitudinal healthcare analytics for early detection and progression of neurological diseases: a clinical decision support system.,"Neurological diseases, including Alzheimer's disease (AD), are rising global health challenges. This study presents a two-stage decision support system (DSS) that uses machine learning and neuroimaging for early AD detection and monitoring. The first stage uses deep learning for predicting AD likelihood. The second leverages a 3D convolutional neural network to identify crucial brain regions in AD progression. Notably, the DSS offers a solution to machine learning's ""black box"" problem using an occlusion map explainability method, enhancing decision transparency. Its design is adaptable to other diseases using imaging data, underscoring its broad healthcare potential. By providing an innovative and interpretable tool for improved disease management, this research helps foster better patient care and outcomes.",NA,https://aisel.aisnet.org/hicss-57/hc/process/4,Hawaii International Conference on System Sciences,"Owusu, Gabriel;Wang, Xuan;Sun, Jun",2024,7,"@inproceedings{2-4775,
  title = {Longitudinal healthcare analytics for early detection and progression of neurological diseases: a clinical decision support system},
  author = {Owusu, Gabriel and Wang, Xuan and Sun, Jun},
  year = {2024},
  booktitle = {Hawaii International Conference on System Sciences}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Explaining, Auditing, Advising","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-4779,aisel,"Machine learning, analytics and strategic decision in the regulated energy industry","Requests have been made for more study in energy informatics where IS research can demonstrate academic social responsibility and create new opportunities within the field (Watson et al. 2010). Other calls have been made to incorporate Big Data, intelligence and intelligent systems to help understand strategic decision making for organizations (Gholami et al. 2016; Ketter et al. 2016). It remains unclear how the use of machine learning (ML) and analytics along with Big Data as part of the larger Information System (IS) in non-profit energy sector firms can be effectively used to inform strategic outcomes and decisions. This research contributes to extant IS literature by advancing insights related to the role of ML, Analytics and Big Data in strategic outcome and decisions by top management in the non-profit firms within the electrical energy sector.",NA,https://aisel.aisnet.org/icis2018/green/Presentations/8,International Conference on Information Systems,"Hodges, Darin C.;Salam, A. F.",2018,2,"@inproceedings{2-4779,
  title={Machine learning, analytics and strategic decision in the regulated energy industry},
  author={Hodges, Darin C. and Salam, A. F.},
  year={2018},
  booktitle={International Conference on Information Systems}
}",Empirical contributions,Environment / Resources / Energy,Organizational,"Analyzing, Advising",Decision-maker,"Alter decision outcomes, Change affective-perceptual, Change cognitive demands","Update AI competence, Change AI responses","prediction of alternative, risk assessment’s prediction",NA,NA,Yes,Yes
2-4787,aisel,"Me, you or ai? How do we feel about delegation","Driven by the growing availability of data and information paired with the human brain's limited pro-cessing capabilities, Artificial Intelligence (AI) has become an increasingly relevant actor in strategic decision making. To leverage the potential AI provides for strategic decision making, firms rely on managers that are willing to transfer authority and control to AI-based decision systems. By analyzing the willingness to delegate strategic decisions and emotional responses to outcomes of delegated deci-sions, this research contributes to a better understanding of human behavior in decision delegation to AI in the context of strategic decisions. Based on a policy-capturing study, we find that humans are less likely to delegate a strategic decision to AI, than to another person. Further, our findings reveal that emotional responses to the outcomes of delegated decisions are more intense when responsibility was delegated to another human being, rather than to an AI-enabled system.",NA,https://aisel.aisnet.org/ecis2019_rp/36,International Conference on Information Systems (ICIS),"Leyer, Michael;Schneider, Sabrina",2019,0,"@inproceedings{2-4787,
  title = {Me, you or AI? How do we feel about delegation},
  author = {Leyer, Michael and Schneider, Sabrina},
  year = {2019},
  booktitle = {International Conference on Information Systems (ICIS)}
}",Empirical contributions,Finance / Business / Economy,Operational,"Advising, Executing",Decision-maker,"Change affective-perceptual, Alter decision outcomes",no such info,NA,"delegation, less likely to delegate a strategic decision to AI",Textual,Yes,Yes
2-4792,aisel,Monte-carlo simulation based on patient-individual distributions for supporting intensive care occupancy management,"Managing Intensive Care Units (ICUs) in hospitals is a highly challenging endeavor. In particular, decisions such as admitting elective patients and discharging patients from the ICU have to be taken under a high level of uncertainty since the occupancy of ICUs does not only depend on these decisions but also on unknown parameters such emergency patient arrivals and lengths of stay of the patients in the ICU. In this paper, we develop a framework for supporting ICU occupation management by quantifying the impact of admission and discharge decisions on the probability of reaching critical ICU occupancy levels in a given planning horizon. A key component of this framework is the use of data-driven approaches for obtaining probability distributions for the parameters affected by uncertainty. In particular, we use standardized treatment and patient health state data to create patient-specific length-of-stay distributions with a Machine Learning approach. These patient-individual distributions are then validated and/or adjusted by medical experts. The validated distributions form the input to a Monte-Carlo Simulation that is used to approximate the probability distributions of the daily ICU occupancy levels resulting from ICU admission and discharge decisions. We experimentally evaluate our framework in a counterfactual simulation based on one year of historical data from 2019 from a medium-sized ICU in a German hospital. In that evaluation, we use a simple ICU management policy based on the probabilistic occupancy forecasts aiming at reducing the risk of running out of ICU capacity. The results show that following this policy would have avoided hitting critical occupancy levels by around 70% and would have had a smoothing effect on ICU occupancy levels.",NA,https://aisel.aisnet.org/hicss-55/hc/process/6,Hawaii International Conference on System Sciences,"Witteborg, Arne;Borgstedt, Rainer;G{\""u}nther, Markus;Jansen, Gerrit;R{\""o}mer, Michael",2022,0,"@inproceedings{2-4792,
  title = {Monte-carlo simulation based on patient-individual distributions for supporting intensive care occupancy management},
  author = {Witteborg, Arne and Borgstedt, Rainer and G{\""u}nther, Markus and Jansen, Gerrit and R{\""o}mer, Michael},
  year = {2022},
  booktitle = {Hawaii International Conference on System Sciences}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-4796,aisel,Natural language processing for medical texts -- a taxonomy to inform integration decisions into clinical practice,"Electronic health records (EHR) have significantly amplified the volume of information accessible in the healthcare sector. Nevertheless, this information load also translates into elevated workloads for clinicians engaged in extracting and generating patient information. Natural Language Process (NLP) aims to overcome this problem by automatically extracting and structuring relevant information from medical texts. While other methods related to artificial intelligence have been implemented successfully in healthcare (e.g., computer vision in radiology), NLP still lacks commercial success in this domain. The lack of a structured overview of NLP systems is exacerbating the problem, especially with the emergence of new technologies like generative pre-trained transformers. Against this background, this paper presents a taxonomy to inform integration decisions of NLP systems into healthcare IT landscapes. We contribute to a better understanding of how NLP systems can be integrated into daily clinical contexts. In total, we reviewed 29 papers and 36 commercial NLP products.",NA,https://aisel.aisnet.org/icis2023/ishealthcare/ishealthcare/8,International Conference on Information Systems,"Braun, Marvin;Kolbe, Lutz;Neumann, Caspar",2023,2,"@inproceedings{2-4796,
  title = {Natural language processing for medical texts -- a taxonomy to inform integration decisions into clinical practice},
  author = {Braun, Marvin and Kolbe, Lutz and Neumann, Caspar},
  year = {2023},
  booktitle = {International Conference on Information Systems}
}",Theoretical contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Decision-maker, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-4798,aisel,Navigating explainability: a comparative field study of how professionals explain ai-made decisions to clients,"As artificial intelligence (AI) systems increasingly make impactful decisions in the workplace, issues of explainability have gained prominence. However, current debates around explainability of AI either take on a technical perspective or focus on the use of AI for augmentation, in which professionals can decide to ignore or override AI outputs when hindered by opacity. Given that current AI tools have the increasing ability to act on their own, this calls for a deeper understanding of how professionals manage explainability in cases of AI automation. Building on a comparative field study, we identify different practices that professionals enacted to produce post hoc explanations to clients of decisions made by AI tools. These practices varied depending on whether professionals relied on their own expertise versus AI techniques and whether they deeply engaged with the AI tool in constructing explanations. Our preliminary findings yield important implications for the literature on AI and professions.",NA,https://aisel.aisnet.org/icis2023/aiinbus/aiinbus/12,International Conference on Information Systems,"Mayer, Anne;van den Broek, Elmira;Karacic, Tomislav;Huysman, Marleen",2023,0,"@inproceedings{2-4798,
  title={Navigating explainability: a comparative field study of how professionals explain ai-made decisions to clients},
  author={Mayer, Anne and van den Broek, Elmira and Karacic, Tomislav and Huysman, Marleen},
  year={2023},
  booktitle={International Conference on Information Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Executing, Advising, Explaining","Decision-maker, Knowledge provider",NA,NA,post hoc explanations,NA,NA,Yes,No
2-4799,aisel,Next frontiers in emergency medical services in germany: identifying gaps between academia and practice,"In recent years, an increase in data availability and computation power led to the ""rise of Artificial Intelligence (AI)"". In many different domains, AI-based methods and more specifically intelligent decision support systems (DSS) are studied in research and already implemented in practice, but not yet so in emergency medical services (EMS). This is especially true for the German EMS system that falls short in terms of digitization in general and the use of well-grounded methods for managing and planning their logistics and processes. As the actual need for intelligent DSS in the German EMS are unclear, we have performed interviews with German EMS experts. Referring to the qualitative data, we compare the decision problems and desired DSS with existing research and identify gaps between academia and practice.",NA,https://aisel.aisnet.org/hicss-54/hc/process/3,Hawaii International Conference on System Sciences,"Reuter-Oppermann, Melanie;Wolff, Clemens;Pumplun, Luisa",2021,4,"@inproceedings{2-4799,
  title={Next frontiers in emergency medical services in Germany: identifying gaps between academia and practice},
  author={Reuter-Oppermann, Melanie and Wolff, Clemens and Pumplun, Luisa},
  year={2021},
  booktitle={Hawaii International Conference on System Sciences}
}",Empirical contributions,Healthcare / Medicine / Surgery,Organizational,"Advising, Executing, Analyzing","Decision-maker, Knowledge provider","Change affective-perceptual, Change trust, Change cognitive demands",Shape AI for accountability,NA,NA,NA,Yes,Yes
2-4800,aisel,No ground truth at sea -- developing high-accuracy ai decision-support for complex environments,"As AI decision-support systems are increasingly developed for applications outside of traditional organizational confinements, developers are confronted with new sources of complexity they need to address. However, we know little about how AI applications are developed for natural use domains with high environmental complexity, stemming from physical influences outside of the developers' control. This study investigates what challenges emerge from such complexity and how developers mitigate them. Drawing upon a rich longitudinal single-case study on the development of AI decision-support for maritime navigation, findings show that achieving high output accuracy is complicated by the physical environment hindering training data creation. Further, developers chose to reduce the output accuracy and adapt the HMI design to successfully situate the AI application in an existing sociotechnical context. This study contributes to IS literature following recent calls for phenomenon-based examination of emerging challenges when extending the scope frontier of AI and provides practical recommendations for developing AI decision-support for complex environments.",NA,https://aisel.aisnet.org/hicss-56/ks/design/3,Hawaii International Conference on System Sciences,"Bumann, Adrian",2023,2,"@inproceedings{2-4800,
  author    = {Bumann, Adrian},
  title     = {No ground truth at sea -- developing high-accuracy AI decision-support for complex environments},
  year      = {2023},
  booktitle = {Hawaii International Conference on System Sciences}
}",Empirical contributions,Transportation / Mobility / Planning,Organizational,"Advising, Forecasting, Analyzing","Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-4802,aisel,Obstacles to human-ai collaboration,"Though human-AI collaboration (HAIC) is increasing, significant challenges persist in its effective adoption. Based on a systematic literature review, we propose a framework encompassing 15 obstacles to the adoption of HAIC in organizations, organized into three components of competence learning: knowledge, skills, and attitudes. Important obstacles include the lack of an AI strategy, limited understanding of AI responses, a technology readiness gap, cultural adoption resistance, as well as ethical and privacy concerns. We apply our framework to a case study of an international automotive OEM. 14 semi-structured interviews with executives provide further insights, such as categorizing the lack of technical readiness into over-reliance on human interaction, digital mindset, fear of AI, and operational readiness. Our framework provides a comprehensive overview of relevant obstacles to HAIC adoption and can help to develop effective strategies for integrating AI, leading to improved productivity and decision-making processes in organizations.",NA,https://aisel.aisnet.org/icis2024/practitioner/practitioner/2,International Conference on Information Systems,"Sudeeptha, Ishara;M{\""u}ller, Wieland;Richter, Alexander;Leyer, Michael;Nolte, Ferry",2024,2,"@inproceedings{2-4802,
  title     = {Obstacles to human-ai collaboration},
  author    = {Sudeeptha, Ishara and M{\""u}ller, Wieland and Richter, Alexander and Leyer, Michael and Nolte, Ferry},
  year      = {2024},
  booktitle = {International Conference on Information Systems}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Organizational,Collaborating,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4805,aisel,"Only a coward hides behind ai? Preferences in surrogate, moral decision-making","While decision-makers previously repeatedly struggled with information scarcity; now, they often face a seemingly unmanageable overload of information. To increase the decision quality and speed as well as to free up resources, mangers increasingly have the option to involve AI-enabled decision-making systems. However, delegating decision to AI is challenging, in particular in surrogate and ethically complex situations, e.g., in lay-off decision situations. To research the decision delegation behavior in such situations, we draw on two experimental designs and complement our findings with qualitative interview data. Our findings reveal opposing perceptions of the same situation, depending on the individual's perspective. Whereas the willingness-to-delegate a layoff decision in a surrogate decision context to AI is lower than in a non-surrogate context (decision-maker perspective), people affected by the decision do not generally prefer humans to AI (decision-affected perspective).",NA,https://aisel.aisnet.org/icis2021/ai_business/ai_business/2,International Conference on Information Systems,"Freisinger, Elena;Schneider, Sabrina",2021,1,"@inproceedings{2-4805,
  title={Only a coward hides behind AI? Preferences in surrogate, moral decision-making},
  author={Freisinger, Elena and Schneider, Sabrina},
  year={2021},
  booktitle={International Conference on Information Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Advising, Executing","Decision-maker, Decision-subject","Change trust, Alter decision outcomes, Shift responsibility",no such info,identity cues,"voting control, refusal to delegate due to accountability, delegation motivated by responsibility avoidance",NA,Yes,Yes
2-4809,aisel,Overcoming anchoring bias: the potential of ai and xai-based decision support,"Information systems (IS) are frequently designed to leverage the negative effect of anchoring bias to influence individuals' decision-making (e.g., by manipulating purchase decisions). Recent advances in Artificial Intelligence (AI) and the explanations of its decisions through explainable AI (XAI) have opened new opportunities for mitigating biased decisions. So far, the potential of these technological advances to overcome anchoring bias remains widely unclear. To this end, we conducted two online experiments with a total of N=390 participants in the context of purchase decisions to examine the impact of AI and XAI-based decision support on anchoring bias. Our results show that AI alone and its combination with XAI help to mitigate the negative effect of anchoring bias. Ultimately, our findings have implications for the design of AI and XAI-based decision support and IS to overcome cognitive biases.",NA,https://aisel.aisnet.org/icis2023/aiinbus/aiinbus/4,International Conference on Information Systems,"Haag, Felix;Stingl, Carlo;Zerfass, Katrin;Hopf, Konstantin;Staake, Thorsten",2023,0,"@inproceedings{2-4809,
  title={Overcoming anchoring bias: The potential of AI and XAI-based decision support},
  author={Haag, Felix and Stingl, Carlo and Zerfass, Katrin and Hopf, Konstantin and Staake, Thorsten},
  year={2023},
  booktitle={International Conference on Information Systems}
}",Empirical contributions,"Finance / Business / Economy, Generic / Abstract / Domain-agnostic",Individual,"Advising, Explaining",Decision-maker,"Shape ethical norms, Alter decision outcomes",no such info,textual explanations,NA,"Textual, Visual",Yes,Yes
2-4811,aisel,Perceptions of fairness and trustworthiness based on explanations in human vs. Automated decision-making,"Automated decision systems (ADS) have become ubiquitous in many high-stakes domains. Those systems typically involve sophisticated yet opaque artificial intelligence (AI) techniques that seldom allow for full comprehension of their inner workings, particularly for affected individuals. As a result, ADS are prone to deficient oversight and calibration, which can lead to undesirable (e.g., unfair) outcomes. In this work, we conduct an online study with 200 participants to examine people's perceptions of fairness and trustworthiness towards ADS in comparison to a scenario where a human instead of an ADS makes a high-stakes decision---and we provide thorough identical explanations regarding decisions in both cases. Surprisingly, we find that people perceive ADS as fairer than human decision-makers. Our analyses also suggest that people's AI literacy affects their perceptions, indicating that people with higher AI literacy favor ADS more strongly over human decision-makers, whereas low-AI-literacy people exhibit no significant differences in their perceptions.",NA,https://aisel.aisnet.org/hicss-55/da/ai_algorithms/4,Hawaii International Conference on System Sciences,"Schoeffer, Jakob;Machowski, Yvette;K{\""u}hl, Niklas",2022,25,"@inproceedings{2-4811,
  title = {Perceptions of Fairness and Trustworthiness Based on Explanations in Human vs. Automated Decision-Making},
  author = {Schoeffer, Jakob and Machowski, Yvette and K{\""u}hl, Niklas},
  year = {2022},
  booktitle = {Hawaii International Conference on System Sciences}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Explaining, Executing","Decision-maker, Decision-subject","Shape ethical norms, Change trust",no such info,"explanations, identity cues",NA,"Textual, Conversational/Natural Language",Yes,Yes
2-4813,aisel,Personalized explanation for machine learning: a conceptualization,"Explanation in machine learning and related fields such as artificial intelligence aims at making machine learning models and their decisions understandable to humans. Existing work suggests that personalizing explanations might help to improve understandability. In this work, we derive a conceptualization of personalized explanation by defining and structuring the problem based on prior work on machine learning explanation, personalization (in machine learning) and concepts and techniques from other domains such as privacy and knowledge elicitation. We perform a categorization of explainee data used in the process of personalization as well as describing means to collect this data. We also identify three key explanation properties that are amendable to personalization: complexity, decision information and presentation. We also enhance existing work on explanation by introducing additional desiderata and measures to quantify the quality of personalized explanations. Keywords: Explainable artificial intelligence, Interpretable machine learning, Personalization, Customization, Interactive machine learning",NA,https://aisel.aisnet.org/ecis2019_rp/171,International Conference on Information Systems (ICIS),"Schneider, Johannes;Handali, Joshua Peter",2019,164,"@inproceedings{2-4813,
  title={Personalized explanation for machine learning: a conceptualization},
  author={Schneider, Johannes and Handali, Joshua Peter},
  year={2019},
  booktitle={International Conference on Information Systems (ICIS)}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,no such info,"Explaining, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4815,aisel,Pilots and pixels: a comparative analysis of machine learning error effects on aviation decision making,"Despite immense improvements in machine learning (ML)-based decision support systems (DSSs), these systems are still prone to errors. For use in high-risk environments such as aviation it is critical, to find out what costs the different types of ML error cause for decision makers. Thus, we provide pilots holding a valid flight license with explainable and non-explainable ML-based DSSs that output different types of ML errors while supporting the visual detection of other aircraft in the vicinity in 222 recorded scenes of flight simulations. The study reveals that both false positives (FPs) and false negatives (FNs) detrimentally affect pilot trust and performance, with a more pronounced effect observed for FNs. While explainable ML output design mitigates some negative effects, it significantly increases the mental workload for pilots when dealing with FPs. These findings inform the development of ML-based DSSs aligned with Error Management Theory to enhance applications in high-stakes environments.",NA,https://aisel.aisnet.org/ecis2024/track06_humanaicollab/track06_humanaicollab/6,European Conference on Information Systems,"Ellenrieder, Sara;Ellenrieder, Nils;Hendriks, Patrick;Mehler, Maren",2024,0,"@inproceedings{2-4815,
  title = {Pilots and pixels: a comparative analysis of machine learning error effects on aviation decision making},
  author = {Ellenrieder, Sara and Ellenrieder, Nils and Hendriks, Patrick and Mehler, Maren},
  year = {2024},
  booktitle = {European Conference on Information Systems}
}",Empirical contributions,Transportation / Mobility / Planning,Operational,"Advising, Auditing, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4821,aisel,Process selection in rpa projects -- towards a quantifiable method of decision making,"The digital age requires companies to invest in value-creating rather than routine activities to drive innovation as a future source of competitiveness and business success. Thus, many companies are reluctant to invest in large-scale, costly backend integration projects and seek adaptable solutions to automate their front-office activities. Bridging artificial intelligence and business process management, robotic process automation (RPA) provides the promise of robots as a virtual workforce that performs these tasks in a self-determined manner. Many studies have highlighted potential benefits of RPA. However, little data is available on operationalizing and automating RPA to maximize its benefits. In this paper, we shed light on the automation potential of processes with RPA and operationalize it. Based on process mining techniques, we propose an automatable indicator system as well as present and evaluate decision support for companies that seek to better prioritize their RPA activities and to maximize their return on investment.",NA,https://aisel.aisnet.org/icis2019/business_models/business_models/6,International Conference on Information Systems,"Wanner, Jonas;Hofmann, Adrian;Fischer, Marcus;Imgrund, Florian;Janiesch, Christian;Geyer-Klingeberg, Jerome",2019,0,"@inproceedings{2-4821,
  title={Process selection in RPA projects: Towards a quantifiable method of decision making},
  author={Wanner, Jonas and Hofmann, Adrian and Fischer, Marcus and Imgrund, Florian and Janiesch, Christian and Geyer-Klingeberg, Jerome},
  year={2019},
  booktitle={International Conference on Information Systems}
}",Methodological contributions,Finance / Business / Economy,Organizational,"Monitoring, Analyzing, Advising","Developer, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-4823,aisel,Promoting learning through explainable artificial intelligence: an experimental study in radiology,"The deployment of machine learning (ML)-based decision support systems (DSSs) in high-risk environments such as radiology is increasing. Despite having achieved high decision accuracy, they are prone to errors. Thus, they are primarily used to assist radiologists in their decision making. However, collaborative decision making poses risks to the decision maker, e.g. automation bias and long-term performance degradation. To address these issues, we propose combining findings of the research streams of explainable artificial intelligence and education to promote human learning through interaction with ML-based DSSs. We provided radiologists with explainable vs non-explainable decision support that was high- vs low-performing in a between-subject experimental study to support manual segmentation of 690 brain tumor scans. Our results show that explainable ML-based DSSs improved human learning outcomes and prevented false learning triggered by incorrect decision support. In fact, radiologists were able to learn from errors made by the low-performing explainable ML-based DSS.",NA,https://aisel.aisnet.org/icis2023/learnandiscurricula/learnandiscurricula/3,International Conference on Information Systems,"Ellenrieder, Sara;Kallina, Emma Marlene;Pumplun, Luisa;Gawlitza, Joshua Felix;Ziegelmayer, Sebastian;Buxmann, Peter",2023,6,"@inproceedings{2-4823,
  title={Promoting learning through explainable artificial intelligence: an experimental study in radiology},
  author={Ellenrieder, Sara and Kallina, Emma Marlene and Pumplun, Luisa and Gawlitza, Joshua Felix and Ziegelmayer, Sebastian and Buxmann, Peter},
  year={2023},
  booktitle={International Conference on Information Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Explaining, Collaborating","Decision-maker, Knowledge provider","Alter decision outcomes, Change affective-perceptual",no such info,local explanations,NA,"Visual, Interactive interface",Yes,Yes
2-4830,aisel,Reimagining the journal editorial process: an ai-augmented versus an ai-driven future,"The editorial process at our leading information systems journals has been pivotal in shaping and growing our field. But this process has grown long in the tooth and is increasingly frustrating and challenging its various stakeholders: editors, reviewers, and authors. The sudden and explosive spread of AI tools, including advances in language models, make them a tempting fit in our efforts to ease and advance the editorial process. But we must carefully consider how the goals and methods of AI tools fit with the core purpose of the editorial process. We present a thought experiment exploring the implications of two distinct futures for the information systems powering today's journal editorial process: an AI-augmented and an AI-driven one. The AI-augmented scenario envisions systems providing algorithmic predictions and recommendations to enhance human decision-making, offering enhanced efficiency while maintaining human judgment and accountability. However, it also requires debate over algorithm transparency, appropriate machine learning methods, and data privacy and security. The AI-driven scenario, meanwhile, imagines a fully autonomous and iterative AI. While potentially even more efficient, this future risks failing to align with academic values and norms, perpetuating data biases, and neglecting the important social bonds and community practices embedded in and strengthened by the human-led editorial process. We consider and contrast the two scenarios in terms of their usefulness and dangers to authors, reviewers, editors, and publishers. We conclude by cautioning against the lure of an AI-driven, metric-focused approach, advocating instead for a future where AI serves as a tool to augment human capacity and strengthen the quality of academic discourse. But more broadly, this thought experiment allows us to distill what the editorial process is about: the building of a premier research community instead of chasing metrics and efficiency. It is up to us to guard these values.",NA,https://aisel.aisnet.org/jais/vol25/iss1/10,Journal of the Association for Information Systems,"Shmueli, Galit;Ray, Soumya",2024,8,"@article{2-4830,
  title = {Reimagining the journal editorial process: an AI-augmented versus an AI-driven future},
  author = {Shmueli, Galit and Ray, Soumya},
  year = {2024},
  journal = {Journal of the Association for Information Systems}
}","Vision contributions, Empirical contributions",Media / Communication / Entertainment,Organizational,"Forecasting, Advising, Collaborating","Decision-maker, Stakeholder","Change trust, Shape ethical norms, Alter decision outcomes, Change cognitive demands",no such info,"prediction of alternative, recommendations",refinement,"Textual, Autonomous System",Yes,Yes
2-4837,aisel,Robot shopping assistants: how emotional versus rational robot designs affect consumer trust and purchase decisions,"Advancements in robotics and artificial intelligence are paving the way for retailers to employ physical robots in assisting consumers with their purchase decisions. However, as research has primarily focused on virtual robots (e.g., chatbots) or relied on hypothetical scenarios without actual consumer--robot interaction, our understanding of how consumers interact with physical robots and how the robot design affects consumer interactions is limited. We address this gap by investigating how an emotional (vs. rational) robot design influences consumers' trust and purchase decisions. Drawing on trust theory and human--robot interaction literature, we propose a lab experiment in which consumers interact with a large language model (LLM)-based robot shopping assistant based on the robot `Furhat' and OpenAI's GPT-4. With our findings, we aim to contribute to research on consumer--robot interaction by providing novel insights into how and why the design of robot shopping assistants impacts consumers' shopping behavior.",NA,https://aisel.aisnet.org/ecis2024/track19_hci/track19_hci/2,European Conference on Information Systems,"Gnewuch, Ulrich;Hanschmann, Leon;Kaiser, Carolin;Schallner, Rene;M{\""a}dche, Alexander",2024,4,"@inproceedings{2-4837,
  title = {Robot Shopping Assistants: How Emotional versus Rational Robot Designs Affect Consumer Trust and Purchase Decisions},
  author = {Gnewuch, Ulrich and Hanschmann, Leon and Kaiser, Carolin and Schallner, Rene and M{\""a}dche, Alexander},
  year = {2024},
  booktitle = {European Conference on Information Systems}
}",Empirical contributions,Finance / Business / Economy,Individual,Advising,"Decision-maker, Decision-subject",Change trust,"Update AI competence, Change AI responses","emotional condition, rational condition, recommendations",NA,"Physical / Embodiment, Conversational/Natural Language",Yes,Yes
2-4840,aisel,Seven elements of phronesis: a framework for understanding judgment in relation to automated decision-making,"This conceptual paper aims to explore judgment in the context of automated decision-making systems (ADS). To achieve this, we adopt a modern version of Aristotle's notion of phronesis to understand judgment. We delineate seven elements of judgment which provide insights into what humans are better at, and what AI is better at in relation to automated decision-making. These elements are sources of knowledge that guide action including not-knowing, emotions, sensory perception, experience, intuition, episteme, and techne. Our analysis suggests that most of these attributes are not transferable to AI systems, because judgment in human decision-making requires the integration of all which involves considering the contextual and affective resources of phronesis, and the competence to make value judgments. The paper contributes to unpack human judgment capacities and what needs to be cultivated to achieve `good' AI systems that serves humanity as well as guiding future information systems researchers to explore human-AI judgment further.",NA,https://aisel.aisnet.org/hicss-56/os/ai_and_organizing/7,Hawaii International Conference on System Sciences,"Koutsikouri, Dina;Hylving, Lena;Lindberg, Susanne;Bornemark, Jonna",2023,6,"@inproceedings{2-4840,
  title={Seven elements of phronesis: a framework for understanding judgment in relation to automated decision-making},
  author={Koutsikouri, Dina and Hylving, Lena and Lindberg, Susanne and Bornemark, Jonna},
  year={2023},
  booktitle={Hawaii International Conference on System Sciences}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,no such info,"Executing, Analyzing, Advising","Decision-maker, Knowledge provider","Shape ethical norms, Change trust, Restrict human agency",Shape AI for accountability,recommendations,"contextual knowledge, human value incorporation, emotion expression, domain knowledge, intuition",Autonomous System,Yes,Yes
2-4841,aisel,Shall we use it or not? Explaining the adoption of artificial intelligence for car manufacturing purposes,"Artificial intelligence (AI) is said to incorporate enormous potential for reducing the operational costs of car manufacturers and their suppliers all over the globe. Nevertheless, recent studies suggest that many of them are still struggling with adopting it at large scale. Therefore, in this article we develop a research model explaining the decision-making of organisations from the automotive industry regarding the question whether to adopt AI for car (part) manufacturing purposes or not. To do so, we contextualise the Technology-Organization-Environment (TOE) framework by taking a multiple case study approach including 39 expert interviews in 25 different firms all over the globe. Based on that, we propose a research model that theorises 16 different context-specific factors that influence that decision. Four of them are already well-known from the TOE framework but are renamed for a better contextual accuracy, whereas twelve can be regarded as new in this contextualised setting. The proposed model addresses research gaps in both the organisational adoption research literature and the literature on the application of AI in the automotive environment.",NA,https://aisel.aisnet.org/ecis2020_rp/177,European Conference on Information Systems,"Demlehner, Quirin;Laumer, Sven",2020,89,"@inproceedings{2-4841,
  title={Shall we use it or not? Explaining the adoption of artificial intelligence for car manufacturing purposes},
  author={Demlehner, Quirin and Laumer, Sven},
  year={2020},
  booktitle={European Conference on Information Systems}
}","Empirical contributions, Theoretical contributions",Manufacturing / Industry / Automation,Organizational,"Executing, Advising","Decision-maker, Knowledge provider",Alter decision outcomes,Shape AI for accountability,"black box reasoning, underlying data","moral responsibility, domain knowledge, technology experience, resource allocation to AI, privacy concern",Autonomous System,Yes,Yes
2-4842,aisel,Show me your claims and i'll tell you your offenses: machine learning-based decision support for fraud detection on medical claim data,"Health insurance claim fraud is a serious issue for the healthcare industry as it drives up costs and inefficiency. Therefore, claim fraud must be effectively detected to provide economical and high-quality healthcare. In practice, however, fraud detection is mainly performed by domain experts resulting in significant cost and resource consumption. This paper presents a novel Convolutional Neural Network-based fraud detection approach that was developed, implemented, and evaluated on Medicare Part B records. The model aids manual fraud detection by classifying potential types of fraud, which can then be specifically analyzed. Our model is the first of its kind for Medicare data, yields an AUC of 0.7 for selected fraud types and provides an applicable method for medical claim fraud detection.",NA,https://aisel.aisnet.org/hicss-55/hc/process/9,Hawaii International Conference on System Sciences,"Matschak, Tizian;Prinz, Christoph;Rampold, Florian;Trang, Simon",2022,8,"@inproceedings{2-4842,
  title={Show me your claims and I'll tell you your offenses: Machine learning-based decision support for fraud detection on medical claim data},
  author={Matschak, Tizian and Prinz, Christoph and Rampold, Florian and Trang, Simon},
  year={2022},
  booktitle={Hawaii International Conference on System Sciences}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Auditing","Decision-maker, Stakeholder, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-4850,aisel,Supporting ai-based innovations: revealing the interplay between framing and innovation process formalization,"While the potentials of AI are widely acknowledged, it remains unclear how framing shapes the support for AI-based innovations, and how it interrelates with innovation process formalization. Drawing on 53 interviews, onsite observations, and archival data in three cases, we reveal that managers use `grounding in the business' and `calling on aspirations' to `mobilize support' for investment decisions on AI-based innovations. We further find that innovation process formalization strengthens framing focused on achieving alignment of AI with business processes. Vice versa, framing addressing managers' aspirations flexibilizes the innovation process, which is crucial for building trust in AI. The absence of process formalization however leads to unstructured framing of AI-based innovations ultimately undermining support. This study extends our understanding of how managers can successfully cope with the challenges of AI-based innovations, reveals how different framing behaviors shape the support for AI-based innovations, and how these behaviors interact with innovation process formalization.",NA,https://aisel.aisnet.org/icis2024/diginnoventren/diginnoventren/23,International Conference on Information Systems,"Alguera Kleine, Rebecca;R{\""o}th, Tobias;Clau{\ss}, Thomas",2024,0,"@inproceedings{2-4850,
  title = {Supporting AI-based Innovations: Revealing the Interplay Between Framing and Innovation Process Formalization},
  author = {Alguera Kleine, Rebecca and R{\""o}th, Tobias and Clau{\ss}, Thomas},
  year = {2024},
  booktitle = {International Conference on Information Systems}
}",Empirical contributions,Finance / Business / Economy,Operational,Advising,"Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-4851,aisel,Supporting managerial decision-making for federated machine learning: design of a technology selection tool,"The insufficient amount of training data is a persisting bottleneck of Machine Learning systems. A large portion of the world's data is scattered and locked in data silos. Breaking up these data silos could alleviate this problem. Federated Machine Learning is a novel model-to-data approach that enables the training of Machine Learning models, on decentralized, potentially siloed data. Despite its promising potential, most Federated Machine Learning projects never leave the prototype stage. This can be attributed to exaggerated expectations and an inappropriate fit between the technology and the use case. Current literature does not offer guidance for assessing the fit between Federated Machine Learning and their use case. Against this backdrop, we design a decision-support tool to aid decision-makers in the suitability and complexity assessment of FedML projects. Thereby, we aim to facilitate the technology selection process, avoid exaggerated expectations and consequently facilitate the success of Federated Machine Learning projects.",NA,https://aisel.aisnet.org/hicss-57/os/topics_in_os/2,Hawaii International Conference on System Sciences,"Zahn, Milena;M{\""u}ller, Tobias;Matthes, Florian",2024,0,"@inproceedings{2-4851,
  title = {Supporting Managerial Decision-Making for Federated Machine Learning: Design of a Technology Selection Tool},
  author = {Zahn, Milena and M{\""u}ller, Tobias and Matthes, Florian},
  year = {2024},
  booktitle = {Hawaii International Conference on System Sciences},
}",System/Artifact contributions,Generic / Abstract / Domain-agnostic,Operational,"Advising, Analyzing","Decision-maker, Developer",NA,NA,NA,NA,NA,Yes,No
2-4855,aisel,Task delegability to ai: evaluation of a framework in a knowledge work context,"With the increased research focus on ways to use AI for augmentation rather than automation of knowledge-intensive work, a myriad of questions on how this should be accomplished arises. To break down the complexity of Human-AI collaboration, this paper pursues the identification of factors that contribute to the delegation of tasks to AI in such a setting, and consequently gain insights into requirements for meaningful task allocation. To address this research gap, we carried out an empirical study on an existing task delegability framework in a knowledge work context. We employed several statistical approaches such as confirmatory factor analysis, linear regression, and analysis of covariance. Results show that an adapted framework with fewer factors fits the data better. As for the framework factors, we show that the factor trust predicts delegability best. Furthermore, we find a significant impact of task on delegability decision. Finally, we derive theoretical and design implications.",NA,https://aisel.aisnet.org/hicss-55/cl/ai_and_future_work/5,Hawaii International Conference on System Sciences,"Cvetkovic, Izabel;Bittner, Eva",2022,12,"@inproceedings{2-4855,
  title     = {Task Delegability to AI: Evaluation of a Framework in a Knowledge Work Context},
  author    = {Cvetkovic, Izabel and Bittner, Eva},
  year      = {2022},
  booktitle = {Proceedings of the Hawaii International Conference on System Sciences},
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Education / Teaching / Research",Institutional,"Collaborating, Advising","Decision-maker, Guardian","Change trust, Alter decision outcomes",no such info,recommendations,"delegation, prefer to delegate demotivating tasks",Textual,Yes,Yes
2-4856,aisel,The bermuda triangle of leadership in the ai era? Emerging trust implications from ``two-leader-situations'' in the eyes of employees,"Artificial Intelligence (AI) and machine learning (ML) algorithms are changing the work in many ways. One hitherto little-studied area is how these technologies are impacting leader-employee relationships, particularly employees' trust relationships in their ``flesh-and-blood'' leaders. In this paper, we discuss how algorithms change the nature of leadership when some leadership functions become automated. As a consequence, employees will often find themselves in a ``two-leader-situation'' with resulting frictions, that create novel leadership focus areas. Three situations, in particular, can be trust-problematic in the eyes of followers: the triad relationship might (1) make responsibilities blur, (2) create conflicting decisions of human leaders and algorithms, and (3) make employees' voice unheard. We argue that these situations can undermine employee perceptions of leaders' trustworthiness as followers might start to question a leaders' ability, benevolence, and integrity if leaders do not understand these novel situations.",NA,https://aisel.aisnet.org/hicss-54/os/trust/3,Hawaii International Conference on System Sciences,"Schafheitle, Simon;Weibel, Antoinette;Rickert, Alice",2021,23,"@inproceedings{2-4856,
  title = {The Bermuda Triangle of Leadership in the AI Era? Emerging Trust Implications from ""Two-Leader-Situations"" in the Eyes of Employees},
  author = {Schafheitle, Simon and Weibel, Antoinette and Rickert, Alice},
  year = {2021},
  booktitle = {Hawaii International Conference on System Sciences}
}",Theoretical contributions,"Everyday / Employment / Public Service, Generic / Abstract / Domain-agnostic",Operational,Executing,"Stakeholder, Decision-subject",NA,NA,prediction of alternative,NA,NA,Yes,No
2-4861,aisel,The effect of ai advice on human confidence in decision-making,"As artificial intelligence advances, it can increasingly be applied in collaborative decision-making contexts with humans. However, questions on the design of different collaborative environments remain open. In the context of AI-assisted human decision-making processes, we analyze the influence of AI advice on human confidence in the final decision. In a laboratory experiment, 458 subjects performed an image classification task. We compare their confidence over three treatments: i) a baseline case where subjects do not receive any AI advice; ii) where subjects receive AI advice; and iii) in addition to AI advice subjects also see the certainty of AI for its choice. Our results suggest that while AI advice can increase human overconfidence, this effect can be mitigated by augmenting the AI advice with its certainty. Our result not only contributes to the growing literature of human-AI collaboration, but also bears important practical implications for the design of collaborative systems.",NA,https://aisel.aisnet.org/hicss-55/cl/human-ai_collaboration/7,Hawaii International Conference on System Sciences,"Taudien, Anna;F{\""u}gener, Andreas;Gupta, Alok;Ketter, Wolfgang",2022,22,"@inproceedings{2-4861,
  title={The effect of AI advice on human confidence in decision-making},
  author={Taudien, Anna and F{\""u}gener, Andreas and Gupta, Alok and Ketter, Wolfgang},
  year={2022},
  booktitle={Hawaii International Conference on System Sciences}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Collaborating, Advising","Decision-maker, Decision-subject","Alter decision outcomes, Change trust, Change affective-perceptual",no such info,"AI advice, the certainty of AI for its choice",NA,"Visual, Textual",Yes,Yes
2-4862,aisel,The effect of interpretable artificial intelligence on repeated managerial decision-making under uncertainty,"Business decisions involving investments, healthcare, and supply chains are often made in uncertain environments. At the same time, despite being optimal initially, such choices may seem incorrect in hindsight, which may explain why decision-makers hesitate to use AI algorithms under high uncertainty. While some studies suggest that making AI and ML applications more understandable can boost their adoption and trust, this hasn't been examined in uncertain conditions where decision-makers must make repetitive business decisions. Our study addresses this issue empirically by analyzing how different interpretability approaches affect AI adoption and trust under varying levels of uncertainty. Surprisingly, we find that providing interpretability does not necessarily increase AI adoption. In some cases, it may even reduce AI adoption. Interestingly, even though AI adoption was higher, trust in the AI recommendations was significantly lower in high uncertainty compared to low uncertainty across all interpretability types. The evidence is clear that showing the cumulative monetary performance of AI to the users as a benchmark, side by side with their own monetary performance, enhances trust in the AI recommendations.",NA,https://aisel.aisnet.org/hicss-57/os/digitization/2,Hawaii International Conference on System Sciences,"Altintas, Onur;Seidmann, Abraham;Gu, Bin;Ma{\v z}ar, Nina",2024,4,"@inproceedings{2-4862,
  title = {The Effect of Interpretable Artificial Intelligence on Repeated Managerial Decision-Making Under Uncertainty},
  author = {Altintas, Onur and Seidmann, Abraham and Gu, Bin and Ma{\v{z}}ar, Nina},
  year = {2024},
  booktitle = {Hawaii International Conference on System Sciences}
}",Empirical contributions,Finance / Business / Economy,Organizational,"Explaining, Advising",Decision-maker,no such info,no such info,"recommendations, reasoning",NA,Textual,Yes,Yes
2-4863,aisel,The formal rationality of artificial intelligence-based algorithms and the problem of bias,"This paper presents a new perspective on the problem of bias in artificial intelligence (AI)-driven decision-making by examining the fundamental difference between AI and human rationality in making sense of data. Current research has focused primarily on software engineers' bounded rationality and bias in the data fed to algorithms but has neglected the crucial role of algorithmic rationality in producing bias. Using a Weberian distinction between formal and substantive rationality, we inquire why AI-based algorithms lack the ability to display common sense in data interpretation, leading to flawed decisions. We first conduct a rigorous text analysis to uncover and exemplify contextual nuances within the sampled data. We then combine unsupervised and supervised learning, revealing that algorithmic decision-making characterizes and judges data categories mechanically as it operates through the formal rationality of mathematical optimization procedures. Next, using an AI tool, we demonstrate how formal rationality embedded in AI-based algorithms limits its capacity to perform adequately in complex contexts, thus leading to bias and poor decisions. Finally, we delineate the boundary conditions and limitations of leveraging formal rationality to automatize algorithmic decision-making. Our study provides a deeper understanding of the rationality-based causes of AI's role in bias and poor decisions, even when data is generated in a largely bias-free context.",NA,https://aisel.aisnet.org/jit/vol39/iss1/2,Journal of Information Technology,"Nishant, Rohit;Schneckenberg, Dirk;Ravishankar, MN",2024,114,"@article{2-4863,
  title = {The formal rationality of artificial intelligence-based algorithms and the problem of bias},
  author = {Nishant, Rohit and Schneckenberg, Dirk and Ravishankar, MN},
  year = {2024},
  journal = {Journal of Information Technology}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Operational,"Analyzing, Executing","Developer, Guardian",NA,NA,"text-based explanations, topic representations, statistical scoring, automated coherence metrics","instruction, language proficiency, corrective feedback, emotion expression, evaluation","Textual, Conversational/Natural Language",Yes,No
2-4865,aisel,The impact of physician-ai collaboration on care quality: empirical evidence in acute diseases,"The integration of AI is reshaping decision-making, especially in healthcare, where it supports diagnostic procedures. However, a significant knowledge gap exists regarding the impact of physician-AI collaboration on patient care. Leveraging proprietary data from a hospital that implemented an AI-based risk evaluation system in 2022, we examine the effects of physician-AI collaboration on patient care outcomes, considering patient risk levels and physician experience. Results indicate that physician-AI collaboration reduces hospital stays and readmissions, particularly benefiting high-risk patients. Notably, high levels of physician experience exhibit complex moderating effects. Our findings underscore the role of AI as an assistant in healthcare, revealing variations based on patient risk and physician diverse experience. They are critical for optimizing AI integration, minimizing decision biases, and emphasizing how AI strengthens medical diagnosis with nuanced implementation.",NA,https://aisel.aisnet.org/icis2024/ishealthcare/ishealthcare/11,International Conference on Information Systems,"Qin, Yaxue;Kwon, Juhee",2024,0,"@inproceedings{2-4865,
  title={The impact of physician-ai collaboration on care quality: empirical evidence in acute diseases},
  author={Qin, Yaxue and Kwon, Juhee},
  year={2024},
  booktitle={International Conference on Information Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Collaborating, Monitoring","Decision-maker, Decision-subject","Alter decision outcomes, Change cognitive demands",no such info,"recommendations, (continuous) support",NA,Autonomous System,Yes,Yes
2-4870,aisel,The moderating role of gamification in reducing algorithm aversion in the adoption of ai-based decision support systems,"Integrating artificial intelligence (AI) into decision-making processes is key to improving organizational performance. However, trust in AI-based decision support systems (DSSs), similar to other information systems, is important for successful integration. A disruptive phenomenon, ``algorithm aversion'', can impede AI trust and, thus, acceptance. Although AI recommendations outperform human recommendations in different decision-making fields, individuals underweight recommendations from AI-based DSSs compared to human decision-makers due to a lack of AI trust. We conducted a lab experiment to investigate the role of AI recommendations in workplace-related tasks, first focusing on the mediating effect of AI trust and the negative impact of algorithm aversion on decision-making performance and the moderating effect of technical competence. Second, we analyzed the ability of gamification to reduce this phenomenon. We provide evidence regarding how to enhance decision-making performance when AI recommendations are deployed and identify countermeasures against algorithm aversion to facilitate the adoption of AI-based DSSs.",NA,https://aisel.aisnet.org/ecis2024/track03_ai/track03_ai/4,European Conference on Information Systems,"Smeets, Mario Richard;Roetzel, Peter Gordon",2024,2,"@inproceedings{2-4870,
  title = {The Moderating Role of Gamification in Reducing Algorithm Aversion in the Adoption of AI-based Decision Support Systems},
  author = {Smeets, Mario Richard and Roetzel, Peter Gordon},
  year = {2024},
  booktitle = {European Conference on Information Systems}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Operational,Advising,Decision-maker,"Alter decision outcomes, Change trust",Update AI competence,recommendations,"domain knowledge, gamification",Textual,Yes,Yes
2-4871,aisel,The narrative ai advantage? A field experiment on generative ai-augmented evaluations of early-stage innovations,"Generative artificial intelligence (AI) transforms creative problem-solving, necessitating new approaches for evaluating innovative solutions. This study explores how human-AI collaboration can enhance early-stage evaluations, focusing on the interplay between objective criteria, which are quantifiable, and subjective criteria, which rely on personal judgment. We conducted a field experiment with MIT Solve, involving 72 experts and 156 community screeners who evaluated 48 solutions to a global health equity challenge. We compared a human-only control group with two AI-assisted treatments: a black box AI and a narrative AI with probabilistic rationale justifying its decisions. Results show that screeners were more likely to fail solutions with AI assistance, especially based on subjective criteria. AI-generated rationales significantly influenced human subjective assessments across all expertise levels, underscoring the importance of developing AI interaction expertise in creative evaluation processes. While AI can standardize decision-making for objective criteria, human oversight remains crucial in subjective assessments.",NA,https://aisel.aisnet.org/icis2024/aiinbus/aiinbus/2,International Conference on Information Systems,"Ayoubi, Charles;Boussioux, Leonard;Chen, YingHao;Ho, Justin;Jackson, Katherine;Lane, Jacquelin;Lin, Camila;Spens, Rebecca",2024,9,"@inproceedings{2-4871,
  title     = {The Narrative AI Advantage? A Field Experiment on Generative AI-Augmented Evaluations of Early-Stage Innovations},
  author    = {Ayoubi, Charles and Boussioux, Leonard and Chen, YingHao and Ho, Justin and Jackson, Katherine and Lane, Jacquelin and Lin, Camila and Spens, Rebecca},
  year      = {2024},
  booktitle = {International Conference on Information Systems}
}",Empirical contributions,Education / Teaching / Research,Operational,"Explaining, Advising, Collaborating","Decision-maker, Knowledge provider, Guardian","Alter decision outcomes, Change trust, Change cognitive demands, Restrict human agency, Shape ethical norms",Change AI responses,"probabilistic rationale justifying, recommendations",NA,"Interactive interface, Textual",Yes,Yes
2-4874,aisel,The return on investment in ai ethics: a holistic framework,"We propose a Holistic Return on Ethics (HROE) framework for understanding the return on organizational investments in artificial intelligence (AI) ethics efforts. This framework is useful for organizations that wish to quantify the return for their investment decisions. The framework identifies the direct economic returns of such investments, the indirect paths to return through intangibles associated with organizational reputation, and real options associated with capabilities. The holistic framework ultimately provides organizations with the competency to employ and justify AI ethics investments.",NA,https://aisel.aisnet.org/hicss-57/os/ai_and_organizing/5,Hawaii International Conference on System Sciences,"Bevilacqua, Marialena;Berente, Nicholas;Domin, Heather;Goehring, Brian;Rossi, Francesca",2024,12,"@inproceedings{2-4874,
  title = {The return on investment in AI ethics: a holistic framework},
  author = {Bevilacqua, Marialena and Berente, Nicholas and Domin, Heather and Goehring, Brian and Rossi, Francesca},
  year = {2024},
  booktitle = {Hawaii International Conference on System Sciences}
}",Theoretical contributions,Finance / Business / Economy,Organizational,Advising,"Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-4875,aisel,The role of ai agents for de-escalating commitment in digital innovation projects,"The increasing complexity of managerial decision-making for digital innovation activities accelerates cognitive biases like escalation of commitment (EoC). Decision aids (e.g., AI agents) can assist managers in avoiding EoC scenarios. However, how AI-based decision aids affect EoC in this context remains a critical yet understudied topic. To address this gap, we develop a theoretical model and propose a randomized controlled post-test vignette experiment with a fictive decision-making simulation to study the de-escalating effect of an AI-based decision aid in the digital innovation context. Our model accounts for moderating (AI familiarity, personality traits) and mediating (decision aid reliance) factors. By entangling the de-escalating effect of AI agent decision aid in decision-making scenarios about digital innovation projects we contribute to the digital innovation, AI agent, and the EoC literature. The future implementation of the proposed research design lays the foundation for designing AI agent decision support systems that de-bias managerial decision-making.",NA,https://aisel.aisnet.org/icis2021/paperathon/paperathon/2,International Conference on Information Systems,"Marx, Carolin;Ampel, Benjamin;Lazarine, Ben",2021,2,"@inproceedings{2-4875,
  title={The role of AI agents for de-escalating commitment in digital innovation projects},
  author={Marx, Carolin and Ampel, Benjamin and Lazarine, Ben},
  year={2021},
  booktitle={International Conference on Information Systems}
}",Theoretical contributions,Design / Creativity / Architecture,Operational,"Advising, Collaborating",Decision-maker,NA,NA,recommendations,NA,NA,Yes,No
2-4877,aisel,The role of trusting beliefs in voice assistants during voice shopping,"Artificial intelligence-based voice assistants (VAs) such as Amazon Alexa deliver personalized product recommendations in order to match consumers' needs. The use of voice assistants for shopping purposes incorporates elements of risk affecting when and how they are considered trusted relationship partners. In this uncertain environment, it is unclear `when' voice assistants are capable of gaining trust and `how' the development of such a trusted relationship affects decisions. This research explores the effect of trusting beliefs towards voice assistants on decision satisfaction through the indirect effect of consideration set size (n. of options), in the context of voice shopping. Findings of an individual-session online experiment (N = 180) show a positive direct effect of trust on customer's satisfaction and a mediating role of set size, confirming consumers' bias towards default choices. This study highlights the consequences of trust in AI-enabled voice assistants for decision-making during utilitarian purchases.",NA,https://aisel.aisnet.org/hicss-54/in/ai_based_assistants/7,Hawaii International Conference on System Sciences,"Mari, Alex;Algesheimer, Ren{\'e },",2021,26,"@inproceedings{2-4877,
  title = {The role of trusting beliefs in voice assistants during voice shopping},
  author = {Mari, Alex and Algesheimer, Ren{\'e}},
  year = {2021},
  booktitle = {Hawaii International Conference on System Sciences}
}",Empirical contributions,Finance / Business / Economy,Individual,"Advising, Collaborating","Decision-subject, Decision-maker","Change affective-perceptual, Alter decision outcomes, Change trust",no such info,number of recommendations,NA,"Textual, Conversational/Natural Language, Auditory",Yes,Yes
2-4879,aisel,To automate or not? How open collaboration communities decide on bot adoption,"AI tools are widely used in open collaboration communities nowadays, yet factors influencing their adoption are not well understood. This study investigates the factors influencing bot approval on English Wikipedia. Leveraging the advanced capabilities of Large Language Models (LLMs) on NLP tasks, we systematically extract humancomputer interaction (HCI) factors from 2,155 Wikipedia Bot Request for Approval pages. We identify ten themes that emerged from community discussion concerning three main aspects: Technological Factors, User Interaction and Experience, and Platform Governance. We further assess the comparative importance of each theme in the community's bot approval decisions. Our findings reveal that Technological Factors play the most significant role, followed by User Interaction and Experience and Platform Governance Standards. This study contributes quantitative evidence to understanding bot approval in open collaboration communities and proposes a novel, generalizable LLM-based approach for extracting and summarizing themes from large text corpora.",NA,https://aisel.aisnet.org/icis2024/humtechinter/humtechinter/23,International Conference on Information Systems,"Zheng, Lei (Nico);Chen, Zihan;Mai, Feng",2024,0,"@inproceedings{2-4879,
  title = {To automate or not? How open collaboration communities decide on bot adoption},
  author = {Zheng, Lei (Nico) and Chen, Zihan and Mai, Feng},
  year = {2024},
  booktitle = {International Conference on Information Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Organizational,"Advising, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-488,aaai,Fair and Interpretable Algorithmic Hiring using Evolutionary Many Objective Optimization,"Hiring is a high-stakes decision-making process that balances the joint objectives of being fair and accurately selecting the top candidates. The industry standard method employs subject-matter experts to manually generate hiring algorithms; however, this method is resource intensive and finds sub-optimal solutions. Despite the recognized need for algorithmic hiring solutions to address these limitations, no reported method currently supports optimizing predictive objectives while complying to legal fairness standards. We present the novel application of Evolutionary Many-Objective Optimization( EMOO) methods to create the first fair, interpretable, and legally compliant algorithmic hiring approach. Using a proposed novel application of Dirichlet-based genetic operators for improved search, we compare state-of-the-art EMOO models( NSGA-III, SPEA2-SDE, bi-goal evolution) to expert solutions, verifying our results across three real world datasets across diverse organizational positions. Experimental results demonstrate the proposed EMOO models outperform human experts, consistently generate fairer hiring algorithms, and can provide additional lift when removing constraints required for human analysis.",10.1609/aaai.v35i17.17737,https://ojs.aaai.org/index.php/AAAI/article/view/17737,AAAI Conference on Artificial Intelligence,Michael Geden;Joshua Andrews,2021,4,"@inproceedings{2-488,
  title = {Fair and Interpretable Algorithmic Hiring using Evolutionary Many Objective Optimization},
  author = {Michael Geden and Joshua Andrews},
  year = {2021},
  doi = {10.1609/aaai.v35i17.17737},
  booktitle = {AAAI Conference on Artificial Intelligence}
}",Algorithmic contributions,Everyday / Employment / Public Service,Operational,"Executing, Analyzing","Decision-subject, Decision-maker, Knowledge provider","Alter decision outcomes, Shape ethical norms, Change cognitive demands",no such info,"differential fairness, ordinal sum, recommendations",fairness constraints,"Textual, Interactive interface",Yes,Yes
2-4882,aisel,Towards a machine learning-based decision support system for dispatching helicopters in new zealand,"Helicopters play an important role in emergency medical service systems worldwide. In sparsely populated countries like New Zealand with long distances between hospitals, helicopters are often the best way to help critically injured patients. As helicopters are extremely costly, they should only be dispatched when really necessary. In this paper, we use data from the South Island of New Zealand to test several Machine Learning approaches and show that they can be used to support dispatchers by identifying emergencies likely to require a helicopter response. We follow a non-static dataset, as the information is successively available during an emergency, and demonstrate that even a limited approach, based only on geographic incident information, can yield an Average Precision of 94% for highlighting critical emergencies. In the latter parts of this paper, we investigate different compositions of training data to assess the impact of a potential concept drift.",NA,https://aisel.aisnet.org/hicss-54/da/service_analytics/2,Hawaii International Conference on System Sciences,"R{\""a}dsch, Tim;Reuter-Oppermann, Melanie;Richards, Dave",2021,2,"@inproceedings{2-4882,
  title = {Towards a Machine Learning-Based Decision Support System for Dispatching Helicopters in New Zealand},
  author = {R{\""a}dsch, Tim and Reuter-Oppermann, Melanie and Richards, Dave},
  year = {2021},
  booktitle = {Hawaii International Conference on System Sciences}
}",Empirical contributions,Defense / Military / Emergency,Operational,"Advising, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-4885,aisel,Towards effective human-ai decision-making: the role of human learning in appropriate reliance on ai advice,"The true potential of human-AI collaboration lies in exploiting the complementary capabilities of humans and AI to achieve a joint performance superior to that of the individual AI or human, i.e., to achieve complementary team performance (CTP). To realize this complementarity potential, humans need to exercise discretion in following AI's advice, i.e., appropriately relying on the AI's advice. While previous work has focused on building a mental model of the AI to assess AI recommendations, recent research has shown that the mental model alone cannot explain appropriate reliance. We hypothesize that, in addition to the mental model, human learning is a key mediator of appropriate reliance and, thus, CTP. In this study, we demonstrate the relationship between learning and appropriate reliance in an experiment with 100 participants. This work provides fundamental concepts for analyzing reliance and derives implications for the effective design of human-AI decision-making.",NA,https://aisel.aisnet.org/icis2023/hti/hti/14,International Conference on Information Systems,"Schemmer, Max;Bartos, Andrea;Spitzer, Philipp;Hemmer, Patrick;K{\""u}hl, Niklas;Liebschner, Jonas;Satzger, Gerhard",2023,21,"@inproceedings{2-4885,
  title = {Towards Effective Human-AI Decision-Making: The Role of Human Learning in Appropriate Reliance on AI Advice},
  author = {Schemmer, Max and Bartos, Andrea and Spitzer, Philipp and Hemmer, Patrick and K{\""u}hl, Niklas and Liebschner, Jonas and Satzger, Gerhard},
  year = {2023},
  booktitle = {International Conference on Information Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Collaborating, Advising",Decision-maker,"Alter decision outcomes, Change cognitive demands, Change trust",no such info,"AI advice, example-based explanations",learning,"Textual, Conversational/Natural Language",Yes,Yes
2-4902,aisel,Unpacking ai advice for decision-making: a novel toulmin-based conceptualization,"As a holistic conceptualization of AI-advised decision-making currently does not exist, we propose such conceptualization by utilizing a proven framework: Toulmin's Model of Argumentation. To achieve this, we break down AI advice into its core elements; namely the AI prediction, the AI explanation, and AI confidence level. We argue that each of these elements can be mapped to the argumentative elements proposed by Toulmin's Model: The prediction constitutes grounds and claim, the explanation warrant and backing, and the confidence level the qualifier. Through this new perspective, this conceptual paper offers three main contributions: 1) We present the first holistic conceptualization for AI-advised decision-making, 2) Building on the proven explanatory powers of TMA, our novel conceptualization deepens our understand of contemporary issues in humans interacting with AI advice, and 3) The conceptualization can be used by practitioners to build more persuasive AI systems for real-world applications.",NA,https://aisel.aisnet.org/ecis2024/track06_humanaicollab/track06_humanaicollab/8,European Conference on Information Systems,"Aslan, Aycan;Greve, Maike",2024,1,"@inproceedings{2-4902,
  title={Unpacking AI Advice for Decision-Making: A Novel Toulmin-Based Conceptualization},
  author={Aslan, Aycan and Greve, Maike},
  year={2024},
  booktitle={European Conference on Information Systems}
}",Theoretical contributions,Generic / Abstract / Domain-agnostic,Institutional,"Advising, Explaining",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-4904,aisel,User acceptance of advice by ai agents: expectation-system fit perspective,"Algorithms have increasing influence on our daily decisions, especially when the recommendations are presented by human-like AI agents. This study applies the Theory of Effective Use to investigate how the fit between the user's role expectation for an AI agent and the agent's interaction style impacts AI advice adoption. We proposed a new concept termed Perceived Expectation-System Fit (PESF) and empirically examined its impact on user perceptions and advice acceptance. We found that low PESF reduces advice acceptance by diminishing cognitive and affective trust in the AI agent. Furthermore, increased algorithm transparency increases PESF's impact on decision-making. Our findings provide both practical implications and theoretical contributions to our understanding of effective system use in the context of human-AI interaction.",NA,https://aisel.aisnet.org/icis2024/humtechinter/humtechinter/25,International Conference on Information Systems,"Cai, Jingyuan;Nah, Fiona",2024,1,"@inproceedings{2-4904,
  title = {User Acceptance of Advice by AI Agents: Expectation-System Fit Perspective},
  author = {Cai, Jingyuan and Nah, Fiona},
  year = {2024},
  booktitle = {International Conference on Information Systems}
}",Empirical contributions,"Generic / Abstract / Domain-agnostic, Media / Communication / Entertainment",Individual,Advising,Decision-maker,"Change cognitive demands, Change trust, Alter decision outcomes, Change affective-perceptual",no such info,"recommendations, textual explanations, algorithm transparency",Perceived Expectation-System Fit (PESF),"Autonomous System, Interactive interface, Auditory",Yes,Yes
2-4906,aisel,Using deep learning and 360 video to detect eating behavior for user assistance systems,"The rising prevalence of non-communicable diseases calls for more sophisticated approaches to support individuals in engaging in healthy lifestyle behaviors, particularly in terms of their dietary intake. Building on recent advances in information technology, user assistance systems hold the potential of combining active and passive data collection methods to monitor dietary intake and, subsequently, to support individuals in making better decisions about their diet. In this paper, we review the state-of-the-art in active and passive dietary monitoring along with the issues being faced. Building on this groundwork, we propose a research framework for user assistance systems that combine active and passive methods with three distinct levels of assistance. Finally, we outline a proof-of-concept study using video obtained from a 360-degree camera to automatically detect eating behavior from video data as a source of passive dietary monitoring for decision support.",NA,https://aisel.aisnet.org/ecis2018_rp/101,International Conference on Information Systems (ICIS),"Rouast, Philipp Vincent;Adam, Marc;Burrows, Tracy;Chiong, Raymond",2018,48,"@inproceedings{2-4906,
  title={Using deep learning and 360 video to detect eating behavior for user assistance systems},
  author={Rouast, Philipp Vincent and Adam, Marc and Burrows, Tracy and Chiong, Raymond},
  year={2018},
  booktitle={International Conference on Information Systems (ICIS)}
}","Methodological contributions, Vision contributions",Healthcare / Medicine / Surgery,Individual,"Auditing, Advising, Monitoring","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-4909,aisel,Watch me improve---algorithm aversion and demonstrating the ability to learn,"Owing to advancements in artificial intelligence (AI) and specifically in machine learning, information technology (IT) systems can support humans in an increasing number of tasks. Yet, previous research indicates that people often prefer human support to support by an IT system, even if the latter provides superior performance -- a phenomenon called algorithm aversion. A possible cause of algorithm aversion put forward in literature is that users lose trust in IT systems they become familiar with and perceive to err, for example, making forecasts that turn out to deviate from the actual value. Therefore, this paper evaluates the effectiveness of demonstrating an AI-based system's ability to learn as a potential countermeasure against algorithm aversion in an incentive-compatible online experiment. The experiment reveals how the nature of an erring advisor (i.e., human vs. algorithmic), its familiarity to the user (i.e., unfamiliar vs. familiar), and its ability to learn (i.e., non-learning vs. learning) influence a decision maker's reliance on the advisor's judgement for an objective and non-personal decision task. The results reveal no difference in the reliance on unfamiliar human and algorithmic advisors, but differences in the reliance on familiar human and algorithmic advisors that err. Demonstrating an advisor's ability to learn, however, offsets the effect of familiarity. Therefore, this study contributes to an enhanced understanding of algorithm aversion and is one of the first to examine how users perceive whether an IT system is able to learn. The findings provide theoretical and practical implications for the employment and design of AI-based systems.",NA,https://aisel.aisnet.org/bise/vol63/iss1/5,Business & Information Systems Engineering,"Berger, Benedikt;Adam, Martin;R{\""u}hr, Alexander;Benlian, Alexander",2021,247,"@article{2-4909,
  title = {Watch me improve---algorithm aversion and demonstrating the ability to learn},
  author = {Berger, Benedikt and Adam, Martin and R{\""u}hr, Alexander and Benlian, Alexander},
  year = {2021},
  journal = {Business \& Information Systems Engineering}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,Advising,Decision-maker,"Alter decision outcomes, Change trust",no such info,ability to learn,NA,"Textual, Conversational/Natural Language",Yes,Yes
2-4910,aisel,What do i do in a world of artificial intelligence? Investigating the impact of substitutive decision-making ai systems on employees' professional role identity,"Artificial intelligence (AI) systems in the workplace increasingly substitute for employees' tasks, responsibilities, and decision-making. Consequently, employees must relinquish core activities of their work processes without the ability to interact with the AI system (e.g., to influence decision-making processes or adapt or overrule decision-making outcomes). To deepen our understanding of how substitutive decision-making AI systems affect employees' professional role identity and how employees adapt their identity in response to the system, we conducted an in-depth case study of a company in the area of loan consulting. We qualitatively analyzed more than 60 interviews with employees and managers. Our research contributes to the literature on IS and identity by disclosing mechanisms through which employees strengthen and protect their professional role identity despite being unable to directly interact with the AI system. Further, we highlight the boundary conditions for introducing an AI system and contribute to the body of empirical research on the potential downsides of AI.",NA,https://aisel.aisnet.org/jais/vol22/iss2/9,Journal of the Association for Information Systems,"Strich, Franz;Mayer, Anne-Sophie;Fiedler, Marina",2021,347,"@article{2-4910,
  title={What do I do in a world of artificial intelligence? Investigating the impact of substitutive decision-making AI systems on employees' professional role identity},
  author={Strich, Franz and Mayer, Anne-Sophie and Fiedler, Marina},
  year={2021},
  journal={Journal of the Association for Information Systems}
}",Empirical contributions,"Everyday / Employment / Public Service, Finance / Business / Economy",Operational,Executing,"Decision-maker, Decision-subject","Alter decision outcomes, Change cognitive demands, Change affective-perceptual, Restrict human agency",Change AI responses,NA,NA,"Textual, Conversational/Natural Language, Autonomous System",Yes,Yes
2-4911,aisel,What if an ai told you that 2 + 2 is 5? Conformity to algorithmic recommendations,"Organizations are increasingly integrating human-AI decision-making processes. Therefore, it is crucial to make sure humans possess the ability to call out algorithms' biases and errors. Biased algorithms were shown to negatively affect access to loans, hiring processes, judicial decisions, and more. Thus, studying workers' ability to balance reliance on algorithmic recommendations and critical judgment towards them, holds immense importance and potential social gain. In this study, we focused on gig-economy platform workers (MTurk) and simple perceptual judgment tasks, in which algorithmic mistakes are relatively visible. In a series of experiments, we present workers with misleading advice perceived to be the results of AI calculations and measure their conformity to the erroneous recommendations. Our initial results indicate that such algorithmic recommendations hold strong persuasive power, even compared to recommendations that are presented as crowd-based. Our study also explores the effectiveness of mechanisms for reducing workers' conformity in these situations.",NA,https://aisel.aisnet.org/icis2020/hci_artintel/hci_artintel/17,International Conference on Information Systems,"Liel, Yotam;Zalmanson, Lior",2020,33,"@inproceedings{2-4911,
  title={What if an AI told you that 2 + 2 is 5? Conformity to algorithmic recommendations},
  author={Liel, Yotam and Zalmanson, Lior},
  year={2020},
  booktitle={International Conference on Information Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,Advising,Decision-maker,"Change trust, Alter decision outcomes",no such info,"wrong recommendations, recommendations",NA,Visual,Yes,Yes
2-4913,aisel,When can ai reduce individuals' anchoring bias and enhance decision accuracy? Evidence from multiple longitudinal experiments,"This study aimed to identify and explain the mechanism underlying decision-making behaviors adaptive to AI advice. We develop a new theoretical framework by drawing on the anchoring effect and the literature on experiential learning. We focus on two factors: (1) the difference between individuals' initial estimates and AI advice and (2) the existence of a second anchor (i.e., previous-year credit scores). We conducted two longitudinal experiments in the corporate credit rating context, where correct answers exist stochastically. We found that individuals exhibit some paradoxical behaviors. With greater differences and no second anchor, individuals are more likely to make adjustment efforts, but their initial estimates remain strong anchors. Yet, in multiple-anchor contexts individuals tend to diminish dependence on their initial estimates. We also found that the accuracy of individuals was dependent on their debiasing efforts.",NA,https://aisel.aisnet.org/hicss-55/da/emerging_markets/3,Hawaii International Conference on System Sciences,"Lee, Kyootai;Woo, Han-Gyun;Cho, Wooje;De Jong, Simon",2022,5,"@inproceedings{2-4913,
  title = {When can AI reduce individuals' anchoring bias and enhance decision accuracy? Evidence from multiple longitudinal experiments},
  author = {Lee, Kyootai and Woo, Han-Gyun and Cho, Wooje and De Jong, Simon},
  year = {2022},
  booktitle = {Hawaii International Conference on System Sciences}
}",Empirical contributions,Finance / Business / Economy,Individual,Advising,Decision-maker,"Change trust, Shape ethical norms, Alter decision outcomes",Update AI competence,"credit score, recommendations",NA,Textual,Yes,Yes
2-4916,aisel,When machines will take over? Algorithms for human-machine collaborative decision making in healthcare,"Artificial intelligence (AI) has increasingly become a popular alternative for performing tasks that are typically performed by humans. Mammography imaging is one context in which the role of AI is growing. Some experts claim that, with recent advancements in image processing algorithms and the increasing availability of data, AI will replace radiologists. Others argue that the rise of AI will change how diagnostic tasks are allocated, eventually paving the way for human-machine collaborative decision-making. In this research, we solve a hospital's AI acquisition problem for mammography imaging and redesign its operations for human-computer collaborative decision-making. To that end, we propose an optimization model for the hospital that minimizes costs related to mammography screening and determines whether and when a complete automation (AI alone) strategy or a delegation (collaboration between humans and machines) strategy is preferable to an expert-alone strategy. We find that the disease incidence relative to the ratio of follow-up against liability costs is an important determinant of whether the delegation strategy is preferable to the automation strategy. In addition, reductions in algorithmic cost could either result in the delegation (sharing of work between humans and machines) or full automation depending on the performance of the algorithm. Our work has implications beyond radiology imaging for the design of work in the AI era and in the human-machine collaboration context.",NA,https://aisel.aisnet.org/hicss-56/os/digital_transformation/11,Hawaii International Conference on System Sciences,"Ahsen, Eren;Ayvaci, Mehmet;Mookerjee, Radha",2023,2,"@inproceedings{2-4916,
  title={When machines will take over? Algorithms for human-machine collaborative decision making in healthcare},
  author={Ahsen, Eren and Ayvaci, Mehmet and Mookerjee, Radha},
  year={2023},
  booktitle={Hawaii International Conference on System Sciences}
}","Algorithmic contributions, Empirical contributions",Healthcare / Medicine / Surgery,Operational,"Analyzing, Collaborating, Executing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-4918,aisel,"Who takes responsibility for ai? A field study on ai-related task shifts, explainability, and responsibility attributions","The opaque and incomprehensible nature of artificial intelligence (AI) raises questions about who can and will take responsibility for AI in organizations. Examining the relation between explainability and responsibility, we explore how AI responsibility attributions unfold when 1) AI shifts tasks and roles of individuals and 2) these individuals lack comprehension of the task elements they are expected to take responsibility for. Through an in-depth qualitative field study in a large organization, we identify three types of responsibility attributions in decision-making with AI: a shared responsibility, a data science-centered responsibility, and a business domain expert-centered responsibility. These three prevalent types of responsibility attributions will be explained by the interaction of different shifts in AI-related tasks and corresponding AI explainability needs and actions in the organization. Our study contributes to the existing literature by demonstrating AI's impact on traditional responsibility assignment in day-to-day organizational practices.",NA,https://aisel.aisnet.org/icis2023/aiinbus/aiinbus/3,International Conference on Information Systems,"Thuis, Tamara;Li, Ting;van Heck, Eric",2023,3,"@inproceedings{2-4918,
  title = {Who Takes Responsibility for AI? A Field Study on AI-Related Task Shifts, Explainability, and Responsibility Attributions},
  author = {Thuis, Tamara and Li, Ting and van Heck, Eric},
  year = {2023},
  booktitle = {International Conference on Information Systems}
}",Empirical contributions,Everyday / Employment / Public Service,Operational,"Analyzing, Advising, Collaborating","Knowledge provider, Decision-maker","Change trust, Shift responsibility, Alter decision outcomes, Change cognitive demands",Shape AI for accountability,"recommendations, explanations",NA,"Textual, Visual, Autonomous System, Conversational/Natural Language",Yes,Yes
2-4919,aisel,Who's a good decision maker? Data-driven expert worker ranking under unobservable quality,"Evaluation of expert workers by their decision quality has substantial practical value, yet using other expert workers for decision quality evaluation tasks is costly and often infeasible. In this work, we frame the Ranking of Expert workers according to their unobserved decision Quality (REQ) -- without resorting to evaluation by other experts -- as a new Data Science problem. This problem is challenging, as the correct decisions are commonly unobservable and substantial parts of the information available to the decision maker is not available for retrospective decision evaluation. We propose a new machine learning approach to address this problem. We evaluate our method on one dataset representing real expert decisions and two public datasets, and find that our approach is successful in generating highly accurate rankings. Moreover, we observe that our approach's superiority over the baseline is particularly prominent as evaluation settings become increasingly challenging.",NA,https://aisel.aisnet.org/icis2016/DataScience/Presentations/7,International Conference on Information Systems,"Geva, Tomer;Saar-Tsechansky, Maytal",2016,8,"@inproceedings{2-4919,
  title = {Who's a good decision maker? Data-driven expert worker ranking under unobservable quality},
  author = {Geva, Tomer and Saar-Tsechansky, Maytal},
  year = {2016},
  booktitle = {International Conference on Information Systems},
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Organizational,"Analyzing, Advising, Monitoring","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-4921,aisel,Why users accept discriminatory pricing: the roles of ai agent's presence and explanation,"Discriminatory pricing practices have raised consumers' negative reactions. This study investigates how AI agent's presence and the use of explanations impact consumers' acceptance of discriminatory pricing. A scenario-based experiment revealed that AI agent's presence negatively moderates the negative relationship between offer unfavorability and offer acceptance, which is mediated by perceived justice and invasion of privacy. Moreover, this research indicated that for unfavored price, environment-based explanation is more effective than user-based explanation and the positive effect of AI agent's presence on offer acceptance is more pronounced when providing user-based explanations. This study contributes to price management literature and AI decision literature by illustrating how the AI agent's presence asymmetrically shapes consumers' perceptions of offer outcomes, enriching our understanding of consumer responses to AI. The findings have implications for firms managing discriminatory pricing, offering insights into optimal AI agents and explanation utilization for enhancing customer experience and business performance.",NA,https://aisel.aisnet.org/icis2023/hti/hti/6,International Conference on Information Systems,"PENG, XIAO;Peng, Xixian;Xu, David (Jingjun)",2023,0,"@inproceedings{2-4921,
  title = {Why users accept discriminatory pricing: the roles of AI agent's presence and explanation},
  author = {Peng, Xiao and Peng, Xixian and Xu, David (Jingjun)},
  year = {2023},
  booktitle = {International Conference on Information Systems}
}",Empirical contributions,Finance / Business / Economy,Individual,"Explaining, Executing",Decision-subject,"Alter decision outcomes, Change cognitive demands, Shape ethical norms",no such info,textual explanations,NA,"Physical / Embodiment, Autonomous System",Yes,Yes
2-4923,aisel,Will humans-in-the-loop become borgs? Merits and pitfalls of working with ai,"We analyze how advice from an AI affects complementarities between humans and AI, in particular what humans know that an AI does not know: ``unique human knowledge.'' In a multi-method study consisting of an analytical model, experimental studies, and a simulation study, our main finding is that human choices converge toward similar responses improving individual accuracy. However, as overall individual accuracy of the group of humans improves, the individual unique human knowledge decreases. Based on this finding, we claim that humans interacting with AI behave like ``Borgs,'' that is, cyborg creatures with strong individual performance but no human individuality. We argue that the loss of unique human knowledge may lead to several undesirable outcomes in a host of human--AI decision environments. We demonstrate this harmful impact on the ``wisdom of crowds.'' Simulation results based on our experimental data suggest that groups of humans interacting with AI are far less effective as compared to human groups without AI assistance. We suggest mitigation techniques to create environments that can provide the best of both worlds (e.g., by personalizing AI advice). We show that such interventions perform well individually as well as in wisdom of crowds settings.",NA,https://aisel.aisnet.org/misq/vol45/iss3/20,Management Information Systems Quarterly,"Fuegener, Andreas;Grahl, J{\""o}rn;Gupta, Alok;Ketter, Wolfgang",2021,560,"@article{2-4923,
  title={Will humans-in-the-loop become borgs? Merits and pitfalls of working with AI},
  author={Fuegener, Andreas and Grahl, J{\""o}rn and Gupta, Alok and Ketter, Wolfgang},
  year={2021},
  journal={Management Information Systems Quarterly}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,Advising,Decision-maker,"Alter decision outcomes, Change trust","Update AI competence, Change AI responses","recommendations, certainty",intuition,Textual,Yes,Yes
2-4924,aisel,Work design in healthcare artificial intelligence applications: the role of advice provision timing,"Artificial intelligence (AI) applications are rapidly revolutionizing the clinical practice in recent years, from imaging to disease screening and diagnosis to treatment planning to clinical outcome prediction. Despite the promising potential of healthcare AI, extant research has primarily focused on the development and validation of AI algorithms. In contrast, few studies have examined how to incorporate AI applications into the existing clinical workflow. More research is thus needed to understand how work design factors influence AI-aided clinical decision making. Drawing from the interpersonal advice taking literature, this study aims to investigate the effects of AI-advice provision timing (i.e., independent versus dependent timing) on physicians' advice taking behavior and decision outcomes in a clinical diagnosis setting. We will also examine the moderating role of physician expertise. To this end, we will conduct an experiment among physicians using a novel AI-based diagnostic algorithm. Theoretical and practical implications are discussed.",NA,https://aisel.aisnet.org/icis2020/is_health/is_health/10,International Conference on Information Systems,"Yin, Jiamin;Ngiam, Kee Yuan;Teo, Hock-Hai",2020,3,"@inproceedings{2-4924,
  title     = {Work Design in Healthcare Artificial Intelligence Applications: The Role of Advice Provision Timing},
  author    = {Yin, Jiamin and Ngiam, Kee Yuan and Teo, Hock-Hai},
  year      = {2020},
  booktitle = {International Conference on Information Systems}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Knowledge provider",NA,NA,AI advice,NA,"Textual, Conversational/Natural Language, Autonomous System",Yes,No
2-4925,aisel,Working with ai: how attitudes shape human-ai collaboration,"Recent research shows that people tend to avoid relying on artificial intelligence (AI) when making decisions at work. Although previous studies have investigated the causes and effects of this phenomenon, the role of attitudes, which are considered as a crucial factor in biased decision-making, remains unexplored. To investigate whether and how attitudes toward AI influence decision-making in work-related situations, we developed and conducted a lab experiment in which 156 participants received AI advice on process improvement while performing a manufacturing task. Our results suggest that negative attitudes toward AI are associated with greater reluctance to accept AI advice, resulting in lower performance in work-related situations. Furthermore, we show that human involvement in the process of providing AI advice cannot mitigate the negative effects of strong negative attitudes.",NA,https://aisel.aisnet.org/icis2023/hti/hti/12,International Conference on Information Systems,"Glienke, Maximilian;Hartwich, Nicole Janine;Antons, David",2023,4,"@inproceedings{2-4925,
  title = {Working with AI: How Attitudes Shape Human-AI Collaboration},
  author = {Glienke, Maximilian and Hartwich, Nicole Janine and Antons, David},
  year = {2023},
  booktitle = {International Conference on Information Systems}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Advising, Collaborating",Decision-maker,"Alter decision outcomes, Change trust, Change affective-perceptual",no such info,recommendations,attitudes,"3D, Visual, Interactive interface",Yes,Yes
2-4978,ama,Appraise-ai tool for quantitative evaluation of ai studies for clinical decision support,"Artificial intelligence (AI) has gained considerable attention in health care, yet concerns have been raised around appropriate methods and fairness. Current AI reporting guidelines do not provide a means of quantifying overall quality of AI research, limiting their ability to compare models addressing the same clinical question.To develop a tool (APPRAISE-AI) to evaluate the methodological and reporting quality of AI prediction models for clinical decision support.This quality improvement study evaluated AI studies in the model development, silent, and clinical trial phases using the APPRAISE-AI tool, a quantitative method for evaluating quality of AI studies across 6 domains: clinical relevance, data quality, methodological conduct, robustness of results, reporting quality, and reproducibility. These domains included 24 items with a maximum overall score of 100 points. Points were assigned to each item, with higher points indicating stronger methodological or reporting quality. The tool was applied to a systematic review on machine learning to estimate sepsis that included articles published until September 13, 2019. Data analysis was performed from September to December 2022.The primary outcomes were interrater and intrarater reliability and the correlation between APPRAISE-AI scores and expert scores, 3-year citation rate, number of Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2) low risk-of-bias domains, and overall adherence to the Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis (TRIPOD) statement.A total of 28 studies were included. Overall APPRAISE-AI scores ranged from 33 (low quality) to 67 (high quality). Most studies were moderate quality. The 5 lowest scoring items included source of data, sample size calculation, bias assessment, error analysis, and transparency. Overall APPRAISE-AI scores were associated with expert scores (Spearman ρ, 0.82; 95\% CI, 0.64-0.91; P \&lt; .001), 3-year citation rate (Spearman ρ, 0.69; 95\% CI, 0.43-0.85; P \&lt; .001), number of QUADAS-2 low risk-of-bias domains (Spearman ρ, 0.56; 95\% CI, 0.24-0.77; P = .002), and adherence to the TRIPOD statement (Spearman ρ, 0.87; 95\% CI, 0.73-0.94; P \&lt; .001). Intraclass correlation coefficient ranges for interrater and intrarater reliability were 0.74 to 1.00 for individual items, 0.81 to 0.99 for individual domains, and 0.91 to 0.98 for overall scores.In this quality improvement study, APPRAISE-AI demonstrated strong interrater and intrarater reliability and correlated well with several study quality measures. This tool may provide a quantitative approach for investigators, reviewers, editors, and funding organizations to compare the research quality across AI studies for clinical decision support.",10.1001/jamanetworkopen.2023.35377,https://doi.org/10.1001/jamanetworkopen.2023.35377,JAMA Network Open,"Kwong, Jethro C. C.;Khondker, Adree;Lajkosz, Katherine;McDermott, Matthew B. A.;Frigola, Xavier Borrat;McCradden, Melissa D.;Mamdani, Muhammad;Kulkarni, Girish S.;Johnson, Alistair E. W.",2023,75,"@article{2-4978,
  title={Appraise-ai tool for quantitative evaluation of ai studies for clinical decision support},
  author={Kwong, Jethro C. C. and Khondker, Adree and Lajkosz, Katherine and McDermott, Matthew B. A. and Frigola, Xavier Borrat and McCradden, Melissa D. and Mamdani, Muhammad and Kulkarni, Girish S. and Johnson, Alistair E. W.},
  year={2023},
  doi={10.1001/jamanetworkopen.2023.35377},
  journal={JAMA Network Open}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-4995,ama,Assessment of diagnostic performance of dermatologists cooperating with a convolutional neural network in a prospective clinical study: human with machine,"Studies suggest that convolutional neural networks (CNNs) perform equally to trained dermatologists in skin lesion classification tasks. Despite the approval of the first neural networks for clinical use, prospective studies demonstrating benefits of human with machine cooperation are lacking.To assess whether dermatologists benefit from cooperation with a market-approved CNN in classifying melanocytic lesions.In this prospective diagnostic 2-center study, dermatologists performed skin cancer screenings using naked-eye examination and dermoscopy. Dermatologists graded suspect melanocytic lesions by the probability of malignancy (range 0-1, threshold for malignancy ≥0.5) and indicated management decisions (no action, follow-up, excision). Next, dermoscopic images of suspect lesions were assessed by a market-approved CNN, Moleanalyzer Pro (FotoFinder Systems). The CNN malignancy scores (range 0-1, threshold for malignancy ≥0.5) were transferred to dermatologists with the request to re-evaluate lesions and revise initial decisions in consideration of CNN results. Reference diagnoses were based on histopathologic examination in 125 (54.8\%) lesions or, in the case of nonexcised lesions, on clinical follow-up data and expert consensus. Data were collected from October 2020 to October 2021.Primary outcome measures were diagnostic sensitivity and specificity of dermatologists alone and dermatologists cooperating with the CNN. Accuracy and receiver operator characteristic area under the curve (ROC AUC) were considered as additional measures.A total of 22 dermatologists detected 228 suspect melanocytic lesions (190 nevi, 38 melanomas) in 188 patients (mean [range] age, 53.4 [19-91] years; 97 [51.6\%] male patients). Diagnostic sensitivity and specificity significantly improved when dermatologists additionally integrated CNN results into decision-making (mean sensitivity from 84.2\% [95\% CI, 69.6\%-92.6\%] to 100.0\% [95\% CI, 90.8\%-100.0\%]; P = .03; mean specificity from 72.1\% [95\% CI, 65.3\%-78.0\%] to 83.7\% [95\% CI, 77.8\%-88.3\%]; P \&lt; .001; mean accuracy from 74.1\% [95\% CI, 68.1\%-79.4\%] to 86.4\% [95\% CI, 81.3\%-90.3\%]; P \&lt; .001; and mean ROC AUC from 0.895 [95\% CI, 0.836-0.954] to 0.968 [95\% CI, 0.948-0.988]; P = .005). In addition, the CNN alone achieved a comparable sensitivity, higher specificity, and higher diagnostic accuracy compared with dermatologists alone in classifying melanocytic lesions. Moreover, unnecessary excisions of benign nevi were reduced by 19.2\%, from 104 (54.7\%) of 190 benign nevi to 84 nevi when dermatologists cooperated with the CNN (P \&lt; .001). Most lesions were examined by dermatologists with 2 to 5 years (96, 42.1\%) or less than 2 years of experience (78, 34.2\%); others (54, 23.7\%) were evaluated by dermatologists with more than 5 years of experience. Dermatologists with less dermoscopy experience cooperating with the CNN had the most diagnostic improvement compared with more experienced dermatologists.In this prospective diagnostic study, these findings suggest that dermatologists may improve their performance when they cooperate with the market-approved CNN and that a broader application of this human with machine approach could be beneficial for dermatologists and patients.",10.1001/jamadermatol.2023.0905,https://doi.org/10.1001/jamadermatol.2023.0905,Journal of the American Medical Association Dermatology,"Winkler, Julia K.;Blum, Andreas;Kommoss, Katharina;Enk, Alexander;Toberer, Ferdinand;Rosenberger, Albert;Haenssle, Holger A.",2023,0,"@article{2-4995,
  title={Assessment of diagnostic performance of dermatologists cooperating with a convolutional neural network in a prospective clinical study: human with machine},
  author={Winkler, Julia K. and Blum, Andreas and Kommoss, Katharina and Enk, Alexander and Toberer, Ferdinand and Rosenberger, Albert and Haenssle, Holger A.},
  year={2023},
  journal={Journal of the American Medical Association Dermatology},
  doi={10.1001/jamadermatol.2023.0905}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Knowledge provider","Alter decision outcomes, Change trust, Change cognitive demands, Change affective-perceptual","Change AI responses, Update AI competence",preliminary diagnoses,"corrective feedback, domain knowledge","Visual, Conversational/Natural Language, Interactive interface",Yes,Yes
2-5032,ama,Automated diagnosis of plus disease in retinopathy of prematurity using deep convolutional neural networks,"Retinopathy of prematurity (ROP) is a leading cause of childhood blindness worldwide. The decision to treat is primarily based on the presence of plus disease, defined as dilation and tortuosity of retinal vessels. However, clinical diagnosis of plus disease is highly subjective and variable.To implement and validate an algorithm based on deep learning to automatically diagnose plus disease from retinal photographs.A deep convolutional neural network was trained using a data set of 5511 retinal photographs. Each image was previously assigned a reference standard diagnosis (RSD) based on consensus of image grading by 3 experts and clinical diagnosis by 1 expert (ie, normal, pre–plus disease, or plus disease). The algorithm was evaluated by 5-fold cross-validation and tested on an independent set of 100 images. Images were collected from 8 academic institutions participating in the Imaging and Informatics in ROP (i-ROP) cohort study. The deep learning algorithm was tested against 8 ROP experts, each of whom had more than 10 years of clinical experience and more than 5 peer-reviewed publications about ROP. Data were collected from July 2011 to December 2016. Data were analyzed from December 2016 to September 2017.A deep learning algorithm trained on retinal photographs.Receiver operating characteristic analysis was performed to evaluate performance of the algorithm against the RSD. Quadratic-weighted κ coefficients were calculated for ternary classification (ie, normal, pre–plus disease, and plus disease) to measure agreement with the RSD and 8 independent experts.Of the 5511 included retinal photographs, 4535 (82.3\%) were graded as normal, 805 (14.6\%) as pre–plus disease, and 172 (3.1\%) as plus disease, based on the RSD. Mean (SD) area under the receiver operating characteristic curve statistics were 0.94 (0.01) for the diagnosis of normal (vs pre–plus disease or plus disease) and 0.98 (0.01) for the diagnosis of plus disease (vs normal or pre–plus disease). For diagnosis of plus disease in an independent test set of 100 retinal images, the algorithm achieved a sensitivity of 93\% with 94\% specificity. For detection of pre–plus disease or worse, the sensitivity and specificity were 100\% and 94\%, respectively. On the same test set, the algorithm achieved a quadratic-weighted κ coefficient of 0.92 compared with the RSD, outperforming 6 of 8 ROP experts.This fully automated algorithm diagnosed plus disease in ROP with comparable or better accuracy than human experts. This has potential applications in disease detection, monitoring, and prognosis in infants at risk of ROP.",10.1001/jamaophthalmol.2018.1934,https://doi.org/10.1001/jamaophthalmol.2018.1934,JAMA Ophthalmology,"Brown, James M.;Campbell, J. Peter;Beers, Andrew;Chang, Ken;Ostmo, Susan;Chan, R. V. Paul;Dy, Jennifer;Erdogmus, Deniz;Ioannidis, Stratis;Kalpathy-Cramer, Jayashree;Chiang, Michael F.;for the Imaging;Informatics in Retinopathy of Prematurity (i-ROP) Research Consortium",2018,0,"@article{2-5032,
  title = {Automated diagnosis of plus disease in retinopathy of prematurity using deep convolutional neural networks},
  author = {Brown, James M. and Campbell, J. Peter and Beers, Andrew and Chang, Ken and Ostmo, Susan and Chan, R. V. Paul and Dy, Jennifer and Erdogmus, Deniz and Ioannidis, Stratis and Kalpathy-Cramer, Jayashree and Chiang, Michael F. and for the Imaging and Informatics in Retinopathy of Prematurity (i-ROP) Research Consortium},
  year = {2018},
  doi = {10.1001/jamaophthalmol.2018.1934},
  journal = {JAMA Ophthalmology}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Forecasting, Executing","Knowledge provider, Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-5033,ama,Automated interpretation of clinical electroencephalograms using artificial intelligence,"Electroencephalograms (EEGs) are a fundamental evaluation in neurology but require special expertise unavailable in many regions of the world. Artificial intelligence (AI) has a potential for addressing these unmet needs. Previous AI models address only limited aspects of EEG interpretation such as distinguishing abnormal from normal or identifying epileptiform activity. A comprehensive, fully automated interpretation of routine EEG based on AI suitable for clinical practice is needed.To develop and validate an AI model (Standardized Computer-based Organized Reporting of EEG–Artificial Intelligence [SCORE-AI]) with the ability to distinguish abnormal from normal EEG recordings and to classify abnormal EEG recordings into categories relevant for clinical decision-making: epileptiform-focal, epileptiform-generalized, nonepileptiform-focal, and nonepileptiform-diffuse.In this multicenter diagnostic accuracy study, a convolutional neural network model, SCORE-AI, was developed and validated using EEGs recorded between 2014 and 2020. Data were analyzed from January 17, 2022, until November 14, 2022. A total of 30 493 recordings of patients referred for EEG were included into the development data set annotated by 17 experts. Patients aged more than 3 months and not critically ill were eligible. The SCORE-AI was validated using 3 independent test data sets: a multicenter data set of 100 representative EEGs evaluated by 11 experts, a single-center data set of 9785 EEGs evaluated by 14 experts, and for benchmarking with previously published AI models, a data set of 60 EEGs with external reference standard. No patients who met eligibility criteria were excluded.Diagnostic accuracy, sensitivity, and specificity compared with the experts and the external reference standard of patients’ habitual clinical episodes obtained during video-EEG recording.The characteristics of the EEG data sets include development data set (N = 30 493; 14 980 men; median age, 25.3 years [95\% CI, 1.3-76.2 years]), multicenter test data set (N = 100; 61 men, median age, 25.8 years [95\% CI, 4.1-85.5 years]), single-center test data set (N = 9785; 5168 men; median age, 35.4 years [95\% CI, 0.6-87.4 years]), and test data set with external reference standard (N = 60; 27 men; median age, 36 years [95\% CI, 3-75 years]). The SCORE-AI achieved high accuracy, with an area under the receiver operating characteristic curve between 0.89 and 0.96 for the different categories of EEG abnormalities, and performance similar to human experts. Benchmarking against 3 previously published AI models was limited to comparing detection of epileptiform abnormalities. The accuracy of SCORE-AI (88.3\%; 95\% CI, 79.2\%-94.9\%) was significantly higher than the 3 previously published models (P \&lt; .001) and similar to human experts.In this study, SCORE-AI achieved human expert level performance in fully automated interpretation of routine EEGs. Application of SCORE-AI may improve diagnosis and patient care in underserved areas and improve efficiency and consistency in specialized epilepsy centers.",10.1001/jamaneurol.2023.1645,https://doi.org/10.1001/jamaneurol.2023.1645,Journal of the American Medical Association Neurology,"Tveit, Jesper;Aurlien, Harald;Plis, Sergey;Calhoun, Vince D.;Tatum, William O.;Schomer, Donald L.;Arntsen, Vibeke;Cox, Fieke;Fahoum, Firas;Gallentine, William B.;Gardella, Elena;Hahn, Cecil D.;Husain, Aatif M.;Kessler, Sudha;Kural, Mustafa Aykut;Nascimento, Fábio A.;Tankisi, Hatice;Ulvin, Line B.;Wennberg, Richard;Beniczky, Sándor",2023,170,"@article{2-5033,
  title={Automated interpretation of clinical electroencephalograms using artificial intelligence},
  author={Tveit, Jesper and Aurlien, Harald and Plis, Sergey and Calhoun, Vince D. and Tatum, William O. and Schomer, Donald L. and Arntsen, Vibeke and Cox, Fieke and Fahoum, Firas and Gallentine, William B. and Gardella, Elena and Hahn, Cecil D. and Husain, Aatif M. and Kessler, Sudha and Kural, Mustafa Aykut and Nascimento, Fábio A. and Tankisi, Hatice and Ulvin, Line B. and Wennberg, Richard and Beniczky, Sándor},
  year={2023},
  doi={10.1001/jamaneurol.2023.1645},
  journal={Journal of the American Medical Association Neurology}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Analyzing","Knowledge provider, Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-5080,ama,"Comparison of an artificial intelligence–enabled patient decision aid vs educational material on decision quality, shared decision-making, patient experience, and functional outcomes in adults with knee osteoarthritis: a randomized clinical trial","Decision aids can help inform appropriate selection of total knee replacement (TKR) for advanced knee osteoarthritis (OA). However, few decision aids combine patient education, preference assessment, and artificial intelligence (AI) using patient-reported outcome measurement data to generate personalized estimations of outcomes to augment shared decision-making (SDM).To assess the effect of an AI-enabled patient decision aid that includes education, preference assessment, and personalized outcome estimations (using patient-reported outcome measurements) on decision quality, patient experience, functional outcomes, and process-level outcomes among individuals with advanced knee OA considering TKR in comparison with education only.This randomized clinical trial at a single US academic orthopedic practice included 129 new adult patients presenting for OA-related knee pain from March 2019 to January 2020. Data were analyzed from April to May 2020.Patients were randomized into a group that received a decision aid including patient education, preference assessment, and personalized outcome estimations (intervention group) or a group receiving educational material only (control group) alongside usual care.The primary outcome was decision quality, measured using the Knee OA Decision Quality Instrument (K-DQI). Secondary outcomes were collaborative decision-making (assessed using the CollaboRATE survey), patient satisfaction with consultation (using a numerical rating scale), Knee Injury and Osteoarthritis Outcome Score Joint Replacement (KOOS JR) score, consultation time, TKR rate, and treatment concordance.A total of 69 patients in the intervention group (46 [67\%] women) and 60 patients in the control group (37 [62\%] women) were included in the analysis. The intervention group showed better decisional quality (K-DQI mean difference, 20.0\%; SE, 3.02; 95\% CI, 14.2\%-26.1\%; P \&lt; .001), collaborative decision-making (CollaboRATE, 8 of 69 [12\%] vs 28 of 60 [47\%] patients below median; P \&lt; .001), satisfaction (numerical rating scale, 9 of 65 [14\%] vs 19 of 58 [33\%] patients below median; P = .01), and improved functional outcomes at 4 to 6 months (mean [SE] KOOS JR, 4.9 [2.24] points higher in intervention group; 95\% CI, 0.8-9.0 points; P = .02). The intervention did not significantly affect consultation time (mean [SE] difference, 2.23 [2.18] minutes; P = .31), TKR rates (16 of 69 [23\%] vs 7 of 60 [12\%] patients; P = .11), or treatment concordance (58 of 69 [84\%] vs 44 of 60 [73\%] patients; P = .19).In this randomized clinical trial, an AI-enabled decision aid significantly improved decision quality, level of SDM, satisfaction, and physical limitations without significantly impacting consultation times, TKR rates, or treatment concordance in patients with knee OA considering TKR. Decision aids using a personalized, data-driven approach can enhance SDM in the management of knee OA.ClinicalTrials.gov Identifier: NCT03956004",10.1001/jamanetworkopen.2020.37107,https://doi.org/10.1001/jamanetworkopen.2020.37107,JAMA Network Open,"Jayakumar, Prakash;Moore, Meredith G.;Furlough, Kenneth A.;Uhler, Lauren M.;Andrawis, John P.;Koenig, Karl M.;Aksan, Nazan;Rathouz, Paul J.;Bozic, Kevin J.",2021,14,"@article{2-5080,
  title = {Comparison of an artificial intelligence–enabled patient decision aid vs educational material on decision quality, shared decision-making, patient experience, and functional outcomes in adults with knee osteoarthritis: a randomized clinical trial},
  author = {Jayakumar, Prakash and Moore, Meredith G. and Furlough, Kenneth A. and Uhler, Lauren M. and Andrawis, John P. and Koenig, Karl M. and Aksan, Nazan and Rathouz, Paul J. and Bozic, Kevin J.},
  year = {2021},
  doi = {10.1001/jamanetworkopen.2020.37107},
  journal = {JAMA Network Open}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-subject, Decision-maker","Alter decision outcomes, Change affective-perceptual",no such info,NA,personalized settings,"Textual, Interactive interface",Yes,Yes
2-5127,ama,Development and clinical evaluation of an artificial intelligence support tool for improving telemedicine photo quality,"Telemedicine use accelerated during the COVID-19 pandemic, and skin conditions were a common use case. However, many images submitted may be of insufficient quality for making a clinical determination.To determine whether an artificial intelligence (AI) decision support tool, a machine learning algorithm, could improve the quality of images submitted for telemedicine by providing real-time feedback and explanations to patients.This quality improvement study with an AI performance component and single-arm clinical pilot study component was conducted from March 2020 to October 2021. After training, the AI decision support tool was tested on 357 retrospectively collected telemedicine images from Stanford telemedicine from March 2020 to June 2021. Subsequently, a single-arm clinical pilot study was conducted to assess feasibility with 98 patients in the Stanford Department of Dermatology across 2 clinical sites from July 2021 to October 2021. For the clinical pilot study, inclusion criteria for patients included being adults (aged ≥18 years), presenting to clinic for a skin condition, and being able to photograph their own skin with a smartphone.During the clinical pilot study, patients were given a handheld smartphone device with a machine learning algorithm interface loaded and were asked to take images of any lesions of concern. Patients were able to review and retake photos prior to submitting, so each submitted photo met the patient’s assumed standard of clinical acceptability. A machine learning algorithm then gave the patient feedback on whether the image was acceptable. If the image was rejected, the patient was provided a reason by the AI decision support tool and allowed to retake the photos.The main outcome of the retrospective image analysis was the receiver operator curve area under the curve (ROC-AUC). The main outcome of the clinical pilot study was the image quality difference between the baseline images and the images approved by AI decision support.Of the 98 patients included, the mean (SD) age was 49.8 (17.6) years, and 50 (51\%) of the patients were male. On retrospective telemedicine images, the machine learning algorithm effectively identified poor-quality images (ROC-AUC of 0.78) and the reason for poor quality (blurry ROC-AUC of 0.84; lighting issues ROC-AUC of 0.70). The performance was consistent across age and sex. In the clinical pilot study, patient use of the machine learning algorithm was associated with improved image quality. An AI algorithm was associated with reduction in the number of patients with a poor-quality image by 68.0\%.In this quality improvement study, patients use of the AI decision support with a machine learning algorithm was associated with improved quality of skin disease photographs submitted for telemedicine use.",10.1001/jamadermatol.2023.0091,https://doi.org/10.1001/jamadermatol.2023.0091,Journal of the American Medical Association Dermatology,"Vodrahalli, Kailas;Ko, Justin;Chiou, Albert S.;Novoa, Roberto;Abid, Abubakar;Phung, Michelle;Yekrang, Kiana;Petrone, Paige;Zou, James;Daneshjou, Roxana",2023,23,"@article{2-5127,
  title={Development and clinical evaluation of an artificial intelligence support tool for improving telemedicine photo quality},
  author={Vodrahalli, Kailas and Ko, Justin and Chiou, Albert S. and Novoa, Roberto and Abid, Abubakar and Phung, Michelle and Yekrang, Kiana and Petrone, Paige and Zou, James and Daneshjou, Roxana},
  year={2023},
  doi={10.1001/jamadermatol.2023.0091},
  journal={Journal of the American Medical Association Dermatology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Executing, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-5129,ama,Development and performance of the pulmonary embolism result forecast model (perform) for computed tomography clinical decision support,"Pulmonary embolism (PE) is a life-threatening clinical problem, and computed tomographic imaging is the standard for diagnosis. Clinical decision support rules based on PE risk-scoring models have been developed to compute pretest probability but are underused and tend to underperform in practice, leading to persistent overuse of CT imaging for PE.To develop a machine learning model to generate a patient-specific risk score for PE by analyzing longitudinal clinical data as clinical decision support for patients referred for CT imaging for PE.In this diagnostic study, the proposed workflow for the machine learning model, the Pulmonary Embolism Result Forecast Model (PERFORM), transforms raw electronic medical record (EMR) data into temporal feature vectors and develops a decision analytical model targeted toward adult patients referred for CT imaging for PE. The model was tested on holdout patient EMR data from 2 large, academic medical practices. A total of 3397 annotated CT imaging examinations for PE from 3214 unique patients seen at Stanford University hospitals and clinics were used for training and validation. The models were externally validated on 240 unique patients seen at Duke University Medical Center. The comparison with clinical scoring systems was done on randomly selected 100 outpatient samples from Stanford University hospitals and clinics and 101 outpatient samples from Duke University Medical Center.Prediction performance of diagnosing acute PE was evaluated using ElasticNet, artificial neural networks, and other machine learning approaches on holdout data sets from both institutions, and performance of models was measured by area under the receiver operating characteristic curve (AUROC).Of the 3214 patients included in the study, 1704 (53.0\%) were women from Stanford University hospitals and clinics; mean (SD) age was 60.53 (19.43) years. The 240 patients from Duke University Medical Center used for validation included 132 women (55.0\%); mean (SD) age was 70.2 (14.2) years. In the samples for clinical scoring system comparisons, the 100 outpatients from Stanford University hospitals and clinics included 67 women (67.0\%); mean (SD) age was 57.74 (19.87) years, and the 101 patients from Duke University Medical Center included 59 women (58.4\%); mean (SD) age was 73.06 (15.3) years. The best-performing model achieved an AUROC performance of predicting a positive PE study of 0.90 (95\% CI, 0.87-0.91) on intrainstitutional holdout data with an AUROC of 0.71 (95\% CI, 0.69-0.72) on an external data set from Duke University Medical Center; superior AUROC performance and cross-institutional generalization of the model of 0.81 (95\% CI, 0.77-0.87) and 0.81 (95\% CI, 0.73-0.82), respectively, were noted on holdout outpatient populations from both intrainstitutional and extrainstitutional data.The machine learning model, PERFORM, may consider multitudes of applicable patient-specific risk factors and dependencies to arrive at a PE risk prediction that generalizes to new population distributions. This approach might be used as an automated clinical decision-support tool for patients referred for CT PE imaging to improve CT use.",10.1001/jamanetworkopen.2019.8719,https://doi.org/10.1001/jamanetworkopen.2019.8719,JAMA Network Open,"Banerjee, Imon;Sofela, Miji;Yang, Jaden;Chen, Jonathan H.;Shah, Nigam H.;Ball, Robyn;Mushlin, Alvin I.;Desai, Manisha;Bledsoe, Joseph;Amrhein, Timothy;Rubin, Daniel L.;Zamanian, Roham;Lungren, Matthew P.",2019,0,"@article{2-5129,
  title={Development and performance of the pulmonary embolism result forecast model (PERFORM) for computed tomography clinical decision support},
  author={Banerjee, Imon and Sofela, Miji and Yang, Jaden and Chen, Jonathan H. and Shah, Nigam H. and Ball, Robyn and Mushlin, Alvin I. and Desai, Manisha and Bledsoe, Joseph and Amrhein, Timothy and Rubin, Daniel L. and Zamanian, Roham and Lungren, Matthew P.},
  year={2019},
  doi={10.1001/jamanetworkopen.2019.8719},
  journal={JAMA Network Open}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-5131,ama,Development and validation of a deep learning algorithm for gleason grading of prostate cancer from biopsy specimens,"For prostate cancer, Gleason grading of the biopsy specimen plays a pivotal role in determining case management. However, Gleason grading is associated with substantial interobserver variability, resulting in a need for decision support tools to improve the reproducibility of Gleason grading in routine clinical practice.To evaluate the ability of a deep learning system (DLS) to grade diagnostic prostate biopsy specimens.The DLS was evaluated using 752 deidentified digitized images of formalin-fixed paraffin-embedded prostate needle core biopsy specimens obtained from 3 institutions in the United States, including 1 institution not used for DLS development. To obtain the Gleason grade group (GG), each specimen was first reviewed by 2 expert urologic subspecialists from a multi-institutional panel of 6 individuals (years of experience: mean, 25 years; range, 18-34 years). A third subspecialist reviewed discordant cases to arrive at a majority opinion. To reduce diagnostic uncertainty, all subspecialists had access to an immunohistochemical-stained section and 3 histologic sections for every biopsied specimen. Their review was conducted from December 2018 to June 2019.The frequency of the exact agreement of the DLS with the majority opinion of the subspecialists in categorizing each tumor-containing specimen as 1 of 5 categories: nontumor, GG1, GG2, GG3, or GG4-5. For comparison, the rate of agreement of 19 general pathologists’ opinions with the subspecialists’ majority opinions was also evaluated.For grading tumor-containing biopsy specimens in the validation set (n = 498), the rate of agreement with subspecialists was significantly higher for the DLS (71.7\%; 95\% CI, 67.9\%-75.3\%) than for general pathologists (58.0\%; 95\% CI, 54.5\%-61.4\%) (P \&lt; .001). In subanalyses of biopsy specimens from an external validation set (n = 322), the Gleason grading performance of the DLS remained similar. For distinguishing nontumor from tumor-containing biopsy specimens (n = 752), the rate of agreement with subspecialists was 94.3\% (95\% CI, 92.4\%-95.9\%) for the DLS and similar at 94.7\% (95\% CI, 92.8\%-96.3\%) for general pathologists (P = .58).In this study, the DLS showed higher proficiency than general pathologists at Gleason grading prostate needle core biopsy specimens and generalized to an independent institution. Future research is necessary to evaluate the potential utility of using the DLS as a decision support tool in clinical workflows and to improve the quality of prostate cancer grading for therapy decisions.",10.1001/jamaoncol.2020.2485,https://doi.org/10.1001/jamaoncol.2020.2485,Journal of the American Medical Association Oncology,"Nagpal, Kunal;Foote, Davis;Tan, Fraser;Liu, Yun;Chen, Po-Hsuan Cameron;Steiner, David F.;Manoj, Naren;Olson, Niels;Smith, Jenny L.;Mohtashamian, Arash;Peterson, Brandon;Amin, Mahul B.;Evans, Andrew J.;Sweet, Joan W.;Cheung, Carol;van der Kwast, Theodorus;Sangoi, Ankur R.;Zhou, Ming;Allan, Robert;Humphrey, Peter A.;Hipp, Jason D.;Gadepalli, Krishna;Corrado, Greg S.;Peng, Lily H.;Stumpe, Martin C.;Mermel, Craig H.",2020,216,"@article{2-5131,
  title={Development and validation of a deep learning algorithm for gleason grading of prostate cancer from biopsy specimens},
  author={Nagpal, Kunal and Foote, Davis and Tan, Fraser and Liu, Yun and Chen, Po-Hsuan Cameron and Steiner, David F. and Manoj, Naren and Olson, Niels and Smith, Jenny L. and Mohtashamian, Arash and Peterson, Brandon and Amin, Mahul B. and Evans, Andrew J. and Sweet, Joan W. and Cheung, Carol and van der Kwast, Theodorus and Sangoi, Ankur R. and Zhou, Ming and Allan, Robert and Humphrey, Peter A. and Hipp, Jason D. and Gadepalli, Krishna and Corrado, Greg S. and Peng, Lily H. and Stumpe, Martin C. and Mermel, Craig H.},
  year={2020},
  doi={10.1001/jamaoncol.2020.2485},
  journal={Journal of the American Medical Association Oncology}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing, Forecasting","Decision-subject, Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-5135,ama,Development and validation of a machine learning model to estimate bacterial sepsis among immunocompromised recipients of stem cell transplant,"Sepsis disproportionately affects recipients of allogeneic hematopoietic cell transplant (allo-HCT), and timely detection is crucial. However, the atypical presentation of sepsis within this population makes detection challenging, and existing clinical sepsis tools have limited prognostic value among this high-risk population.To develop a full risk factor (demographic, transplant, clinical, and laboratory factors) and clinical factor–specific automated bacterial sepsis decision support tool for recipients of allo-HCT with potential bloodstream infections (PBIs).This prognostic study used data from adult recipients of allo-HCT transplanted at the Fred Hutchinson Cancer Research Center, Seattle, Washington, between June 2010 and June 2019 randomly divided into 70\% modeling and 30\% validation data sets. Tools were developed using the area under the curve (AUC) optimized SuperLearner, and their performance was compared with existing clinical sepsis tools: National Early Warning Score (NEWS), quick Sequential Organ Failure Assessment (qSOFA), and Systemic Inflammatory Response Syndrome (SIRS), using the validation data set. Data were analyzed between January and October of 2020.The primary outcome was high–sepsis risk bacteremia (culture confirmed gram-negative species, Staphylococcus aureus, or Streptococcus spp bacteremia), and the secondary outcomes were 10- and 28-day mortality. Tool discrimination and calibration were examined using accuracy metrics and expected vs observed probabilities.Between June 2010 and June 2019, 1943 recipients of allo-HCT received their first transplant, and 1594 recipients (median [interquartile range] age at transplant, 54 [43-63] years; 911 [57.2\%] men; 1242 individuals [77.9\%] identifying as White) experienced at least 1 PBI. Of 8131 observed PBIs, 238 (2.9\%) were high–sepsis risk bacteremia. Compared with high–sepsis risk bacteremia, the full decision support tool had the highest AUC (0.85; 95\% CI, 0.81-0.89), followed by the clinical factor–specific tool (0.72; 95\% CI, 0.66-0.78). SIRS had the highest AUC of existing tools (0.64; 95\% CI, 0.57-0.71). The full decision support tool had the highest AUCs for PBIs identified in inpatient (0.82; 95\% CI, 0.76-0.89) and outpatient (0.82; 95\% CI, 0.75-0.89) settings and for 10-day (0.85; 95\% CI, 0.79-0.91) and 28-day (0.80; 95\% CI, 0.75-0.84) mortality.These findings suggest that compared with existing tools and the clinical factor–specific tool, the full decision support tool had superior prognostic accuracy for the primary (high–sepsis risk bacteremia) and secondary (short-term mortality) outcomes in inpatient and outpatient settings. If used at the time of culture collection, the full decision support tool may inform more timely sepsis detection among recipients of allo-HCT.",10.1001/jamanetworkopen.2021.4514,https://doi.org/10.1001/jamanetworkopen.2021.4514,JAMA Network Open,"Lind, Margaret L.;Mooney, Stephen J.;Carone, Marco;Althouse, Benjamin M.;Liu, Catherine;Evans, Laura E.;Patel, Kevin;Vo, Phuong T.;Pergam, Steven A.;Phipps, Amanda I.",2021,24,"@article{2-5135,
  title={Development and validation of a machine learning model to estimate bacterial sepsis among immunocompromised recipients of stem cell transplant},
  author={Lind, Margaret L. and Mooney, Stephen J. and Carone, Marco and Althouse, Benjamin M. and Liu, Catherine and Evans, Laura E. and Patel, Kevin and Vo, Phuong T. and Pergam, Steven A. and Phipps, Amanda I.},
  year={2021},
  journal={JAMA Network Open},
  volume={},
  number={},
  pages={},
  doi={10.1001/jamanetworkopen.2021.4514}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-5136,ama,Development and validation of a machine learning model to explore tyrosine kinase inhibitor response in patients with stage iv egfr variant–positive non–small cell lung cancer,"An end-to-end efficacy evaluation approach for identifying progression risk after epidermal growth factor receptor (EGFR)–tyrosine kinase inhibitor (TKI) therapy in patients with stage IV EGFR variant–positive non–small cell lung cancer (NSCLC) is lacking.To propose a clinically applicable large-scale bidirectional generative adversarial network for predicting the efficacy of EGFR-TKI therapy in patients with NSCLC.This diagnostic/prognostic study enrolled 465 patients from January 1, 2010, to August 1, 2017, with follow-up from February 1, 2010, to June 1, 2020. A deep learning (DL) semantic signature to predict progression-free survival (PFS) was constructed in the training cohort, validated in 2 external validation and 2 control cohorts, and compared with the radiomics signature.An end-to-end bidirectional generative adversarial network framework was designed to predict the progression risk in patients with NSCLC.The primary end point was PFS, considering the time from the initiation of therapy to the date of recurrence, confirmed disease progression, or death.A total of 342 patients with stage IV EGFR variant–positive NSCLC receiving EGFR-TKI therapy met the inclusion criteria. Of these, 145 patients from 2 of the hospitals (n = 117 and 28) formed a training cohort (mean [SD] age, 61 [11] years; 87 [60.0\%] female), and the patients from 2 other hospitals comprised 2 external validation cohorts (validation cohort 1: n = 101; mean [SD] age, 57 [12] years; 60 [59.4\%] female; and validation cohort 2: n = 96, mean [SD] age, 58 [9] years; 55 [57.3\%] female). Fifty-six patients with advanced-stage EGFR variant–positive NSCLC (mean [SD] age, 52 [11] years; 26 [46.4\%] female) and 67 patients with advanced-stage EGFR wild-type NSCLC (mean [SD] age, 54 [10] years; 10 [15.0\%] female) who received first-line chemotherapy were included. A total of 90 (26\%) receiving EGFR-TKI therapy with a high risk of rapid disease progression were identified (median [range] PFS, 7.3 [1.4-32.0] months in the training cohort, 5.0 [0.6-34.6] months in validation cohort 1, and 6.4 [1.8-20.1] months, in validation cohort 2) using the DL semantic signature.The PFS decreased by 36\% (hazard ratio, 2.13; 95\% CI, 1.30-3.49; P \&lt; .001) compared with that in other patients (median [range] PFS, 11.5 [1.5-64.2] months in the training cohort, 10.9 [1.1-50.5] in validation cohort 1, and 8.9 [0.8-40.6] months in validation cohort 2. No significant differences were observed when comparing the PFS of high-risk patients receiving EGFR-TKI therapy with the chemotherapy cohorts (median PFS, 6.9 vs 4.4 months; P = .08). In terms of predicting the tumor progression risk after EGFR-TKI therapy, clinical decisions based on the DL semantic signature led to better survival outcomes than those based on radiomics signature across all risk probabilities by the decision curve analysis.This diagnostic/prognostic study provides a clinically applicable approach for identifying patients with stage IV EGFR variant–positive NSCLC who are not likely to benefit from EGFR-TKI therapy. The end-to-end DL-derived semantic features eliminated all manual interventions required while using previous radiomics methods and have a better prognostic performance.",10.1001/jamanetworkopen.2020.30442,https://doi.org/10.1001/jamanetworkopen.2020.30442,JAMA Network Open,"Song, Jiangdian;Wang, Lu;Ng, Nathan Norton;Zhao, Mingfang;Shi, Jingyun;Wu, Ning;Li, Weimin;Liu, Zaiyi;Yeom, Kristen W.;Tian, Jie",2020,69,"@article{2-5136,
  title={Development and validation of a machine learning model to explore tyrosine kinase inhibitor response in patients with stage IV EGFR variant-positive non-small cell lung cancer},
  author={Song, Jiangdian and Wang, Lu and Ng, Nathan Norton and Zhao, Mingfang and Shi, Jingyun and Wu, Ning and Li, Weimin and Liu, Zaiyi and Yeom, Kristen W. and Tian, Jie},
  year={2020},
  journal={JAMA Network Open},
  volume={},
  number={},
  pages={},
  doi={10.1001/jamanetworkopen.2020.30442}
}","Methodological contributions, Algorithmic contributions",Healthcare / Medicine / Surgery,Operational,"Analyzing, Forecasting, Executing","Decision-subject, Developer, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-5147,ama,Development and validation of an automated classifier to diagnose acute otitis media in children,"Acute otitis media (AOM) is a frequently diagnosed illness in children, yet the accuracy of diagnosis has been consistently low. Multiple neural networks have been developed to recognize the presence of AOM with limited clinical application.To develop and internally validate an artificial intelligence decision-support tool to interpret videos of the tympanic membrane and enhance accuracy in the diagnosis of AOM.This diagnostic study analyzed otoscopic videos of the tympanic membrane captured using a smartphone during outpatient clinic visits at 2 sites in Pennsylvania between 2018 and 2023. Eligible participants included children who presented for sick visits or wellness visits.Otoscopic examination.Using the otoscopic videos that were annotated by validated otoscopists, a deep residual-recurrent neural network was trained to predict both features of the tympanic membrane and the diagnosis of AOM vs no AOM. The accuracy of this network was compared with a second network trained using a decision tree approach. A noise quality filter was also trained to prompt users that the video segment acquired may not be adequate for diagnostic purposes.Using 1151 videos from 635 children (majority younger than 3 years of age), the deep residual-recurrent neural network had almost identical diagnostic accuracy as the decision tree network. The finalized deep residual-recurrent neural network algorithm classified tympanic membrane videos into AOM vs no AOM categories with a sensitivity of 93.8\% (95\% CI, 92.6\%-95.0\%) and specificity of 93.5\% (95\% CI, 92.8\%-94.3\%) and the decision tree model had a sensitivity of 93.7\% (95\% CI, 92.4\%-94.9\%) and specificity of 93.3\% (92.5\%-94.1\%). Of the tympanic membrane features outputted, bulging of the TM most closely aligned with the predicted diagnosis; bulging was present in 230 of 230 cases (100\%) in which the diagnosis was predicted to be AOM in the test set.These findings suggest that given its high accuracy, the algorithm and medical-grade application that facilitates image acquisition and quality filtering could reasonably be used in primary care or acute care settings to aid with automated diagnosis of AOM and decisions regarding treatment.",10.1001/jamapediatrics.2024.0011,https://doi.org/10.1001/jamapediatrics.2024.0011,Journal of the American Medical Association Pediatrics,"Shaikh, Nader;Conway, Shannon J.;Kovačević, Jelena;Condessa, Filipe;Shope, Timothy R.;Haralam, Mary Ann;Campese, Catherine;Lee, Matthew C.;Larsson, Tomas;Cavdar, Zafer;Hoberman, Alejandro",2024,67,"@article{2-5147,
  title={Development and validation of an automated classifier to diagnose acute otitis media in children},
  author={Shaikh, Nader and Conway, Shannon J. and Kova{\v{c}}evi{\'c}, Jelena and Condessa, Filipe and Shope, Timothy R. and Haralam, Mary Ann and Campese, Catherine and Lee, Matthew C. and Larsson, Tomas and Cavdar, Zafer and Hoberman, Alejandro},
  year={2024},
  doi={10.1001/jamapediatrics.2024.0011},
  journal={Journal of the American Medical Association Pediatrics}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Knowledge provider, Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-5283,ama,Integration of artificial intelligence decision aids to reduce workload and enhance efficiency in thyroid nodule management,"To optimize the integration of artificial intelligence (AI) decision aids and reduce workload in thyroid nodule management, it is critical to incorporate personalized AI into the decision-making processes of radiologists with varying levels of expertise.To develop an optimized integration of AI decision aids for reducing radiologists’ workload while maintaining diagnostic performance compared with traditional AI-assisted strategy.In this diagnostic study, a retrospective set of 1754 ultrasonographic images of 1048 patients with 1754 thyroid nodules from July 1, 2018, to July 31, 2019, was used to build an optimized strategy based on how 16 junior and senior radiologists incorporated AI-assisted diagnosis results with different image features. In the prospective set of this diagnostic study, 300 ultrasonographic images of 268 patients with 300 thyroid nodules from May 1 to December 31, 2021, were used to compare the optimized strategy with the traditional all-AI strategy in terms of diagnostic performance and workload reduction. Data analyses were completed in September 2022.The retrospective set of images was used to develop an optimized integration of AI decision aids for junior and senior radiologists based on the selection of AI-assisted significant or nonsignificant features. In the prospective set of images, the diagnostic performance, time-based cost, and assisted diagnosis were compared between the optimized strategy and the traditional all-AI strategy.The retrospective set included 1754 ultrasonographic images from 1048 patients (mean [SD] age, 42.1 [13.2] years; 749 women [71.5\%]) with 1754 thyroid nodules (mean [SD] size, 16.4 [10.6] mm); 748 nodules (42.6\%) were benign, and 1006 (57.4\%) were malignant. The prospective set included 300 ultrasonographic images from 268 patients (mean [SD] age, 41.7 [14.1] years; 194 women [72.4\%]) with 300 thyroid nodules (mean [SD] size, 17.2 [6.8] mm); 125 nodules (41.7\%) were benign, and 175 (58.3\%) were malignant. For junior radiologists, the ultrasonographic features that were not improved by AI assistance included cystic or almost completely cystic nodules, anechoic nodules, spongiform nodules, and nodules smaller than 5 mm, whereas for senior radiologists the features that were not improved by AI assistance were cystic or almost completely cystic nodules, anechoic nodules, spongiform nodules, very hypoechoic nodules, nodules taller than wide, lobulated or irregular nodules, and extrathyroidal extension. Compared with the traditional all-AI strategy, the optimized strategy was associated with increased mean task completion times for junior radiologists (reader 11, from 15.2 seconds [95\% CI, 13.2-17.2 seconds] to 19.4 seconds [95\% CI, 15.6-23.3 seconds]; reader 12, from 12.7 seconds [95\% CI, 11.4-13.9 seconds] to 15.6 seconds [95\% CI, 13.6-17.7 seconds]), but shorter times for senior radiologists (reader 14, from 19.4 seconds [95\% CI, 18.1-20.7 seconds] to 16.8 seconds [95\% CI, 15.3-18.3 seconds]; reader 16, from 12.5 seconds [95\% CI, 12.1-12.9 seconds] to 10.0 seconds [95\% CI, 9.5-10.5 seconds]). There was no significant difference in sensitivity (range, 91\%-100\%) or specificity (range, 94\%-98\%) between the 2 strategies for readers 11 to 16.This diagnostic study suggests that an optimized AI strategy in thyroid nodule management may reduce diagnostic time-based costs without sacrificing diagnostic accuracy for senior radiologists, while the traditional all-AI strategy may still be more beneficial for junior radiologists.",10.1001/jamanetworkopen.2023.13674,https://doi.org/10.1001/jamanetworkopen.2023.13674,JAMA Network Open,"Tong, Wen-Juan;Wu, Shao-Hong;Cheng, Mei-Qing;Huang, Hui;Liang, Jin-Yu;Li, Chao-Qun;Guo, Huan-Ling;He, Dan-Ni;Liu, Yi-Hao;Xiao, Han;Hu, Hang-Tong;Ruan, Si-Min;Li, Ming-De;Lu, Ming-De;Wang, Wei",2023,50,"@article{2-5283,
  title={Integration of artificial intelligence decision aids to reduce workload and enhance efficiency in thyroid nodule management},
  author={Tong, Wen-Juan and Wu, Shao-Hong and Cheng, Mei-Qing and Huang, Hui and Liang, Jin-Yu and Li, Chao-Qun and Guo, Huan-Ling and He, Dan-Ni and Liu, Yi-Hao and Xiao, Han and Hu, Hang-Tong and Ruan, Si-Min and Li, Ming-De and Lu, Ming-De and Wang, Wei},
  year={2023},
  doi={10.1001/jamanetworkopen.2023.13674},
  journal={JAMA Network Open}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Knowledge provider, Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-53,aaai,A Voting-Based System for Ethical Decision Making,"We present a general approach to automating ethical decisions, drawing on machine learning and computational social choice. In a nutshell, we propose to learn a model of societal preferences, and, when faced with a specific ethical dilemma at runtime, efficiently aggregate those preferences to identify a desirable choice. We provide a concrete algorithm that instantiates our approach; some of its crucial steps are informed by a new theory of swap-dominance efficient voting rules. Finally, we implement and evaluate a system for ethical decision making in the autonomous vehicle domain, using preference data collected from 1. 3 million people through the Moral Machine website.",10.1609/aaai.v32i1.11512,https://ojs.aaai.org/index.php/AAAI/article/view/11512,AAAI Conference on Artificial Intelligence,Ritesh Noothigattu;Snehalkumar Gaikwad;Edmond Awad;Sohan Dsouza;Iyad Rahwan;Pradeep Ravikumar;Ariel Procaccia,2018,270,"@inproceedings{2-53,
  title     = {A Voting-Based System for Ethical Decision Making},
  author    = {Ritesh Noothigattu and Snehalkumar Gaikwad and Edmond Awad and Sohan Dsouza and Iyad Rahwan and Pradeep Ravikumar and Ariel Procaccia},
  year      = {2018},
  booktitle = {AAAI Conference on Artificial Intelligence},
  doi       = {10.1609/aaai.v32i1.11512}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Transportation / Mobility / Planning",Individual,Executing,"Decision-subject, Guardian, Knowledge provider, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-532,aaai,Human Uncertainty Inference via Deterministic Ensemble Neural Networks,"The estimation and inference of human predictive uncertainty have great potential to improve the sampling efficiency and prediction reliability of human-in-the-loop systems for smart healthcare, smart education, and human-computer interactions. Predictive uncertainty in humans is highly interpretable, but its measurement is poorly accessible. Contrarily, the predictive uncertainty of machine learning models, albeit with poor interpretability, is relatively easily accessible. Here, we demonstrate that the poor accessibility of human uncertainty can be resolved by exploiting simple and universally accessible deterministic neural networks. We propose a new model for human uncertainty inference, called proxy ensemble network( PEN). Simulations with a few benchmark datasets demonstrated that the model can efficiently learn human uncertainty from a small amount of data. To show its applicability in real-world problems, we performed behavioral experiments, in which 64 physicians classified medical images and reported their level of confidence. We showed that the PEN could predict both the uncertainty range and diagnoses given by subjects with high accuracy. Our results demonstrate the ability of machine learning in guiding human decision making; it can also help humans in learning more efficiently and accurately. To the best of our knowledge, this is the first study that explored the possibility of accessing human uncertainty via the lens of deterministic neural networks.",10.1609/aaai.v35i7.16735,https://ojs.aaai.org/index.php/AAAI/article/view/16735,AAAI Conference on Artificial Intelligence,Yujin Cha;Sang Wan Lee,2021,5,"@inproceedings{2-532,
  title     = {Human Uncertainty Inference via Deterministic Ensemble Neural Networks},
  author    = {Yujin Cha and Sang Wan Lee},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year      = {2021},
  doi       = {10.1609/aaai.v35i7.16735}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing, Forecasting","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-5381,ama,Patient perspectives on the use of artificial intelligence for skin cancer screening: a qualitative study,"The use of artificial intelligence (AI) is expanding throughout the field of medicine. In dermatology, researchers are evaluating the potential for direct-to-patient and clinician decision-support AI tools to classify skin lesions. Although AI is poised to change how patients engage in health care, patient perspectives remain poorly understood.To explore how patients conceptualize AI and perceive the use of AI for skin cancer screening.A qualitative study using a grounded theory approach to semistructured interview analysis was conducted in general dermatology clinics at the Brigham and Women’s Hospital and melanoma clinics at the Dana-Farber Cancer Institute. Forty-eight patients were enrolled. Each interview was independently coded by 2 researchers with interrater reliability measurement; reconciled codes were used to assess code frequency. The study was conducted from May 6 to July 8, 2019.Artificial intelligence concept, perceived benefits and risks of AI, strengths and weaknesses of AI, AI implementation, response to conflict between human and AI clinical decision-making, and recommendation for or against AI.Of 48 patients enrolled, 26 participants (54\%) were women; mean (SD) age was 53.3 (21.7) years. Sixteen patients (33\%) had a history of melanoma, 16 patients (33\%) had a history of nonmelanoma skin cancer only, and 16 patients (33\%) had no history of skin cancer. Twenty-four patients were interviewed about a direct-to-patient AI tool and 24 patients were interviewed about a clinician decision-support AI tool. Interrater reliability ratings for the 2 coding teams were κ = 0.94 and κ = 0.89. Patients primarily conceptualized AI in terms of cognition. Increased diagnostic speed (29 participants [60\%]) and health care access (29 [60\%]) were the most commonly perceived benefits of AI for skin cancer screening; increased patient anxiety was the most commonly perceived risk (19 [40\%]). Patients perceived both more accurate diagnosis (33 [69\%]) and less accurate diagnosis (41 [85\%]) to be the greatest strength and weakness of AI, respectively. The dominant theme that emerged was the importance of symbiosis between humans and AI (45 [94\%]). Seeking biopsy was the most common response to conflict between human and AI clinical decision-making (32 [67\%]). Overall, 36 patients (75\%) would recommend AI to family members and friends.In this qualitative study, patients appeared to be receptive to the use of AI for skin cancer screening if implemented in a manner that preserves the integrity of the human physician-patient relationship.",10.1001/jamadermatol.2019.5014,https://doi.org/10.1001/jamadermatol.2019.5014,Journal of the American Medical Association Dermatology,"Nelson, Caroline A.;Pérez-Chada, Lourdes Maria;Creadore, Andrew;Li, Sara Jiayang;Lo, Kelly;Manjaly, Priya;Pournamdari, Ashley Bahareh;Tkachenko, Elizabeth;Barbieri, John S.;Ko, Justin M.;Menon, Alka V.;Hartman, Rebecca Ivy;Mostaghimi, Arash",2020,220,"@article{2-5381,
  title={Patient perspectives on the use of artificial intelligence for skin cancer screening: a qualitative study},
  author={Nelson, Caroline A. and P{\'e}rez-Chada, Lourdes Maria and Creadore, Andrew and Li, Sara Jiayang and Lo, Kelly and Manjaly, Priya and Pournamdari, Ashley Bahareh and Tkachenko, Elizabeth and Barbieri, John S. and Ko, Justin M. and Menon, Alka V. and Hartman, Rebecca Ivy and Mostaghimi, Arash},
  year={2020},
  doi={10.1001/jamadermatol.2019.5014},
  journal={Journal of the American Medical Association Dermatology}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject","Alter decision outcomes, Change affective-perceptual, Change trust",no such info,NA,importance of symbiosis,Visual,Yes,Yes
2-570,aaai,Human Assisted Learning by Evolutionary Multi-Objective Optimization,"Machine learning models have liberated manpower greatly in many real-world tasks, but their predictions are still worse than humans on some specific instances. To improve the performance, it is natural to optimize machine learning models to take decisions for most instances while delivering a few tricky instances to humans, resulting in the problem of Human Assisted Learning( HAL). Previous works mainly formulated HAL as a constrained optimization problem that tries to find a limited subset of instances for human decision such that the sum of model and human errors can be minimized; and employed the greedy algorithms, whose performance, however, may be limited due to the greedy nature. In this paper, we propose a new framework HAL-EMO based on Evolutionary Multi-objective Optimization, which reformulates HAL as a bi-objective optimization problem that minimizes the number of selected instances for human decision and the total errors simultaneously, and employs a Multi-Objective Evolutionary Algorithm( MOEA) to solve it. We implement HAL-EMO using two MOEAs, the popular NSGA-II as well as the theoretically grounded GSEMO. We also propose a specific MOEA, called BSEMO, with biased selection and balanced mutation for HAL-EMO, and prove that for human assisted regression and classification, HAL-EMO using BSEMO can achieve better and same theoretical guarantees than previous greedy algorithms, respectively. Experiments on the tasks of medical diagnosis and content moderation show the superiority of HAL-EMO( with either NSGA-II, GSEMO or BSEMO) over previous algorithms, and that using BSEMO leads to the best performance of HAL-EMO.",10.1609/aaai.v37i10.26467,https://ojs.aaai.org/index.php/AAAI/article/view/26467,AAAI Conference on Artificial Intelligence,Dan-Xuan Liu;Xin Mu;Chao Qian,2023,2,"@inproceedings{2-570,
  title = {Human Assisted Learning by Evolutionary Multi-Objective Optimization},
  author = {Liu, Dan-Xuan and Mu, Xin and Qian, Chao},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year = {2023},
  doi = {10.1609/aaai.v37i10.26467}
}",Algorithmic contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery",Operational,"Forecasting, Advising","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-571,aaai,Human-Guided Moral Decision Making in Text-Based Games,"Training reinforcement learning( RL) agents to achieve desired goals while also acting morally is a challenging problem. Transformer-based language models( LMs) have shown some promise in moral awareness, but their use in different contexts is problematic because of the complexity and implicitness of human morality. In this paper, we build on text-based games, which are challenging environments for current RL agents, and propose the HuMAL( Human-guided Morality Awareness Learning) algorithm, which adaptively learns personal values through human-agent collaboration with minimal manual feedback. We evaluate HuMAL on the Jiminy Cricket benchmark, a set of text-based games with various scenes and dense morality annotations, using both simulated and actual human feedback. The experimental results demonstrate that with a small amount of human feedback, HuMAL can improve task performance and reduce immoral behavior in a variety of games and is adaptable to different personal values.",10.1609/aaai.v38i19.30155,https://ojs.aaai.org/index.php/AAAI/article/view/30155,AAAI Conference on Artificial Intelligence,Zijing Shi;Meng Fang;Ling Chen;Yali Du;Jun Wang,2024,1,"@inproceedings{2-571,
  title = {Human-Guided Moral Decision Making in Text-Based Games},
  author = {Shi, Zijing and Fang, Meng and Chen, Ling and Du, Yali and Wang, Jun},
  year = {2024},
  doi = {10.1609/aaai.v38i19.30155},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence}
}",Algorithmic contributions,"Media / Communication / Entertainment, Generic / Abstract / Domain-agnostic",Individual,"Executing, Collaborating","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-575,aaai,I Prefer Not to Say: Protecting User Consent in Models with Optional Personal Data,"We examine machine learning models in a setup where individuals have the choice to share optional personal information with a decision-making system, as seen in modern insurance pricing models. Some users consent to their data being used whereas others object and keep their data undisclosed. In this work, we show that the decision not to share data can be considered as information in itself that should be protected to respect users privacy. This observation raises the overlooked problem of how to ensure that users who protect their personal data do not suffer any disadvantages as a result. To address this problem, we formalize protection requirements for models which only use the information for which active user consent was obtained. This excludes implicit information contained in the decision to share data or not. We offer the first solution to this problem by proposing the notion of Protected User Consent( PUC) , which we prove to be loss-optimal under our protection requirement. We observe that privacy and performance are not fundamentally at odds with each other and that it is possible for a decision maker to benefit from additional data while respecting users consent. To learn PUC-compliant models, we devise a model-agnostic data augmentation strategy with finite sample convergence guarantees. Finally, we analyze the implications of PUC on challenging real datasets, tasks, and models.",10.1609/aaai.v38i19.30126,https://ojs.aaai.org/index.php/AAAI/article/view/30126,AAAI Conference on Artificial Intelligence,Tobias Leemann;Martin Pawelczyk;Christian Thomas Eberle;Gjergji Kasneci,2024,1,"@inproceedings{2-575,
  title = {I Prefer Not to Say: Protecting User Consent in Models with Optional Personal Data},
  author = {Tobias Leemann and Martin Pawelczyk and Christian Thomas Eberle and Gjergji Kasneci},
  year = {2024},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v38i19.30126}
}",Methodological contributions,"Healthcare / Medicine / Surgery, Generic / Abstract / Domain-agnostic, Law / Policy / Governance, Environment / Resources / Energy, Finance / Business / Economy, Transportation / Mobility / Planning",Operational,Executing,"Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-5755,apa,Auditing the ai auditors: a framework for evaluating fairness and bias in high stakes ai predictive models.,"Researchers, governments, ethics watchdogs, and the public are increasingly voicing concerns about unfairness and bias in artificial intelligence (AI)-based decision tools. Psychology’s more-than-a-century of research on the measurement of psychological traits and the prediction of human behavior can benefit such conversations, yet psychological researchers often find themselves excluded due to mismatches in terminology, values, and goals across disciplines. In the present paper, we begin to build a shared interdisciplinary understanding of AI fairness and bias by first presenting three major lenses, which vary in focus and prototypicality by discipline, from which to consider relevant issues: (a) individual attitudes, (b) legality, ethicality, and morality, and (c) embedded meanings within technical domains. Using these lenses, we next present psychological audits as a standardized approach for evaluating the fairness and bias of AI systems that make predictions about humans across disciplinary perspectives. We present 12 crucial components to audits across three categories: (a) components related to AI models in terms of their source data, design, development, features, processes, and outputs, (b) components related to how information about models and their applications are presented, discussed, and understood from the perspectives of those employing the algorithm, those affected by decisions made using its predictions, and third-party observers, and (c) meta-components that must be considered across all other auditing components, including cultural context, respect for persons, and the integrity of individual research designs used to support all model developer claims. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",10.1037/amp0000972,https://doi.org/10.1037/amp0000972,American Psychologist,"Landers, Richard N.;Behrend, Tara S.",2023,5,"@article{2-5755,
  title={Auditing the AI auditors: A framework for evaluating fairness and bias in high stakes AI predictive models},
  author={Landers, Richard N. and Behrend, Tara S.},
  year={2023},
  journal={American Psychologist},
  doi={10.1037/amp0000972}
}","Theoretical contributions, Methodological contributions",Generic / Abstract / Domain-agnostic,Institutional,"Forecasting, Auditing","Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-59,aaai,Beyond Distributive Fairness in Algorithmic Decision Making: Feature Selection for Procedurally Fair Learning,"With widespread use of machine learning methods in numerous domains involving humans, several studies have raised questions about the potential for unfairness towards certain individuals or groups. A number of recent works have proposed methods to measure and eliminate unfairness from machine learning models. However, most of this work has focused on only one dimension of fair decision making: distributive fairness, i. e. , the fairness of the decision outcomes. In this work, we leverage the rich literature on organizational justice and focus on another dimension of fair decision making: procedural fairness, i. e. , the fairness of the decision making process. We propose measures for procedural fairness that consider the input features used in the decision process, and evaluate the moral judgments of humans regarding the use of these features. We operationalize these measures on two real world datasets using human surveys on the Amazon Mechanical Turk( AMT) platform, demonstrating that our measures capture important properties of procedurally fair decision making. We provide fast submodular mechanisms to optimize the tradeoff between procedural fairness and prediction accuracy. On our datasets, we observe empirically that procedural fairness may be achieved with little cost to outcome fairness, but that some loss of accuracy is unavoidable.",10.1609/aaai.v32i1.11296,https://ojs.aaai.org/index.php/AAAI/article/view/11296,AAAI Conference on Artificial Intelligence,Nina Grgić-Hlača;Muhammad Bilal Zafar;Krishna P. Gummadi;Adrian Weller,2018,293,"@inproceedings{2-59,
  title = {Beyond Distributive Fairness in Algorithmic Decision Making: Feature Selection for Procedurally Fair Learning},
  author = {Grgić-Hlača, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P. and Weller, Adrian},
  year = {2018},
  doi = {10.1609/aaai.v32i1.11296},
  booktitle = {AAAI Conference on Artificial Intelligence}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Law / Policy / Governance",Institutional,"Advising, Auditing","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-5937,apa,Does artificial intelligence (ai) assistance mitigate biased evaluations of eyewitness identifications?,"Artificial intelligence (AI) is playing an increasing role in human decision making. We use eyewitness lineup identification to show when AI assistance can and cannot help people avoid a cognitive bias known as the featural justification effect. People are biased to judge highly confident eyewitnesses as less likely to be correct when their lineup identification is based on an observable feature than on an expression of recognition. Our participants (N = 1,010) saw an eyewitness’s lineup identification, accompanied by the eyewitness’s verbal confidence statement (e.g., “I’m certain”) and either a featural (“I remember his eyes”) or a recognition justification (“I remember him”). They then rated the likely accuracy of the eyewitness’s identification. AI assistance eliminated the featural justification effect but only in participants who regarded the AI as very useful. This project is the first step in evaluating human–algorithm interactions before the widespread use of AI assistance by law enforcement. (PsycInfo Database Record (c) 2024 APA, all rights reserved)",10.1037/mac0000192,https://doi.org/10.1037/mac0000192,Journal of Applied Research in Memory and Cognition,"Kelso, Lauren E.;Grabman, Jesse H.;Dobolyi, David G.;Dodson, Chad S.",2024,4,"@article{2-5937,
  title = {Does artificial intelligence (AI) assistance mitigate biased evaluations of eyewitness identifications?},
  author = {Kelso, Lauren E. and Grabman, Jesse H. and Dobolyi, David G. and Dodson, Chad S.},
  year = {2024},
  doi = {10.1037/mac0000192},
  journal = {Journal of Applied Research in Memory and Cognition}
}",Empirical contributions,Law / Policy / Governance,Operational,"Advising, Analyzing","Decision-maker, Decision-subject","Shape ethical norms, Alter decision outcomes, Change trust",no such info,"prediction of alternative, system accuracy",NA,"Textual, Visual",Yes,Yes
2-596,aaai,Incentivizing Truthfulness Through Audits in Strategic Classification,"In many societal resource allocation domains, machine learning methods are increasingly used to either score or rank agents in order to decide which ones should receive either resources( e. g. , homeless services) or scrutiny( e. g. , child welfare investigations) from social services agencies. An agencys scoring function typically operates on a feature vector that contains a combination of self-reported features and information available to the agency about individuals or households. This can create incentives for agents to misrepresent their self-reported features in order to receive resources or avoid scrutiny, but agencies may be able to selectively audit agents to verify the veracity of their reports. We study the problem of optimal auditing of agents in such settings. When decisions are made using a threshold on an agents score, the optimal audit policy has a surprisingly simple structure, uniformly auditing all agents who could benefit from lying. While this policy can, in general be hard to compute because of the difficulty of identifying the set of agents who could benefit from lying given a complete set of reported types, we also present sufficient conditions under which it is tractable. We show that the scarce resource setting is more difficult, and exhibit an approximately optimal audit policy in this case. In addition, we show that in either setting verifying whether it is possible to incentivize exact truthfulness is hard even to approximate. However, we also exhibit sufficient conditions for solving this problem optimally, and for obtaining good approximations.",10.1609/aaai.v35i6.16674,https://ojs.aaai.org/index.php/AAAI/article/view/16674,AAAI Conference on Artificial Intelligence,Andrew Estornell;Sanmay Das;Yevgeniy Vorobeychik,2021,16,"@inproceedings{2-596,
  title={Incentivizing Truthfulness Through Audits in Strategic Classification},
  author={Estornell, Andrew and Das, Sanmay and Vorobeychik, Yevgeniy},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2021},
  volume={35},
  number={6},
  pages={4923--4931},
  doi={10.1609/aaai.v35i6.16674}
}",Theoretical contributions,"Manufacturing / Industry / Automation, Everyday / Employment / Public Service",Institutional,Executing,"Stakeholder, Guardian, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-5975,apa,Efficient visual representations for learning and decision making.,"The efficient representation of visual information is essential for learning and decision making due to the complexity and uncertainty of the world, as well as inherent constraints on the capacity of cognitive systems. We hypothesize that biological agents learn to efficiently represent visual information in a manner that balances performance across multiple potentially competing objectives. In this article, we examine two such objectives: storing information in a manner that supports accurate recollection (maximizing veridicality) and in a manner that facilitates utility-based decision making (maximizing behavioral utility). That these two objectives may be in conflict is not immediately obvious. Our hypothesis suggests that neither behavior nor representation formation can be fully understood by studying either in isolation, with information processing constraints exerting an overarching influence. Alongside this hypothesis we develop a computational model of representation formation and behavior motivated by recent methods in machine learning and neuroscience. The resulting model explains both the beneficial aspects of human visual learning, such as fast acquisition and high generalization, as well as the biases that result from information constraints. To test this model, we developed two experimental paradigms, in decision making and learning, to evaluate how well the model’s predictions match human behavior. A key feature of the proposed model is that it predicts the occurrence of commonly found biases in human decision making, resulting from the desire to form efficient representations of visual information that are useful for behavioral goals in learning and decision making and optimized under an information processing constraint. (PsycInfo Database Record (c) 2024 APA, all rights reserved)",10.1037/rev0000498,https://doi.org/10.1037/rev0000498,Psychological Review,"Malloy, Tyler;Sims, Chris R.",2024,7,"@article{2-5975,
  title = {Efficient visual representations for learning and decision making},
  author = {Malloy, Tyler and Sims, Chris R.},
  year = {2024},
  doi = {10.1037/rev0000498},
  journal = {Psychological Review}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,no such info,"Analyzing, Forecasting, Explaining",Decision-maker,Alter decision outcomes,no such info,predicting the occurrence of commonly found biases,NA,NA,Yes,Yes
2-605,aaai,Investigations of Performance and Bias in Human-AI Teamwork in Hiring,"In AI-assisted decision-making, effective hybrid( human-AI) teamwork is not solely dependent on AI performance alone, but also on its impact on human decision-making. While prior work studies the effects of model accuracy on humans, we endeavour here to investigate the complex dynamics of how both a models predictive performance and bias may transfer to humans in a recommendation-aided decision task. We consider the domain of ML-assisted hiring, where humans---operating in a constrained selection setting---can choose whether they wish to utilize a trained models inferences to help select candidates from written biographies. We conduct a large-scale user study leveraging a re-created dataset of real bios from prior work, where humans predict the ground truth occupation of given candidates with and without the help of three different NLP classifiers( random, bag-of-words, and deep neural network). Our results demonstrate that while high-performance models significantly improve human performance in a hybrid setting, some models mitigate hybrid bias while others accentuate it. We examine these findings through the lens of decision conformity and observe that our model architecture choices have an impact on human-AI conformity and bias, motivating the explicit need to assess these complex dynamics prior to deployment.",10.1609/aaai.v36i11.21468,https://ojs.aaai.org/index.php/AAAI/article/view/21468,AAAI Conference on Artificial Intelligence,Andi Peng;Besmira Nushi;Emre Kiciman;Kori Inkpen;Ece Kamar,2022,56,"@inproceedings{2-605,
  title     = {Investigations of Performance and Bias in Human-AI Teamwork in Hiring},
  author    = {Andi Peng and Besmira Nushi and Emre Kiciman and Kori Inkpen and Ece Kamar},
  year      = {2022},
  doi       = {10.1609/aaai.v36i11.21468},
  booktitle = {AAAI Conference on Artificial Intelligence}
}",Empirical contributions,Everyday / Employment / Public Service,Operational,"Forecasting, Advising","Decision-maker, Decision-subject","Shape ethical norms, Change trust, Alter decision outcomes",no such info,recommendations,language proficiency,"Textual, Interactive interface",Yes,Yes
2-610,aaai,Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork,"AI practitioners typically strive to develop the most accurate systems, making an implicit assumption that the AI system will function autonomously. However, in practice, AI systems often are used to provide advice to people in domains ranging from criminal justice and finance to healthcare. In such AI-advised decision making, humans and machines form a team, where the human is responsible for making final decisions. But is the most accurate AI the best teammate? We argue not necessarily --- predictable performance may be worth a slight sacrifice in AI accuracy. Instead, we argue that AI systems should be trained in a human-centered manner, directly optimized for team performance. We study this proposal for a specific type of human-AI teaming, where the human overseer chooses to either accept the AI recommendation or solve the task themselves. To optimize the team performance for this setting we maximize the teams expected utility, expressed in terms of the quality of the final decision, cost of verifying, and individual accuracies of people and machines. Our experiments with linear and non-linear models on real-world, high-stakes datasets show that the most accuracy AI may not lead to highest team performance and show the benefit of modeling teamwork during training through improvements in expected team utility across datasets, considering parameters such as human skill and the cost of mistakes. We discuss the shortcoming of current optimization approaches beyond well-studied loss functions such as log-loss, and encourage future work on AI optimization problems motivated by human-AI collaboration.",10.1609/aaai.v35i13.17359,https://ojs.aaai.org/index.php/AAAI/article/view/17359,AAAI Conference on Artificial Intelligence,Gagan Bansal;Besmira Nushi;Ece Kamar;Eric Horvitz;Daniel S. Weld,2021,1,"@inproceedings{2-610,
  title     = {Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork},
  author    = {Gagan Bansal and Besmira Nushi and Ece Kamar and Eric Horvitz and Daniel S. Weld},
  year      = {2021},
  doi       = {10.1609/aaai.v35i13.17359},
  booktitle = {AAAI Conference on Artificial Intelligence}
}",Algorithmic contributions,"Healthcare / Medicine / Surgery, Finance / Business / Economy, Law / Policy / Governance, Generic / Abstract / Domain-agnostic",Operational,"Advising, Collaborating","Guardian, Knowledge provider, Decision-maker","Change cognitive demands, Alter decision outcomes, Change trust",Update AI competence,recommendations,NA,"Textual, Visual",Yes,Yes
2-6115,apa,Identifying resource-rational heuristics for risky choice.,"Perfectly rational decision making is almost always out of reach for people because their computational resources are limited. Instead, people may rely on computationally frugal heuristics that usually yield good outcomes. Although previous research has identified many such heuristics, discovering good heuristics and predicting when they will be used remains challenging. Here, we present a theoretical framework that allows us to use methods from machine learning to automatically derive the best heuristic to use in any given situation by considering how to make the best use of limited cognitive resources. To demonstrate the generalizability and accuracy of our method, we compare the heuristics it discovers against those used by people across a wide range of multi-attribute risky choice environments in a behavioral experiment that is an order of magnitude larger than any previous experiments of its type. Our method rediscovered known heuristics, identifying them as rational strategies for specific environments, and discovered novel heuristics that had been previously overlooked. Our results show that people adapt their decision strategies to the structure of the environment and generally make good use of their limited cognitive resources, although their strategy choices do not always fully exploit the structure of the environment. (PsycInfo Database Record (c) 2024 APA, all rights reserved)",10.1037/rev0000456,https://doi.org/10.1037/rev0000456,Psychological Review,"Krueger, Paul M.;Callaway, Frederick;Gul, Sayan;Griffiths, Thomas L.;Lieder, Falk",2024,40,"@article{2-6115,
  title={Identifying resource-rational heuristics for risky choice},
  author={Krueger, Paul M. and Callaway, Frederick and Gul, Sayan and Griffiths, Thomas L. and Lieder, Falk},
  year={2024},
  journal={Psychological Review},
  doi={10.1037/rev0000456}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,no such info,"Analyzing, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-6133,apa,"Inequality threat increases laypeople’s, but not judges’, acceptance of algorithmic decision making in court.","Objective: Algorithmic decision making (ADM) takes on increasingly complex tasks in the criminal justice system. Whereas new developments in machine learning could help to improve the quality of judicial decisions, there are legal and ethical concerns that thwart the widespread use of algorithms. Against the backdrop of current efforts to promote the digitization of the German judicial system, this research investigates motivational factors (pragmatic motives, fairness concerns, and self-image-related considerations) that drive or impede the acceptance of ADM in court. Hypotheses: We tested two hypotheses: (1) Perceived threat of inequality in legal judgments increases ADM acceptance, and (2) experts (judges) are more skeptical toward technological innovation than novices (general population). Method: We conducted a preregistered experiment with 298 participants from the German general population and 267 judges at regional courts in Bavaria to study how inequality threat (vs. control) relates to ADM acceptance in court, usage intentions, and attitudes. Results: In partial support of the first prediction, inequality threat increased ADM acceptance, effect size d = 0.24, 95% confidence interval (CI) [0.01, 0.47], and usage intentions (d = 0.23, 95% CI [0.00, 0.46]) of laypeople. Unexpectedly, however, this was not the case for experts. Moreover, ADM attitudes remained unaffected by the experimental manipulation in both groups. As predicted, judges held more negative attitudes toward ADM than the general population (d = −0.71, 95% CI [−0.88, −0.54]). Exploratory analysis suggested that generalized attitudes emerged as the strongest predictor of judges’ intentions to use ADM in their own court proceedings. Conclusions: These findings elucidate the motivational forces that drive algorithm aversion and acceptance in a criminal justice context and inform the ongoing debate about perceptions of fairness in human–computer interaction. Implications for judicial praxis and the regulation of ADM in the German legal framework are discussed. (PsycInfo Database Record (c) 2024 APA, all rights reserved)",10.1037/lhb0000577,https://doi.org/10.1037/lhb0000577,Law and Human Behavior,"Ludwig, Jonas;Heineck, Paul-Michael;Hess, Marie-Theres;Kremeti, Eleni;Tauschhuber, Max;Hilgendorf, Eric;Deutsch, Roland",2024,3,"@article{2-6133,
  title = {Inequality threat increases laypeople's, but not judges', acceptance of algorithmic decision making in court},
  author = {Ludwig, Jonas and Heineck, Paul-Michael and Hess, Marie-Theres and Kremeti, Eleni and Tauschhuber, Max and Hilgendorf, Eric and Deutsch, Roland},
  year = {2024},
  doi = {10.1037/lhb0000577},
  journal = {Law and Human Behavior}
}",Empirical contributions,Law / Policy / Governance,Institutional,Advising,"Guardian, Decision-subject, Decision-maker","Change trust, Alter decision outcomes",no such info,NA,NA,Textual,Yes,Yes
2-626,aaai,Interpretable Low-Resource Legal Decision Making,"Over the past several years, legal applications of deep learning have been on the rise. However, as with other high-stakes decision making areas, the requirement for interpretability is of crucial importance. Current models utilized by legal practitioners are more of the conventional machine learning type, wherein they are inherently interpretable, yet unable to harness the performance capabilities of data-driven deep learning models. In this work, we utilize deep learning models in the area of trademark law to shed light on the issue of likelihood of confusion between trademarks. Specifically, we introduce a model-agnostic interpretable intermediate layer, a technique which proves to be effective for legal documents. Furthermore, we utilize weakly supervised learning by means of a curriculum learning strategy, effectively demonstrating the improved performance of a deep learning model. This is in contrast to the conventional models which are only able to utilize the limited number of expensive manually-annotated samples by legal experts. Although the methods presented in this work tackles the task of risk of confusion for trademarks, it is straightforward to extend them to other fields of law, or more generally, to other similar high-stakes application scenarios.",10.1609/aaai.v36i11.21438,https://ojs.aaai.org/index.php/AAAI/article/view/21438,AAAI Conference on Artificial Intelligence,Rohan Bhambhoria;Hui Liu;Samuel Dahan;Xiaodan Zhu,2022,61,"@inproceedings{2-626,
  title = {Interpretable Low-Resource Legal Decision Making},
  author = {Bhambhoria, Rohan and Liu, Hui and Dahan, Samuel and Zhu, Xiaodan},
  year = {2022},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v36i11.21438}
}",Methodological contributions,Law / Policy / Governance,Operational,"Advising, Analyzing, Explaining","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-6272,apa,One model to rule them all? Using machine learning algorithms to determine the number of factors in exploratory factor analysis.,"Determining the number of factors is one of the most crucial decisions a researcher has to face when conducting an exploratory factor analysis. As no common factor retention criterion can be seen as generally superior, a new approach is proposed—combining extensive data simulation with state-of-the-art machine learning algorithms. First, data was simulated under a broad range of realistic conditions and 3 algorithms were trained using specially designed features based on the correlation matrices of the simulated data sets. Subsequently, the new approach was compared with 4 common factor retention criteria with regard to its accuracy in determining the correct number of factors in a large-scale simulation experiment. Sample size, variables per factor, correlations between factors, primary and cross-loadings as well as the correct number of factors were varied to gain comprehensive knowledge of the efficiency of our new method. A gradient boosting model outperformed all other criteria, so in a second step, we improved this model by tuning several hyperparameters of the algorithm and using common retention criteria as additional features. This model reached an out-of-sample accuracy of 99.3% (the pretrained model can be obtained from https://osf.io/mvrau/). A great advantage of this approach is the possibility to continuously extend the data basis (e.g., using ordinal data) as well as the set of features to improve the predictive performance and to increase generalizability. (PsycInfo Database Record (c) 2020 APA, all rights reserved)",10.1037/met0000262,https://doi.org/10.1037/met0000262,Psychological Methods,"Goretzko, David;Bühner, Markus",2020,74,"@article{2-6272,
  title={One model to rule them all? Using machine learning algorithms to determine the number of factors in exploratory factor analysis},
  author={Goretzko, David and B{\""u}hner, Markus},
  year={2020},
  journal={Psychological Methods},
  doi={10.1037/met0000262}
}",Methodological contributions,Education / Teaching / Research,no such info,"Forecasting, Analyzing, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-6339,apa,Psychometric and machine learning approaches for diagnostic assessment and tests of individual classification.,"Assessments are commonly used to make a decision about an individual, such as grade placement, treatment assignment, job selection, or to inform a diagnosis. A psychometric approach to classify respondents based on the assessment would aggregate items into a score, and then each respondent’s score is compared to a cut score. In contrast, a machine learning approach to classify respondents would build a model to predict the probability of belonging to a specific class from assessment items, and then respondents are classified based on their predicted probability of belonging to that class. It remains unclear whether psychometric and machine learning methods have comparable classification accuracy or if 1 method is preferable in all or some situations. In the context of diagnostic assessment, this study used Monte Carlo simulation methods to compare the classification accuracy of psychometric and machine learning methods as a function of the diagnosis-test correlation, prevalence, sample size, and the structure of the diagnostic assessment. Results suggest that machine learning models using logistic regression or random forest could have comparable classification accuracy to the psychometric methods using estimated item response theory scores. Therefore, machine learning models could provide a viable alternative for classification when psychometric methods are not feasible. Methods are illustrated with an empirical example predicting an oppositional defiant disorder diagnosis from a behavior disorders scale in children of age seven. Strengths and limitations for each of the methods are examined, and the overlap between the field of machine learning and psychometrics is discussed. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",10.1037/met0000317,https://doi.org/10.1037/met0000317,Psychological Methods,"Gonzalez, Oscar",2021,61,"@article{2-6339,
  title={Psychometric and machine learning approaches for diagnostic assessment and tests of individual classification},
  author={Gonzalez, Oscar},
  year={2021},
  doi={10.1037/met0000317},
  journal={Psychological Methods}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Analyzing","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-6697,elsevier,‘Smart’ choice? Evaluating ai-based mobile decision bots for in-store decision-making,"To address a research gap on how AI can be leveraged to enhance customers’ in-store journeys, this study evaluates the effectiveness of an AI-powered conversational decision bot (mobile messaging app) employing two laboratory experiments in a simulated store. Study 1 revealed that consumers found in-store shopping to be more enjoyable and effortful with a decision bot than without, demonstrating that these bots reinforce consumers’ in-store hedonic and utilitarian experiences. Study 2 evaluated the effectiveness of two types of decision bots (attribute- vs. alternative-based), revealing that consumers’ perceived usefulness and reuse/recommend intentions for the bot were higher when using an attribute- as compared to an alternative-based bot. When shopping in-store, consumers benefit from a decision bot that retrieves attribute-level information for the product category rather than one that focuses on single product alternatives. This study makes important theoretical contributions to constructive decision theory, the Elaboration Likelihood Model, and language-based adaptive intelligence.",https://doi.org/10.1016/j.jbusres.2024.114801,https://www.sciencedirect.com/science/article/pii/S0148296324003059,Journal of Business Research,Veena Chattaraman;Wi-Suk Kwon;Kassandra Ross;Jihyun Sung;Kiana Alikhademi;Brianna Richardson;Juan E. Gilbert,2024,10,"@article{2-6697,
  title = {‘Smart’ choice? Evaluating ai-based mobile decision bots for in-store decision-making},
  author = {Veena Chattaraman and Wi-Suk Kwon and Kassandra Ross and Jihyun Sung and Kiana Alikhademi and Brianna Richardson and Juan E. Gilbert},
  year = {2024},
  journal = {Journal of Business Research},
  doi = {10.1016/j.jbusres.2024.114801}
}",Empirical contributions,Finance / Business / Economy,Individual,"Advising, Collaborating",Decision-maker,"Change affective-perceptual, Alter decision outcomes, Change cognitive demands",no such info,"attribute-level information for the product category, single product alternatives",NA,Conversational/Natural Language,Yes,Yes
2-6700,elsevier,“Let the driver off the hook?” Moral decisions of autonomous cars and their impact on consumer well-being,"Equipped with sophisticated, AI-based driver assistance systems, passenger cars are becoming increasingly intelligent. It seems that in a matter of a few years, fully autonomous vehicles will operate without any driver intervention. In this context, researchers are addressing the question of how fully automated vehicles should make decisions in critical situations. Should they spare the driver, children jumping out into the road or elderly people standing on the sidewalk? Projects such as MIT’s Moral Machine are investigating the preferences of people from different nations and cultures for ethical decision algorithms. Evaluations of these automated decisions and how the may impact consumer perception and well-being are still scarce. In our experimental study, participants experienced a simulator-based driving situation in a fully autonomous car, after which they were confronted with alternative scenarios requiring automated action by the car in a critical situation. We measured the emotional status and well-being of our test-persons (N=33) in those critical situations using facial expression recognition (FER), electroencephalography (EEG), and standardized questions. The results show that there are detectable differences between the scenarios with respect to emotions as well as subjective well-being and behavioral intentions in the test group’s responses to the questionnaire. Regarding FER and EEG, no statistically significant differences could be shown due to the small subsample.",https://doi.org/10.1016/j.tra.2024.104224,https://www.sciencedirect.com/science/article/pii/S0965856424002726,Transportation Research Part A: Policy and Practice,Marc Kuhn;Vanessa Reit;Maximilian Schwing;Sarah Selinka,2024,3,"@article{2-6700,
  title = {“Let the driver off the hook?” Moral decisions of autonomous cars and their impact on consumer well-being},
  author = {Marc Kuhn and Vanessa Reit and Maximilian Schwing and Sarah Selinka},
  year = {2024},
  doi = {10.1016/j.tra.2024.104224},
  journal = {Transportation Research Part A: Policy and Practice}
}",Empirical contributions,Transportation / Mobility / Planning,"Individual, Institutional",Executing,Decision-subject,"Alter decision outcomes, Shape ethical norms, Change cognitive demands",no such info,"recommendations, visual explanations","emotion expression, trust","Autonomous System, Visual",Yes,Yes
2-6728,elsevier,A 3d game theoretical framework for the evaluation of unmanned aircraft systems airspace integration concepts,"Predicting the outcomes of integrating Unmanned Aerial System (UAS) into the National Airspace System (NAS) is a complex problem, which is required to be addressed by simulation studies before allowing the routine access of UAS into the NAS. This paper focuses on providing a 3-dimensional (3D) simulation framework using a game-theoretical methodology to evaluate integration concepts using scenarios where manned and unmanned air vehicles co-exist. In the proposed method, the human pilot interactive decision-making process is incorporated into airspace models which can fill the gap in the literature where the pilot behavior is generally assumed to be known a priori. The proposed human pilot behavior is modeled using a dynamic level-k reasoning concept and approximate reinforcement learning. The level-k reasoning concept is a notion in game theory and is based on the assumption that humans have various levels of decision making. In the conventional “static” approach, each agent makes assumptions about his or her opponents and chooses his or her actions accordingly. On the other hand, in the dynamic level-k reasoning, agents can update their beliefs about their opponents and revise their level-k rule. In this study, Neural Fitted Q Iteration, which is an approximate reinforcement learning method, is used to model time-extended decisions of pilots with 3D maneuvers. An analysis of UAS integration is conducted using an Example 3D scenario in the presence of manned aircraft and fully autonomous UAS equipped with sense and avoid algorithms.",https://doi.org/10.1016/j.trc.2021.103417,https://www.sciencedirect.com/science/article/pii/S0968090X21004113,Transportation Research Part C: Emerging Technologies,Berat Mert Albaba;Negin Musavi;Yildiray Yildiz,2021,13,"@article{2-6728,
  title = {A 3D Game Theoretical Framework for the Evaluation of Unmanned Aircraft Systems Airspace Integration Concepts},
  author = {Berat Mert Albaba and Negin Musavi and Yildiray Yildiz},
  year = {2021},
  doi = {10.1016/j.trc.2021.103417},
  journal = {Transportation Research Part C: Emerging Technologies}
}",Methodological contributions,Transportation / Mobility / Planning,Operational,"Analyzing, Executing, Forecasting","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-6750,elsevier,A case-based reasoning system for aiding detection and classification of nosocomial infections,"Nowadays, it is recognized worldwide that healthcare-associated infections are responsible for an increase in patient morbidity, mortality, and higher costs related to prolonged hospital stays. As electronic health data are increasingly available today, there is a unique opportunity to implement real-time decision support systems for automating the surveillance of healthcare-associated infections. As a consequence, different electronic surveillance systems have been implemented to date with varying degrees of success. However, there have been few instances in which clinical data and physician narratives with the potential to significantly improve electronic surveillance alternatives have been adopted. In this context, the present work introduces a case-based reasoning system for the automatic surveillance and diagnosis of healthcare-associated infections. The developed system makes use of different machine learning techniques in order to (i) automatically extract evidence from different types of data including clinical unstructured documents, (ii) incorporate static a priori knowledge handled by infection preventionists, and (iii) dynamically generate new knowledge as well as understandable explanations about the system's decisions. Results obtained from a real deployment in a public hospital belonging to the Spanish National Health System trained with 2569 samples belonging to 1800 patients during more than 10 consecutive months recognize the usefulness of the system.",https://doi.org/10.1016/j.dss.2016.02.005,https://www.sciencedirect.com/science/article/pii/S0167923616300161,Decision Support Systems,H.J. Gómez-Vallejo;B. Uriel-Latorre;M. Sande-Meijide;B. Villamarín-Bello;R. Pavón;F. Fdez-Riverola;D. Glez-Peña,2016,83,"@article{2-6750,
  title={A case-based reasoning system for aiding detection and classification of nosocomial infections},
  author={G\'omez-Vallejo, H.J. and Uriel-Latorre, B. and Sande-Meijide, M. and Villamar\'in-Bello, B. and Pav\'on, R. and Fdez-Riverola, F. and Glez-Pe\~na, D.},
  year={2016},
  journal={Decision Support Systems},
  doi={10.1016/j.dss.2016.02.005}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Explaining","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-6758,elsevier,A clinical decision support system for sleep staging tasks with explanations from artificial intelligence: user-centered design and evaluation study,"Background Despite the unprecedented performance of deep learning algorithms in clinical domains, full reviews of algorithmic predictions by human experts remain mandatory. Under these circumstances, artificial intelligence (AI) models are primarily designed as clinical decision support systems (CDSSs). However, from the perspective of clinical practitioners, the lack of clinical interpretability and user-centered interfaces hinders the adoption of these AI systems in practice. Objective This study aims to develop an AI-based CDSS for assisting polysomnographic technicians in reviewing AI-predicted sleep staging results. This study proposed and evaluated a CDSS that provides clinically sound explanations for AI predictions in a user-centered manner. Methods Our study is based on a user-centered design framework for developing explanations in a CDSS that identifies why explanations are needed, what information should be contained in explanations, and how explanations can be provided in the CDSS. We conducted user interviews, user observation sessions, and an iterative design process to identify three key aspects for designing explanations in the CDSS. After constructing the CDSS, the tool was evaluated to investigate how the CDSS explanations helped technicians. We measured the accuracy of sleep staging and interrater reliability with macro-F1 and Cohen κ scores to assess quantitative improvements after our tool was adopted. We assessed qualitative improvements through participant interviews that established how participants perceived and used the tool. Results The user study revealed that technicians desire explanations that are relevant to key electroencephalogram (EEG) patterns for sleep staging when assessing the correctness of AI predictions. Here, technicians wanted explanations that could be used to evaluate whether the AI models properly locate and use these patterns during prediction. On the basis of this, information that is closely related to sleep EEG patterns was formulated for the AI models. In the iterative design phase, we developed a different visualization strategy for each pattern based on how technicians interpreted the EEG recordings with these patterns during their workflows. Our evaluation study on 9 polysomnographic technicians quantitatively and qualitatively investigated the helpfulness of the tool. For technicians with <5 years of work experience, their quantitative sleep staging performance improved significantly from 56.75 to 60.59 with a P value of .05. Qualitatively, participants reported that the information provided effectively supported them, and they could develop notable adoption strategies for the tool. Conclusions Our findings indicate that formulating clinical explanations for automated predictions using the information in the AI with a user-centered design process is an effective strategy for developing a CDSS for sleep staging.",https://doi.org/10.2196/28659,https://www.sciencedirect.com/science/article/pii/S1438887122000401,Journal of Medical Internet Research,Jeonghwan Hwang;Taeheon Lee;Honggu Lee;Seonjeong Byun,2022,36,"@article{2-6758,
  title = {A clinical decision support system for sleep staging tasks with explanations from artificial intelligence: user-centered design and evaluation study},
  author = {Jeonghwan Hwang and Taeheon Lee and Honggu Lee and Seonjeong Byun},
  year = {2022},
  journal = {Journal of Medical Internet Research},
  doi = {https://doi.org/10.2196/28659}
}","Empirical contributions, System/Artifact contributions",Healthcare / Medicine / Surgery,Operational,"Explaining, Forecasting, Advising","Decision-maker, Developer","Alter decision outcomes, Change trust, Change cognitive demands",no such info,prediction of alternative,NA,Autonomous System,Yes,Yes
2-6799,elsevier,A comprehensive multi-stage decision-making model for supplier selection and order allocation approach in the digital economy,"The increasingly serious environmental issues and fierce competition caused by globalization have brought pressure on supply chain managers who seek to allocate multiple purchase demands comprehensively, highlighting the significance of supplier assessment considering sustainability and technique. Moreover, many multi-criteria decision-making (MCDM) methods fail to quantify the risk preference of decision-makers (DMs) when conducting the supplier assessment process. Indeed, a hybrid supplier selection and order allocation model that integrates such requirements is yet to be proposed. Thus, this work develops a comprehensive decision-making model that constructs a deep learning model to forecast the potential demand and addresses the sustainable supplier selection based on cumulative prospect theory (CPT) and multi-material order allocation problem simultaneously. The proposed order allocation model is solved by the second generation of adaptive geometry estimation based many-objective evolutionary algorithm, with the technique of order preference similarity to the ideal solution used to filter out the best Pareto solution for DMs as the reference. Through implementing an illustrative case study of a leading Chinese engineering machinery manufacturer followed by a sensitivity analysis, the relatively strong applicability and scalability of the proposed model and methods are demonstrated. The results show that introducing Weibull distribution to estimate the theoretical obsolescence rate of historically sold accessories can result in higher demand prediction accuracy for consumable mechanical accessories. Integrating CPT into the MCDM framework allows us to evaluate suppliers more comprehensively by capturing the effect of DMs’ risk preferences and gain or loss sensitivity.",https://doi.org/10.1016/j.aei.2024.102961,https://www.sciencedirect.com/science/article/pii/S1474034624006128,Advanced Engineering Informatics,Xuhui Chen;Yong He;Golnaz {Hooshmand Pakdel};Xiaofan Liu;Sai Wang,2025,1,"@article{2-6799,
  title = {A comprehensive multi-stage decision-making model for supplier selection and order allocation approach in the digital economy},
  author = {Xuhui Chen and Yong He and Golnaz Hooshmand Pakdel and Xiaofan Liu and Sai Wang},
  year = {2025},
  doi = {10.1016/j.aei.2024.102961},
  journal = {Advanced Engineering Informatics}
}",Algorithmic contributions,Manufacturing / Industry / Automation,Organizational,"Forecasting, Analyzing, Advising","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-6860,elsevier,A decision-making framework to support urban heat mitigation by local governments,"Local governments have made extensive efforts to mitigate urban overheating, cool streetscapes and cities, and protect vulnerable people. However, there is uncertainty about which urban heat mitigation strategies (UHMSs) can provide better solutions for a specific urban context. There is a compelling need for local governments to automate the decision-making process and optimise the combination of UHMSs to maximise the mitigation outcomes for their cities. We develop a novel decision-making framework that incorporates artificial intelligence (AI) techniques into urban heat mitigation in the built environment to enable an automated process of decision making. The novel decision-making framework comprises: the ontology-based knowledge representation of UHMSs and their relationships with urban contexts and performance assessment to share knowledge in urban heat mitigation domain; sensitivity analysis of the environmental, social and economic performance of UHMSs to get key variables for UHMSs; and genetic algorithm-based multi-objective optimisation of UHMSs. The novel decision-making framework enables generating the context-based optimised UHMS combinations to support local governments’ decision-making. The research outcomes will advance interdisciplinary knowledge about using AI techniques in the decision-making process for urban heat mitigation.",https://doi.org/10.1016/j.resconrec.2022.106420,https://www.sciencedirect.com/science/article/pii/S0921344922002646,"Resources, Conservation and Recycling",Jinda Qi;Lan Ding;Samsung Lim,2022,53,"@article{2-6860,
  title = {A decision-making framework to support urban heat mitigation by local governments},
  author = {Qi, Jinda and Ding, Lan and Lim, Samsung},
  year = {2022},
  doi = {10.1016/j.resconrec.2022.106420},
  journal = {Resources, Conservation and Recycling}
}",Methodological contributions,"Transportation / Mobility / Planning, Environment / Resources / Energy",Organizational,"Advising, Executing, Analyzing","Decision-maker, Guardian, Stakeholder, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-6895,elsevier,A deep neural network–based decision support tool for the detection of lymph node metastases in colorectal cancer specimens,"The identification of lymph node metastases in colorectal cancer (CRC) specimens is crucial for the planning of postoperative treatment and can be a time-consuming task for pathologists. In this study, we developed a deep neural network (DNN) algorithm for the detection of metastatic CRC in digitized histologic sections of lymph nodes and evaluated its performance as a diagnostic support tool. First, the DNN algorithm was trained using pixel-level annotations of cancerous areas on 758 whole slide images (360 with cancerous areas). The algorithm’s performance was evaluated on 74 whole slide images (43 with cancerous areas). Second, the algorithm was evaluated as a decision support tool on 288 whole slide images covering 1517 lymph node sections, randomized in 16 batches. Two senior pathologists (C.K. and C.O.) assessed each batch with and without the help of the algorithm in a 2 × 2 crossover design, with a washout period of 1 month in between. The time needed for the evaluation of each node section was recorded. The DNN algorithm achieved a median pixel-level accuracy of 0.952 on slides with cancerous areas and 0.996 on slides with benign samples. N+ disease (metastases, micrometastases, or tumor deposits) was present in 103 of the 1517 sections. The algorithm highlighted cancerous areas in 102 of these sections, with a sensitivity of 0.990. Assisted by the algorithm, the median time needed for evaluation was significantly shortened for both pathologists (median time for pathologist 1, 26 vs 14 seconds; P < .001; 95% CI, 11.0-12.0; median time for pathologist 2, 25 vs 23 seconds; P < .001; 95% CI, 2.0-4.0). Our DNN showed high accuracy for detecting metastatic CRC in digitized histologic sections of lymph nodes. This decision support tool has the potential to improve the diagnostic workflow by shortening the time needed for the evaluation of lymph nodes in CRC specimens without impairing diagnostic accuracy.",https://doi.org/10.1016/j.modpat.2022.100015,https://www.sciencedirect.com/science/article/pii/S0893395222000151,Modern Pathology,Csaba Kindler;Stefan Elfwing;John Öhrvik;Maziar Nikberg,2023,11,"@article{2-6895,
  title = {A Deep Neural Network–Based Decision Support Tool for the Detection of Lymph Node Metastases in Colorectal Cancer Specimens},
  author = {Csaba Kindler and Stefan Elfwing and John Öhrvik and Maziar Nikberg},
  year = {2023},
  doi = {10.1016/j.modpat.2022.100015},
  journal = {Modern Pathology}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Auditing, Forecasting","Knowledge provider, Guardian, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-6929,elsevier,A dynamic intelligent building retrofit decision-making model in response to climate change,"Building energy-saving retrofitting has become an essential way for the building sector to cope with climate change. Furthermore, climate change affects building retrofit strategies. The current building stock is massive, which means there is a large demand for building retrofitting. Additionally, climatic conditions are changing, posing a significant challenge to the current time-consuming and labor-intensive decision-making process. To solve these problems, a dynamic intelligent building decision-making model was established in this study. The static and dynamic features of building retrofit decision-making were identified, four machine learning algorithms were considered, and a case base containing records for 301 retrofitted buildings was established for knowledge mining. The findings demonstrate that the XGBoost algorithm performs well in terms of building retrofit strategy prediction, with 77% accuracy for the prediction of building envelope retrofits and 76% accuracy for the prediction of HVAC system retrofits. In addition, the trends of building retrofit strategy decision-making considering dynamic climate conditions were observed. The demand for building envelope retrofitting and heating retrofitting has declined, while the demand for cooling retrofitting has increased. Some buildings are extremely sensitive to climate change, and some redundant retrofitting strategies should be avoided. The proposed intelligent decision-making model can provide valuable information for future building retrofit strategy decision-making.",https://doi.org/10.1016/j.enbuild.2023.112832,https://www.sciencedirect.com/science/article/pii/S0378778823000622,Energy and Buildings,Dingyuan Ma;Xiaodong Li;Borong Lin;Yimin Zhu;Siyu Yue,2023,50,"@article{2-6929,
  title = {A dynamic intelligent building retrofit decision-making model in response to climate change},
  author = {Dingyuan Ma and Xiaodong Li and Borong Lin and Yimin Zhu and Siyu Yue},
  year = {2023},
  journal = {Energy and Buildings},
  doi = {https://doi.org/10.1016/j.enbuild.2023.112832}
}",System/Artifact contributions,"Environment / Resources / Energy, Design / Creativity / Architecture",Organizational,"Forecasting, Advising","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-6949,elsevier,A framework for context-aware heterogeneous group decision making in business processes,"In Business Process Management great attention is given to Computational Intelligence for supporting process life-cycle. Several approaches have been defined to support human decision making. The main drawback is that there are no solid criteria for determining optimal decisions since context, matter of discussion, and involved actors may differ at each execution. This work focuses on the definition of a framework to support and trace human decision making activities, in business processes, when heterogeneous decision-makers have to find a consensus to select most promising alternative to follow. The framework relies on Fuzzy Consensus Model and implements Reinforcement Learning algorithm to learn weight of the decision-makers through the analysis of past process executions considering context and performances of business processes. Context awareness relies on semantic web technologies enabling ontological reasoning to evaluate context similarity used to assign the right weight to the involved decision-makers also in the case when more general or more specific context occurs. The framework has been instantiated in the case study of Supply Chain Management. The analysis of the simulation results reveal that the proposed weight learning algorithm and the considered initial weight association strategies (Starting Weight and Training Executions), even if the cold start, give to decision-makers the chance to fill the gap with respect to more experienced decision makers.",https://doi.org/10.1016/j.knosys.2016.03.019,https://www.sciencedirect.com/science/article/pii/S0950705116300090,Knowledge-Based Systems,Carmen {De Maio};Giuseppe Fenza;Vincenzo Loia;Francesco Orciuoli;Enrique Herrera-Viedma,2016,56,"@article{2-6949,
  title={A framework for context-aware heterogeneous group decision making in business processes},
  author={De Maio, Carmen and Fenza, Giuseppe and Loia, Vincenzo and Orciuoli, Francesco and Herrera-Viedma, Enrique},
  year={2016},
  journal={Knowledge-Based Systems},
  doi={10.1016/j.knosys.2016.03.019}
}",System/Artifact contributions,Finance / Business / Economy,Organizational,"Analyzing, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-6952,elsevier,A framework for manufacturing system reconfiguration and optimisation utilising digital twins and modular artificial intelligence,"Digital twins and artificial intelligence have shown promise for improving the robustness, responsiveness, and productivity of industrial systems. However, traditional digital twin approaches are often only employed to augment single, static systems to optimise a particular process. This article presents a paradigm for combining digital twins and modular artificial intelligence algorithms to dynamically reconfigure manufacturing systems, including the layout, process parameters, and operation times of numerous assets to allow system decision-making in response to changing customer or market needs. A knowledge graph has been used as the enabler for this system-level decision-making. A simulation environment has been constructed to replicate the manufacturing process, with the example here of an industrial robotic manufacturing cell. The simulation environment is connected to a data pipeline and an application programming interface to assist the integration of multiple artificial intelligence methods. These methods are used to improve system decision-making and optimise the configuration of a manufacturing system to maximise user-selectable key performance indicators. In contrast to previous research, this framework incorporates artificial intelligence for decision-making and production line optimisation to provide a framework that can be used for a wide variety of manufacturing applications. The framework has been applied and validated in a real use case, with the automatic reconfiguration resulting in a process time improvement of approximately 10%.",https://doi.org/10.1016/j.rcim.2022.102524,https://www.sciencedirect.com/science/article/pii/S073658452200206X,Robotics and Computer-Integrated Manufacturing,Fan Mo;Hamood Ur Rehman;Fabio Marco Monetti;Jack C. Chaplin;David Sanderson;Atanas Popov;Antonio Maffei;Svetan Ratchev,2023,5,"@article{2-6952,
  title = {A framework for manufacturing system reconfiguration and optimisation utilising digital twins and modular artificial intelligence},
  author = {Fan Mo and Hamood Ur Rehman and Fabio Marco Monetti and Jack C. Chaplin and David Sanderson and Atanas Popov and Antonio Maffei and Svetan Ratchev},
  year = {2023},
  journal = {Robotics and Computer-Integrated Manufacturing},
  doi = {10.1016/j.rcim.2022.102524}
}",System/Artifact contributions,Manufacturing / Industry / Automation,"Organizational, Operational","Executing, Analyzing","Developer, Stakeholder, Guardian",NA,NA,NA,NA,NA,Yes,No
2-6975,elsevier,A general explicable forecasting framework for weather events based on ordinal classification and inductive rules combined with fuzzy logic,"This paper presents a method for providing explainability in the integration of artificial intelligence (AI) and data mining techniques when dealing with meteorological prediction. Explainable artificial intelligence (XAI) refers to the transparency of AI systems in providing explanations for their predictions and decision-making processes, and contribute to improve prediction accuracy and enhance trust in AI systems. The focus of this paper relies on the interpretability challenges in ordinal classification problems within weather forecasting. Ordinal classification involves predicting weather phenomena with ordered classes, such as temperature ranges, wind speed, precipitation levels, and others. To address this challenge, a novel and general explicable forecasting framework, that combines inductive rules and fuzzy logic, is proposed in this work. Inductive rules, derived from historical weather data, provide a logical and interpretable basis for forecasting; while fuzzy logic handles the uncertainty and imprecision in the weather data. The system predicts a set of probabilities that the incoming sample belongs to each considered class. Moreover, it allows the expert decision-making process to be strengthened by relying on the transparency and physical explainability of the model, and not only on the output of a black-box algorithm. The proposed framework is evaluated using two real-world weather databases related to wind speed and low-visibility events due to fog. The results are compared to both ML classifiers and specific methods for ordinal classification problems, achieving very competitive results in terms of ordinal performance metrics while offering a higher level of explainability and transparency compared to existing approaches.",https://doi.org/10.1016/j.knosys.2024.111556,https://www.sciencedirect.com/science/article/pii/S0950705124001916,Knowledge-Based Systems,C. Peláez-Rodríguez;J. Pérez-Aracil;C.M. Marina;L. Prieto-Godino;C. Casanova-Mateo;P.A. Gutiérrez;S. Salcedo-Sanz,2024,18,"@article{2-6975,
  title={A general explicable forecasting framework for weather events based on ordinal classification and inductive rules combined with fuzzy logic},
  author={Pel{\'a}ez-Rodr{\'i}guez, C. and P{\'e}rez-Aracil, J. and Marina, C.M. and Prieto-Godino, L. and Casanova-Mateo, C. and Guti{\'e}rrez, P.A. and Salcedo-Sanz, S.},
  year={2024},
  doi={10.1016/j.knosys.2024.111556},
  journal={Knowledge-Based Systems}
}",Methodological contributions,Environment / Resources / Energy,Operational,"Forecasting, Advising, Explaining","Decision-maker, Guardian, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-7019,elsevier,A hybrid cnn + bilstm deep learning-based dss for efficient prediction of judicial case decisions,"A gradual increase in the historical data available in the legal domain over the years has compelled industry experts to collect, assemble, and analyze this data to predict judicial case decisions. However, using this legal data to predict and justify court decisions is no small feat. At present, existing studies on predicting court decisions using limited-size datasets from experimentation have produced a variety of unexpected estimates that were made using machine learning (ML) classifiers with the assistance of common approaches for encoding categorical data. These early works also used convolutional neural networks (CNN), a class of deep neural networks; to extract features without keeping track of sequencing information. This present study proposes predicting court decisions by applying a hybrid deep learning (DL)-based decision support system (DSS); namely CNN with bidirectional long/short-term memory (BiLSTM); to efficiently predict court decisions from historical legal data. Using feature selection, only the most appropriate features were chosen by prioritizing and selecting features that ranked high in the given legal data set. The CNN + BiLSTM hybrid model was then used to predict judicial case decisions. In comparison to other similar studies, the experimental findings of this hybrid model were encouraging; 91.52 % accuracy, 91.74 % precision, 89.04 % recall, and an F1-score of 90.44 %.",https://doi.org/10.1016/j.eswa.2022.118318,https://www.sciencedirect.com/science/article/pii/S0957417422014476,Expert Systems with Applications,Shakeel Ahmad;Muhammad Zubair Asghar;Fahad Mazaed Alotaibi;Yasser D. Al-Otaibi,2022,36,"@article{2-7019,
  title     = {A Hybrid CNN + BiLSTM Deep Learning-Based DSS for Efficient Prediction of Judicial Case Decisions},
  author    = {Shakeel Ahmad and Muhammad Zubair Asghar and Fahad Mazaed Alotaibi and Yasser D. Al-Otaibi},
  year      = {2022},
  doi       = {https://doi.org/10.1016/j.eswa.2022.118318},
  journal   = {Expert Systems with Applications}
}",System/Artifact contributions,Law / Policy / Governance,Operational,"Advising, Forecasting, Analyzing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-7022,elsevier,A hybrid decision support system for adaptive trading strategies: combining a rule-based expert system with a deep reinforcement learning strategy,"Stock trading strategies pose challenging applications of machine learning for significant commercial yields in the finance industry, drawing the attention of both economists and computer scientists. Until now, many researchers have proposed various methods to implement intelligent trading strategy systems that can support decisions regarding stock trading. Some studies have shown that the problem of trading strategies can be successfully addressed by applying hybrid approaches. Motivated by this, we propose a hybrid decision support system for adaptive trading strategies that combines a rule-based system with deep reinforcement learning to self-improve by learning with human expertise. This study overcomes the limitations of previous hybrid models that mainly have focused on optimizing trading decisions and improving forecasting accuracy. The proposed hybrid model combines decision-making information from a rule-based model to enable the agent of reinforcement learning to capture more trading opportunities. In addition, the investor's available balance states facilitate adaptive learning by interacting with the environment. Moreover, the proposed trading mechanism adjusts the volume size using the policy gradient algorithm's action probabilities, resulting in improved risk-adjusted returns. The proposed hybrid model has the potential to be a reliable trading system in real-world applications through its ability to adapt to different market scenarios, withstand stressful market conditions, reduce transaction costs, scale to various index funds, and extend the proposed hybrid structure. This study highlights the applicability of more advanced machine learning in financial areas, and we also suggest expanding this approach to adaptive decision-making systems in other fields.",https://doi.org/10.1016/j.dss.2023.114100,https://www.sciencedirect.com/science/article/pii/S0167923623001756,Decision Support Systems,Yuhee Kwon;Zoonky Lee,2024,45,"@article{2-7022,
  title = {A Hybrid Decision Support System for Adaptive Trading Strategies: Combining a Rule-Based Expert System with a Deep Reinforcement Learning Strategy},
  author = {Yuhee Kwon and Zoonky Lee},
  year = {2024},
  doi = {10.1016/j.dss.2023.114100},
  journal = {Decision Support Systems}
}",System/Artifact contributions,Finance / Business / Economy,Operational,"Advising, Executing","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-7036,elsevier,A hybrid machine learning framework for analyzing human decision-making through learning preferences,"Multiple criteria decision aiding (MCDA) is a family of analytic approaches to depicting the rationale of human decisions. To better interpret the contributions of individual attributes to the decision maker, the conventional MCDA approaches assume that the attributes are monotonic and preference independence. However, the capacity in describing the decision maker’s preferences is sacrificed as a result of model simplification. To meet the decision maker’s demand for more accurate and interpretable decision models, we propose a novel hybrid method, namely Neural Network-based Multiple Criteria Decision Aiding (NN-MCDA), which combines MCDA model and machine learning to achieve better prediction performance while capturing the relationships between individual attributes and the prediction. NN-MCDA uses a linear component (in an additive form of a set of polynomial functions) to characterize such relationships through providing explicit non-monotonic marginal value functions, and a nonlinear component (in a standard multilayer perceptron form) to capture the implicit high-order interactions among attributes and their complex nonlinear transformations. We demonstrate the effectiveness of NN-MCDA with extensive simulation studies and three real-world datasets. The study sheds light on how to improve the prediction performance of MCDA models using machine learning techniques, and how to enhance the interpretability of machine learning models using MCDA approaches.",https://doi.org/10.1016/j.omega.2020.102263,https://www.sciencedirect.com/science/article/pii/S0305048319312575,Omega - The International Journal of Management Science,Mengzhuo Guo;Qingpeng Zhang;Xiuwu Liao;Frank Youhua Chen;Daniel Dajun Zeng,2021,72,"@article{2-7036,
  title = {A Hybrid Machine Learning Framework for Analyzing Human Decision-Making through Learning Preferences},
  author = {Mengzhuo Guo and Qingpeng Zhang and Xiuwu Liao and Frank Youhua Chen and Daniel Dajun Zeng},
  year = {2021},
  journal = {Omega - The International Journal of Management Science},
  volume = {},
  number = {},
  pages = {},
  doi = {10.1016/j.omega.2020.102263}
}",Algorithmic contributions,Generic / Abstract / Domain-agnostic,Operational,"Analyzing, Forecasting, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-7194,elsevier,A national survey of oncologists' perspectives on the ethical implications of artificial intelligence in cancer care,"Introduction Artificial Intelligence (AI) is an emerging technology that promises advancements across blood cancer discovery and care delivery. There are already FDA-approved uses of AI in hematology and oncology but few data available about its use or utility. Concerns have been raised over AI's potential biases, “explainability,” and who is responsible for its use. Oncologists' familiarity with AI and perspectives on these issues are not well known and are critical to understanding the ethical deployment of AI in blood cancer care. Methods From January to July 2023, we performed a national cross-sectional survey of practicing oncologists. A draft survey instrument was developed by a team with content and methodological expertise. Iterative refinement occurred through cognitive debriefing with 5 practicing hematologist/oncologists. The final instrument included 24 questions covering domains of AI familiarity, predictions, explainability, and responsibilities. A random sample of US oncologists was identified using the National Plan & Provider Enumeration System. Paper surveys were mailed with $25 incentivizing gift cards. Reminder letters and phone calls were used for non-responders with an electronic survey option provided. Responses were analyzed descriptively. X 2 testing assessed bivariate associations between responses and participant practice setting, with odds ratios (OR) reported. Results Of 399 mailed surveys, 12 were undeliverable and 201 were completed for a response rate of 51.9%. Question missingness was <1%. Participants were 63.2% White, 16.9% Indian Asian, 10.0% Eastern Asian or Pacific Islander, and 4.5% Black; 6.0% were Hispanic. They represented 37 states. The majority were male (63.7%), 40-59 years old (54.2%), and had no specific training in AI (52.2%); 68.2% had at least 11 years in practice and 29.9% practiced at a primary academic hospital. Questions assessing overall familiarity with and predictions related to AI are shown in Table 1. Those at primary academic hospitals were more likely to be familiar with radiology models (OR 2.82, p<0.001) but not pathology, prognostic, or clinical decision models (all p>0.05). They were more likely to predict that AI would improve side effect management (OR 2.01, p=0.03) and end-of-life decision making (OR 2.07, p=0.02). High proportions of respondents thought that AI prognostic (67.1%) and clinical decision models (62.1%) needed to be explainable by clinicians and researchers to be used in clinic. When posed with a scenario where an FDA-approved AI model selects a different regimen than the oncologist planned to propose (Figure 1), the largest proportion reported they would present both options and let the patient decide (37.3%). Of those recommending a regimen, respondents from primary academic hospitals more frequently recommend the AI-generated regimen (56.1%), while respondents from elsewhere recommended their original regimen (69.0%; OR 2.85, p=0.007). Acceptability of some direct-to-patient AI model applications is shown in Table 1. A majority of respondents (90.5%) thought that AI developers should be responsible for medico-legal problems caused by AI use; 47.8% thought that physicians using the tool should be responsible. Though most (76.1%) agreed that it was oncologists' responsibility to protect patients from biased AI tools, few (27.3%) were confident in their ability to identify how representative the data used in an AI model was relative to the population with that cancer (including 66.7% of those who agreed it was oncologists' responsibility). More respondents thought that patients should need to consent to the use of AI tools in cancer treatment decisions (81.1%) than cancer diagnosis (55.7%); those from primary academic hospitals were less likely to respond that patients should consent for treatment or diagnosis (OR 0.43, p=0.02 and OR 0.44, p=0.009). Conclusions In this national survey of oncologists' views on AI, there was optimism about its impact on care with notable caveats related to end-of-life care, patient trust, and care disparities. Despite many approving use of AI models too complex for patients to explain, a substantial proportion deferred to patient choice when AI and physician recommendations differed. There was also dissonance between accepting responsibility to protect patients from bias and respondents' self-perceived ability to do so.",https://doi.org/10.1182/blood-2023-175030,https://www.sciencedirect.com/science/article/pii/S0006497123089498,Blood,Andrew Hantel;Jonathan M. Marron;Kenneth Kehl;Richard Sharp;Thomas P Walsh;Eliezer {Van Allen};Gregory A Abel,2023,1,"@article{2-7194,
  title = {A national survey of oncologists' perspectives on the ethical implications of artificial intelligence in cancer care},
  author = {Andrew Hantel and Jonathan M. Marron and Kenneth Kehl and Richard Sharp and Thomas P. Walsh and Eliezer {Van Allen} and Gregory A. Abel},
  year = {2023},
  doi = {10.1182/blood-2023-175030},
  journal = {Blood}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Guardian, Stakeholder, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-7206,elsevier,A new deep learning approach integrated with clinical data for the dermoscopic differentiation of early melanomas from atypical nevi,"Background Timely recognition of malignant melanoma (MM) is challenging for dermatologists worldwide and represents the main determinant for mortality. Dermoscopic examination is influenced by dermatologists’ experience and fails to achieve adequate accuracy and reproducibility in discriminating atypical nevi (AN) from early melanomas (EM). Objective We aimed to develop a Deep Convolutional Neural Network (DCNN) model able to support dermatologists in the classification and management of atypical melanocytic skin lesions (aMSL). Methods A training set (630 images), a validation set (135) and a testing set (214) were derived from the idScore dataset of 979 challenging aMSL cases in which the dermoscopic image is integrated with clinical data (age, sex, body site and diameter) and associated with histological data. A DCNN_aMSL architecture was designed and then trained on both dermoscopic images of aMSL and the clinical/anamnestic data, resulting in the integrated “iDCNN_aMSL” model. Responses of 111 dermatologists with different experience levels on both aMSL classification (intuitive diagnosis) and management decisions (no/long follow-up; short follow-up; excision/preventive excision) were compared with the DCNNs models. Results In the lesion classification study, the iDCNN_aMSL achieved the best accuracy, reaching an AUC = 90.3 %, SE = 86.5 % and SP = 73.6 %, compared to DCNN_aMSL (SE = 89.2 %, SP = 65.7 %) and intuitive diagnosis of dermatologists (SE = 77.0 %; SP = 61.4 %). Conclusions The iDCNN_aMSL proved to be the best support tool for management decisions reducing the ratio of inappropriate excision. The proposed iDCNN_aMSL model can represent a valid support for dermatologists in discriminating AN from EM with high accuracy and for medical decision making by reducing their rates of inappropriate excisions.",https://doi.org/10.1016/j.jdermsci.2020.11.009,https://www.sciencedirect.com/science/article/pii/S0923181120303947,Journal of Dermatological Science,Linda Tognetti;Simone Bonechi;Paolo Andreini;Monica Bianchini;Franco Scarselli;Gabriele Cevenini;Elvira Moscarella;Francesca Farnetani;Caterina Longo;Aimilios Lallas;Cristina Carrera;Susana Puig;Danica Tiodorovic;Jean Luc Perrot;Giovanni Pellacani;Giuseppe Argenziano;Elisa Cinotti;Gennaro Cataldo;Alberto Balistreri;Alessandro Mecocci;Marco Gori;Pietro Rubegni;Alessandra Cartocci,2021,48,"@article{2-7206,
  title = {A new deep learning approach integrated with clinical data for the dermoscopic differentiation of early melanomas from atypical nevi},
  author = {Linda Tognetti and Simone Bonechi and Paolo Andreini and Monica Bianchini and Franco Scarselli and Gabriele Cevenini and Elvira Moscarella and Francesca Farnetani and Caterina Longo and Aimilios Lallas and Cristina Carrera and Susana Puig and Danica Tiodorovic and Jean Luc Perrot and Giovanni Pellacani and Giuseppe Argenziano and Elisa Cinotti and Gennaro Cataldo and Alberto Balistreri and Alessandro Mecocci and Marco Gori and Pietro Rubegni and Alessandra Cartocci},
  year = {2021},
  journal = {Journal of Dermatological Science},
  doi = {10.1016/j.jdermsci.2020.11.009}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-7263,elsevier,A novel decision support system for managing predictive maintenance strategies based on machine learning approaches,"Nowadays, the industrial environment is characterised by growing competitiveness, short response times, cost reduction and reliability of production to meet customer needs. Thus, the new industrial paradigm of Industry 4.0 has gained interest worldwide, leading many manufacturers to a significant digital transformation. Digital technologies have enabled a novel approach to decision-making processes based on data-driven strategies, where knowledge extraction relies on the analysis of a large amount of data from sensor-equipped factories. In this context, Predictive Maintenance (PdM) based on Machine Learning (ML) is one of the most prominent data-driven analytical approaches for monitoring industrial systems aiming to maximise reliability and efficiency. In fact, PdM aims not only to reduce equipment failure rates but also to minimise operating costs by maximising equipment life. When considering industrial applications, industries deal with different issues and constraints relating to process digitalisation. The main purpose of this study is to develop a new decision support system based on decision trees (DTs) that guides the decision-making process of PdM implementation, considering context-aware information, quality and maturity of collected data, severity, occurrence and detectability of potential failures (identified through FMECA analysis) and direct and indirect maintenance costs. The decision trees allow the study of different scenarios to identify the conditions under which a PdM policy, based on the ML algorithm, is economically profitable compared to corrective maintenance, considered to be the current scenario. The results show that the proposed methodology is a simple and easy way to implement tool to support the decision process by assessing the different levels of occurrence and severity of failures. For each level, savings and the potential costs have been evaluated at leaf nodes of the trees aimed at defining the most suitable maintenance strategy implementation. Finally, the proposed DTs are applied to a real industrial case to illustrate their applicability and robustness.",https://doi.org/10.1016/j.ssci.2021.105529,https://www.sciencedirect.com/science/article/pii/S0925753521003726,Safety Science,S. Arena;E. Florian;I. Zennaro;P.F. Orrù;F. Sgarbossa,2022,181,"@article{2-7263,
  title = {A novel decision support system for managing predictive maintenance strategies based on machine learning approaches},
  author = {S. Arena and E. Florian and I. Zennaro and P. F. Orrù and F. Sgarbossa},
  year = {2022},
  journal = {Safety Science},
  doi = {10.1016/j.ssci.2021.105529}
}",System/Artifact contributions,Manufacturing / Industry / Automation,Operational,"Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-7291,elsevier,A novel integration strategy for uncertain knowledge in group decision-making with artificial opinions: a dsfit-soa-dematel approach,"The inherent knowledge limitations possessed by human experts often yield suboptimal accuracy in linguistic decisions within traditional Decision Making Experimentation and Evaluation Laboratory (DEMATEL) methods. Conversely, artificial decision-making approaches, employed to mitigate these limitations, often introduce substantial instability due to technical constraints and ethical biases. This study introduces a novel decision aggregation approach, termed Dempster-Shafer Fusion fitness function (DSFIT)-spatially Optimal Aggregation (SOA), with the primary objective of mitigating uncertainty in DEMATEL assessment decisions through synergizing human expertise and Artificial Intelligence (AI) capabilities. First, artificial decision-maker (ADM) evidence was acquired through talking with an open-source large language model (LLM); Second, DSFIT quantified uncertainty using the equivalent uncertainty and amplifying uncertainty algorithms, thereby assigning appropriate weights; Third, the SOA method combined evidence from all experts into a comprehensive group perspective. Finally, an analysis of human decision-makers' satisfaction factors in interactive multi-objective optimization, utilizing the DEMATEL method, probed the influence of uncertainty operators on ADM weights. Furthermore, we compared ADM knowledge supplementation, ADM acquisition prompt design, and uncertainty assessment methods to enhance the methodology's accuracy during ADM intervention. This work introduces innovative methods to mitigate decision instability during human and LLM collaboration, providing valuable support for strengthening human–machine partnerships.",https://doi.org/10.1016/j.eswa.2023.122886,https://www.sciencedirect.com/science/article/pii/S0957417423033882,Expert Systems with Applications,Lin Sheng;Zhenyu Gu;Fangyuan Chang,2024,1,"@article{2-7291,
  title={A novel integration strategy for uncertain knowledge in group decision-making with artificial opinions: a dsfit-soa-dematel approach},
  author={Sheng, Lin and Gu, Zhenyu and Chang, Fangyuan},
  year={2024},
  journal={Expert Systems with Applications},
  doi={10.1016/j.eswa.2023.122886}
}",Methodological contributions,Generic / Abstract / Domain-agnostic,Organizational,"Executing, Advising, Collaborating","Decision-maker, Knowledge provider","Change cognitive demands, Change affective-perceptual","Update AI competence, Change AI responses, Shape AI for accountability","uncertainty, textual explanations, recommendations","domain knowledge, corrective feedback","Textual, Visual",Yes,Yes
2-7366,elsevier,A practical computerized decision support system for predicting the severity of alzheimer's disease of an individual,"Computerized clinical decision support systems can help to provide objective, standardized, and timely dementia diagnosis. However, current computerized systems are mainly based on group analysis, discrete classification of disease stages, or expensive and not readily accessible biomarkers, while current clinical practice relies relatively heavily on cognitive and functional assessments (CFA). In this study, we developed a computational framework using a suite of machine learning tools for identifying key markers in predicting the severity of Alzheimer's disease (AD) from a large set of biological and clinical measures. Six machine learning approaches, namely Kernel Ridge Regression (KRR), Support Vector Regression, and k-Nearest Neighbor for regression and Support Vector Machine (SVM), Random Forest, and k-Nearest Neighbor for classification, were used for the development of predictive models. We demonstrated high predictive power of CFA. Predictive performance of models incorporating CFA was shown to consistently have higher accuracy than those based solely on biomarker modalities. We found that KRR and SVM were the best performing regression and classification methods respectively. The optimal SVM performance was observed for a set of four CFA test scores (FAQ, ADAS13, MoCA, MMSE) with multi-class classification accuracy of 83.0%, 95%CI = (72.1%, 93.8%) while the best performance of the KRR model was reported with combined CFA and MRI neuroimaging data, i.e., R2 = 0.874, 95%CI = (0.827, 0.922). Given the high predictive power of CFA and their widespread use in clinical practice, we designed a data-driven and self-adaptive computerized clinical decision support system (CDSS) prototype for evaluating the severity of AD of an individual on a continuous spectrum. The system implemented an automated computational approach for data pre-processing, modelling, and validation and used exclusively the scores of selected cognitive measures as data entries. Taken together, we have developed an objective and practical CDSS to aid AD diagnosis.",https://doi.org/10.1016/j.eswa.2019.04.022,https://www.sciencedirect.com/science/article/pii/S0957417419302520,Expert Systems with Applications,Magda Bucholc;Xuemei Ding;Haiying Wang;David H. Glass;Hui Wang;Girijesh Prasad;Liam P. Maguire;Anthony J. Bjourson;Paula L. McClean;Stephen Todd;David P. Finn;KongFatt Wong-Lin,2019,117,"@article{2-7366,
  title={A practical computerized decision support system for predicting the severity of Alzheimer's disease of an individual},
  author={Bucholc, Magda and Ding, Xuemei and Wang, Haiying and Glass, David H. and Wang, Hui and Prasad, Girijesh and Maguire, Liam P. and Bjourson, Anthony J. and McClean, Paula L. and Todd, Stephen and Finn, David P. and Wong-Lin, KongFatt},
  year={2019},
  journal={Expert Systems with Applications},
  doi={10.1016/j.eswa.2019.04.022}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising, Executing","Decision-maker, Guardian, Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-7385,elsevier,A proactive decision support method based on deep reinforcement learning and state partition,"Big streaming data is an important kind of big data which we need new technology to process. Getting knowledge from online streaming data and making decision online can help us get more value from Big data. A proactive decision support system can predict future states and mitigate or eliminate undesired future states by taking some actions proactively. But it is difficult to handle some issues like the data distribution change in streaming data, combination of prediction and decision making, and the huge state space in decision making. In this paper, we propose a proactive decision support method based on deep reinforcement learning and state partition. The predictive analytics part uses deep belief networks with two level incremental training method. The deep reinforcement learning part uses deep belief networks as function approximation which is learned by semi-gradient method. Off-policy is supported through important sampling. Two kinds of state partition and parallel execution methods are proposed to improve the performance. The experimental evaluation in traffic congestion control application shows this method works well in both accuracy and performance.",https://doi.org/10.1016/j.knosys.2017.11.005,https://www.sciencedirect.com/science/article/pii/S095070511730504X,Knowledge-Based Systems,Yongheng Wang;Shaofeng Geng;Hui Gao,2018,1,"@article{2-7385,
  title = {A proactive decision support method based on deep reinforcement learning and state partition},
  author = {Wang, Yongheng and Geng, Shaofeng and Gao, Hui},
  year = {2018},
  journal = {Knowledge-Based Systems},
  doi = {10.1016/j.knosys.2017.11.005}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,"Executing, Forecasting","Guardian, Stakeholder, Developer",NA,NA,NA,NA,NA,Yes,No
2-7411,elsevier,A radiomics-based decision support tool improves lung cancer diagnosis in combination with the herder score in large lung nodules,"Summary Background Large lung nodules (≥15 mm) have the highest risk of malignancy, and may exhibit important differences in phenotypic or clinical characteristics to their smaller counterparts. Existing risk models do not stratify large nodules well. We aimed to develop and validate an integrated segmentation and classification pipeline, incorporating deep-learning and traditional radiomics, to classify large lung nodules according to cancer risk. Methods 502 patients from five U.K. centres were recruited to the large-nodule arm of the retrospective LIBRA study between July 2020 and April 2022. 838 CT scans were used for model development, split into training and test sets (70% and 30% respectively). An nnUNet model was trained to automate lung nodule segmentation. A radiomics signature was developed to classify nodules according to malignancy risk. Performance of the radiomics model, termed the large-nodule radiomics predictive vector (LN-RPV), was compared to three radiologists and the Brock and Herder scores. Findings 499 patients had technically evaluable scans (mean age 69 ± 11, 257 men, 242 women). In the test set of 252 scans, the nnUNet achieved a DICE score of 0.86, and the LN-RPV achieved an AUC of 0.83 (95% CI 0.77–0.88) for malignancy classification. Performance was higher than the median radiologist (AUC 0.75 [95% CI 0.70–0.81], DeLong p = 0.03). LN-RPV was robust to auto-segmentation (ICC 0.94). For baseline solid nodules in the test set (117 patients), LN-RPV had an AUC of 0.87 (95% CI 0.80–0.93) compared to 0.67 (95% CI 0.55–0.76, DeLong p = 0.002) for the Brock score and 0.83 (95% CI 0.75–0.90, DeLong p = 0.4) for the Herder score. In the international external test set (n = 151), LN-RPV maintained an AUC of 0.75 (95% CI 0.63–0.85). 18 out of 22 (82%) malignant nodules in the Herder 10–70% category in the test set were identified as high risk by the decision-support tool, and may have been referred for earlier intervention. Interpretation The model accurately segments and classifies large lung nodules, and may improve upon existing clinical models. Funding This project represents independent research funded by: 1) Royal Marsden Partners Cancer Alliance, 2) the Royal Marsden Cancer Charity, 3) the National Institute for Health Research (NIHR) Biomedical Research Centre at the Royal Marsden NHS Foundation Trust and The Institute of Cancer Research, London, 4) the National Institute for Health Research (NIHR) Biomedical Research Centre at Imperial College London, 5) Cancer Research UK (C309/A31316).",https://doi.org/10.1016/j.ebiom.2022.104344,https://www.sciencedirect.com/science/article/pii/S2352396422005266,eBioMedicine,Benjamin Hunter;Mitchell Chen;Prashanthi Ratnakumar;Esubalew Alemu;Andrew Logan;Kristofer Linton-Reid;Daniel Tong;Nishanthi Senthivel;Amyn Bhamani;Susannah Bloch;Samuel V. Kemp;Laura Boddy;Sejal Jain;Shafick Gareeboo;Bhavin Rawal;Simon Doran;Neal Navani;Arjun Nair;Catey Bunce;Stan Kaye;Matthew Blackledge;Eric O. Aboagye;Anand Devaraj;Richard W. Lee,2022,46,"@article{2-7411,
  title = {A radiomics-based decision support tool improves lung cancer diagnosis in combination with the herder score in large lung nodules},
  author = {Hunter, Benjamin and Chen, Mitchell and Ratnakumar, Prashanthi and Alemu, Esubalew and Logan, Andrew and Linton-Reid, Kristofer and Tong, Daniel and Senthivel, Nishanthi and Bhamani, Amyn and Bloch, Susannah and Kemp, Samuel V. and Boddy, Laura and Jain, Sejal and Gareeboo, Shafick and Rawal, Bhavin and Doran, Simon and Navani, Neal and Nair, Arjun and Bunce, Catey and Kaye, Stan and Blackledge, Matthew and Aboagye, Eric O. and Devaraj, Anand and Lee, Richard W.},
  year = {2022},
  doi = {10.1016/j.ebiom.2022.104344},
  journal = {eBioMedicine}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-748,aaai,Modeling Human Trust and Reliance in AI-Assisted Decision Making: A Markovian Approach,"The increased integration of artificial intelligence( AI) technologies in human workflows has resulted in a new paradigm of AI-assisted decision making, in which an AI model provides decision recommendations while humans make the final decisions. To best support humans in decision making, it is critical to obtain a quantitative understanding of how humans interact with and rely on AI. Previous studies often model humans reliance on AI as an analytical process, i. e. , reliance decisions are made based on cost-benefit analysis. However, theoretical models in psychology suggest that the reliance decisions can often be driven by emotions like humans trust in AI models. In this paper, we propose a hidden Markov model to capture the affective process underlying the human-AI interaction in AI-assisted decision making, by characterizing how decision makers adjust their trust in AI over time and make reliance decisions based on their trust. Evaluations on real human behavior data collected from human-subject experiments show that the proposed model outperforms various baselines in accurately predicting humans reliance behavior in AI-assisted decision making. Based on the proposed model, we further provide insights into how humans trust and reliance dynamics in AI-assisted decision making is influenced by contextual factors like decision stakes and their interaction experiences.",10.1609/aaai.v37i5.25748,https://ojs.aaai.org/index.php/AAAI/article/view/25748,AAAI Conference on Artificial Intelligence,Zhuoyan Li;Zhuoran Lu;Ming Yin,2023,0,"@inproceedings{2-748,
  title={Modeling Human Trust and Reliance in AI-Assisted Decision Making: A Markovian Approach},
  author={Li, Zhuoyan and Lu, Zhuoran and Yin, Ming},
  year={2023},
  doi={10.1609/aaai.v37i5.25748},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Finance / Business / Economy",Operational,"Forecasting, Advising","Decision-maker, Knowledge provider","Change trust, Alter decision outcomes",no such info,prediction of alternative,NA,Textual,Yes,Yes
2-75,aaai,Adapting a Kidney Exchange Algorithm to Align With Human Values,"The efficient allocation of limited resources is a classical problem in economics and computer science. In kidney exchanges, a central market maker allocates living kidney donors to patients in need of an organ. Patients and donors in kidney exchanges are prioritized using ad-hoc weights decided on by committee and then fed into an allocation algorithm that determines who get what—and who does not. In this paper, we provide an end-to-end methodology for estimating weights of individual participant profiles in a kidney exchange. We first elicit from human subjects a list of patient attributes they consider acceptable for the purpose of prioritizing patients( e. g. , medical characteristics, lifestyle choices, and so on). Then, we ask subjects comparison queries between patient profiles and estimate weights in a principled way from their responses. We show how to use these weights in kidney exchange market clearing algorithms. We then evaluate the impact of the weights in simulations and find that the precise numerical values of the weights we computed matter little, other than the ordering of profiles that they imply. However, compared to not prioritizing patients at all, there is a significant effect, with certain classes of patients being( de) prioritized based on the human-elicited value judgments.",10.1609/aaai.v32i1.11505,https://ojs.aaai.org/index.php/AAAI/article/view/11505,AAAI Conference on Artificial Intelligence,Rachel Freedman;Jana Schaich Borg;Walter Sinnott-Armstrong;John Dickerson;Vincent Conitzer,2018,158,"@inproceedings{2-75,
  title = {Adapting a Kidney Exchange Algorithm to Align With Human Values},
  author = {Rachel Freedman and Jana Schaich Borg and Walter Sinnott-Armstrong and John Dickerson and Vincent Conitzer},
  year = {2018},
  booktitle = {AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v32i1.11505}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing","Decision-maker, Stakeholder, Guardian, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-7500,elsevier,A smart sustainable decision support system for water management of power plants in water stress regions,"Power Plants (PPs) is considered as critical facilities in each region because of essential role through energy generation processes. These facilities are also depended to water availability especially in water stress areas. Due to the critical water shortage in many areas around the world, it is necessary to make an optimal condition among water consumption and the increasing demand for electricity to prevent any further conflict of interests between industry, householders and the environmental goals. There are different techniques for controlling Water Consumption (WC) in these industries. This paper develops a smart Decision Support System (DSS) for monitoring, prediction and control sections based on Artificial Intelligent (AI) and integration of the PESTEL matrix and Multi Criteria Decision Making (MCDM) methods. Monitoring section comprises Fuel Consumption (FC), Atmospheric Temperature (AT), Power Plant Temperature (PPT) and Power Plant Efficiency (PPE), in which FC has the most influence on WC based on ANOVA evaluations in both cold and warm seasons. The prediction results have illustrated that Adaptive Neuro Fuzzy Inference System model is more efficient for the WC estimation with a correlation coefficient over 0.99. Ordered Weighted Averaging (OWA) also demonstrated that in the optimistic and pessimistic states, the most priority is linked to E3 (Establishment of evaporation control systems by contractor companies and concluding a guaranteed purchase contract with a power plant worth one and a half times the current amount of water price). In the last step of technical approaches, the smart controlling system is added for execution of water-energy nexus in the PP based on proportional–integral–derivative controller system. Finally, the performance of the DSS is approved with more than 80% agreement of experts and more than 90% precision in prediction procedure through this investigation. Application of this DSS can also be helpful for developing countries to achieve the UN Sustainable Development Goals.",https://doi.org/10.1016/j.eswa.2023.120752,https://www.sciencedirect.com/science/article/pii/S095741742301254X,Expert Systems with Applications,Mahdi Nakhaei;Amirhossein Ahmadi;Mohammad Gheibi;Benyamin Chahkandi;Mostafa Hajiaghaei-Keshteli;Kourosh Behzadian,2023,11,"@article{2-7500,
  title={A smart sustainable decision support system for water management of power plants in water stress regions},
  author={Nakhaei, Mahdi and Ahmadi, Amirhossein and Gheibi, Mohammad and Chahkandi, Benyamin and Hajiaghaei-Keshteli, Mostafa and Behzadian, Kourosh},
  year={2023},
  journal={Expert Systems with Applications},
  doi={10.1016/j.eswa.2023.120752}
}",System/Artifact contributions,Environment / Resources / Energy,Operational,"Forecasting, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-7512,elsevier,A strategic decision-making architecture toward hybrid teams for dynamic competitive problems,"Advances in artificial intelligence create new opportunities for computers to support humans as peers in hybrid teams in several complex problem-solving situations. This paper proposes a decision-making architecture for adaptively informing decisions in human-computer collaboration for large-scale competitive problems under dynamic environments. The proposed architecture integrates methods from sequence learning, model predictive control, and game theory. Computers in this architecture learn objectives and strategies from experimental data to support humans with strategic decisions while operational decisions are made by humans. The paper also presents data-driven methods for partitioning tasks among a team of computers in this architecture. The generalized methodology is illustrated on the real-time strategy game Starcraft II. The results from this application show that low-performing players can benefit from the game-theoretic decision support whereas this support can be overly conservative for high-performing players. The proposed approach provides safe though suboptimal suggestions particularly against an opponent with an unknown level of expertise. The results further show that problem solution with a team of computers based on non-intuitive task partitioning significantly improves the quality of decisions compared to an all-in-one solution with a single computer.",https://doi.org/10.1016/j.dss.2020.113490,https://www.sciencedirect.com/science/article/pii/S0167923620302451,Decision Support Systems,Alparslan Emrah Bayrak;Christopher McComb;Jonathan Cagan;Kenneth Kotovsky,2021,0,"@article{2-7512,
  title={A strategic decision-making architecture toward hybrid teams for dynamic competitive problems},
  author={Bayrak, Alparslan Emrah and McComb, Christopher and Cagan, Jonathan and Kotovsky, Kenneth},
  year={2021},
  journal={Decision Support Systems},
  doi={10.1016/j.dss.2020.113490}
}",Methodological contributions,"Generic / Abstract / Domain-agnostic, Media / Communication / Entertainment",Operational,Advising,Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-7515,elsevier,A streaming data visualization framework for supporting decision-making in the intensive care unit,"The number of reporting activities in real time has increased over the last years. This situation has pushed the need for providing real time analysis and visualizations to support decision-making. We propose a visualization framework for exploratory data analysis of multivariate data streams that relies on dimensionality reduction and machine learning techniques for plotting the data in two dimensions. Users can demarcate regions of interest for their study, and use them to make predictions or to decide when to train a new model. The knowledge gained from these visualizations allows users to: (i) characterize the data stream scenario; (ii) track the evolution of a case of interest; and (iii) configure and raise alarms according to the user-defined regions. We illustrate the effectiveness of our proposal through a case study analyzing real-world streaming data to identify patients with multi-drug resistant bacteria when they are in a hospital intensive care unit. Our visualization framework enables the patient follow-up which can allow clinicians to support decisions about the health status evolution of a particular patient. This could provide information for deciding on a particular treatment or whether to isolate patients with a high risk of having multi-drug resistant bacteria since their presence boosts infections in intensive care units.",https://doi.org/10.1016/j.eswa.2023.120252,https://www.sciencedirect.com/science/article/pii/S0957417423007546,Expert Systems with Applications,Miguel A. Mohedano-Munoz;Cristina Soguero-Ruiz;Inmaculada Mora-Jiménez;Manuel Rubio-Sánchez;Joaquín Álvarez-Rodríguez;Alberto Sanchez,2023,0,"@article{2-7515,
  title     = {A streaming data visualization framework for supporting decision-making in the intensive care unit},
  author    = {Miguel A. Mohedano-Munoz and Cristina Soguero-Ruiz and Inmaculada Mora-Jiménez and Manuel Rubio-Sánchez and Joaquín Álvarez-Rodríguez and Alberto Sanchez},
  year      = {2023},
  doi       = {10.1016/j.eswa.2023.120252},
  journal   = {Expert Systems with Applications}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-7543,elsevier,A sustainable decision support system for soil bioremediation of toluene incorporating un sustainable development goals,"Decision Support System (DSS) is a novel approach for smart, sustainable controlling of environmental phenomena and purification processes. Toluene is one of the most widely used petroleum products, which adversely impacts on human health. In this study, Fusarium Solani fungi are utilized as the engine of the toluene bioremediation procedure for the monitoring part of DSS. Experiments are optimized by Central Composite Design (CCD) - Response Surface Methodology (RSM), and the behavior of the mentioned fungi is estimated by M5 Pruned model tree (M5P), Gaussian Processes (GP), and Sequential Minimal Optimization (SMOreg) algorithms as the prediction section of DSS. Finally, the control stage of DSS is provided by integrated Petri Net modeling and Failure Modes and Effects Analysis (FMEA). The findings showed that Aeration Intensity (AI) and Fungi load/Biological Waste (F/BW) are the most influential mechanical and biological factors, with P-value of 0.0001 and 0.0003, respectively. Likewise, the optimal values of main mechanical parameters include AI, and the space between pipes (S) are equal to 13.76 m3/h and 15.99 cm, respectively. Also, the optimum conditions of biological features containing F/BW and pH are 0.001 mg/g and 7.56. In accordance with the kinetic study, bioremediation of toluene by Fusarium Solani is done based on a first-order reaction with a 0.034 s-1 kinetic coefficient. Finally, the machine learning practices showed that the GP (R2 = 0.98) and M5P (R2 = 0.94) have the most precision for predicting Removal Percentage (RP) for mechanical and biological factors, respectively. At the end of the present research, it is found that by controlling seven possible risk factors in bioremediation operation through the FMEA- Petri Net technique, efficiency of the process can be adjusted to optimum value.",https://doi.org/10.1016/j.envpol.2022.119587,https://www.sciencedirect.com/science/article/pii/S0269749122008016,Environmental Pollution,Hadi Akbarian;Farhad Mahmoudi Jalali;Mohammad Gheibi;Mostafa Hajiaghaei-Keshteli;Mehran Akrami;Ajit K. Sarmah,2022,22,"@article{2-7543,
  title = {A sustainable decision support system for soil bioremediation of toluene incorporating UN sustainable development goals},
  author = {Hadi Akbarian and Farhad Mahmoudi Jalali and Mohammad Gheibi and Mostafa Hajiaghaei-Keshteli and Mehran Akrami and Ajit K. Sarmah},
  year = {2022},
  doi = {10.1016/j.envpol.2022.119587},
  journal = {Environmental Pollution}
}",System/Artifact contributions,Environment / Resources / Energy,Operational,"Forecasting, Advising, Monitoring","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-7548,elsevier,A system based on an artificial neural network of the second generation for decision support in especially significant situations,"Currently, specialized system models implemented on the basis of decision support in exceptional (emergency) situations (states) using machine learning, artificial intelligence (including using neural networks) to reproduce, predict and prevent (or minimize the risk of consequences) in exceptional situations are useful and are becoming increasingly popular. Floods also fall under such exceptional situations and states, on the basis of which the task of early forecasting of an exceptional situation arises, using the example of rising water levels at stationary hydrological posts in order to prevent (or minimize the risk) the transition of the territory management system under consideration to an exceptional state (emergency situation). To solve this problem, a decision support system is proposed for early prediction of water rise levels, based on a neural network (intelligent) analysis of retrospective data (code of a stationary hydrological post / automatic station, date, water level at a stationary hydrological post / automatic station, atmospheric pressure, wind speed, snow cover thickness, amount of precipitation, time and air temperature) in order to calculate the values of water levels for 5 days in advance. The artificial neural network itself is based on the freely distributed TensorFlow machine learning software library, and a modified error backpropagation method is used as training, the main difference of which is the addition of an artificial neural network (ANN) learning rate increase factor. A mathematical model of the subject area is constructed in the form of a multidimensional space and an integrated multidimensional data model. A single integrated model describes the subject area quite fully and can be used as a single source of information, which provides system information support, defined as a set of distributed databases. All this makes it possible to improve the accuracy of forecast values of water levels at stationary hydrological posts and automatic stations. The main difficulty in early forecasting is a sharp rise in water levels over a short period of time (usually 1 day) for various reasons. Our neural network model takes this into account and allows us to give more accurate results. The results of the analysis of the effectiveness of the method showed that the proposed decision support system is more accurate: the average error does not exceed 55.68 cm for the entire flood period, the average error between the real and calculated values in the framework of forecasting for 5 days does not exceed 2.10 % compared to existing common methods/systems (8.36 %). This will give the necessary time to special services to carry out anti–flood measures to prepare for the protection of technical facilities of enterprises.",https://doi.org/10.1016/j.jhydrol.2022.128844,https://www.sciencedirect.com/science/article/pii/S0022169422014147,Journal of Hydrology,Evgeny Palchevsky;Vyacheslav Antonov;Rustem Radomirovich Enikeev;Tim Breikin,2023,12,"@article{2-7548,
  title = {A system based on an artificial neural network of the second generation for decision support in especially significant situations},
  author = {Evgeny Palchevsky and Vyacheslav Antonov and Rustem Radomirovich Enikeev and Tim Breikin},
  year = {2023},
  doi = {10.1016/j.jhydrol.2022.128844},
  journal = {Journal of Hydrology}
}",System/Artifact contributions,Defense / Military / Emergency,Organizational,"Advising, Forecasting, Analyzing","Decision-maker, Guardian, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-7565,elsevier,A task-level emergency experience reuse method for freeway accidents onsite disposal with policy distilled reinforcement learning,"A large number of freeway accident disposals are well-recorded by accident reports and surveillance videos, but it is not easy to get the emergency experience reused from past recorded accidents. To reuse emergency experience for better emergency decision-making, this paper proposed a knowledge-based experience transfer method to transfer task-level freeway accident disposal experience via multi-agent reinforcement learning algorithm with policy distillation. First, the Markov decision process is used to simulate the emergency decision-making process of multi-type freeway accident scene at the task level. Then, an adaptive knowledge transfer method named policy distilled multi-agent deep deterministic policy gradient (PD-MADDPG) algorithm is proposed to reuse experience from past freeway accident records to current accidents for fast decision-making and optimal onsite disposal. The performance of the proposed algorithm is evaluated on instantiated cases of freeway accidents that occurred on the freeway in Shaanxi Province, China. Aside from achieving better emergency decisions performance than various typical decision-making methods, the result shows decision maker with transferred knowledge owns 65.22%, 11.37%, 9.23%, 7.76% and 1.71% higher average reward than those without in the five studied cases, respectively. Indicating that the emergency experience transferred from past accidents contributes to fast emergency decision-making and optimal accident onsite disposal.",https://doi.org/10.1016/j.aap.2023.107179,https://www.sciencedirect.com/science/article/pii/S0001457523002269,Accident Analysis & Prevention,Longhao Yan;Ping Wang;Fan Qi;Zhuohang Xu;Ronghui Zhang;Yu Han,2023,1,"@article{2-7565,
  title = {A Task-Level Emergency Experience Reuse Method for Freeway Accidents Onsite Disposal with Policy Distilled Reinforcement Learning},
  author = {Longhao Yan and Ping Wang and Fan Qi and Zhuohang Xu and Ronghui Zhang and Yu Han},
  year = {2023},
  doi = {10.1016/j.aap.2023.107179},
  journal = {Accident Analysis & Prevention}
}",Algorithmic contributions,Transportation / Mobility / Planning,Organizational,Executing,"Knowledge provider, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-7583,elsevier,A two-stage classification approach for ai technical service supplier selection based on multi-stakeholder concern,"Artificial intelligence (AI) technology, which can help supply chains improve operational efficiency and reduce production costs, has gradually been adopted by many enterprises in recent years. In general, stakeholders in the supply chain have their own requirements for AI technical services. However, most traditional service supplier selection methods rarely explicitly consider the interest demands of multi-stakeholders and coordinate them, which may lead to unsatisfactory decision results. Therefore, to support the successful implementation of AI technology in supply chain management, this paper proposes a two-stage AI technical service supplier classification method that considers the requirements of multi-stakeholders. In the first stage, we identify the opinions of stakeholders via cluster analysis and determine group coordination needs while fully considering fairness concerns. Moreover, in the second stage, we evaluate and classify AI technical service suppliers based on the group opinions of stakeholders and decision makers' risk preferences. As AI technical service suppliers are mostly emerging internet enterprises, we construct a criteria system to help assess these suppliers in advance. To verify the practicability and effectiveness of our method, we conduct a case study on the automobile supply chain. Furthermore, this work can provide guidance for AI technical service suppliers to improve their enterprise construction.",https://doi.org/10.1016/j.ins.2023.119762,https://www.sciencedirect.com/science/article/pii/S0020025523013476,Information Sciences,Decui Liang;Wen Cao;Yinrunjie Zhang;Zeshui Xu,2024,16,"@article{2-7583,
  title = {A two-stage classification approach for AI technical service supplier selection based on multi-stakeholder concern},
  author = {Decui Liang and Wen Cao and Yinrunjie Zhang and Zeshui Xu},
  year = {2024},
  doi = {10.1016/j.ins.2023.119762},
  journal = {Information Sciences}
}",Methodological contributions,Manufacturing / Industry / Automation,Organizational,"Advising, Analyzing","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-7649,elsevier,Adapt and overcome: perceptions of adaptive autonomous agents for human-ai teaming,"Rapid advances in AI technologies have caused teams to explore the use of AI agents as full, active members of the team. The complex environments that teams occupy require human team members to constantly adapt their behaviors, and thus the ability of AI teammates to similarly adapt to changing situations significantly enhances the team’s chances to succeed. In order to design such agents, it is important that we understand not only how to identify the amount of autonomous control AI agents have over their decisions, but also how changes to this control cognitively affects the rest of the team. Professional organizations often break their work cycles into phases that set limits on the team members’ actions, and we propose that a similar process could be used to define the autonomy levels of AI teammates. Cyber incident response is an ideal context for this proposal, as we were able to use incident response phases to explore how a team’s work cycle could guide an AI agent’s changing level of autonomy. Using a mixed methods approach, we recruited 103 participants to complete a factorial survey containing ten contextual vignettes focused on an AI teammate’s level of autonomy in incident response contexts, and from these participants we conducted twenty-two follow-on qualitative interviews that further explored how the participants felt an AI agent’s adaptive capabilities would affect team performance and cohesiveness. Our results showed that work cycles can be used to assign autonomy levels to adaptive AI agents based upon the degree of formal processes and predictability of the team’s tasks during the cycle, and that dynamic, human-like adaptation methods are vital to effective human-AI teams. This research provides significant contributions to the HCI community by proposing design recommendations for the development of adaptive autonomous teammates that both enhance Human-AI teams’ productivity and promote positive team dynamics.",https://doi.org/10.1016/j.chb.2022.107451,https://www.sciencedirect.com/science/article/pii/S0747563222002722,Computers in Human Behavior,Allyson I. Hauptman;Beau G. Schelble;Nathan J. McNeese;Kapil Chalil Madathil,2023,124,"@article{2-7649,
  title = {Adapt and overcome: perceptions of adaptive autonomous agents for human-ai teaming},
  author = {Hauptman, Allyson I. and Schelble, Beau G. and McNeese, Nathan J. and Madathil, Kapil Chalil},
  year = {2023},
  journal = {Computers in Human Behavior},
  doi = {10.1016/j.chb.2022.107451}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Operational,"Executing, Collaborating","Decision-maker, Guardian","Shift responsibility, Change trust, Change affective-perceptual",no such info,NA,"varying comfort with high autonomy across phases, leveraging higher levels of autonomy with experienced teams, dynamic autonomy, need time to accept",NA,Yes,Yes
2-7727,elsevier,Ai and digitalization in relationship management: impact of adopting ai-embedded crm system,"The purpose of this study is to determine the impact of adopting a artificial intelligence-embedded customer relationship management (CRM) system for business-to-business relationship management. After reviewing the literature and considering the theory, a conceptual model was developed. The model was validated using the PLS-SEM technique with 312 responses from 14 firms in the B2B context. The study finds that an AI-embedded CRM system has a significant positive impact towards B2B relationship satisfaction and firm performance. Also, the study highlights that there is a negative impact of the moderator ‘technology turbulence’ on the relations of ‘automated decision making’ and ‘operational efficiency’ with ‘B2B relationship satisfaction’, whereas there is a positive impact of moderator ‘leadership support’ on ‘B2B relationship satisfaction’ and ‘firm performance’. There is presently no study on the impact of AI-CRM in the B2B context. Also, the study contributes to the existing literature by incorporating the moderator impact of ‘technology turbulence’ and ‘leadership support’ in the context of AI-embedded CRM systems for B2B relationship management.",https://doi.org/10.1016/j.jbusres.2022.06.033,https://www.sciencedirect.com/science/article/pii/S0148296322005719,Journal of Business Research,Sheshadri Chatterjee;Ranjan Chaudhuri;Demetris Vrontis,2022,159,"@article{2-7727,
  title={AI and digitalization in relationship management: impact of adopting AI-embedded CRM system},
  author={Chatterjee, Sheshadri and Chaudhuri, Ranjan and Vrontis, Demetris},
  year={2022},
  doi={10.1016/j.jbusres.2022.06.033},
  journal={Journal of Business Research}
}",Empirical contributions,Finance / Business / Economy,Operational,"Executing, Analyzing, Advising","Stakeholder, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-7732,elsevier,Ai model for analyzing construction litigation precedents to support decision-making,"Litigation among stakeholders in construction projects has a significantly negative impact on successful project completion and overall performance. Prompt decision-making in relation to litigation is crucial, but the manual review of extensive document sets is time-consuming. In this paper, the natural language processing (NLP) technique was applied to litigation data to develop a model for case summarization and winner prediction. By automatically summarizing the data and predicting litigation outcomes, the proposed model aids practitioners in making timely decisions and enhances document management during disputes. This paper contributes to existing knowledge in two ways. Firstly, the model aids practitioners in making timely decisions about proceeding with litigation. Secondly, unlike previous studies that manually processed raw data such as contracts and specifications, this study utilized NLP to process raw litigation case data automatically. As big data becomes increasingly common, the methodology employed in this study holds academic significance.",https://doi.org/10.1016/j.autcon.2024.105824,https://www.sciencedirect.com/science/article/pii/S0926580524005600,Automation in Construction,Wonkyoung Seo;Youngcheol Kang,2024,6,"@article{2-7732,
  title={AI model for analyzing construction litigation precedents to support decision-making},
  author={Seo, Wonkyoung and Kang, Youngcheol},
  year={2024},
  journal={Automation in Construction},
  volume={},
  pages={},
  doi={10.1016/j.autcon.2024.105824}
}",Algorithmic contributions,Law / Policy / Governance,Operational,"Forecasting, Advising, Analyzing","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-7738,elsevier,Ai-assisted clinical decision making in interventional cardiology: the potential of commercially available large language models,NA,https://doi.org/10.1016/j.jcin.2024.06.013,https://www.sciencedirect.com/science/article/pii/S1936879824009014,Journal of the American College of Cardiology: Cardiovascular Interventions,Edward Itelman;Guy Witberg;Ran Kornowski,2024,11,"@article{2-7738,
        author = {Edward Itelman and Guy Witberg and Ran Kornowski},
        journal = {JACC: Cardiovascular Interventions},
        number = {15},
        pages = {1858-1860},
        title = {AI-Assisted Clinical Decision Making in Interventional Cardiology: The Potential of Commercially Available Large Language Models},
        volume = {17},
        year = {2024},
        doi={10.1016/j.jcin.2024.06.013}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-7745,elsevier,Ai-clinician collaboration via disagreement prediction: a decision pipeline and retrospective analysis of real-world radiologist-ai interactions,"Summary Clinical decision support tools can improve diagnostic performance or reduce variability, but they are also subject to post-deployment underperformance. Although using AI in an assistive setting offsets many concerns with autonomous AI in medicine, systems that present all predictions equivalently fail to protect against key AI safety concerns. We design a decision pipeline that supports the diagnostic model with an ecosystem of models, integrating disagreement prediction, clinical significance categorization, and prediction quality modeling to guide prediction presentation. We characterize disagreement using data from a deployed chest X-ray interpretation aid and compare clinician burden in this proposed pipeline to the diagnostic model in isolation. The average disagreement rate is 6.5%, and the expected burden reduction is 4.8%, even if 5% of disagreements on urgent findings receive a second read. We conclude that, in our production setting, we can adequately balance risk mitigation with clinician burden if disagreement false positives are reduced.",https://doi.org/10.1016/j.xcrm.2023.101207,https://www.sciencedirect.com/science/article/pii/S2666379123003749,Cell Reports Medicine,Morgan Sanchez;Kyle Alford;Viswesh Krishna;Thanh M. Huynh;Chanh D.T. Nguyen;Matthew P. Lungren;Steven Q.H. Truong;Pranav Rajpurkar,2023,12,"@article{2-7745,
  title     = {Ai-clinician collaboration via disagreement prediction: a decision pipeline and retrospective analysis of real-world radiologist-ai interactions},
  author    = {Morgan Sanchez and Kyle Alford and Viswesh Krishna and Thanh M. Huynh and Chanh D.T. Nguyen and Matthew P. Lungren and Steven Q.H. Truong and Pranav Rajpurkar},
  year      = {2023},
  doi       = {10.1016/j.xcrm.2023.101207},
  journal   = {Cell Reports Medicine}
}","Methodological contributions, System/Artifact contributions",Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising, Collaborating","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-7751,elsevier,Ai-enabled investment advice: will users buy it?,"The objective of this paper is to develop and empirically validate a conceptual model that explains individuals' behavioral intention to accept AI-based recommendations as a function of attitude toward AI, trust, perceived accuracy and uncertainty level. The conceptual model was tested through a between-participants experiment using a simulated AI-enabled investment recommendation system. A total of 368 participants were randomly and evenly assigned to one of the two experimental conditions, one depicting low-uncertainty investment recommendation involving blue-chip stocks while the other depicting high-uncertainty investment recommendation involving penny stocks. Results show that attitude toward AI was positively associated with behavioral intention to accept AI-based recommendations, trust in AI, and perceived accuracy of AI. Furthermore, uncertainty level moderated how attitude, trust and perceived accuracy varied with behavioral intention to accept AI-based recommendations. When uncertainty was low, a favorable attitude toward AI seemed sufficient to promote reliance on automation. However, when uncertainty was high, a favorable attitude toward AI was a necessary but no longer sufficient condition for AI acceptance. Thus, the paper contributes to the human-AI interaction literature by not only shedding light on the underlying psychological mechanism of how users decide to accept AI-enabled advice but also adding to the scholarly understanding of AI recommendation systems in tasks that call for intuition in high involvement services.",https://doi.org/10.1016/j.chb.2022.107481,https://www.sciencedirect.com/science/article/pii/S0747563222003016,Computers in Human Behavior,Alton Y.K. Chua;Anjan Pal;Snehasish Banerjee,2023,0,"@article{2-7751,
  title = {AI-enabled Investment Advice: Will Users Buy It?},
  author = {Alton Y.K. Chua and Anjan Pal and Snehasish Banerjee},
  year = {2023},
  doi = {10.1016/j.chb.2022.107481},
  journal = {Computers in Human Behavior}
}",Empirical contributions,Finance / Business / Economy,Individual,"Advising, Forecasting","Decision-maker, Guardian","Alter decision outcomes, Change trust, Change affective-perceptual",no such info,"recommendations, uncertainty",NA,Textual,Yes,Yes
2-7752,elsevier,Ai-guided resource allocation and rescue decision system for medical applications,"With the robust growth in the social environment, millions of families have vast medical plans nowadays. Each client requires a personalized medical rescue decision, making an intelligent recommendation system highly important. In practice, wealthy families tend to purchase tailored medical services, while others tend to seek medical services from local community hospitals. Noticeably, trillions-scale of medical service transactions occurs daily, making the development of a fully automatic intelligent recommendation system to match the suitable medical services to clients urgent. In this work, we propose a novel Internet-scale deep architecture that automatically recommends personalized and tailored medical rescue services based on the previously discovered clients with different financial tolerance. Specifically, given a massive number of client families, first, we create a set of features that represents the multiple medically related attributes of each family. These features capture the income, health status, type of diseases, occupation, and so on. The features are then combined into a descriptive feature by multi-view learning, which can dynamically assign the importance of each feature channels. Based on the combined feature, affinity graphs are created on a large scale to represent the relations between the client families. Also, an efficient dense subgraph mining is used to categorize the million-scale clients into 10 different medical groups. Finally, various customized medical services can be tailored for each client in a fully automatic and efficient manner. Comprehensive experiments on our collected data sets demonstrated the competitiveness of our proposed intelligent recommendation system. Moreover, the results of visualized dense subgraph mining showed that the client families with different wealth levels can be distinguished accurately.",https://doi.org/10.1016/j.future.2020.12.010,https://www.sciencedirect.com/science/article/pii/S0167739X20330727,Future Generation Computer Systems,Ye Yu;Zhongheng Zhang;Rongju Sun;Haiping Liu;Suwei Yuan;Ting Jiang;Meng Wu;Cheng Guo;Yuelei Guo;Jianchao Weng;Xingdong Zheng;Feng Yuan,2021,0,"@article{2-7752,
  title={AI-guided resource allocation and rescue decision system for medical applications},
  author={Ye, Yu and Zhang, Zhongheng and Sun, Rongju and Liu, Haiping and Yuan, Suwei and Jiang, Ting and Wu, Meng and Guo, Cheng and Guo, Yuelei and Weng, Jianchao and Zheng, Xingdong and Yuan, Feng},
  year={2021},
  journal={Future Generation Computer Systems},
  doi={10.1016/j.future.2020.12.010}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Organizational,"Analyzing, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-7754,elsevier,Ai-induced indifference: unfair ai reduces prosociality,"The growing prevalence of artificial intelligence (AI) in our lives has brought the impact of AI-based decisions on human judgments to the forefront of academic scholarship and public debate. Despite growth in research on people's receptivity towards AI, little is known about how interacting with AI shapes subsequent interactions among people. We explore this question in the context of unfair decisions determined by AI versus humans and focus on the spillover effects of experiencing such decisions on the propensity to act prosocially. Four experiments (combined N = 2425) show that receiving an unfair allocation by an AI (versus a human) actor leads to lower rates of prosocial behavior towards other humans in a subsequent decision—an effect we term AI-induced indifference. In Experiment 1, after receiving an unfair monetary allocation by an AI (versus a human) actor, people were less likely to act prosocially, defined as punishing an unfair human actor at a personal cost in a subsequent, unrelated decision. Experiments 2a and 2b provide evidence for the underlying mechanism: People blame AI actors less than their human counterparts for unfair behavior, decreasing people's desire to subsequently sanction injustice by punishing the unfair actor. In an incentive-compatible design, Experiment 3 shows that AI-induced indifference manifests even when the initial unfair decision and subsequent interaction occur in different contexts. These findings illustrate the spillover effect of human-AI interaction on human-to-human interactions and suggest that interacting with unfair AI may desensitize people to the bad behavior of others, reducing their likelihood to act prosocially. Implications for future research are discussed. All preregistrations, data, code, statistical outputs, stimuli qsf files, and the Supplementary Appendix are posted on OSF at: https://bit.ly/OSF_unfairAI",https://doi.org/10.1016/j.cognition.2024.105937,https://www.sciencedirect.com/science/article/pii/S0010027724002233,Cognition,Raina Zexuan Zhang;Ellie J. Kyung;Chiara Longoni;Luca Cian;Kellen Mrkva,2025,11,"@article{2-7754,
  title={Ai-induced indifference: unfair AI reduces prosociality},
  author={Zhang, Raina Zexuan and Kyung, Ellie J. and Longoni, Chiara and Cian, Luca and Mrkva, Kellen},
  year={2025},
  journal={Cognition},
  doi={10.1016/j.cognition.2024.105937}
}",Empirical contributions,"Everyday / Employment / Public Service, Law / Policy / Governance, Generic / Abstract / Domain-agnostic",Individual,Executing,"Knowledge provider, Decision-subject","Shape ethical norms, Change trust, Shift responsibility, Alter decision outcomes",no such info,unfairness,NA,Textual,Yes,Yes
2-7768,elsevier,Algorithms as partners in crime: a lesson in ethics by design,"The human in the loop is often advocated as a panacea against concerns about AI-powered machines, which increasingly take decisions of consequence in all realms of life. However, can we rely on humans to prevent unethical decisions by machines? We run online experiments modeling both the case where the machine serves as a corrective to the human and where the human serves as a corrective to the machine. Our results suggest that, in the former case, humans make similar decisions whether the corrective is a machine or another human. In the latter case, humans take advantage of rather than correct bad decisions by machines, turning into partners in crime. These findings caution us not to count too much on the human in the loop as a moral corrective. Instead, they tend to argue for human–machine decision-making where the human makes the decision and the machine is the corrective.",https://doi.org/10.1016/j.chb.2022.107483,https://www.sciencedirect.com/science/article/pii/S074756322200303X,Computers in Human Behavior,Sebastian Krügel;Andreas Ostermaier;Matthias Uhl,2023,22,"@article{2-7768,
  title = {Algorithms as partners in crime: a lesson in ethics by design},
  author = {Sebastian Krügel and Andreas Ostermaier and Matthias Uhl},
  year = {2023},
  doi = {10.1016/j.chb.2022.107483},
  journal = {Computers in Human Behavior}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Executing, Monitoring, Advising","Decision-maker, Decision-subject, Guardian","Alter decision outcomes, Shape ethical norms",Shape AI for accountability,NA,human in the loop have limited moral corrective influence on AI,Autonomous System,Yes,Yes
2-7828,elsevier,An artificial intelligence algorithmic approach to ethical decision-making in human resource management processes,"Management scholars and practitioners have highlighted the importance of ethical dimensions in the selection of strategies. However, to date, there has been little effort aimed at theoretically understanding the ethical positions of individuals/organizations concerning human resource management (HRM) decision-making processes, the selection of specific ethical positions and strategies, or the post-decision accounting for those decisions. To this end, we present a Throughput model framework that describes individuals' decision-making processes in an algorithmic HRM context. The model depicts how perceptions, judgments, and the use of information affect strategy selection, identifying how diverse strategies may be supported by the employment of certain ethical decision-making algorithmic pathways. In focusing on concerns relating to the impact and acceptance of artificial intelligence (AI) integration in HRM, this research draws insights from multidisciplinary theoretical lenses, such as AI-augmented (HRM(AI)) and HRM(AI) assimilation processes, AI-mediated social exchange, and the judgment and choice literature. We highlight the use of algorithmic ethical positions in the adoption of AI for better HRM outcomes in terms of intelligibility and accountability of AI-generated HRM decision-making, which is often underexplored in existing research, and we propose their key role in HRM strategy selection.",https://doi.org/10.1016/j.hrmr.2022.100925,https://www.sciencedirect.com/science/article/pii/S1053482222000432,Human Resource Management Review,Waymond Rodgers;James M. Murray;Abraham Stefanidis;William Y. Degbey;Shlomo Y. Tarba,2023,497,"@article{2-7828,
  title={An artificial intelligence algorithmic approach to ethical decision-making in human resource management processes},
  author={Rodgers, Waymond and Murray, James M. and Stefanidis, Abraham and Degbey, William Y. and Tarba, Shlomo Y.},
  year={2023},
  journal={Human Resource Management Review},
  doi={10.1016/j.hrmr.2022.100925}
}",Theoretical contributions,Everyday / Employment / Public Service,Organizational,"Advising, Executing","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-7832,elsevier,An artificial neural network tool to support the decision making of designers for environmentally conscious product development,"The consideration of sustainability aspects in initial stages of product development is understood as an effective approach to plan a sustainable product life cycle. It can be achieved by integrating the sustainability considerations into decision making of companies in early design phases. This study aims to develop an Artificial Intelligence (AI) tool that can assist the designers in their decision making to choose environmentally benign design parameters of products. The proposed tool is based on an Artificial Neural Network (ANN) model which takes the life cycle design parameters (viz. size of product, density of material, manufacturing process, transport mode and recyclability) as inputs and provides the corresponding outputs in terms of ‘carbon footprint’ and ‘life cycle cost’ of a product. These outputs assist the designers to realize the trade-off between the environmental load and cost effectiveness of a design alternative. Thus, it enables the decision making of companies to select a more sustainable design. A Graphical User Interface (GUI) is developed for the AI tool so that the designers can efficiently use this tool. The results predicted by the proposed tool are compared with the results obtained through the life cycle assessment carried out by using GaBi 9.2. The comparison shows that the results predicted by the tool have a reasonable accuracy of more than 90% which is significant, especially in the design stages of environmentally conscious product development. Also, the time efficiency of the tool to compute the environmental impact was compared with that of GaBi 9.2 by using a T-test. Results showed that the time efficiency of the proposed AI tool is significantly higher than that of GaBi 9.2.",https://doi.org/10.1016/j.eswa.2022.118679,https://www.sciencedirect.com/science/article/pii/S0957417422017146,Expert Systems with Applications,Prashant Kumar Singh;Prabir Sarkar,2023,5,"@article{2-7832,
  title={An artificial neural network tool to support the decision making of designers for environmentally conscious product development},
  author={Singh, Prashant Kumar and Sarkar, Prabir},
  year={2023},
  journal={Expert Systems with Applications},
  doi={10.1016/j.eswa.2022.118679}
}",System/Artifact contributions,Design / Creativity / Architecture,Operational,"Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-7885,elsevier,An evaluation of the capabilities of language models and nurses in providing neonatal clinical decision support,"Aim To assess the clinical reasoning capabilities of two large language models, ChatGPT-4 and Claude-2.0, compared to those of neonatal nurses during neonatal care scenarios. Design A cross-sectional study with a comparative evaluation using a survey instrument that included six neonatal intensive care unit clinical scenarios. Participants 32 neonatal intensive care nurses with 5–10 years of experience working in the neonatal intensive care units of three medical centers. Methods Participants responded to 6 written clinical scenarios. Simultaneously, we asked ChatGPT-4 and Claude-2.0 to provide initial assessments and treatment recommendations for the same scenarios. The responses from ChatGPT-4 and Claude-2.0 were then scored by certified neonatal nurse practitioners for accuracy, completeness, and response time. Results Both models demonstrated capabilities in clinical reasoning for neonatal care, with Claude-2.0 significantly outperforming ChatGPT-4 in clinical accuracy and speed. However, limitations were identified across the cases in diagnostic precision, treatment specificity, and response lag. Conclusions While showing promise, current limitations reinforce the need for deep refinement before ChatGPT-4 and Claude-2.0 can be considered for integration into clinical practice. Additional validation of these tools is important to safely leverage this Artificial Intelligence technology for enhancing clinical decision-making. Impact The study provides an understanding of the reasoning accuracy of new Artificial Intelligence models in neonatal clinical care. The current accuracy gaps of ChatGPT-4 and Claude-2.0 need to be addressed prior to clinical usage.",https://doi.org/10.1016/j.ijnurstu.2024.104771,https://www.sciencedirect.com/science/article/pii/S002074892400083X,International Journal of Nursing Studies,Chedva Levin;Tehilla Kagan;Shani Rosen;Mor Saban,2024,19,"@article{2-7885,
  title = {An evaluation of the capabilities of language models and nurses in providing neonatal clinical decision support},
  author = {Levin, Chedva and Kagan, Tehilla and Rosen, Shani and Saban, Mor},
  year = {2024},
  doi = {10.1016/j.ijnurstu.2024.104771},
  journal = {International Journal of Nursing Studies}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Guardian, Knowledge provider",no such info,Change AI responses,"recommendations, preliminary diagnoses",domain knowledge,Textual,Yes,Yes
2-7897,elsevier,An explainable ai decision-support-system to automate loan underwriting,"Widespread adoption of automated decision making by artificial intelligence (AI) is witnessed due to specular advances in computation power and improvements in optimization algorithms especially in machine learning (ML). Complex ML models provide good prediction accuracy; however, the opacity of ML models does not provide sufficient assurance for their adoption in the automation of lending decisions. This paper presents an explainable AI decision-support-system to automate the loan underwriting process by belief-rule-base (BRB). This system can accommodate human knowledge and can also learn from historical data by supervised learning. The hierarchical structure of BRB can accommodates factual and heuristic rules. The system can explain the chain of events leading to a decision for a loan application by the importance of an activated rule and the contribution of antecedent attributes in the rule. A business case study on automation of mortgage underwriting is demonstrated to show that the BRB system can provide a good trade-off between accuracy and explainability. The textual explanation produced by the activation of rules could be used as a reason for denial of a loan. The decision-making process for an application can be comprehended by the significance of rules in providing the decision and contribution of its antecedent attributes.",https://doi.org/10.1016/j.eswa.2019.113100,https://www.sciencedirect.com/science/article/pii/S0957417419308176,Expert Systems with Applications,Swati Sachan;Jian-Bo Yang;Dong-Ling Xu;David Eraso Benavides;Yang Li,2020,213,"@article{2-7897,
  title={An explainable AI decision-support-system to automate loan underwriting},
  author={Sachan, Swati and Yang, Jian-Bo and Xu, Dong-Ling and Benavides, David Eraso and Li, Yang},
  year={2020},
  journal={Expert Systems with Applications},
  doi={10.1016/j.eswa.2019.113100}
}",System/Artifact contributions,Finance / Business / Economy,Operational,"Explaining, Executing",Decision-subject,"Alter decision outcomes, Change trust",Update AI competence,"textual explanations, factual-rule-base, heuristic-rule-base, the contribution of attributes in activated rules",domain knowledge,Autonomous System,Yes,Yes
2-7901,elsevier,An explainable data-driven decision support framework for strategic customer development,"Financial institutions benefit from the advanced predictive performance of machine learning algorithms in automatic decision-making for credit scoring. However, two main challenges hamper machine learning algorithms’ applicability in practice: the complex and black-box nature of algorithms that hinder their understandability and the inability to guide rejected customers to have a successful application. Regarding customer relationship management is one of the main responsibilities of financial institutions; they must clarify the decision-making process to guide them. However, financial institutions are not willing to disclose their decision-making procedure to prevent potential risks from customers or competitors side. Hence, in this study, a decision support framework is proposed to clarify the decision-making process and model strategic decision-making to guide rejected customers simultaneously. To do so, after classifying customers in their corresponding groups, the capability of Shapley additive exPlanations method is exploited to extract the most impactful features to the prediction’s outcome globally and locally. Then, based on the benchmarking approach, the equivalent approved peer is found for the rejected customer for target setting to modify the application. To find the optimal modified values for a counterfactual prediction, a multi-objective gamed-based counterfactual explanation model is developed using the prisoner’s dilemma game as the constraint to simulate strategic decision-making. After optimization, the decision is reported to the customers concerning the credential background. A public data set is used to elaborate on the proposed framework. This framework can generate counterfactual predictions successfully by modifying perspective features.",https://doi.org/10.1016/j.knosys.2024.111761,https://www.sciencedirect.com/science/article/pii/S0950705124003964,Knowledge-Based Systems,Mohsen Abbaspour Onari;Mustafa Jahangoshai Rezaee;Morteza Saberi;Marco S. Nobile,2024,16,"@article{2-7901,
  title={An explainable data-driven decision support framework for strategic customer development},
  author={Mohsen Abbaspour Onari and Mustafa Jahangoshai Rezaee and Morteza Saberi and Marco S. Nobile},
  year={2024},
  journal={Knowledge-Based Systems},
  doi={10.1016/j.knosys.2024.111761}
}",Theoretical contributions,Finance / Business / Economy,Organizational,"Forecasting, Explaining, Advising","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-7951,elsevier,An integrated optimization + learning approach to optimal dynamic pricing for the retailer with multi-type customers in smart grids,"In this paper, we consider a realistic and meaningful scenario in the context of smart grids where an electricity retailer serves three different types of customers, i.e., customers with an optimal home energy management system embedded in their smart meters (C-HEMS), customers with only smart meters (C-SM), and customers without smart meters (C-NONE). The main objective of this paper is to support the retailer to make optimal day-ahead dynamic pricing decisions in such a mixed customer pool. To this end, we propose a two-level decision-making framework where the retailer acting as upper-level agent firstly announces its electricity prices of next 24 h and customers acting as lower-level agents subsequently schedule their energy usages accordingly. For the lower level problem, we model the price responsiveness of different customers according to their unique characteristics. For the upper level problem, we optimize the dynamic prices for the retailer to maximize its profit subject to realistic market constraints. The above two-level model is tackled by genetic algorithms (GA) based distributed optimization methods while its feasibility and effectiveness are confirmed via simulation results.",https://doi.org/10.1016/j.ins.2018.03.039,https://www.sciencedirect.com/science/article/pii/S002002551730292X,Information Sciences,Fanlin Meng;Xiao-Jun Zeng;Yan Zhang;Chris J. Dent;Dunwei Gong,2018,42,"@article{2-7951,
  title={An integrated optimization + learning approach to optimal dynamic pricing for the retailer with multi-type customers in smart grids},
  author={Meng, Fanlin and Zeng, Xiao-Jun and Zhang, Yan and Dent, Chris J. and Gong, Dunwei},
  year={2018},
  journal={Information Sciences},
  doi={10.1016/j.ins.2018.03.039}
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Executing, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-7965,elsevier,An intelligent recognition model for dynamic air traffic decision-making,"Air traffic flow management system (ATFMS) is becoming increasingly important due to the rapid growth of air traffic and serious flight delay nowadays. To aware the air traffic flow density and identify the heat airspace in terminal areas of large hub airports is essential for an ATFMS. Due to numerous parameters in air traffic flow, traditional methods based on one single parameter fail to reflect the true complexity relationship between these parameters. This study aims to develop an intelligent air traffic flow heat airspace recognition model using advanced data science technique for establishing a real-time cloud map in the terminal airspace of airports, which attempts to use machine learning models to represent the complex relationship among these parameters. In the proposed intelligent recognition model, high dimensions of parameters (basic parameters, additional parameters and time parameters) are processed to achieve a comprehensive and accurate situation awareness for support dynamic air traffic decision-making. An aircraft trajectories points clustering method is developed to generate a 4D heat airspace map. The basic parameters and time parameters are used to identify the heat airspaces; the changes of additional parameters which influence the heat airspaces are identified and analyzed by use of grid graphs of flight trajectories; probability fitting graphs are used to verify accuracy of 4D results in order to support air traffic decision-making. A case study on Beijing International Airport (PEK) is conducted to test our model and has obtained two main research findings: there are two areas of PEK that have the high density and there are hot peaks at two different heights; flight trajectories and speed of trajectories also effect on the heat airspace. The study realizes that the proposed 4D heat airspace model is better for detailed and accurate information construction, expression of spatial changes, and visualization of multiple parameters of temporal and spatial density and range. It can assist the decisions on airspace allocation, and also have a definite reference meaning on alleviating the contradiction between the current air traffic demand and airspace resource constraints.",https://doi.org/10.1016/j.knosys.2019.105274,https://www.sciencedirect.com/science/article/pii/S095070511930574X,Knowledge-Based Systems,Xinru Du;Zi Lu;Dianshuang Wu,2020,9,"@article{2-7965,
  title={An intelligent recognition model for dynamic air traffic decision-making},
  author={Du, Xinru and Lu, Zi and Wu, Dianshuang},
  year={2020},
  journal={Knowledge-Based Systems},
  doi={10.1016/j.knosys.2019.105274}
}",System/Artifact contributions,Transportation / Mobility / Planning,Operational,"Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-7966,elsevier,An intelligent retrofit decision-making model for building program planning considering tacit knowledge and multiple objectives,"Due to the enormous building stock and high energy consumption of the construction sector, green retrofitting of existing buildings has recently become a critical issue. China has implemented building retrofitting on a large scale based on various norms and standards. In practice, however, the effectiveness of the whole building retrofit program is often jeopardized because some decision-makers are limited by their experience and fail to evaluate the program in its entirety. To overcome this problem, this study offers an intelligent decision support model considering tacit knowledge for program decision-making with conflicting objectives. By comparing several data mining approaches and using 152 retrofitted existing buildings as examples, a tacit knowledge mining model based on the XGBoost algorithm with an accuracy of 73.91% is constructed. The predicted results of the knowledge mining model can be used as the input of the multi-objective decision-making model. In addition, the retrofit cost, thermal insulation requirement, and total retrofit area are chosen as the objectives of the multi-objective decision-making model. Then, the model's applicability to building retrofit programs is tested using five buildings as examples. Finally, the results demonstrate that the proposed model can partially replace experts in supporting policymakers and owners throughout the planning stage.",https://doi.org/10.1016/j.energy.2022.125704,https://www.sciencedirect.com/science/article/pii/S0360544222025907,Energy,Dingyuan Ma;Xiaodong Li;Borong Lin;Yimin Zhu,2023,20,"@article{2-7966,
  title={An intelligent retrofit decision-making model for building program planning considering tacit knowledge and multiple objectives},
  author={Ma, Dingyuan and Li, Xiaodong and Lin, Borong and Zhu, Yimin},
  year={2023},
  journal={Energy},
  doi={10.1016/j.energy.2022.125704}
}",System/Artifact contributions,Environment / Resources / Energy,Organizational,"Executing, Advising","Guardian, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-7973,elsevier,An international observational study suggests that artificial intelligence for clinical decision support optimizes anemia management in hemodialysis patients,"Managing anemia in hemodialysis patients can be challenging because of competing therapeutic targets and individual variability. Because therapy recommendations provided by a decision support system can benefit both patients and doctors, we evaluated the impact of an artificial intelligence decision support system, the Anemia Control Model (ACM), on anemia outcomes. Based on patient profiles, the ACM was built to recommend suitable erythropoietic-stimulating agent doses. Our retrospective study consisted of a 12-month control phase (standard anemia care), followed by a 12-month observation phase (ACM-guided care) encompassing 752 patients undergoing hemodialysis therapy in 3 NephroCare clinics located in separate countries. The percentage of hemoglobin values on target, the median darbepoetin dose, and individual hemoglobin fluctuation (estimated from the intrapatient hemoglobin standard deviation) were deemed primary outcomes. In the observation phase, median darbepoetin consumption significantly decreased from 0.63 to 0.46 μg/kg/month, whereas on-target hemoglobin values significantly increased from 70.6% to 76.6%, reaching 83.2% when the ACM suggestions were implemented. Moreover, ACM introduction led to a significant decrease in hemoglobin fluctuation (intrapatient standard deviation decreased from 0.95 g/dl to 0.83 g/dl). Thus, ACM support helped improve anemia outcomes of hemodialysis patients, minimizing erythropoietic-stimulating agent use with the potential to reduce the cost of treatment.",https://doi.org/10.1016/j.kint.2016.03.036,https://www.sciencedirect.com/science/article/pii/S0085253816301326,Kidney International,Carlo Barbieri;Manuel Molina;Pedro Ponce;Monika Tothova;Isabella Cattinelli;Jasmine {Ion Titapiccolo};Flavio Mari;Claudia Amato;Frank Leipold;Wolfgang Wehmeyer;Stefano Stuard;Andrea Stopper;Bernard Canaud,2016,95,"@article{2-7973,
  title = {An international observational study suggests that artificial intelligence for clinical decision support optimizes anemia management in hemodialysis patients},
  author = {Carlo Barbieri and Manuel Molina and Pedro Ponce and Monika Tothova and Isabella Cattinelli and Jasmine {Ion Titapiccolo} and Flavio Mari and Claudia Amato and Frank Leipold and Wolfgang Wehmeyer and Stefano Stuard and Andrea Stopper and Bernard Canaud},
  year = {2016},
  doi = {10.1016/j.kint.2016.03.036},
  journal = {Kidney International}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting, Executing","Decision-subject, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-7977,elsevier,An interpretable decision-support systems for daily cryptocurrency trading,"Cryptocurrencies, especially Bitcoin (BTC), have become an important commodity for both individual and corporate investors within the last decade. The limited supply, high volatility, and random price fluctuations have increased investors' interest in BTC, especially in daily trading. Although BTC has been yielding a high rate of returns, price fluctuations and constant speculations make the investors wary of unexpected price movements. Predictive modeling suffers from the complexity of the datasets (i.e., the high number of features employed to forecast BTC movements) as well as the black-box nature of most machine learning algorithms (which is especially problematic for corporate investors since they are obligated to disclose their investment decisions to their clients). Therefore, the main goal of the current study is to assist individual and corporate investors in making transparent and interpretable daily BTC trading decisions by developing a predictive analytics framework. To address the complexities posed by the datasets, a comprehensive tri-level feature selection approach is proposed. The selected features are then, fed into the Classification & Regression Tree (C&RT) to build a highly parsimonious, transparent, and interpretable prediction model. The resultant model was not only evaluated on the test (holdout) sample but was also tested on challenging time periods, including the first half of 2020 (the start of the pandemic era) to exhibit the viability and reliability of the proposed framework. Finally, a decision support tool is developed for the practical implementation of the model. The tool can be used by short-term investors not only due to its highly simplistic, transparent, and interpretable structure, but also its higher accuracy, sensitivity, and specificity results when compared to the extant literature.",https://doi.org/10.1016/j.eswa.2022.117409,https://www.sciencedirect.com/science/article/pii/S0957417422007515,Expert Systems with Applications,Hamidreza Ahady Dolatsara;Eyyub Kibis;Musa Caglar;Serhat Simsek;Ali Dag;Gelareh Ahadi Dolatsara;Dursun Delen,2022,25,"@article{2-7977,
  title = {An interpretable decision-support systems for daily cryptocurrency trading},
  author = {Hamidreza Ahady Dolatsara and Eyyub Kibis and Musa Caglar and Serhat Simsek and Ali Dag and Gelareh Ahadi Dolatsara and Dursun Delen},
  year = {2022},
  doi = {10.1016/j.eswa.2022.117409},
  journal = {Expert Systems with Applications}
}",System/Artifact contributions,Finance / Business / Economy,"Operational, Individual","Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-804,aaai,On Testing for Discrimination Using Causal Models,"Consider a bank that uses an AI system to decide which loan applications to approve. We want to ensure that the system is fair, that is, it does not discriminate against applicants based on a predefined list of sensitive attributes, such as gender and ethnicity. We expect there to be a regulator whose job it is to certify the bank’s system as fair or unfair. We consider issues that the regulator will have to confront when making such a decision, including the precise definition of fairness, dealing with proxy variables, and dealing with what we call allowed variables, that is, variables such as salary on which the decision is allowed to depend, despite being correlated with sensitive variables. We show( among other things) that the problem of deciding fairness as we have defined it is co-NP-complete, but then argue that, despite that, in practice the problem should be manageable.",10.1609/aaai.v36i5.20494,https://ojs.aaai.org/index.php/AAAI/article/view/20494,AAAI Conference on Artificial Intelligence,Hana Chockler;Joseph Y. Halpern,2022,7,"@inproceedings{2-804,
  title     = {On Testing for Discrimination Using Causal Models},
  author    = {Hana Chockler and Joseph Y. Halpern},
  year      = {2022},
  doi       = {10.1609/aaai.v36i5.20494},
  booktitle = {AAAI Conference on Artificial Intelligence}
}",Theoretical contributions,"Finance / Business / Economy, Law / Policy / Governance",Operational,"Advising, Forecasting, Auditing","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-8067,elsevier,Application of interpretable machine learning models for the intelligent decision,"In this study, an interpretable machine learning algorithm is proposed for the issues of intelligent decision through predicting the firms’ efficiency of innovation. Based on the unbalanced panel data collected in Zhongguancun Science Parks from year 2005 to 2015, the efficiency of over 10,000 firms have been analysed in this study, and the change and growth of these firms have been captured over time. The linear regression, decision tree, random forests, neural network and XGBoost models are applied to figure out the impact factors of innovation. After comparing the results of different models, it has been found that the accuracy of XGBoost for R&D efficiency labelled, commercial efficiency labelled and overall efficiency labelled classification problems are 73.65%, 70.02% and 70.09%, which outperform the other four models. Moreover, the interpretability of XGBoost is also better than other models. Thus, the XGBoost model makes it possible for managers to predict the firm's future innovation performance derived from their innovation strategies in the current stage. Furthermore, it helps firms to build an intelligent decision support system, which is of great importance for them to deal with complex decision environments, and to increase their efficiency of innovation in the long-term dynamic competition with other firms.",https://doi.org/10.1016/j.neucom.2018.12.012,https://www.sciencedirect.com/science/article/pii/S0925231218314668,Neurocomputing,Yawen Li;Liu Yang;Bohan Yang;Ning Wang;Tian Wu,2019,79,"@article{2-8067,
  title={Application of interpretable machine learning models for the intelligent decision},
  author={Li, Yawen and Yang, Liu and Yang, Bohan and Wang, Ning and Wu, Tian},
  year={2019},
  journal={Neurocomputing},
  doi={10.1016/j.neucom.2018.12.012}
}",Empirical contributions,Finance / Business / Economy,Organizational,"Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-8148,elsevier,Artificial intelligence and moral dilemmas: perception of ethical decision-making in ai,"Artificial intelligence (AI) has become deeply integrated into daily life; thus, it is important to examine how people perceive AI as it functions as a decision-maker, especially in situations involving moral dilemmas. Across four studies (N = 804), we found that people perceive AI as more likely to make utilitarian choices than human beings are (Studies1–4). We then measured people's perceptions (both warmth and competence) toward AI and explored their potential contributions to our predicted main effect (Study 2). In addition, our main effect was replicated in impersonal moral dilemma and personal high-conflict moral dilemma situations (Studies 3 and 4). We discuss the implications of these findings on moral dilemma and human–computer interactions.",https://doi.org/10.1016/j.jesp.2022.104327,https://www.sciencedirect.com/science/article/pii/S0022103122000464,Journal of Experimental Social Psychology,Zaixuan Zhang;Zhansheng Chen;Liying Xu,2022,70,"@article{2-8148,
  title = {Artificial intelligence and moral dilemmas: perception of ethical decision-making in AI},
  author = {Zhang, Zaixuan and Chen, Zhansheng and Xu, Liying},
  year = {2022},
  journal = {Journal of Experimental Social Psychology},
  doi = {10.1016/j.jesp.2022.104327}
}",Empirical contributions,Law / Policy / Governance,Institutional,Executing,"Guardian, Stakeholder, Decision-subject",Alter decision outcomes,no such info,reasoning,NA,"Textual, Visual",Yes,Yes
2-8150,elsevier,Artificial intelligence and policy making; can small municipalities enable digital transformation?,"This study investigates digital transformation and the usability of emerging technologies in policymaking. Prior studies categorised digital transformation into three distinct phases of digitisation, digitalisation, and digital transformation. They mainly focus on the operational or functional levels, however, this study considers digital transformation at the strategic level. Previous studies confirmed that using new emerging AI-based technologies will enable organisations to use digital transformation to achieve higher efficiency. A novel methodological AI-based approach for policymaking was constructed into three phases through the lens of organisational learning theory. The proposed framework was validated using a case study in the transportation industry of a small municipality. In the selected case study, a confirmatory model was developed and tested utilising the Structural Equation Modelling with data collected from a survey of 494 local stakeholders. Artificial Neural Network was utilised to predict and then to identify the most appropriate policy according to cost, feasibility, and impact criteria amongst six policies extracted from the literature. The results from this research confirm that utilisation of the AI-based strategic decision-making through the proposed generative AI platform at strategic level outperforms human decision-making in terms of applicability, efficiency, and accuracy.",https://doi.org/10.1016/j.ijpe.2024.109324,https://www.sciencedirect.com/science/article/pii/S0925527324001816,International Journal of Production Economics,Ioannis Koliousis;Abdulrahman Al-Surmi;Mahdi Bashiri,2024,11,"@article{2-8150,
  title = {Artificial intelligence and policy making; can small municipalities enable digital transformation?},
  author = {Koliousis, Ioannis and Al-Surmi, Abdulrahman and Bashiri, Mahdi},
  year = {2024},
  journal = {International Journal of Production Economics},
  doi = {10.1016/j.ijpe.2024.109324}
}","Methodological contributions, Empirical contributions",Law / Policy / Governance,Organizational,"Forecasting, Executing","Decision-subject, Stakeholder, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-8171,elsevier,Artificial intelligence for caries detection: randomized trial,"Objectives: We aimed to assess the impact of an artificial intelligence (AI)-based diagnostic-support software for proximal caries detection on bitewing radiographs. Methods: A cluster-randomized cross-over controlled trial was conducted. A commercially available software employing a fully convolutional neural network for caries detection (dentalXrai Pro, dentalXrai Ltd.) was randomly employed by 22 dentists, supporting their caries detection on 20 bitewings randomly chosen from a pool of 140 bitewings, with 10 bitewings randomly being supported by AI and 10 not. The reference test had been established by 4 + 1 independent experts in a pixelwise fashion. Caries was subgrouped as enamel, early dentin and advanced dentin caries, and accuracy and treatment decisions for each caries lesion assessed. Results: Dentists with AI showed a significantly higher mean (95% CI) area under the Receiver-Operating-Characteristics curve (0.89; 0.87–0.90) than those without AI (0.85; 0.83–0.86; p<0.05), mainly as their sensitivity was significantly higher (0.81; 0.74–0.87 compared with 0.72; 0.64–0.79; p<0.05) while the specificity was not significantly affected (p>0.05). This increase in sensitivity was found for enamel, but not early or advanced dentin lesions. Higher sensitivity came with an increase in non-invasive, but also invasive treatment decisions (p<0.05). Conclusion: AI can increase dentists’ diagnostic accuracy but may also increase invasive treatment decisions. Clinical significance: AI can increase dentists’ diagnostic accuracy, mainly via increasing their sensitivity for detecting enamel lesions, but may also increase invasive therapy decisions. Differences in the effects of AI for different dentists should be explored, and dentists should be guided as to which therapy to choose when detecting caries lesions using AI support.",https://doi.org/10.1016/j.jdent.2021.103849,https://www.sciencedirect.com/science/article/pii/S0300571221002724,Journal of Dentistry,Sarah Mertens;Joachim Krois;Anselmo Garcia Cantu;Lubaina T. Arsiwala;Falk Schwendicke,2021,159,"@article{2-8171,
  title = {Artificial intelligence for caries detection: randomized trial},
  author = {Sarah Mertens and Joachim Krois and Anselmo Garcia Cantu and Lubaina T. Arsiwala and Falk Schwendicke},
  year = {2021},
  journal = {Journal of Dentistry},
  doi = {https://doi.org/10.1016/j.jdent.2021.103849}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Knowledge provider",Alter decision outcomes,no such info,"prediction of alternative, preliminary diagnoses",NA,Visual,Yes,Yes
2-8186,elsevier,Artificial intelligence for the public sector: results of landscaping the use of ai in government across the european union,"Artificial Intelligence is increasingly being used by public sector organisations. Previous research highlighted that the use of AI technologies in government could improve policy making processes, public service delivery and the internal management of public administrations. In this article, we explore to which extent the use of AI in the public sector impacts these core governance functions. Findings from the review of a sample of 250 cases across the European Union, show that AI is used mainly to support improving public service delivery, followed by enhancing internal management and only in a limited number assist directly or indirectly policy decision-making. The analysis suggests that different types of AI technologies and applications are used in different governance functions, highlighting the need to further in-depth investigation to better understand the role and impact of use in what is being defined the governance “of, with and by AI”.",https://doi.org/10.1016/j.giq.2022.101714,https://www.sciencedirect.com/science/article/pii/S0740624X22000478,Government Information Quarterly,Colin {van Noordt};Gianluca Misuraca,2022,351,"@article{2-8186,
  title = {Artificial Intelligence for the Public Sector: Results of Landscaping the Use of AI in Government across the European Union},
  author = {Colin {van Noordt} and Gianluca Misuraca},
  year = {2022},
  doi = {10.1016/j.giq.2022.101714},
  journal = {Government Information Quarterly}
}",Empirical contributions,Law / Policy / Governance,Institutional,"Analyzing, Advising","Decision-maker, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-8227,elsevier,Artificial intelligence vs. autonomous decision-making in streaming platforms: a mixed-method approach,"Although the empowerment of technology is of great value to society, little is known about its downstream effects on consumers' decisions. This research draws on the expectation–confirmation theory and autonomy in artificial intelligence (AI) and investigates how AI (vs. autonomous choice) has detrimental effects on consumer outcomes, creating an autonomy-technology tension — i.e., the conflict arising from AI technology diminishing consumers' autonomy in their choices. Four studies using a mixed-method approach reveal that the use of AI recommendations in streaming platforms creates an autonomy-technology tension that reduces consumers' performance expectancy, thus lowering their satisfaction. However, such effects are contingent on the nature of the AI recommendations. While a mismatch between AI recommendations and consumer preferences might backfire, AI's negative effect is mitigated when choices match consumers' preferences. We make significant theoretical and practical contributions to empirical research on consumers' sense of autonomy while interacting with AI.",https://doi.org/10.1016/j.ijinfomgt.2023.102748,https://www.sciencedirect.com/science/article/pii/S0268401223001299,International Journal of Information Management,Ana Rita Gonçalves;Diego Costa Pinto;Saleh Shuqair;Marlon Dalmoro;Anna S. Mattila,2024,45,"@article{2-8227,
  title={Artificial intelligence vs. autonomous decision-making in streaming platforms: a mixed-method approach},
  author={Gonçalves, Ana Rita and Pinto, Diego Costa and Shuqair, Saleh and Dalmoro, Marlon and Mattila, Anna S.},
  year={2024},
  journal={International Journal of Information Management},
  doi={10.1016/j.ijinfomgt.2023.102748}
}",Methodological contributions,Media / Communication / Entertainment,Individual,"Advising, Executing","Decision-subject, Guardian, Decision-maker","Alter decision outcomes, Change trust, Change cognitive demands, Change affective-perceptual, Restrict human agency",no such info,recommendations,autonomy-technology tension,Interactive interface,Yes,Yes
2-8236,elsevier,Artificial intelligence-based public healthcare systems: g2g knowledge-based exchange to enhance the decision-making process,"With the rapid evolution of data over the last few years, many new technologies have arisen with artificial intelligent (AI) technologies at the top. Artificial intelligence (AI), with its infinite power, holds the potential to transform patient healthcare. Given the gaps revealed by the 2020 COVID-19 pandemic in healthcare systems, this research investigates the effects of using an artificial intelligence-driven public healthcare framework to enhance the decision-making process using an extended model of Shaft and Vessey (2006) cognitive fit model in healthcare organizations in Saudi Arabia. The model was validated based on empirical data collected using an online questionnaire distributed to healthcare organizations in Saudi Arabia. The main sample participants were healthcare CEOs, senior managers/managers, doctors, nurses, and other relevant healthcare practitioners under the MoH involved in the decision-making process relating to COVID-19. The measurement model was validated using SEM analyses. Empirical results largely supported the conceptual model proposed as all research hypotheses are significantly approved. This study makes several theoretical contributions. For example, it expands the theoretical horizon of Shaft and Vessey's (2006) CFT by considering new mechanisms, such as the inclusion of G2G Knowledge-based Exchange in addition to the moderation effect of Experience-based decision-making (EDBM) for enhancing the decision-making process related to the COVID-19 pandemic. More discussion regarding research limitations and future research directions are provided as well at the end of this study.",https://doi.org/10.1016/j.giq.2021.101618,https://www.sciencedirect.com/science/article/pii/S0740624X2100054X,Government Information Quarterly,Omar A. Nasseef;Abdullah M. Baabdullah;Ali Abdallah Alalwan;Banita Lal;Yogesh K. Dwivedi,2022,88,"@article{2-8236,
  title = {Artificial Intelligence-Based Public Healthcare Systems: G2G Knowledge-Based Exchange to Enhance the Decision-Making Process},
  author = {Nasseef, Omar A. and Baabdullah, Abdullah M. and Alalwan, Ali Abdallah and Lal, Banita and Dwivedi, Yogesh K.},
  year = {2022},
  journal = {Government Information Quarterly},
  doi = {10.1016/j.giq.2021.101618}
}",Empirical contributions,Healthcare / Medicine / Surgery,Organizational,Advising,"Decision-maker, Stakeholder, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-8244,elsevier,Artificial intelligence–based clinical decision support systems in geriatrics: an ethical analysis,"Objectives To provide an ethical analysis of the implications of the usage of artificial intelligence–supported clinical decision support systems (AI-CDSS) in geriatrics. Design Ethical analysis based on the normative arguments regarding the use of AI-CDSS in geriatrics using a principle-based ethical framework. Setting and Participants Normative arguments identified in 29 articles on AI-CDSS in geriatrics. Methods Our analysis is based on a literature search that was done to determine ethical arguments that are currently discussed regarding AI-CDSS. The relevant articles were subjected to a detailed qualitative analysis regarding the ethical considerations Supplementary Datamentioned therein. We then discussed the identified arguments within the frame of the 4 principles of medical ethics according to Beauchamp and Childress and with respect to the needs of frail older adults. Results We found a total of 5089 articles; 29 articles met the inclusion criteria and were subsequently subjected to a detailed qualitative analysis. We could not identify any systematic analysis of the ethical implications of AI-CDSS in geriatrics. The ethical considerations are very unsystematic and scattered, and the existing literature has a predominantly technical focus emphasizing the technology's utility. In an extensive ethical analysis, we systematically discuss the ethical implications of the usage of AI-CDSS in geriatrics. Conclusions and Implications AI-CDSS in geriatrics can be a great asset, especially when dealing with patients with cognitive disorders; however, from an ethical perspective, we see the need for further research. By using AI-CDSS, older patients’ values and beliefs might be overlooked, and the quality of the doctor-patient relationship might be altered, endangering compliance to the 4 ethical principles of Beauchamp and Childress.",https://doi.org/10.1016/j.jamda.2023.06.008,https://www.sciencedirect.com/science/article/pii/S1525861023005558,Journal of the American Medical Directors Association,Tobias Skuban-Eiseler;Marcin Orzechowski;Michael Denkinger;Thomas Derya Kocar;Christoph Leinert;Florian Steger,2023,24,"@article{2-8244,
  title = {Artificial intelligence--based clinical decision support systems in geriatrics: an ethical analysis},
  author = {Skuban-Eiseler, Tobias and Orzechowski, Marcin and Denkinger, Michael and Kocar, Thomas Derya and Leinert, Christoph and Steger, Florian},
  year = {2023},
  journal = {Journal of the American Medical Directors Association},
  doi = {10.1016/j.jamda.2023.06.008}
}",Theoretical contributions,Healthcare / Medicine / Surgery,Operational,Advising,"Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-8263,elsevier,Askmusic: leveraging a clinical registry to develop a new machine learning model to inform patients of prostate cancer treatments chosen by similar men,"Background Clinical registries provide physicians with a means for making data-driven decisions but few opportunities exist for patients to interact with registry data to help make decisions. Objective We sought to develop a web-based system that uses a prostate cancer (CaP) registry to provide newly diagnosed men with a platform to view predicted treatment decisions based on patients with similar characteristics. Design, setting, and participants The Michigan Urological Surgery Improvement Collaborative (MUSIC) is a quality improvement consortium of urology practices that maintains a prospective registry of men with CaP. We used registry data from 45 MUSIC urology practices from 2015 to 2017 to develop and validate a random forest machine learning model. After fitting the random forest model to a derivation cohort consisting of a random two-thirds sample of patients after stratifying by practice location, we evaluated the model performance in a validation cohort consisting of the remaining one-third of patients using a multiclass area under the curve (AUC) measure and calibration plots. Results and limitations We identified 7543 men diagnosed with CaP, of whom 45% underwent radical prostatectomy, 30% surveillance, 17% radiation therapy, 5.6% androgen deprivation, and 1.8% watchful waiting. The personalized prediction for patients in the validation cohort was highly accurate (AUC 0.81). Conclusions Using clinical registry data and machine learning methods, we created a web-based platform for patients that generates accurate predictions for most CaP treatments. Patient summary We have developed and tested a tool to help men newly diagnosed with prostate cancer to view predicted treatment decisions based on similar patients from our registry. We have made this tool available online for patients to use.",https://doi.org/10.1016/j.eururo.2018.09.050,https://www.sciencedirect.com/science/article/pii/S0302283818307401,European Urology,Gregory B. Auffenberg;Khurshid R. Ghani;Shreyas Ramani;Etiowo Usoro;Brian Denton;Craig Rogers;Benjamin Stockton;David C. Miller;Karandeep Singh,2019,47,"@article{2-8263,
  title={Askmusic: leveraging a clinical registry to develop a new machine learning model to inform patients of prostate cancer treatments chosen by similar men},
  author={Auffenberg, Gregory B. and Ghani, Khurshid R. and Ramani, Shreyas and Usoro, Etiowo and Denton, Brian and Rogers, Craig and Stockton, Benjamin and Miller, David C. and Singh, Karandeep},
  year={2019},
  journal={European Urology},
  doi={10.1016/j.eururo.2018.09.050}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Analyzing","Decision-subject, Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-8281,elsevier,"Assessing the communication gap between ai models and healthcare professionals: explainability, utility and trust in ai-driven clinical decision-making","This paper contributes with a pragmatic evaluation framework for explainable Machine Learning (ML) models for clinical decision support. The study revealed a more nuanced role for ML explanation models, when these are pragmatically embedded in the clinical context. Despite the general positive attitude of healthcare professionals (HCPs) towards explanations as a safety and trust mechanism, for a significant set of participants there were negative effects associated with confirmation bias, accentuating model over-reliance and increased effort to interact with the model. Also, contradicting one of its main intended functions, standard explanatory models showed limited ability to support a critical understanding of the limitations of the model. However, we found new significant positive effects which repositions the role of explanations within a clinical context: these include reduction of automation bias, addressing ambiguous clinical cases (cases where HCPs were not certain about their decision) and support of less experienced HCPs in the acquisition of new domain knowledge.",https://doi.org/10.1016/j.artint.2022.103839,https://www.sciencedirect.com/science/article/pii/S0004370222001795,Artificial Intelligence,Oskar Wysocki;Jessica Katharine Davies;Markel Vigo;Anne Caroline Armstrong;Dónal Landers;Rebecca Lee;André Freitas,2023,157,"@article{2-8281,
  title={Assessing the communication gap between AI models and healthcare professionals: explainability, utility and trust in AI-driven clinical decision-making},
  author={Wysocki, Oskar and Davies, Jessica Katharine and Vigo, Markel and Armstrong, Anne Caroline and Landers, D{\'o}nal and Lee, Rebecca and Freitas, Andr{\'e}},
  year={2023},
  doi={10.1016/j.artint.2022.103839},
  journal={Artificial Intelligence}
}",Methodological contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Advising","Decision-maker, Knowledge provider, Decision-subject","Alter decision outcomes, Change affective-perceptual, Change cognitive demands, Shape ethical norms, Change trust",no such info,"recommendations, visual explanations",NA,Textual,Yes,Yes
2-8347,elsevier,"Augmenting organizational decision-making with deep learning algorithms: principles, promises, and challenges","The current expansion of theory and research on artificial intelligence in management and organization studies has revitalized the theory and research on decision-making in organizations. In particular, recent advances in deep learning (DL) algorithms promise benefits for decision-making within organizations, such as assisting employees with information processing, thereby augment their analytical capabilities and perhaps help their transition to more creative work. We conceptualize the decision-making process in organizations augmented with DL algorithm outcomes (such as predictions or robust patterns from unstructured data) as deep learning–augmented decision-making (DLADM). We contribute to the understanding and application of DL for decision-making in organizations by (a) providing an accessible tutorial on DL algorithms and (b) illustrating DLADM with two case studies drawing on image recognition and sentiment analysis tasks performed on datasets from Zalando, a European e-commerce firm, and Rotten Tomatoes, a review aggregation website for movies, respectively. Finally, promises and challenges of DLADM as well as recommendations for managers in attending to these challenges are also discussed.",https://doi.org/10.1016/j.jbusres.2020.09.068,https://www.sciencedirect.com/science/article/pii/S0148296320306512,Journal of Business Research,Yash Raj Shrestha;Vaibhav Krishna;Georg {von Krogh},2021,0,"@article{2-8347,
  title = {Augmenting organizational decision-making with deep learning algorithms: principles, promises, and challenges},
  author = {Shrestha, Yash Raj and Krishna, Vaibhav and von Krogh, Georg},
  year = {2021},
  doi = {https://doi.org/10.1016/j.jbusres.2020.09.068},
  journal = {Journal of Business Research}
}","Theoretical contributions, Methodological contributions","Everyday / Employment / Public Service, Finance / Business / Economy",no such info,"Analyzing, Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-8420,elsevier,Automation bias in public administration – an interdisciplinary perspective from law and psychology,"The objective of this paper is to break down the widely presumed dichotomy, especially in law, between fully automated decisions and human decisions from a psychological and normative perspective. This is particularly relevant as human oversight is seen as an effective means of quality control, including in the current AI Act. The phenomenon of automation bias argues against this assumption. We have investigated this phenomenon of automation bias, as a behavioral effect of and its implications in normative institutional decision-making situations. The phenomenon of automation bias, whereby individuals overly rely on machine-generated decisions or proposals, has far-reaching implications. Excessive reliance may result in a failure to meaningfully engage with the decision at hand, resulting in an inability to detect automation failures, and an overall deterioration in decision quality, potentially up to a net-negative impact of the decision support system. As legal systems emphasize the role of human decisions in ensuring fairness and quality, this paper critically examines the inadequacies of current EU and national legal frameworks in addressing the risks of automation bias. Contributing a novel perspective, this article integrates psychological, technical, and normative elements to analyze automation bias and its legal implications. Anchoring human decisions within legal principles, it navigates the intersections between AI and human-machine interactions from a normative point of view. An exploration of the AI Act sheds light on potential avenues for improvement. In conclusion, our paper proposes four steps aimed at effectively countering the potential perils posed by automation bias. By linking psychological insights, legal analysis, and technical implications, this paper advocates a holistic approach to evolving legal frameworks in an increasingly automated world.",https://doi.org/10.1016/j.giq.2024.101953,https://www.sciencedirect.com/science/article/pii/S0740624X24000455,Government Information Quarterly,Hannah Ruschemeier;Lukas J. Hondrich,2024,2,"@article{2-8420,
  title = {Automation bias in public administration – an interdisciplinary perspective from law and psychology},
  author = {Hannah Ruschemeier and Lukas J. Hondrich},
  year = {2024},
  journal = {Government Information Quarterly},
  doi = {10.1016/j.giq.2024.101953}
}",Theoretical contributions,Law / Policy / Governance,Institutional,"Executing, Advising","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-8441,elsevier,Avoidable biopsies? Validating artificial intelligence–based decision support software in indeterminate thyroid nodules,"Background Multiple artificial intelligence (AI) systems have been approved to risk-stratify thyroid nodules through sonographic characterization. We sought to validate the ability of one such AI system, Koios DS (Koios Medical, Chicago, IL), to aid in improving risk stratification of indeterminate thyroid nodules. Methods A retrospective single-institution dataset was compiled of 28 cytologically indeterminate thyroid nodules having undergone molecular testing and surgical resection, with surgical pathology categorized as malignant or benign. Nodules were retrospectively evaluated with Koios DS. After nodule selection, automated and AI-adapter–derived Thyroid Imaging Reporting and Data System (TI-RADS) levels were recorded, and agreement with radiologist-derived levels was assessed using Cohen’s κ statistic. The performance of malignancy classification was compared between the radiologist and AI-adapter. Biopsy thresholds were re-evaluated using the AI-adapter. Results In this cohort, 7 (25%) nodules were malignant on surgical pathology. The median nodule size was 2.4 cm (interquartile range: 1.8–2.9 cm). Median radiologist and automated TI-RADS levels were both 4, with κ 0.25 (“fair agreement”). Malignancy classification by the radiologist provided sensitivity 100%, specificity 33.3%, positive predictive value (PPV) 33.3%, and negative predictive value (NPV) 100%, compared with the AI-adapter’s performance with sensitivity 85.7%, specificity 76.2%, PPV 54.5%, and NPV 94.1%. Using the AI-adapter, 14 of 28 biopsies would have been deferred, 13 of which were surgically benign. Conclusion Koios automated and radiologist-derived TI-RADS levels were in consistent agreement for indeterminate thyroid nodules. Malignancy reclassification with the AI-adapter improved PPV at minimal cost to NPV. Risk stratification with the addition of the AI-adapter may allow for more accurate patient counseling and the avoidance of biopsies in select cases that would otherwise be cytologically indeterminate.",https://doi.org/10.1016/j.surg.2024.07.074,https://www.sciencedirect.com/science/article/pii/S0039606024006974,Surgery,Christopher J. Carnabatu;David T. Fetzer;Alexander Tessnow;Shelby Holt;Vivek R. Sant,2024,2,"@article{2-8441,
  title={Avoidable biopsies? Validating artificial intelligence--based decision support software in indeterminate thyroid nodules},
  author={Carnabatu, Christopher J. and Fetzer, David T. and Tessnow, Alexander and Holt, Shelby and Sant, Vivek R.},
  year={2024},
  journal={Surgery},
  doi={10.1016/j.surg.2024.07.074}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting, Analyzing","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-846,aaai,Optimal Kidney Exchange with Immunosuppressants,"Algorithms for exchange of kidneys is one of the key successful applications in market design, artificial intelligence, and operations research. Potent immunosuppressant drugs suppress the bodys ability to reject a transplanted organ up to the point that a transplant across bloodor tissue-type incompatibility becomes possible. In contrast to the standard kidney exchange problem, we consider a setting that also involves the decision about which recipients receive from the limited supply of immunosuppressants that make them compatible with originally incompatible kidneys. We firstly present a general computational framework to model this problem. Our main contribution is a range of efficient algorithms that provide flexibility in terms of meeting meaningful objectives. Motivated by the current reality of kidney exchanges using sophisticated mathematical-programming-based clearing algorithms, we then present a general but scalable approach to optimal clearing with immunosuppression; we validate our approach on realistic data from a large fielded exchange.",10.1609/aaai.v35i1.16073,https://ojs.aaai.org/index.php/AAAI/article/view/16073,AAAI Conference on Artificial Intelligence,Haris Aziz;Ágnes Cseh;John P. Dickerson;Duncan C. McElfresh,2021,0,"@inproceedings{2-846,
  title={Optimal Kidney Exchange with Immunosuppressants},
  author={Aziz, Haris and Cseh, Ágnes and Dickerson, John P. and McElfresh, Duncan C.},
  year={2021},
  doi={10.1609/aaai.v35i1.16073},
  booktitle={AAAI Conference on Artificial Intelligence}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Organizational,"Advising, Forecasting, Analyzing","Decision-maker, Decision-subject, Guardian",NA,NA,NA,NA,NA,Yes,No
2-8474,elsevier,Behavioral nudges as patient decision support for medication adherence: the encourage randomized controlled trial,"Background Medication adherence is generally low and challenging to address because patient actions control healthcare delivery outside of medical environments. Behavioral nudging changes clinician behavior, but nudging patient decision-making requires further testing. This trial evaluated whether behavioral nudges can increase statin adherence, measured as the proportion of days covered (PDC). Methods In a 12-month parallel-group, unblinded, randomized controlled trial, adult patients in Intermountain Healthcare cardiology clinics were enrolled. Inclusion required an indication for statins and membership in SelectHealth insurance. Subjects were randomized 1:1 to control or nudges. Nudge content, timing, frequency, and delivery route were personalized by CareCentra using machine learning of subject motivations and abilities from psychographic assessment, demographics, social determinants, and the Intermountain Mortality Risk Score. PDC calculation used SelectHealth claims data. Results Among 182 subjects, age averaged 63.2±8.5 years, 25.8% were female, baseline LDL-C was 82.5±32.7 mg/dL, and 93.4% had coronary disease. Characteristics were balanced between nudge (n = 89) and control arms (n = 93). The statin PDC was greater at 12 months in the nudge group (PDC: 0.742±0.318) compared to controls (PDC: 0.639±0.358, P = 0.042). Adherent subjects (PDC ≥80%) were more concentrated in the nudge group (66.3% vs controls: 50.5%, P = 0.036) while a composite of death, myocardial infarction, stroke, and revascularization was non-significant (nudges: 6.7% vs control: 10.8%, P = 0.44). Conclusions Persuasive behavioral nudges driven by artificial intelligence resulted in a clinically important increase in statin adherence in general cardiology patients. This precision patient decision support utilized computerized nudge design and delivery with minimal on-going human input.",https://doi.org/10.1016/j.ahj.2021.11.001,https://www.sciencedirect.com/science/article/pii/S0002870321004440,American Heart Journal,Benjamin D. Horne;Joseph B. Muhlestein;Donald L. Lappé;Heidi T. May;Viet T. Le;Tami L. Bair;Daniel Babcock;Daniel Bride;Kirk U. Knowlton;Jeffrey L. Anderson,2022,54,"@article{2-8474,
  title={Behavioral nudges as patient decision support for medication adherence: the encourage randomized controlled trial},
  author={Horne, Benjamin D. and Muhlestein, Joseph B. and Lapp{\'e}, Donald L. and May, Heidi T. and Le, Viet T. and Bair, Tami L. and Babcock, Daniel and Bride, Daniel and Knowlton, Kirk U. and Anderson, Jeffrey L.},
  year={2022},
  journal={American Heart Journal},
  doi={10.1016/j.ahj.2021.11.001}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Decision-maker, Decision-subject",Alter decision outcomes,"Change AI responses, Update AI competence",behavioral nudges,NA,NA,Yes,Yes
2-8506,elsevier,Binarized multi-gate mixture of bayesian experts for cardiac syndrome x diagnosis: a clinician-in-the-loop scenario with a belief-uncertainty fusion paradigm,"Cardiac Syndrome X (CSX) is a very dangerous cardiovascular disease characterized by angina-like chest discomfort and pain on effort despite normal epicardial coronary arteries at angiography. In this study, we used a CSX dataset from the coronary angiography registry of Tehran’s Heart Center at Tehran University of Medical Sciences in Iran to develop several machine learning (ML) methods combined with uncertainty quantification of the obtained results. Uncertainty quantification plays a significant role in both traditional machine learning (ML) and deep learning (DL) studies allowing researchers to create trustable clinical detection systems. We propose a novel Mixture-of-Experts (MoE) model, called Binarized Multi-Gate Mixture of Bayesian Experts (MoBE), which is an effective ensemble technique for accurately classifying CSX data. The proposed binarized multi-gate model relies on a double quantified uncertainty strategy at the feature selection and decision making stages. First, we use a clinician-in-the-loop scenario with a belief-uncertainty paradigm at the feature selection stage. Second, we use Bayesian neural networks (BNNs) as experts in MoBE and Monte Carlo (MC) dropout for gates at the decision making uncertainty quantification stage. The proposed binarized multi-gate model reaches an accuracy of 85% when applied to our benchmark CSX dataset from Tehran’s Heart Center.",https://doi.org/10.1016/j.inffus.2023.101813,https://www.sciencedirect.com/science/article/pii/S1566253523001227,Information Fusion,Moloud Abdar;Arash Mehrzadi;Milad Goudarzi;Farzad Masoudkabir;Leonardo Rundo;Mohammad Mamouei;Evis Sala;Abbas Khosravi;Vladimir Makarenkov;U. Rajendra Acharya;Seyedmohammad Saadatagah;Mohammadreza Naderian;Salvador García;Nizal Sarrafzadegan;Saeid Nahavandi,2023,10,"@article{2-8506,
  title = {Binarized Multi-Gate Mixture of Bayesian Experts for Cardiac Syndrome X Diagnosis: A Clinician-in-the-Loop Scenario with a Belief-Uncertainty Fusion Paradigm},
  author = {Moloud Abdar and Arash Mehrzadi and Milad Goudarzi and Farzad Masoudkabir and Leonardo Rundo and Mohammad Mamouei and Evis Sala and Abbas Khosravi and Vladimir Makarenkov and U. Rajendra Acharya and Seyedmohammad Saadatagah and Mohammadreza Naderian and Salvador García and Nizal Sarrafzadegan and Saeid Nahavandi},
  year = {2023},
  doi = {10.1016/j.inffus.2023.101813},
  journal = {Information Fusion}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-851,aaai,Optimal Sequential Drilling for Hydrocarbon Field Development Planning,"We present a novel approach for planning the development of hydrocarbon fields, taking into account the sequential nature of well drilling decisions and the possibility to react to future information. In a dynamic fashion, we want to optimally decide where to drill each well conditional on every possible piece of information that could be obtained from previous wells. We formulate this sequential drilling optimization problem as a POMDP, and propose an algorithm to search for an optimal drilling policy. We show that our new approach leads to better results compared to the current standard in the oil and gas( O&G) industry.",10.1609/aaai.v31i2.19103,https://ojs.aaai.org/index.php/AAAI/article/view/19103,AAAI Conference on Artificial Intelligence,Ruben Rodriguez Torrado;Jesus Rios;Gerald Tesauro,2017,20,"@inproceedings{2-851,
  title     = {Optimal Sequential Drilling for Hydrocarbon Field Development Planning},
  author    = {Ruben Rodriguez Torrado and Jesus Rios and Gerald Tesauro},
  year      = {2017},
  doi       = {10.1609/aaai.v31i2.19103},
  booktitle = {AAAI Conference on Artificial Intelligence}
}",Algorithmic contributions,Environment / Resources / Energy,Organizational,"Analyzing, Advising, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-8555,elsevier,Breaking the vicious circle: a case study on why ai for software analytics and business intelligence does not take off in practice,"In recent years, the application of artificial intelligence (AI) has become an integral part of a wide range of areas, including software engineering. By analyzing various data sources generated in software engineering, it can provide valuable insights into customer behavior, product performance, bugs and errors, and many more. In practice, however, AI for software analytics and business intelligence often remains at a prototypical stage, and the results are rarely used to make decisions based on data. To understand the underlying causes of this phenomenon, we conduct an explanatory case study consisting of and interview study and a survey on the challenges of realizing and utilizing artificial intelligence in the context of software-intensive businesses. As a result, we identify a vicious circle that prevents practitioners from moving from prototypical AI-based analytics to continuous and productively usable software analytics and business intelligence solutions. In order to break the vicious circle in a targeted manner, we identify a set of solutions based on existing literature as well as the previously conducted interviews and survey. Finally, these solutions are validated by a focus group of experts.",https://doi.org/10.1016/j.jss.2021.111135,https://www.sciencedirect.com/science/article/pii/S0164121221002326,Journal of Systems and Software,Iris Figalist;Christoph Elsner;Jan Bosch;Helena Holmström Olsson,2022,26,"@article{2-8555,
  title={Breaking the vicious circle: a case study on why AI for software analytics and business intelligence does not take off in practice},
  author={Figalist, Iris and Elsner, Christoph and Bosch, Jan and Holmström Olsson, Helena},
  year={2022},
  journal={Journal of Systems and Software},
  doi={10.1016/j.jss.2021.111135},
  note={\corpusFull}
}",Empirical contributions,"Finance / Business / Economy, Software / Systems / Security",Operational,Analyzing,"Decision-maker, Developer","Change affective-perceptual, Change trust, Change cognitive demands","Update AI competence, Change AI responses",NA,NA,NA,Yes,Yes
2-8565,elsevier,Bridging the gap: towards an expanded toolkit for ai-driven decision-making in the public sector,"AI-driven decision-making systems are becoming instrumental in the public sector, with applications spanning areas like criminal justice, social welfare, financial fraud detection, and public health. While these systems offer great potential benefits to institutional decision-making processes, such as improved efficiency and reliability, these systems face the challenge of aligning machine learning (ML) models with the complex realities of public sector decision-making. In this paper, we examine five key challenges where misalignment can occur, including distribution shifts, label bias, the influence of past decision-making on the data side, as well as competing objectives and human-in-the-loop on the model output side. Our findings suggest that standard ML methods often rely on assumptions that do not fully account for these complexities, potentially leading to unreliable and harmful predictions. To address this, we propose a shift in modeling efforts from focusing solely on predictive accuracy to improving decision-making outcomes. We offer guidance for selecting appropriate modeling frameworks, including counterfactual prediction and policy learning, by considering how the model estimand connects to the decision-maker's utility. Additionally, we outline technical methods that address specific challenges within each modeling approach. Finally, we argue for the importance of external input from domain experts and stakeholders to ensure that model assumptions and design choices align with real-world policy objectives, taking a step towards harmonizing AI and public sector objectives.",https://doi.org/10.1016/j.giq.2024.101976,https://www.sciencedirect.com/science/article/pii/S0740624X24000686,Government Information Quarterly,Unai Fischer-Abaigar;Christoph Kern;Noam Barda;Frauke Kreuter,2024,16,"@article{2-8565,
  title = {Bridging the Gap: Towards an Expanded Toolkit for AI-Driven Decision-Making in the Public Sector},
  author = {Unai Fischer-Abaigar and Christoph Kern and Noam Barda and Frauke Kreuter},
  year = {2024},
  journal = {Government Information Quarterly},
  doi = {10.1016/j.giq.2024.101976}
}",Theoretical contributions,Law / Policy / Governance,Institutional,"Forecasting, Advising","Decision-maker, Guardian, Knowledge provider",NA,NA,"counterfactual predictions, policy learning",NA,NA,Yes,No
2-8694,elsevier,Churn and net promoter score forecasting for business decision-making through a new stepwise regression methodology,"Companies typically have to make relevant decisions regarding their clients’ fidelity and retention on the basis of analytical models developed to predict both their churn probability and Net Promoter Score (NPS). Although the predictive capability of these models is important, interpretability is a crucial factor to look for as well, because the decisions to be made from their results have to be properly justified. In this paper, a novel methodology to develop analytical models balancing predictive performance and interpretability is proposed, with the aim of enabling a better decision-making. It proceeds by fitting logistic regression models through a modified stepwise variable selection procedure, which automatically selects input variables while keeping their business logic, previously validated by an expert. In synergy with this procedure, a new method for transforming independent variables in order to better deal with ordinal targets and avoiding some logistic regression issues with outliers and missing data is also proposed. The combination of these two proposals with some competitive machine-learning methods earned the leading position in the NPS forecasting task of an international university talent challenge posed by a well-known global bank. The application of the proposed methodology and the results it obtained at this challenge are described as a case-study.",https://doi.org/10.1016/j.knosys.2020.105762,https://www.sciencedirect.com/science/article/pii/S0950705120301684,Knowledge-Based Systems,D. Vélez;A. Ayuso;C. Perales-González;J. Tinguaro Rodríguez,2020,20,"@article{2-8694,
  title = {Churn and net promoter score forecasting for business decision-making through a new stepwise regression methodology},
  author = {Vélez, D. and Ayuso, A. and Perales-González, C. and Rodríguez, J. Tinguaro},
  year = {2020},
  journal = {Knowledge-Based Systems},
  doi = {https://doi.org/10.1016/j.knosys.2020.105762}
}",Methodological contributions,Finance / Business / Economy,Operational,"Forecasting, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-8705,elsevier,Citizens' acceptance of artificial intelligence in public services: evidence from a conjoint experiment about processing permit applications,"Citizens' acceptance of artificial intelligence (AI) in public service delivery is important for its legitimate and effective use by government. Human involvement in AI systems has been suggested as a way to boost citizens' acceptance and perceptions of these systems' fairness. However, there is little empirical evidence to assess these claims. To address this gap, we conducted a pre-registered conjoint experiment in the UK regarding acceptance of AI in processing public permits: for immigration visas and parking permits. We hypothesise that greater human involvement boosts acceptance of AI in decision-making and associated perceptions of its fairness. We further hypothesise that greater human involvement mitigates the negative impact of certain AI features, such as inaccuracy, high cost, or data sharing. From our study, we find that more human involvement tends to increase acceptance, and that perceptions of fairness were less influenced. Yet, when substantial human discretion was introduced in parking permit scenarios, respondents preferred more limited human input. We found little evidence that human involvement moderates the impact of AI's unfavourable attributes. System-level factors such as high accuracy, the presence of an appeals system, increased transparency, reduced cost, non-sharing of data, and the absence of private company involvement all boost both acceptance and perceived procedural fairness. We find limited evidence that individual characteristics affect these results. The findings show how the design of AI systems can increase its acceptability to citizens for use in public services.",https://doi.org/10.1016/j.giq.2023.101876,https://www.sciencedirect.com/science/article/pii/S0740624X2300076X,Government Information Quarterly,Laszlo Horvath;Oliver James;Susan Banducci;Ana Beduschi,2023,50,"@article{2-8705,
  title = {Citizens' acceptance of artificial intelligence in public services: evidence from a conjoint experiment about processing permit applications},
  author = {Laszlo Horvath and Oliver James and Susan Banducci and Ana Beduschi},
  year = {2023},
  doi = {10.1016/j.giq.2023.101876},
  journal = {Government Information Quarterly}
}",Empirical contributions,Law / Policy / Governance,Organizational,"Advising, Executing","Decision-maker, Decision-subject","Shape ethical norms, Alter decision outcomes",no such info,"system accuracy, procedural fairness",NA,Textual,Yes,Yes
2-8707,elsevier,Clarus: an interactive explainable ai platform for manual counterfactuals in graph neural networks,"Background: Lack of trust in artificial intelligence (AI) models in medicine is still the key blockage for the use of AI in clinical decision support systems (CDSS). Although AI models are already performing excellently in systems medicine, their black-box nature entails that patient-specific decisions are incomprehensible for the physician. Explainable AI (XAI) algorithms aim to “explain” to a human domain expert, which input features influenced a specific recommendation. However, in the clinical domain, these explanations must lead to some degree of causal understanding by a clinician. Results: We developed the CLARUS platform, aiming to promote human understanding of graph neural network (GNN) predictions. CLARUS enables the visualisation of patient-specific networks, as well as, relevance values for genes and interactions, computed by XAI methods, such as GNNExplainer. This enables domain experts to gain deeper insights into the network and more importantly, the expert can interactively alter the patient-specific network based on the acquired understanding and initiate re-prediction or retraining. This interactivity allows us to ask manual counterfactual questions and analyse the effects on the GNN prediction. Conclusion: We present the first interactive XAI platform prototype, CLARUS, that allows not only the evaluation of specific human counterfactual questions based on user-defined alterations of patient networks and a re-prediction of the clinical outcome but also a retraining of the entire GNN after changing the underlying graph structures. The platform is currently hosted by the GWDG on https://rshiny.gwdg.de/apps/clarus/.",https://doi.org/10.1016/j.jbi.2024.104600,https://www.sciencedirect.com/science/article/pii/S1532046424000182,Journal of Biomedical Informatics,Jacqueline Michelle Metsch;Anna Saranti;Alessa Angerschmid;Bastian Pfeifer;Vanessa Klemt;Andreas Holzinger;Anne-Christin Hauschild,2024,2,"@article{2-8707,
  title = {Clarus: an interactive explainable AI platform for manual counterfactuals in graph neural networks},
  author = {Jacqueline Michelle Metsch and Anna Saranti and Alessa Angerschmid and Bastian Pfeifer and Vanessa Klemt and Andreas Holzinger and Anne-Christin Hauschild},
  year = {2024},
  journal = {Journal of Biomedical Informatics},
  doi = {https://doi.org/10.1016/j.jbi.2024.104600}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Explaining, Forecasting","Knowledge provider, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-8744,elsevier,Clinical decision support of radiotherapy treatment planning: a data-driven machine learning strategy for patient-specific dosimetric decision making,"Background and purpose Clinical decision support systems are a growing class of tools with the potential to impact healthcare. This study investigates the construction of a decision support system through which clinicians can efficiently identify which previously approved historical treatment plans are achievable for a new patient to aid in selection of therapy. Material and methods Treatment data were collected for early-stage lung and postoperative oropharyngeal cancers treated using photon (lung and head and neck) and proton (head and neck) radiotherapy. Machine-learning classifiers were constructed using patient-specific feature-sets and a library of historical plans. Model accuracy was analyzed using learning curves, and historical treatment plan matching was investigated. Results Learning curves demonstrate that for these datasets, approximately 45, 60, and 30 patients are needed for a sufficiently accurate classification model for radiotherapy for early-stage lung, postoperative oropharyngeal photon, and postoperative oropharyngeal proton, respectively. The resulting classification model provides a database of previously approved treatment plans that are achievable for a new patient. An exemplary case, highlighting tradeoffs between the heart and chest wall dose while holding target dose constant in two historical plans is provided. Conclusions We report on the first artificial-intelligence based clinical decision support system that connects patients to past discrete treatment plans in radiation oncology and demonstrate for the first time how this tool can enable clinicians to use past decisions to help inform current assessments. Clinicians can be informed of dose tradeoffs between critical structures early in the treatment process, enabling more time spent on finding the optimal course of treatment for individual patients.",https://doi.org/10.1016/j.radonc.2017.10.014,https://www.sciencedirect.com/science/article/pii/S0167814017326543,Radiotherapy and Oncology,Gilmer Valdes;Charles B. Simone;Josephine Chen;Alexander Lin;Sue S. Yom;Adam J. Pattison;Colin M. Carpenter;Timothy D. Solberg,2017,124,"@article{2-8744,
  title = {Clinical decision support of radiotherapy treatment planning: a data-driven machine learning strategy for patient-specific dosimetric decision making},
  author = {Gilmer Valdes and Charles B. Simone and Josephine Chen and Alexander Lin and Sue S. Yom and Adam J. Pattison and Colin M. Carpenter and Timothy D. Solberg},
  year = {2017},
  doi = {10.1016/j.radonc.2017.10.014},
  journal = {Radiotherapy and Oncology}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-8782,elsevier,Co-designing opportunities for human-centred machine learning in supporting type 1 diabetes decision-making,"Type 1 Diabetes (T1D) self-management requires hundreds of daily decisions. Diabetes technologies that use machine learning have significant potential to simplify this process and provide better decision support, but often rely on cumbersome data logging and cognitively demanding reflection on collected data. We set out to use co-design to identify opportunities for machine learning to support diabetes self-management in everyday settings. However, over nine months of interviews and design workshops with 15 people with T1D, we had to re-assess our assumptions about user needs. Our participants reported confidence in their personal knowledge and rejected machine learning based decision support when coping with routine situations, but highlighted the need for technological support in the context of unfamiliar or unexpected situations (holidays, illness, etc.). However, these are the situations where prior data are often lacking and drawing data-driven conclusions is challenging. Reflecting this challenge, we provide suggestions on how machine learning and other artificial intelligence approaches, e.g., expert systems, could enable decision-making support in both routine and unexpected situations.",https://doi.org/10.1016/j.ijhcs.2023.103003,https://www.sciencedirect.com/science/article/pii/S1071581923000095,International Journal of Human-Computer Studies,Katarzyna Stawarz;Dmitri Katz;Amid Ayobi;Paul Marshall;Taku Yamagata;Raul Santos-Rodriguez;Peter Flach;Aisling Ann O’Kane,2023,3,"@article{2-8782,
  title={Co-designing opportunities for human-centred machine learning in supporting type 1 diabetes decision-making},
  author={Stawarz, Katarzyna and Katz, Dmitri and Ayobi, Amid and Marshall, Paul and Yamagata, Taku and Santos-Rodriguez, Raul and Flach, Peter and O’Kane, Aisling Ann},
  year={2023},
  journal={International Journal of Human-Computer Studies},
  doi={10.1016/j.ijhcs.2023.103003}
}",Empirical contributions,Healthcare / Medicine / Surgery,Individual,"Executing, Advising, Collaborating","Decision-subject, Decision-maker",no such info,"Shape AI for accountability, Change AI responses",NA,"self-tracking, lacking motivation/awareness or time to document events, distinguish between data collected in routine vs non-routine situations, ""prefernece for dialogue to verify advice, further build trust, and provide additional information""",Physical / Embodiment,Yes,Yes
2-8797,elsevier,Colergs-constrained safe reinforcement learning for realising mass's risk-informed collision avoidance decision making,"Maritime autonomous surface ship (MASS) represents a significant advancement in maritime technology, offering the potential for increased efficiency, reduced operational costs, and enhanced maritime traffic safety. However, MASS navigation in complex maritime traffic and congested water areas presents challenges, especially in Collision Avoidance Decision Making (CADM) during multi-ship encounter scenarios. Through a robust risk assessment design for time-sequential and joint-target ships (TSs) encounter scenarios, a novel risk and reliability critic-enhanced safe hierarchical reinforcement learning (RA-SHRL), constrained by the International Regulations for Preventing Collisions at Sea (COLREGs), is proposed to realize the autonomous navigation and CADM of MASS. Finally, experimental simulations are conducted against a time-sequenced obstacle avoidance scenario and a swarm obstacle avoidance scenario. The experimental results demonstrate that RA-SHRL generates safe, efficient, and reliable collision avoidance strategies in both time-sequential dynamic obstacles and mixed joint-TSs environments. Additionally, the RA-SHRL is capable of assessing risk and avoiding multiple joint-TSs. Compared with Deep Q-network (DQN) and Constrained Policy Optimization (CPO), the search efficiency of the algorithm proposed in this paper is improved by 40% and 12%, respectively. Moreover, it achieved a 91.3% success rate of collision avoidance during training. The methodology could also benefit other autonomous systems in dynamic environments.",https://doi.org/10.1016/j.knosys.2024.112205,https://www.sciencedirect.com/science/article/pii/S0950705124008396,Knowledge-Based Systems,Chengbo Wang;Xinyu Zhang;Hongbo Gao;Musa Bashir;Huanhuan Li;Zaili Yang,2024,30,"@article{2-8797,
  title={Colergs-constrained safe reinforcement learning for realising mass's risk-informed collision avoidance decision making},
  author={Wang, Chengbo and Zhang, Xinyu and Gao, Hongbo and Bashir, Musa and Li, Huanhuan and Yang, Zaili},
  year={2024},
  journal={Knowledge-Based Systems},
  doi={10.1016/j.knosys.2024.112205}
}",Algorithmic contributions,Transportation / Mobility / Planning,Organizational,Executing,"Developer, Stakeholder, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-8800,elsevier,Collaborative emergency decision-making: a framework for deep learning with social media data,"Emergency decision-making (EDM) problems based on social media data have recently attracted considerable attention. However, few studies have considered collaborative EDM based on public opinion and expert knowledge. To improve the effectiveness and interpretability of EDM, we propose a knowledge+opinion driven multi-phase collaborative emergency decision-making model, which combines social media data that represents public opinion with the knowledge and experience of experts. First, a text-mining algorithm extracts the keywords and their weights from the social media data. Then, we define 2-tuple emergency attributes to simplify and quantify the keywords with social media data. Furthermore, a sentiment analysis model based on the XLNet-Att deep learning algorithm is proposed to obtain sentiment polarities for emergencies and provide timely support for government EDM in the future. Moreover, a real-world case concerning the Southern China flood disaster in 2020 is applied to validate our proposed model. We find that for similar emergencies, the focus of public attention have similar characteristics at different periods, and the analysis results show different perspectives of public attention to emergencies at different stages, providing reliable data and experience support for future EDM of similar emergencies. Finally, we conduct a sensitivity analysis to demonstrate the stability of our deep learning model and a comparative study using existing models to verify the effectiveness of our model.",https://doi.org/10.1016/j.ijpe.2023.109072,https://www.sciencedirect.com/science/article/pii/S0925527323003043,International Journal of Production Economics,Jindong Qin;Minxuan Li;Xiaojun Wang;Witold Pedrycz,2024,13,"@article{2-8800,
  title={Collaborative emergency decision-making: a framework for deep learning with social media data},
  author={Qin, Jindong and Li, Minxuan and Wang, Xiaojun and Pedrycz, Witold},
  year={2024},
  journal={International Journal of Production Economics},
  doi={10.1016/j.ijpe.2023.109072}
}",Methodological contributions,Media / Communication / Entertainment,Organizational,"Forecasting, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-8828,elsevier,Combining the strengths of radiologists and ai for breast cancer screening: a retrospective analysis,"Summary Background We propose a decision-referral approach for integrating artificial intelligence (AI) into the breast-cancer screening pathway, whereby the algorithm makes predictions on the basis of its quantification of uncertainty. Algorithmic assessments with high certainty are done automatically, whereas assessments with lower certainty are referred to the radiologist. This two-part AI system can triage normal mammography exams and provide post-hoc cancer detection to maintain a high degree of sensitivity. This study aimed to evaluate the performance of this AI system on sensitivity and specificity when used either as a standalone system or within a decision-referral approach, compared with the original radiologist decision. Methods We used a retrospective dataset consisting of 1 193 197 full-field, digital mammography studies carried out between Jan 1, 2007, and Dec 31, 2020, from eight screening sites participating in the German national breast-cancer screening programme. We derived an internal-test dataset from six screening sites (1670 screen-detected cancers and 19 997 normal mammography exams), and an external-test dataset of breast cancer screening exams (2793 screen-detected cancers and 80 058 normal exams) from two additional screening sites to evaluate the performance of an AI algorithm on sensitivity and specificity when used either as a standalone system or within a decision-referral approach, compared with the original individual radiologist decision at the point-of-screen reading ahead of the consensus conference. Different configurations of the AI algorithm were evaluated. To account for the enrichment of the datasets caused by oversampling cancer cases, weights were applied to reflect the actual distribution of study types in the screening programme. Triaging performance was evaluated as the rate of exams correctly identified as normal. Sensitivity across clinically relevant subgroups, screening sites, and device manufacturers was compared between standalone AI, the radiologist, and decision referral. We present receiver operating characteristic (ROC) curves and area under the ROC (AUROC) to evaluate AI-system performance over its entire operating range. Comparison with radiologists and subgroup analysis was based on sensitivity and specificity at clinically relevant configurations. Findings The exemplary configuration of the AI system in standalone mode achieved a sensitivity of 84·2% (95% CI 82·4–85·8) and a specificity of 89·5% (89·0–89·9) on internal-test data, and a sensitivity of 84·6% (83·3–85·9) and a specificity of 91·3% (91·1–91·5) on external-test data, but was less accurate than the average unaided radiologist. By contrast, the simulated decision-referral approach significantly improved upon radiologist sensitivity by 2·6 percentage points and specificity by 1·0 percentage points, corresponding to a triaging performance at 63·0% on the external dataset; the AUROC was 0·982 (95% CI 0·978–0·986) on the subset of studies assessed by AI, surpassing radiologist performance. The decision-referral approach also yielded significant increases in sensitivity for a number of clinically relevant subgroups, including subgroups of small lesion sizes and invasive carcinomas. Sensitivity of the decision-referral approach was consistent across the eight included screening sites and three device manufacturers. Interpretation The decision-referral approach leverages the strengths of both the radiologist and AI, demonstrating improvements in sensitivity and specificity surpassing that of the individual radiologist and of the standalone AI system. This approach has the potential to improve the screening accuracy of radiologists, is adaptive to the requirements of screening, and could allow for the reduction of workload ahead of the consensus conference, without discarding the generalised knowledge of radiologists. Funding Vara.",https://doi.org/10.1016/S2589-7500(22)00070-X,https://www.sciencedirect.com/science/article/pii/S258975002200070X,The Lancet Digital Health,Christian Leibig;Moritz Brehmer;Stefan Bunk;Danalyn Byng;Katja Pinker;Lale Umutlu,2022,350,"@article{2-8828,
  title={Combining the strengths of radiologists and AI for breast cancer screening: a retrospective analysis},
  author={Leibig, Christian and Brehmer, Moritz and Bunk, Stefan and Byng, Danalyn and Pinker, Katja and Umutlu, Lale},
  year={2022},
  journal={The Lancet Digital Health},
  doi={10.1016/S2589-7500(22)00070-X}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising, Collaborating","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-8857,elsevier,Comparing clinical judgment with the mysurgeryrisk algorithm for preoperative risk assessment: a pilot usability study,"Background Major postoperative complications are associated with increased cost and mortality. The complexity of electronic health records overwhelms physicians’ abilities to use the information for optimal and timely preoperative risk assessment. We hypothesized that data-driven, predictive-risk algorithms implemented in an intelligent decision-support platform simplify and augment physicians’ risk assessments. Methods This prospective, nonrandomized pilot study of 20 physicians at a quaternary academic medical center compared the usability and accuracy of preoperative risk assessment between physicians and MySurgeryRisk, a validated, machine-learning algorithm, using a simulated workflow for the real-time, intelligent decision-support platform. We used area under the receiver operating characteristic curve to compare the accuracy of physicians’ risk assessment for six postoperative complications before and after interaction with the algorithm for 150 clinical cases. Results The area under the receiver operating characteristic curve of the MySurgeryRisk algorithm ranged between 0.73 and 0.85 and was significantly better than physicians' initial risk assessments (area under the receiver operating characteristic curve between 0.47 and 0.69) for all postoperative complications except cardiovascular. After interaction with the algorithm, the physicians significantly improved their risk assessment for acute kidney injury and for an intensive care unit admission greater than 48 hours, resulting in a net improvement of reclassification of 12% and 16%, respectively. Physicians rated the algorithm as easy to use and useful. Conclusion Implementation of a validated, MySurgeryRisk computational algorithm for real-time predictive analytics with data derived from the electronic health records to augment physicians’ decision-making is feasible and accepted by physicians. Early involvement of physicians as key stakeholders in both design and implementation of this technology will be crucial for its future success.",https://doi.org/10.1016/j.surg.2019.01.002,https://www.sciencedirect.com/science/article/pii/S003960601930008X,Surgery,Meghan Brennan;Sahil Puri;Tezcan Ozrazgat-Baslanti;Zheng Feng;Matthew Ruppert;Haleh Hashemighouchani;Petar Momcilovic;Xiaolin Li;Daisy Zhe Wang;Azra Bihorac,2019,7,"@article{2-8857,
  title = {Comparing clinical judgment with the mysurgeryrisk algorithm for preoperative risk assessment: a pilot usability study},
  author = {Meghan Brennan and Sahil Puri and Tezcan Ozrazgat-Baslanti and Zheng Feng and Matthew Ruppert and Haleh Hashemighouchani and Petar Momcilovic and Xiaolin Li and Daisy Zhe Wang and Azra Bihorac},
  year = {2019},
  doi = {10.1016/j.surg.2019.01.002},
  journal = {Surgery}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Analyzing, Advising","Decision-maker, Stakeholder, Guardian, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-8874,elsevier,"Comparison of humans versus mobile phone-powered artificial intelligence for the diagnosis and management of pigmented skin cancer in secondary care: a multicentre, prospective, diagnostic, clinical trial","Summary Background Diagnosis of skin cancer requires medical expertise, which is scarce. Mobile phone-powered artificial intelligence (AI) could aid diagnosis, but it is unclear how this technology performs in a clinical scenario. Our primary aim was to test in the clinic whether there was equivalence between AI algorithms and clinicians for the diagnosis and management of pigmented skin lesions. Methods In this multicentre, prospective, diagnostic, clinical trial, we included specialist and novice clinicians and patients from two tertiary referral centres in Australia and Austria. Specialists had a specialist medical qualification related to diagnosing and managing pigmented skin lesions, whereas novices were dermatology junior doctors or registrars in trainee positions who had experience in examining and managing these lesions. Eligible patients were aged 18–99 years and had a modified Fitzpatrick I–III skin type; those in the diagnostic trial were undergoing routine excision or biopsy of one or more suspicious pigmented skin lesions bigger than 3 mm in the longest diameter, and those in the management trial had baseline total-body photographs taken within 1–4 years. We used two mobile phone-powered AI instruments incorporating a simple optical attachment: a new 7-class AI algorithm and the International Skin Imaging Collaboration (ISIC) AI algorithm, which was previously tested in a large online reader study. The reference standard for excised lesions in the diagnostic trial was histopathological examination; in the management trial, the reference standard was a descending hierarchy based on histopathological examination, comparison of baseline total-body photographs, digital monitoring, and telediagnosis. The main outcome of this study was to compare the accuracy of expert and novice diagnostic and management decisions with the two AI instruments. Possible decisions in the management trial were dismissal, biopsy, or 3-month monitoring. Decisions to monitor were considered equivalent to dismissal (scenario A) or biopsy of malignant lesions (scenario B). The trial was registered at the Australian New Zealand Clinical Trials Registry ACTRN12620000695909 (Universal trial number U1111–1251–8995). Findings The diagnostic study included 172 suspicious pigmented lesions (84 malignant) from 124 patients and the management study included 5696 pigmented lesions (18 malignant) from the whole body of 66 high-risk patients. The diagnoses of the 7-class AI algorithm were equivalent to the specialists’ diagnoses (absolute accuracy difference 1·2% [95% CI –6·9 to 9·2]) and significantly superior to the novices’ ones (21·5% [13·1 to 30·0]). The diagnoses of the ISIC AI algorithm were significantly inferior to the specialists’ diagnoses (–11·6% [–20·3 to –3·0]) but significantly superior to the novices’ ones (8·7% [–0·5 to 18·0]). The best 7-class management AI was significantly inferior to specialists’ management (absolute accuracy difference in correct management decision –0·5% [95% CI –0·7 to –0·2] in scenario A and –0·4% [–0·8 to –0·05] in scenario B). Compared with the novices’ management, the 7-class management AI was significantly inferior (–0·4% [–0·6 to –0·2]) in scenario A but significantly superior (0·4% [0·0 to 0·9]) in scenario B. Interpretation The mobile phone-powered AI technology is simple, practical, and accurate for the diagnosis of suspicious pigmented skin cancer in patients presenting to a specialist setting, although its usage for management decisions requires more careful execution. An AI algorithm that was superior in experimental studies was significantly inferior to specialists in a real-world scenario, suggesting that caution is needed when extrapolating results of experimental studies to clinical practice. Funding MetaOptima Technology.",https://doi.org/10.1016/S2589-7500(23)00130-9,https://www.sciencedirect.com/science/article/pii/S2589750023001309,The Lancet Digital Health,Scott W Menzies;Christoph Sinz;Michelle Menzies;Serigne N Lo;William Yolland;Johann Lingohr;Majid Razmara;Philipp Tschandl;Pascale Guitera;Richard A Scolyer;Florentina Boltz;Liliane Borik-Heil;Hsien {Herbert Chan};David Chromy;David J Coker;Helena Collgros;Maryam Eghtedari;Marina {Corral Forteza};Emily Forward;Bruna Gallo;Stephanie Geisler;Matthew Gibson;Amelie Hampel;Genevieve Ho;Laura Junez;Philipp Kienzl;Arthur Martin;Fergal J Moloney;Amanda {Regio Pereira};Julia Maria Ressler;Susanne Richter;Katharina Silic;Thomas Silly;Michael Skoll;Julia Tittes;Philipp Weber;Wolfgang Weninger;Doris Weiss;Ping Woo-Sampson;Catherine Zilberg;Harald Kittler,2023,0,"@article{2-8874,
  title={Comparison of humans versus mobile phone-powered artificial intelligence for the diagnosis and management of pigmented skin cancer in secondary care: a multicentre, prospective, diagnostic, clinical trial},
  author={Menzies, Scott W and Sinz, Christoph and Menzies, Michelle and Lo, Serigne N and Yolland, William and Lingohr, Johann and Razmara, Majid and Tschandl, Philipp and Guitera, Pascale and Scolyer, Richard A and Boltz, Florentina and Borik-Heil, Liliane and Chan, Hsien {Herbert} and Chromy, David and Coker, David J and Collgros, Helena and Eghtedari, Maryam and {Corral Forteza}, Marina and Forward, Emily and Gallo, Bruna and Geisler, Stephanie and Gibson, Matthew and Hampel, Amelie and Ho, Genevieve and Junez, Laura and Kienzl, Philipp and Martin, Arthur and Moloney, Fergal J and {Regio Pereira}, Amanda and Ressler, Julia Maria and Richter, Susanne and Silic, Katharina and Silly, Thomas and Skoll, Michael and Tittes, Julia and Weber, Philipp and Weninger, Wolfgang and Weiss, Doris and Woo-Sampson, Ping and Zilberg, Catherine and Kittler, Harald},
  year={2023},
  doi={10.1016/S2589-7500(23)00130-9},
  journal={The Lancet Digital Health}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-89,aaai,AI-Empowered Decision Support for COVID-19 Social Distancing,"The COVID-19 pandemic is one of the most severe challenges the world faces today. In order to contain the transmission of COVID-19, people around the world have been advised to practise social distancing. However, maintaining social distance is a challenging problem, as we often do not know beforehand how crowded the places we intend to visit are. In this paper, we demonstrate crowded. sg, an AI-empowered platform that leverages on Unmanned Aerial Vehicles( UAVs) , crowdsourced images, and computer vision techniques to provide social distancing decision support.",10.1609/aaai.v35i18.18007,https://ojs.aaai.org/index.php/AAAI/article/view/18007,AAAI Conference on Artificial Intelligence,Hongchao Jiang;Wei Yang Bryan Lim;Jer Shyuan Ng;Harold Ze Chie Teng;Han Yu;Zehui Xiong;Dusit Niyato;Chunyan Miao,2021,4,"@inproceedings{2-89,
  title = {AI-Empowered Decision Support for COVID-19 Social Distancing},
  author = {Jiang, Hongchao and Lim, Wei Yang Bryan and Ng, Jer Shyuan and Teng, Harold Ze Chie and Yu, Han and Xiong, Zehui and Niyato, Dusit and Miao, Chunyan},
  year = {2021},
  doi = {10.1609/aaai.v35i18.18007},
  booktitle = {AAAI Conference on Artificial Intelligence}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Individual,"Forecasting, Advising, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-8918,elsevier,"Computational decision framework for enhancing resilience of the energy, water and food nexus in risky environments","The energy, water and food (EWF) nexus modelling and analysis frameworks proposed recently have demonstrated their effectiveness in the assessment and quantification of synergies and trade-offs in the interlinkages between the three sectors. They largely rely on static, deterministic or equilibrium-based models that facilitate in making decisions for well-behaved and predictable resource systems over time. These frameworks, however, are partly limited in their functionality due to the fact that they do not consider the exposure of systems to the dynamic nature of extrinsic uncertainties and the associated risks in the nexus. Hence, there is a need for a sequential learning, planning and optimal control modelling framework which could help achieve adaptive systems under volatile conditions with the objective to maximise economic output and enhance their operational resilience. In this paper, the authors discuss the development of a novel computational framework which incorporates “algorithmic resilience thinking” to achieve adaptive and robust inter-networked systems. Here, the question of adaptive systems for EWF nexus resilience is posed as a reinforcement learning problem based on sequential decision-making called the Markov decision process (MDP). The authors further discuss a case study, considering weather volatility, its spatial impact on vegetation, and the consequent risks on the water-food nexus for outdoor agricultural operations in the State of Qatar. The application of the developed framework particularly demonstrates promise in providing the functionality to track and mitigate emerging risks that have the potential to cause unprecedented disruption in the operations of integrated natural resource systems. The outcome of this study has positive implications for the advancement and effectiveness of EWF nexus planning and risk management to avert resource shortages and price risks, socio-economic disruption, and cascading failures of critical infrastructures, particularly when the global supply chains are subjected to stresses and shocks, such as extreme weather conditions.",https://doi.org/10.1016/j.rser.2019.06.015,https://www.sciencedirect.com/science/article/pii/S1364032119304083,Renewable and Sustainable Energy Reviews,Rajesh Govindan;Tareq Al-Ansari,2019,123,"@article{2-8918,
  title={Computational decision framework for enhancing resilience of the energy, water and food nexus in risky environments},
  author={Govindan, Rajesh and Al-Ansari, Tareq},
  year={2019},
  journal={Renewable and Sustainable Energy Reviews},
  doi={10.1016/j.rser.2019.06.015}
}",Methodological contributions,Environment / Resources / Energy,Institutional,"Analyzing, Executing, Forecasting",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-8934,elsevier,Computer-assisted decision support system in pulmonary cancer detection and stage classification on ct images,"Pulmonary cancer is considered as one of the major causes of death worldwide. For the detection of lung cancer, computer-assisted diagnosis (CADx) systems have been designed. Internet-of-Things (IoT) has enabled ubiquitous internet access to biomedical datasets and techniques; in result, the progress in CADx is significant. Unlike the conventional CADx, deep learning techniques have the basic advantage of an automatic exploitation feature as they have the ability to learn mid and high level image representations. We proposed a Computer-Assisted Decision Support System in Pulmonary Cancer by using the novel deep learning based model and metastasis information obtained from MBAN (Medical Body Area Network). The proposed model, DFCNet, is based on the deep fully convolutional neural network (FCNN) which is used for classification of each detected pulmonary nodule into four lung cancer stages. The performance of proposed work is evaluated on different datasets with varying scan conditions. Comparison of proposed classifier is done with the existing CNN techniques. Overall accuracy of CNN and DFCNet was 77.6% and 84.58%, respectively. Experimental results illustrate the effectiveness of proposed method for the detection and classification of lung cancer nodules. These results demonstrate the potential for the proposed technique in helping the radiologists in improving nodule detection accuracy with efficiency.",https://doi.org/10.1016/j.jbi.2018.01.005,https://www.sciencedirect.com/science/article/pii/S1532046418300078,Journal of Biomedical Informatics,Anum Masood;Bin Sheng;Ping Li;Xuhong Hou;Xiaoer Wei;Jing Qin;Dagan Feng,2018,7,"@article{2-8934,
  title = {Computer-assisted decision support system in pulmonary cancer detection and stage classification on CT images},
  author = {Masood, Anum and Sheng, Bin and Li, Ping and Hou, Xuhong and Wei, Xiaoer and Qin, Jing and Feng, Dagan},
  year = {2018},
  doi = {10.1016/j.jbi.2018.01.005},
  journal = {Journal of Biomedical Informatics}
}","Algorithmic contributions, System/Artifact contributions",Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-8961,elsevier,Constraint guided search for aircraft sequencing,"Aircraft sequencing problem (ASP) is an NP-Hard problem. It involves allocation of aircraft to runways for landing and takeoff, minimising total tardiness. ASP has made significant progress in recent years. However, within practical time limits, existing incomplete algorithms still either find low quality solutions or struggle with large problems. One key reason behind this is the typical way of using generic heuristics or metaheuristics that usually lack problem specific structural knowledge. As a result, existing such methods use either an exhaustive or a random neighbourhood generation strategy. So their search guidance comes only from the evaluation function that is used mainly after the neighbourhood generation. In this work, we aim to advance ASP search by better exploiting the problem specific structural knowledge. We use the constraint and the objective functions to obtain such problem specific knowledge and we exploit such knowledge both in a constructive search method and in a local search method. Our motivation comes from the constraint optimisation paradigm in artificial intelligence, where instead of random decisions, constraint-guided more informed optimisation decisions are of particular interest. We run our experiments on a range of standard benchmark problem instances that include instances from real airports and instances crafted using real airport parameters, and contain scenarios involving multiple runways and both landing and takeoff operations. We show that our proposed algorithms significantly outperform existing state-of-the-art aircraft sequencing algorithms.",https://doi.org/10.1016/j.eswa.2018.10.033,https://www.sciencedirect.com/science/article/pii/S0957417418306833,Expert Systems with Applications,Vahid Riahi;M.A. Hakim Newton;M.M.A. Polash;Kaile Su;Abdul Sattar,2019,25,"@article{2-8961,
  title={Constraint guided search for aircraft sequencing},
  author={Riahi, Vahid and Newton, M.A. Hakim and Polash, M.M.A. and Su, Kaile and Sattar, Abdul},
  year={2019},
  journal={Expert Systems with Applications},
  doi={10.1016/j.eswa.2018.10.033}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,Executing,"Knowledge provider, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-9006,elsevier,Cooperative and distributed decision-making in a multi-agent perception system for improvised land mines detection,"This work presents a novel intelligent system designed using a multi-agent hardware platform to detect improvised explosive devices concealed in the ground. Each agent is equipped with a different sensor, (i.e. a ground-penetrating radar, a thermal sensor and three cameras each covering a different spectrum) and processes dedicated AI decision-making capabilities. The proposed system has a unique hardware structure, with a distributed design and effective selection of sensors, and a novel multi-phase and cooperative decision-making framework. Agents operate independently via a customised logic adjusting their sensor positions - to achieve optimal acquisition; performing a preliminary “local decision-making” - to classify buried objects; sharing information with the other agents. Once sufficient information is shared by the agents, a collaborative behaviour emerges in the so-called “cooperative decision-making” process, which performs the final detection. In this paper, 120 variations of the proposed system, obtained by combining both classic aggregation operators as well as advanced neural and fuzzy systems, are presented, tested and evaluated. Results show a good detection accuracy and robustness to environmental and data sets changes, in particular when the cooperative decision-making is implemented with the neuroevolution paradigm.",https://doi.org/10.1016/j.inffus.2020.06.009,https://www.sciencedirect.com/science/article/pii/S1566253520303067,Information Fusion,Johana Florez-Lozano;Fabio Caraffini;Carlos Parra;Mario Gongora,2020,30,"@article{2-9006,
  title = {Cooperative and distributed decision-making in a multi-agent perception system for improvised land mines detection},
  author = {Johana Florez-Lozano and Fabio Caraffini and Carlos Parra and Mario Gongora},
  year = {2020},
  doi = {10.1016/j.inffus.2020.06.009},
  journal = {Information Fusion}
}",System/Artifact contributions,Defense / Military / Emergency,Operational,"Executing, Forecasting","Decision-maker, Guardian, Stakeholder",Alter decision outcomes,"Update AI competence, Shape AI for accountability",shared information,NA,Physical / Embodiment,Yes,Yes
2-9030,elsevier,Cost-effectiveness of ai for caries detection: randomized trial,"Objectives: We assessed the cost-effectiveness of AI-supported detection of proximal caries in a randomized controlled clustered cross-over superiority trial. Methods: Twenty-three dentists were sampled to assess 20 bitewings; 10 were randomly evaluated supported by an AI-based software (dentalXrai Pro 1.0.4, dentalXrai Ltd, Berlin, Germany) and the other 10 without AI support. The reference test had been established by four independent experts and an additional review. We evaluated the proportion of true and false positive and negative detections and the treatment decisions assigned to each detection (non-invasive, micro-invasive, invasive). Cost-effectiveness was assessed using a mixed public-private-payer perspective in German healthcare. Using the accuracy and treatment decision data from the trial, a Markov simulation model was populated and posterior permanent teeth in initially 31-years old individuals followed over their lifetime. The model allowed extrapolation from the initial detection and therapy to treatment success, re-treatments and, eventually, tooth loss and replacement, capturing long-term effectiveness (tooth retention) and costs (cumulative in Euro). Costs were estimated using the German public and private fee catalogues. Monte-Carlo microsimulations were used and incremental cost-effectiveness at different willingness-to-pay ceiling thresholds assessed. Results: In the trial, AI-supported detection was significantly more sensitive than detection without AI. However, in the AI group, lesions were more often treated invasively. As a result, AI and no AI showed identical effectiveness (tooth retention for a mean (2.5-97.5%) 49 (48-51) years) and nearly identical costs (AI: 330 (250-409) Euro, no AI: 330 (248-410) Euro). 41% simulations found AI and 43% no AI to be more cost-effective. The resulting cost-effectiveness remained uncertain regardless of a payer's willingness-to-pay. Conclusions: Higher accuracy of AI did not lead to higher cost-effectiveness, as more invasive treatment approaches generated costs and diminished possible effectiveness advantages. Clinical significance: The cost-effectiveness of AI could be improved by supporting not only caries detection, but also subsequent management.",https://doi.org/10.1016/j.jdent.2022.104080,https://www.sciencedirect.com/science/article/pii/S0300571222001373,Journal of Dentistry,Falk Schwendicke;Sarah Mertens;Anselmo Garcia Cantu;Akhilanand Chaurasia;Hendrik Meyer-Lueckel;Joachim Krois,2022,159,"@article{2-9030,
  title={Cost-effectiveness of AI for caries detection: randomized trial},
  author={Schwendicke, Falk and Mertens, Sarah and Garcia Cantu, Anselmo and Chaurasia, Akhilanand and Meyer-Lueckel, Hendrik and Krois, Joachim},
  year={2022},
  journal={Journal of Dentistry},
  doi={10.1016/j.jdent.2022.104080}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Guardian, Decision-subject",Alter decision outcomes,no such info,augmented image,NA,"Visual, Interactive interface",Yes,Yes
2-9053,elsevier,Creating a workforce of fatigued cynics? A randomized controlled trial of implementing an algorithmic decision-making support tool,"In recent decades, public service provision has become increasingly digitalized. However, while digitalization and artificial intelligence holds many promises, there is surprisingly little causal evidence on how it affects the employees who provide such services in the frontline. Based on cognitive and social psychological theories, we argue that IT projects can increase employees' cynicism towards change and change fatigue. In liaison with a Danish unemployment insurance fund, we test our hypotheses in a pre-registered randomized controlled trial that introduced an algorithmic decision-making support tool to underpin the counselling of newly unemployed clients. We do not find evidence that implementation of this tool resulted in negative employee outcomes. However, exploratory analyses indicate that this conclusion may mask smaller or heterogenous effects depending on employees' years of service with the insurance fund. We end the paper by discussing the implications of organizational change in the public sector.",https://doi.org/10.1016/j.giq.2024.101911,https://www.sciencedirect.com/science/article/pii/S0740624X24000030,Government Information Quarterly,Matthias Döring;Kim Sass Mikkelsen;Jonas Krogh Madsen;Kristian Bloch Haug,2024,12,"@article{2-9053,
  title = {Creating a workforce of fatigued cynics? A randomized controlled trial of implementing an algorithmic decision-making support tool},
  author = {Matthias Döring and Kim Sass Mikkelsen and Jonas Krogh Madsen and Kristian Bloch Haug},
  year = {2024},
  journal = {Government Information Quarterly},
  doi = {10.1016/j.giq.2024.101911}
}",Empirical contributions,Everyday / Employment / Public Service,Operational,Advising,"Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-9068,elsevier,Cross-check qa: a quality assurance workflow to prevent missed diagnoses by alerting inadvertent discordance between the radiologist and artificial intelligence in the interpretation of high-acuity ct scans,"Purpose The aim of this study was to implement and evaluate a quality assurance (QA) workflow that leverages natural language processing to rapidly resolve inadvertent discordance between radiologists and an artificial intelligence (AI) decision support system (DSS) in the interpretation of high-acuity CT studies when the radiologist does not engage with AI DSS output. Methods All consecutive high-acuity adult CT examinations performed in a health system between March 1, 2020, and September 20, 2022, were interpreted alongside an AI DSS (Aidoc) for intracranial hemorrhage, cervical spine fracture, and pulmonary embolus. CT studies were flagged for this QA workflow if they met three criteria: (1) negative results by radiologist report, (2) a high probability of positive results by the AI DSS, and (3) unviewed AI DSS output. In these cases, an automated e-mail notification was sent to our quality team. If discordance was confirmed on secondary review—an initially missed diagnosis—addendum and communication documentation was performed. Results Of 111,674 high-acuity CT examinations interpreted alongside the AI DSS over this 2.5-year time period, the frequency of missed diagnoses (intracranial hemorrhage, pulmonary embolus, and cervical spine fracture) uncovered by this workflow was 0.02% (n = 26). Of 12,412 CT studies prioritized as depicting positive findings by the AI DSS, 0.4% (n = 46) were discordant, unengaged, and flagged for QA. Among these discordant cases, 57% (26 of 46) were determined to be true positives. Addendum and communication documentation was performed within 24 hours of the initial report signing in 85% of these cases. Conclusions Inadvertent discordance between radiologists and the AI DSS occurred in a small number of cases. This QA workflow leveraged natural language processing to rapidly detect, notify, and resolve these discrepancies and prevent potential missed diagnoses.",https://doi.org/10.1016/j.jacr.2023.06.010,https://www.sciencedirect.com/science/article/pii/S1546144023004866,Journal of the American College of Radiology,Mariam Chekmeyan;Steven J. Baccei;Elisabeth R. Garwood,2023,3,"@article{2-9068,
  title = {Cross-check qa: a quality assurance workflow to prevent missed diagnoses by alerting inadvertent discordance between the radiologist and artificial intelligence in the interpretation of high-acuity CT scans},
  author = {Mariam Chekmeyan and Steven J. Baccei and Elisabeth R. Garwood},
  year = {2023},
  doi = {10.1016/j.jacr.2023.06.010},
  journal = {Journal of the American College of Radiology}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,Monitoring,"Decision-maker, Guardian","Alter decision outcomes, Change cognitive demands",no such info,notification,NA,Textual,Yes,Yes
2-9105,elsevier,Data analytics-based decision support workflow for high-mix low-volume production systems,"In order to answer the ever-fluctuating demand of high-mix low-volume production environments, reconfiguring the production systems and improving their performance rely heavily on the application of advanced decision support tools. Estimating the expected values of the performance measures (KPIs) in the face of these decisions, however, is even more challenging in such an environment as the complex structure, behavior and input demand creates an enormously large variable domain restraining the analysis. The paper introduces a novel workflow for providing simulation-based decision support for improving KPIs of high-mix low-volume production systems by reducing the size of the input domain with the application of unsupervised machine learning techniques.",https://doi.org/10.1016/j.cirp.2019.04.001,https://www.sciencedirect.com/science/article/pii/S000785061930023X,CIRP Annals - Manufacturing Technology,István Gödri;Csaba Kardos;András Pfeiffer;József Váncza,2019,18,"@article{2-9105,
  title = {Data analytics-based decision support workflow for high-mix low-volume production systems},
  author = {Istv{\'a}n G{\""o}dri and Csaba Kardos and Andr{\'a}s Pfeiffer and J{\'o}zsef V{\'a}ncza},
  year = {2019},
  doi = {10.1016/j.cirp.2019.04.001},
  journal = {CIRP Annals - Manufacturing Technology}
}",Methodological contributions,Manufacturing / Industry / Automation,Operational,"Analyzing, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-9194,elsevier,Decision control and explanations in human-ai collaboration: improving user perceptions and compliance,"Human-AI collaboration has become common, integrating highly complex AI systems into the workplace. Still, it is often ineffective; impaired perceptions – such as low trust or limited understanding – reduce compliance with recommendations provided by the AI system. Drawing from cognitive load theory, we examine two techniques of human-AI collaboration as potential remedies. In three experimental studies, we grant users decision control by empowering them to adjust the system’s recommendations, and we offer explanations for the system’s reasoning. We find decision control positively affects user perceptions of trust and understanding, and improves user compliance with system recommendations. Next, we isolate different effects of providing explanations that may help explain inconsistent findings in recent literature: while explanations help reenact the system’s reasoning, they also increase task complexity. Further, the effectiveness of providing an explanation depends on the specific user’s cognitive ability to handle complex tasks. In summary, our study shows that users benefit from enhanced decision control, while explanations – unless appropriately designed for the specific user – may even harm user perceptions and compliance. This work bears both theoretical and practical implications for the management of human-AI collaboration.",https://doi.org/10.1016/j.chb.2023.107714,https://www.sciencedirect.com/science/article/pii/S0747563223000651,Computers in Human Behavior,Monika Westphal;Michael Vössing;Gerhard Satzger;Galit B. Yom-Tov;Anat Rafaeli,2023,0,"@article{2-9194,
  title = {Decision control and explanations in human-ai collaboration: improving user perceptions and compliance},
  author = {Westphal, Monika and V{\""o}ssing, Michael and Satzger, Gerhard and Yom-Tov, Galit B. and Rafaeli, Anat},
  year = {2023},
  doi = {10.1016/j.chb.2023.107714},
  journal = {Computers in Human Behavior}
}",Empirical contributions,"Finance / Business / Economy, Generic / Abstract / Domain-agnostic",no such info,"Explaining, Advising",Decision-maker,"Restrict human agency, Change trust, Change cognitive demands, Alter decision outcomes",no such info,"visual explanations, prediction of alternative",NA,Visual,Yes,Yes
2-9196,elsevier,Decision making of autonomous vehicles in lane change scenarios: deep reinforcement learning approaches with risk awareness,"Driving safety is the most important element that needs to be considered for autonomous vehicles (AVs). To ensure driving safety, we proposed a lane change decision-making framework based on deep reinforcement learning to find a risk-aware driving decision strategy with the minimum expected risk for autonomous driving. Firstly, a probabilistic-model based risk assessment method was proposed to assess the driving risk using position uncertainty and distance-based safety metrics. Then, a risk aware decision making algorithm was proposed to find a strategy with the minimum expected risk using deep reinforcement learning. Finally, our proposed methods were evaluated in CARLA in two scenarios (one with static obstacles and one with dynamically moving vehicles). The results show that our proposed methods can generate robust safe driving strategies and achieve better driving performances than previous methods.",https://doi.org/10.1016/j.trc.2021.103452,https://www.sciencedirect.com/science/article/pii/S0968090X21004411,Transportation Research Part C: Emerging Technologies,Guofa Li;Yifan Yang;Shen Li;Xingda Qu;Nengchao Lyu;Shengbo Eben Li,2022,323,"@article{2-9196,
  title={Decision making of autonomous vehicles in lane change scenarios: deep reinforcement learning approaches with risk awareness},
  author={Li, Guofa and Yang, Yifan and Li, Shen and Qu, Xingda and Lyu, Nengchao and Li, Shengbo Eben},
  year={2022},
  journal={Transportation Research Part C: Emerging Technologies},
  doi={10.1016/j.trc.2021.103452}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,Executing,"Knowledge provider, Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-9198,elsevier,Decision support analysis for risk identification and control of patients affected by covid-19 based on bayesian networks,"In the context of the outbreak of coronavirus disease (COVID-19), this paper proposes an innovative and systematic decision support model based on Bayesian networks (BNs) to identify and control the risk of COVID-19 patients spreading the virus, which requires the following three steps. First, by consulting the related literature and combining this with expert knowledge, we identify and classify the characteristics (risk factors) of COVID-19 and obtain a conceptual framework for COVID-19 Risk Assessment Bayesian Networks (CRABNs). Second, data on COVID-19 patients with expert scoring results on patient risk levels were collected from hospitals in Hubei Province of China and are used as the training set, and the structure and parameters of the CRABNs model are obtained through machine learning. Finally, we propose two indicators, namely, Model Bias and Model Accuracy, and use the remaining data to verify the feasibility and effectiveness of the CRABNs model to ensure that there are no significant differences between the predicted results of the model and the actual results provided by experts who have relevant experience in treating COVID-19. At the same time, we compared the CRABNs model with the support vector machine (SVM), random forest (RF), and k-nearest neighbour (KNN) models through four indicators: accuracy, sensitivity, specificity, and F-score. The results suggest the reliability of the model and show that it has promising application potential. The proposed model can be used globally by doctors in hospitals as a decision support tool to improve the accuracy of assessing the severity of COVID-19 symptoms in patients. Furthermore, with the further improvement of the model in the future, it can be used for risk assessments in the field of epidemics.",https://doi.org/10.1016/j.eswa.2022.116547,https://www.sciencedirect.com/science/article/pii/S095741742200046X,Expert Systems with Applications,Jiang Shen;Fusheng Liu;Man Xu;Lipeng Fu;Zhenhe Dong;Jiachao Wu,2022,20,"@article{2-9198,
  title = {Decision support analysis for risk identification and control of patients affected by COVID-19 based on Bayesian networks},
  author = {Shen, Jiang and Liu, Fusheng and Xu, Man and Fu, Lipeng and Dong, Zhenhe and Wu, Jiachao},
  year = {2022},
  journal = {Expert Systems with Applications},
  doi = {10.1016/j.eswa.2022.116547}
}",Methodological contributions,Healthcare / Medicine / Surgery,Organizational,"Advising, Forecasting","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-9202,elsevier,Decision support from financial disclosures with deep neural networks and transfer learning,"Company disclosures greatly aid in the process of financial decision-making; therefore, they are consulted by financial investors and automated traders before exercising ownership in stocks. While humans are usually able to correctly interpret the content, the same is rarely true of computerized decision support systems, which struggle with the complexity and ambiguity of natural language. A possible remedy is represented by deep learning, which overcomes several shortcomings of traditional methods of text mining. For instance, recurrent neural networks, such as long short-term memories, employ hierarchical structures, together with a large number of hidden layers, to automatically extract features from ordered sequences of words and capture highly non-linear relationships such as context-dependent meanings. However, deep learning has only recently started to receive traction, possibly because its performance is largely untested. Hence, this paper studies the use of deep neural networks for financial decision support. We additionally experiment with transfer learning, in which we pre-train the network on a different corpus with a length of 139.1 million words. Our results reveal a higher directional accuracy as compared to traditional machine learning when predicting stock price movements in response to financial disclosures. Our work thereby helps to highlight the business value of deep learning and provides recommendations to practitioners and executives.",https://doi.org/10.1016/j.dss.2017.10.001,https://www.sciencedirect.com/science/article/pii/S0167923617301793,Decision Support Systems,Mathias Kraus;Stefan Feuerriegel,2017,153,"@article{2-9202,
  title={Decision support from financial disclosures with deep neural networks and transfer learning},
  author={Kraus, Mathias and Feuerriegel, Stefan},
  year={2017},
  journal={Decision Support Systems},
  doi={10.1016/j.dss.2017.10.001}
}",Empirical contributions,Finance / Business / Economy,Operational,"Forecasting, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-9204,elsevier,Decision support system for an intelligent operator of utility tunnel boring machines,"In tunnel construction projects, delays entail high costs. Thus, tunnel boring machine (TBM) operators aim for high advance rates without compromising safety, a difficult mission in uncertain subterranean environments. Finding the optimal control parameters based on the TBM sensors' measurements remains an open research question with significant practical relevance. In this paper, we present an intelligent decision support system developed in three steps. First, we propose an optimality score to evaluate TBM operation performance, taking into account the advance rate and the working pressure safety. A deep learning model then learns the mapping between the TBM measurements and this optimality score. Finally, in the context of a real application, the model provides incremental recommendations in order to improve the optimality, taking into account the current setting and measurements of the TBM. The proposed approach is evaluated on a real micro-tunnelling project and demonstrates great promise for future applications.",https://doi.org/10.1016/j.autcon.2021.103880,https://www.sciencedirect.com/science/article/pii/S0926580521003319,Automation in Construction,Gabriel Rodriguez Garcia;Gabriel Michau;Herbert H. Einstein;Olga Fink,2021,0,"@article{2-9204,
  title={Decision support system for an intelligent operator of utility tunnel boring machines},
  author={Rodriguez Garcia, Gabriel and Michau, Gabriel and Einstein, Herbert H. and Fink, Olga},
  year={2021},
  doi={10.1016/j.autcon.2021.103880},
  journal={Automation in Construction}
}",System/Artifact contributions,"Transportation / Mobility / Planning, Manufacturing / Industry / Automation",Operational,"Advising, Forecasting, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-9224,elsevier,Decision-making model for selecting products through online product reviews utilizing natural language processing techniques,"Online product reviews are evaluations of products shared by customers on various online platforms, such as electronic commerce websites, social media, or dedicated review sites. These reviews offer valuable insights and opinions about products, potentially influencing the purchasing decisions of other prospective buyers. However, due to the vast volume of content provided by internet product evaluations, it can be challenging for new buyers to conduct a comprehensive qualitative assessment of competitive products. Utilizing natural language processing techniques, analyzing online product reviews posted on social media platforms can aid customers in making informed purchasing decisions. Hence, this article employs a 2-tuple linguistic q-rung orthopair fuzzy set for product selection based on online product reviews. In this study, we introduce a novel hybrid approach combining the Criteria Importance through Inter-Criteria Correlation (CRITIC) method with the Weighted Aggregated Sum Product Assessment (WASPAS) method. This hybrid method assists potential customers in evaluating alternative products by considering consumer opinions regarding product performance in the 2-tuple linguistic q-rung orthopair fuzzy environment. Moreover, for multi-attribute group decision-making problems, we develop a weighted average aggregation operator based on the 2-tuple linguistic q-rung orthopair fuzzy set. Finally, we apply the proposed approach to a real evaluation decision, validating its validity and practicality through parameter and comparison analyses.",https://doi.org/10.1016/j.neucom.2024.128593,https://www.sciencedirect.com/science/article/pii/S092523122401364X,Neurocomputing,Sumera Naz;Aqsa Shafiq;Shariq Aziz Butt;Rabia Tasneem;Dragan Pamucar;Zhoe Comas Gonzalez,2025,26,"@article{2-9224,
  title = {Decision-making model for selecting products through online product reviews utilizing natural language processing techniques},
  author = {Sumera Naz and Aqsa Shafiq and Shariq Aziz Butt and Rabia Tasneem and Dragan Pamucar and Zhoe Comas Gonzalez},
  year = {2025},
  doi = {10.1016/j.neucom.2024.128593},
  journal = {Neurocomputing}
}",Algorithmic contributions,Finance / Business / Economy,Individual,"Analyzing, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-9289,elsevier,Deep learning for diagnosis and survival prediction in soft tissue sarcoma,"Background Clinical management of soft tissue sarcoma (STS) is particularly challenging. Here, we used digital pathology and deep learning (DL) for diagnosis and prognosis prediction of STS. Patients and methods Our retrospective, multicenter study included a total of 506 histopathological slides from 291 patients with STS. The Cancer Genome Atlas cohort (240 patients) served as training and validation set. A second, multicenter cohort (51 patients) served as an additional test set. The use of the DL model (DLM) as a clinical decision support system was evaluated by nine pathologists with different levels of expertise. For prognosis prediction, 139 slides from 85 patients with leiomyosarcoma (LMS) were used. Area under the receiver operating characteristic (AUROC) and accuracy served as main outcome measures. Results The DLM achieved a mean AUROC of 0.97 (±0.01) and an accuracy of 79.9% (±6.1%) in diagnosing the five most common STS subtypes. The DLM significantly improved the accuracy of the pathologists from 46.3% (±15.5%) to 87.1% (±11.1%). Furthermore, they were significantly faster and more certain in their diagnosis. In LMS, the mean AUROC in predicting the disease-specific survival status was 0.91 (±0.1) and the accuracy was 88.9% (±9.9%). Cox regression showed the DLM’s prediction to be a significant independent prognostic factor (P = 0.008, hazard ratio 5.5, 95% confidence interval 1.56-19.7) in these patients, outperforming other risk factors. Conclusions DL can be used to accurately diagnose frequent subtypes of STS from conventional histopathological slides. It might be used for prognosis prediction in LMS, the most prevalent STS subtype in our cohort. It can also help pathologists to make faster and more accurate diagnoses. This could substantially improve the clinical management of STS patients.",https://doi.org/10.1016/j.annonc.2021.06.007,https://www.sciencedirect.com/science/article/pii/S092375342102055X,Annals of Oncology,S. Foersch;M. Eckstein;D.-C. Wagner;F. Gach;A.-C. Woerl;J. Geiger;C. Glasner;S. Schelbert;S. Schulz;S. Porubsky;A. Kreft;A. Hartmann;A. Agaimy;W. Roth,2021,125,"@article{2-9289,
  title = {Deep Learning for Diagnosis and Survival Prediction in Soft Tissue Sarcoma},
  author = {Foersch, S. and Eckstein, M. and Wagner, D.-C. and Gach, F. and Woerl, A.-C. and Geiger, J. and Glasner, C. and Schelbert, S. and Schulz, S. and Porubsky, S. and Kreft, A. and Hartmann, A. and Agaimy, A. and Roth, W.},
  year = {2021},
  journal = {Annals of Oncology},
  doi = {10.1016/j.annonc.2021.06.007}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject","Alter decision outcomes, Change affective-perceptual, Change cognitive demands",no such info,"recommendations, prediction of alternative",NA,Visual,Yes,Yes
2-9301,elsevier,Deep learning in computed tomography pulmonary angiography imaging: a dual-pronged approach for pulmonary embolism detection,"The increasing reliance on Computed Tomography Pulmonary Angiography (CTPA) for Pulmonary Embolism (PE) diagnosis presents challenges and a pressing need for improved diagnostic solutions. The primary objective of this study is to leverage deep learning techniques to enhance the Computer Assisted Diagnosis (CAD) of PE. With this aim, we propose a classifier-guided detection approach that effectively leverages the classifier’s probabilistic inference to direct the detection predictions, marking a novel contribution in the domain of automated PE diagnosis. Our classification system includes an Attention-Guided Convolutional Neural Network (AG-CNN) that uses local context by employing an attention mechanism. This approach emulates a human expert's attention by looking at both global appearances and local lesion regions before making a decision. The classifier demonstrates robust performance on the FUMPE dataset, achieving an AUROC of 0.927, sensitivity of 0.862, specificity of 0.879, and an F1-score of 0.805 with the Inception-v3 backbone architecture. Moreover, AG-CNN outperforms the baseline DenseNet-121 model, achieving an 8.1% AUROC gain. While previous research has mostly focused on finding PE in the main arteries, our use of cutting-edge object detection models and ensembling techniques greatly improves the accuracy of detecting small embolisms in the peripheral arteries. Finally, our proposed classifier-guided detection approach further refines the detection metrics, contributing new state-of-the-art to the community: mAP50, sensitivity, and F1-score of 0.846, 0.901, and 0.779, respectively, outperforming the former benchmark with a significant 3.7% improvement in mAP50. Our research aims to elevate PE patient care by integrating AI solutions into clinical workflows, highlighting the potential of human-AI collaboration in medical diagnostics.",https://doi.org/10.1016/j.eswa.2023.123029,https://www.sciencedirect.com/science/article/pii/S0957417423035315,Expert Systems with Applications,Fabiha Bushra;Muhammad E.H. Chowdhury;Rusab Sarmun;Saidul Kabir;Menatalla Said;Sohaib {Bassam Zoghoul};Adam Mushtak;Israa Al-Hashimi;Abdulrahman Alqahtani;Anwarul Hasan,2024,17,"@article{2-9301,
  title={Deep learning in computed tomography pulmonary angiography imaging: a dual-pronged approach for pulmonary embolism detection},
  author={Bushra, Fabiha and Chowdhury, Muhammad E.H. and Sarmun, Rusab and Kabir, Saidul and Said, Menatalla and Zoghoul, Sohaib Bassam and Mushtak, Adam and Al-Hashimi, Israa and Alqahtani, Abdulrahman and Hasan, Anwarul},
  year={2024},
  journal={Expert Systems with Applications},
  doi={10.1016/j.eswa.2023.123029}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Forecasting, Advising","Knowledge provider, Guardian, Decision-maker",NA,NA,NA,NA,NA,Yes,No
2-9326,elsevier,Deep learning-assisted identification and quantification of aneurysmal subarachnoid hemorrhage in non-contrast ct scans: development and external validation of hybrid 2d/3d unet,"Accurate stroke assessment and consequent favorable clinical outcomes rely on the early identification and quantification of aneurysmal subarachnoid hemorrhage (aSAH) in non-contrast computed tomography (NCCT) images. However, hemorrhagic lesions can be complex and difficult to distinguish manually. To solve these problems, here we propose a novel Hybrid 2D/3D UNet deep-learning framework for automatic aSAH identification and quantification in NCCT images. We evaluated 1824 consecutive patients admitted with aSAH to four hospitals in China between June 2018 and May 2022. Accuracy and precision, Dice scores and intersection over union (IoU), and interclass correlation coefficients (ICC) were calculated to assess model performance, segmentation performance, and correlations between automatic and manual segmentation, respectively. A total of 1355 patients with aSAH were enrolled: 931, 101, 179, and 144 in four datasets, of whom 326 were scanned with Siemens, 640 with Philips, and 389 with GE Medical Systems scanners. Our proposed deep-learning method accurately identified (accuracies 0.993–0.999) and segmented (Dice scores 0.550–0.897) hemorrhage in both the internal and external datasets, even combinations of hemorrhage subtypes. We further developed a convenient AI-assisted platform based on our algorithm to assist clinical workflows, whose performance was comparable to manual measurements by experienced neurosurgeons (ICCs 0.815–0.957) but with greater efficiency and reduced cost. While this tool has not yet been prospectively tested in clinical practice, our innovative hybrid network algorithm and platform can accurately identify and quantify aSAH, paving the way for fast and cheap NCCT interpretation and a reliable AI-based approach to expedite clinical decision-making for aSAH patients.",https://doi.org/10.1016/j.neuroimage.2023.120321,https://www.sciencedirect.com/science/article/pii/S105381192300472X,NeuroImage,Ping Hu;Haizhu Zhou;Tengfeng Yan;Hongping Miu;Feng Xiao;Xinyi Zhu;Lei Shu;Shuang Yang;Ruiyun Jin;Wenlei Dou;Baoyu Ren;Lizhen Zhu;Wanrong Liu;Yihan Zhang;Kaisheng Zeng;Minhua Ye;Shigang Lv;Miaojing Wu;Gang Deng;Rong Hu;Renya Zhan;Qianxue Chen;Dong Zhang;Xingen Zhu,2023,27,"@article{2-9326,
  title = {Deep learning-assisted identification and quantification of aneurysmal subarachnoid hemorrhage in non-contrast CT scans: development and external validation of hybrid 2D/3D UNet},
  author = {Ping Hu and Haizhu Zhou and Tengfeng Yan and Hongping Miu and Feng Xiao and Xinyi Zhu and Lei Shu and Shuang Yang and Ruiyun Jin and Wenlei Dou and Baoyu Ren and Lizhen Zhu and Wanrong Liu and Yihan Zhang and Kaisheng Zeng and Minhua Ye and Shigang Lv and Miaojing Wu and Gang Deng and Rong Hu and Renya Zhan and Qianxue Chen and Dong Zhang and Xingen Zhu},
  year = {2023},
  doi = {10.1016/j.neuroimage.2023.120321},
  journal = {NeuroImage}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-9347,elsevier,Deep learning–enabled diagnosis of liver adenocarcinoma,"Background & Aims Diagnosis of adenocarcinoma in the liver is a frequent scenario in routine pathology and has a critical impact on clinical decision making. However, rendering a correct diagnosis can be challenging, and often requires the integration of clinical, radiologic, and immunohistochemical information. We present a deep learning model (HEPNET) to distinguish intrahepatic cholangiocarcinoma from colorectal liver metastasis, as the most frequent primary and secondary forms of liver adenocarcinoma, with clinical grade accuracy using H&E-stained whole-slide images. Methods HEPNET was trained on 714,589 image tiles from 456 patients who were randomly selected in a stratified manner from a pool of 571 patients who underwent surgical resection or biopsy at Heidelberg University Hospital. Model performance was evaluated on a hold-out internal test set comprising 115 patients and externally validated on 159 patients recruited at Mainz University Hospital. Results On the hold-out internal test set, HEPNET achieved an area under the receiver operating characteristic curve of 0.994 (95% CI, 0.989–1.000) and an accuracy of 96.522% (95% CI, 94.521%–98.694%) at the patient level. Validation on the external test set yielded an area under the receiver operating characteristic curve of 0.997 (95% CI, 0.995–1.000), corresponding to an accuracy of 98.113% (95% CI, 96.907%–100.000%). HEPNET surpassed the performance of 6 pathology experts with different levels of experience in a reader study of 50 patients (P = .0005), boosted the performance of resident pathologists to the level of senior pathologists, and reduced potential downstream analyses. Conclusions We provided a ready-to-use tool with clinical grade performance that may facilitate routine pathology by rendering a definitive diagnosis and guiding ancillary testing. The incorporation of HEPNET into pathology laboratories may optimize the diagnostic workflow, complemented by test-related labor and cost savings.",https://doi.org/10.1053/j.gastro.2023.07.026,https://www.sciencedirect.com/science/article/pii/S0016508523048837,Gastroenterology,Thomas Albrecht;Annik Rossberg;Jana Dorothea Albrecht;Jan Peter Nicolay;Beate Katharina Straub;Tiemo Sven Gerber;Michael Albrecht;Fritz Brinkmann;Alphonse Charbel;Constantin Schwab;Johannes Schreck;Alexander Brobeil;Christa Flechtenmacher;Moritz {von Winterfeld};Bruno Christian Köhler;Christoph Springfeld;Arianeb Mehrabi;Stephan Singer;Monika Nadja Vogel;Olaf Neumann;Albrecht Stenzinger;Peter Schirmacher;Cleo-Aron Weis;Stephanie Roessler;Jakob Nikolas Kather;Benjamin Goeppert,2023,24,"@article{2-9347,
  title = {Deep learning–enabled diagnosis of liver adenocarcinoma},
  author = {Thomas Albrecht and Annik Rossberg and Jana Dorothea Albrecht and Jan Peter Nicolay and Beate Katharina Straub and Tiemo Sven Gerber and Michael Albrecht and Fritz Brinkmann and Alphonse Charbel and Constantin Schwab and Johannes Schreck and Alexander Brobeil and Christa Flechtenmacher and Moritz {von Winterfeld} and Bruno Christian Köhler and Christoph Springfeld and Arianeb Mehrabi and Stephan Singer and Monika Nadja Vogel and Olaf Neumann and Albrecht Stenzinger and Peter Schirmacher and Cleo-Aron Weis and Stephanie Roessler and Jakob Nikolas Kather and Benjamin Goeppert},
  year = {2023},
  doi = {10.1053/j.gastro.2023.07.026},
  journal = {Gastroenterology}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Analyzing, Forecasting","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-9396,elsevier,Deep reinforcement learning-based decision support system for transportation infrastructure management under hurricane events,"The high wind, large surge/wave and heavy rainfall during a hurricane may significantly impact the performance of transportation infrastructure and associated traffic safety. Accordingly, stakeholders need to make a sequence of decisions to close (or restrict) the traffic of vulnerable components in the transportation network (e.g., aerodynamics-sensitive long-span bridges, hydrodynamics-sensitive coastal bridges and inundation-sensitive road segments) for the balance of safety and mobility. Due to large uncertainty involved in hurricane weather and traffic conditions, it is essentially a stochastic sequential decision problem and can be effectively formulated as a Markov decision process. This study proposes a deep reinforcement learning (RL)-based decision support system for stakeholders to optimally manage these critical components for the purpose of minimizing the network-level losses induced by hurricanes. Specifically, the RL policy (i.e., mapping from high-dimensional continuous traffic/weather information to management decisions) is represented by a deep neural network (DNN) and the optimal decision (corresponding to a set of DNN weights) is obtained using the deep Q learning algorithm. A case study based on a hypothetical traffic network is utilized to demonstrate the good performance of the developed deep RL-based decision support system in the application of transportation infrastructure management under hurricane conditions.",https://doi.org/10.1016/j.strusafe.2022.102254,https://www.sciencedirect.com/science/article/pii/S0167473022000637,Structural Safety,Shaopeng Li;Teng Wu,2022,24,"@article{2-9396,
  title = {Deep reinforcement learning-based decision support system for transportation infrastructure management under hurricane events},
  author = {Shaopeng Li and Teng Wu},
  year = {2022},
  doi = {10.1016/j.strusafe.2022.102254},
  journal = {Structural Safety}
}",System/Artifact contributions,"Defense / Military / Emergency, Transportation / Mobility / Planning",Organizational,"Advising, Executing","Decision-maker, Guardian, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-9416,elsevier,Deepad: an integrated decision-making framework for intelligent autonomous driving,"Autonomous vehicles have the potential to revolutionize intelligent transportation by improving traffic safety, increasing energy efficiency, and reducing congestion. In this study, a novel framework termed DeepAD was proposed and validated for decision making in intelligent autonomous driving via deep reinforcement learning. This framework incorporates multiple driving objectives such as efficiency, safety, and comfort to make informed decisions regarding autonomous vehicles (AVs). The decision-making process utilizes the origin–destination information for macrolevel routing and determines microlevel car-following and lane-changing behaviors. The lane-changing behavior is discretized and learned through a deep Q-network, and the continuous car-following behavior is learned through a deep deterministic policy gradient. Comprehensive simulation experiments on a real-world network demonstrated that DeepAD outperformed human driving while maintaining a desirable level of efficiency, safety, and comfort. In the real-world road networks experiment, multiple indexes of vehicles in the high AVs penetration rate group significantly outperformed that of the group with lower AVs penetration rate. Overall, the proposed framework provides insights into intelligent autonomous driving to improve urban mobility.",https://doi.org/10.1016/j.tra.2024.104069,https://www.sciencedirect.com/science/article/pii/S0965856424001174,Transportation Research Part A: Policy and Practice,Yunyang Shi;Jinghan Liu;Chengqi Liu;Ziyuan Gu,2024,3,"@article{2-9416,
  title={DeepAD: An Integrated Decision-Making Framework for Intelligent Autonomous Driving},
  author={Shi, Yunyang and Liu, Jinghan and Liu, Chengqi and Gu, Ziyuan},
  year={2024},
  doi={10.1016/j.tra.2024.104069},
  journal={Transportation Research Part A: Policy and Practice}
}",System/Artifact contributions,Transportation / Mobility / Planning,Individual,Executing,"Guardian, Stakeholder, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-9443,elsevier,Delegation of purchasing tasks to ai: the role of perceived choice and decision autonomy,"Although artificial intelligence (AI) outperforms humans in many tasks, research suggests some consumers are still averse to having AI perform tasks on their behalf. Informed by the literature of customer decision-making process, we propose and show that consumer autonomy is a significant predictor of customers' decision to adopt AI in the purchasing context. Across three experiments, we found that the delegation of purchasing tasks to AI, which restricts choice and decision dimensions of consumers' perceived autonomy, reduces the likelihood of AI adoption. Our results show that the effects of choice and decision autonomy on AI adoption holds even when product choice evaluation is complex. We also found that identity-relevant consumption moderates this relationship, such that it interacts with choice and decision autonomy. Specifically, despite lacking choice and decision autonomy, those who identify strongly with a given activity are more likely to use an AI-enabled app to purchase the product needed to perform this activity. With these insights into when and why consumers are likely to use AI-enabled technology, firms might effectively increase its adoption.",https://doi.org/10.1016/j.dss.2023.114166,https://www.sciencedirect.com/science/article/pii/S0167923623002415,Decision Support Systems,Mariyani {Ahmad Husairi};Patricia Rossi,2024,50,"@article{2-9443,
  title     = {Delegation of purchasing tasks to AI: The role of perceived choice and decision autonomy},
  author    = {Mariyani {Ahmad Husairi} and Patricia Rossi},
  year      = {2024},
  doi       = {10.1016/j.dss.2023.114166},
  journal   = {Decision Support Systems}
}",Empirical contributions,Finance / Business / Economy,Individual,Executing,Decision-subject,"Alter decision outcomes, Restrict human agency",no such info,NA,"choice autonomy, decision autonomy, more likely to adopt the AI-enabled technology when choice and decision autonomy are present",Interactive interface,Yes,Yes
2-9540,elsevier,Developing a virtual reality healthcare product based on data-driven concepts: a case study,"The rapid development of artificial intelligence (AI) and big data technologies has profoundly changed the way of human life and work, and in the healthcare field, the related product design is moving towards the trend of digitalization, intelligence, and personalization. The growing demands for personalised healthcare have contributed to a boom in product design theories in related fields. However, it is difficult to measure product users' preferences in precise numerical terms to make strategic decisions in product design. To fill this gap, this study proposes a framework for virtual reality (VR) healthcare product design based on a data-driven concept and develops a VR virtual roaming system for system users guided by this theory. The data-driven concept incorporates AI and big data technologies into product design, helping to shift the design of medical products towards a data-centred approach. Ultimately, an ergonomic experiment was conducted and a case study was carried out to assess the user's experience using brain function data to enable data-driven VR value-added services. This study provides a new perspective on rehabilitation product design, opening the way for better personalised healthcare delivery.",https://doi.org/10.1016/j.aei.2023.102118,https://www.sciencedirect.com/science/article/pii/S147403462300246X,Advanced Engineering Informatics,Jing Qu;Yinuo Zhang;Weizhong Tang;Wenming Cheng;Yu Zhang;Lingguo Bu,2023,22,"@article{2-9540,
  title={Developing a virtual reality healthcare product based on data-driven concepts: a case study},
  author={Qu, Jing and Zhang, Yinuo and Tang, Weizhong and Cheng, Wenming and Zhang, Yu and Bu, Lingguo},
  year={2023},
  journal={Advanced Engineering Informatics},
  doi={10.1016/j.aei.2023.102118}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Analyzing, Advising","Decision-maker, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-9542,elsevier,Developing ai enabled sensors and decision support for military operators in the field,"Wearable sensors enable down range data collection of physiological and cognitive performance of the warfighter. However, autonomous teams may find the sensor data impractical to interpret and hence influence real-time decisions without the support of subject matter experts. Decision support tools can reduce the burden of interpreting physiological data in the field and incorporate a systems perspective where noisy field data can contain useful additional signals. We present a methodology of how artificial intelligence can be used for modeling human performance with decision-making to achieve actionable decision support. We provide a framework for systems design and advancing from the laboratory to real world environments. The result is a validated measure of down-range human performance with a low burden of operation.",https://doi.org/10.1016/j.jsams.2023.03.001,https://www.sciencedirect.com/science/article/pii/S1440244023000397,Journal of Science and Medicine in Sport,Brian K. Russell;Josh McGeown;Bettina L. Beard,2023,8,"@article{2-9542,
  title = {Developing AI Enabled Sensors and Decision Support for Military Operators in the Field},
  author = {Brian K. Russell and Josh McGeown and Bettina L. Beard},
  year = {2023},
  doi = {10.1016/j.jsams.2023.03.001},
  journal = {Journal of Science and Medicine in Sport}
}",Methodological contributions,Defense / Military / Emergency,Operational,"Advising, Forecasting, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-9556,elsevier,"Developing sustainable, resilient, and responsive biofuel production and distribution management system: a neutrosophic fuzzy optimization approach based on artificial intelligence and geographic information systems","Amidst the surging energy demand, contemporary biofuel production and distribution systems face the challenge of being sustainable, resilient, and responsive to mitigate environmental impact, withstand disruption, and meet regulatory requirements while remaining competitive in the market. However, existing literature predominantly focuses on sustainability alone, leaving a research gap in understanding the complex relationship among sustainability, resilience, and responsiveness within biofuel production systems. This study aims to bridge this gap by proposing a novel approach that integrates sustainability, resilience, and responsiveness within the (lean, agile, responsive, green) LARG framework through the development of a decision-making system. To mitigate operational uncertainty in decision-making, the study employs neutrosophic fuzzy optimization (NFO), support vector regression (SVR) for biomass supply prediction, and geographic information systems (GIS) for potential biorefinery site identification. The proposed model and solution approach undergo validation in a real case study, accompanied by sensitivity analyses. Key findings reveal a substantial influence of the decision maker's risk attitude on strategic decisions in the biofuel production system, with risk-averse approaches resulting in lower objective attainment levels. Additionally, strategic decisions concerning biorefinery locations emerge as critical factors in the biofuel production and distribution system, significantly affecting both upstream and downstream operations and the attainment levels of sustainability, resilience, and responsiveness dimensions. Furthermore, the study observes that introducing redundancy to the biodiesel production network without adequate planning may not necessarily enhance resilience. However, reinforcing critical nodes with redundancy proves to be a significant factor in improving overall network resilience, hence withstanding disruptive events. Biofuel producers, along with policymakers, regulatory bodies, and related stakeholders, stand to potentially benefit from this research.",https://doi.org/10.1016/j.apenergy.2024.123683,https://www.sciencedirect.com/science/article/pii/S0306261924010663,Applied Energy,Muhammad Salman Habib;Seung-June Hwang,2024,24,"@article{2-9556,
  title={Developing sustainable, resilient, and responsive biofuel production and distribution management system: a neutrosophic fuzzy optimization approach based on artificial intelligence and geographic information systems},
  author={Habib, Muhammad Salman and Hwang, Seung-June},
  year={2024},
  journal={Applied Energy},
  doi={10.1016/j.apenergy.2024.123683}
}",System/Artifact contributions,Environment / Resources / Energy,Organizational,"Analyzing, Auditing, Forecasting, Advising","Decision-maker, Guardian, Stakeholder",NA,NA,NA,NA,NA,Yes,No
2-9574,elsevier,"Development and validation of a machine learning–based, point-of-care risk calculator for post-ercp pancreatitis and prophylaxis selection","Background and Aims A robust model of post-ERCP pancreatitis (PEP) risk is not currently available. We aimed to develop a machine learning–based tool for PEP risk prediction to aid in clinical decision making related to periprocedural prophylaxis selection and postprocedural monitoring. Methods Feature selection, model training, and validation were performed using patient-level data from 12 randomized controlled trials. A gradient-boosted machine (GBM) model was trained to estimate PEP risk, and the performance of the resulting model was evaluated using the area under the receiver operating curve (AUC) with 5-fold cross-validation. A web-based clinical decision-making tool was created, and a prospective pilot study was performed using data from ERCPs performed at the Johns Hopkins Hospital over a 1-month period. Results A total of 7389 patients were included in the GBM with an 8.6% rate of PEP. The model was trained on 20 PEP risk factors and 5 prophylactic interventions (rectal nonsteroidal anti-inflammatory drugs [NSAIDs], aggressive hydration, combined rectal NSAIDs and aggressive hydration, pancreatic duct stenting, and combined rectal NSAIDs and pancreatic duct stenting). The resulting GBM model had an AUC of 0.70 (65% specificity, 65% sensitivity, 95% negative predictive value, and 15% positive predictive value). A total of 135 patients were included in the prospective pilot study, resulting in an AUC of 0.74. Conclusions This study demonstrates the feasibility and utility of a novel machine learning–based PEP risk estimation tool with high negative predictive value to aid in prophylaxis selection and identify patients at low risk who may not require extended postprocedure monitoring.",https://doi.org/10.1016/j.gie.2024.08.009,https://www.sciencedirect.com/science/article/pii/S0016510724034175,Gastrointestinal Endoscopy,Todd Brenner;Albert Kuo;Christina J. {Sperna Weiland};Ayesha Kamal;B. Joseph Elmunzer;Hui Luo;James Buxbaum;Timothy B. Gardner;Shaffer S. Mok;Evan S. Fogel;Veit Phillip;Jun-Ho Choi;Guan W. Lua;Ching-Chung Lin;D. Nageshwar Reddy;Sundeep Lakhtakia;Mahesh K. Goenka;Rakesh Kochhar;Mouen A. Khashab;Erwin J.M. {van Geenen};Vikesh K. Singh;Cristian Tomasetti;Venkata S. Akshintala,2024,11,"@article{2-9574,
  title={Development and validation of a machine learning--based, point-of-care risk calculator for post-ercp pancreatitis and prophylaxis selection},
  author={Brenner, Todd and Kuo, Albert and Sperna Weiland, Christina J. and Kamal, Ayesha and Elmunzer, B. Joseph and Luo, Hui and Buxbaum, James and Gardner, Timothy B. and Mok, Shaffer S. and Fogel, Evan S. and Phillip, Veit and Choi, Jun-Ho and Lua, Guan W. and Lin, Ching-Chung and Reddy, D. Nageshwar and Lakhtakia, Sundeep and Goenka, Mahesh K. and Kochhar, Rakesh and Khashab, Mouen A. and van Geenen, Erwin J.M. and Singh, Vikesh K. and Tomasetti, Cristian and Akshintala, Venkata S.},
  year={2024},
  doi={10.1016/j.gie.2024.08.009},
  journal={Gastrointestinal Endoscopy}
}",System/Artifact contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting, Analyzing","Knowledge provider, Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-9578,elsevier,Development and validation of a natural language processing model to identify low-risk pulmonary embolism in real time to facilitate safe outpatient management,"Study objective This study aimed to (1) develop and validate a natural language processing model to identify the presence of pulmonary embolism (PE) based on real-time radiology reports and (2) identify low-risk PE patients based on previously validated risk stratification scores using variables extracted from the electronic health record at the time of diagnosis. The combination of these approaches yielded an natural language processing-based clinical decision support tool that can identify patients presenting to the emergency department (ED) with low-risk PE as candidates for outpatient management. Methods Data were curated from all patients who received a PE-protocol computed tomography pulmonary angiogram (PE-CTPA) imaging study in the ED of a 3-hospital academic health system between June 1, 2018 and December 31, 2020 (n=12,183). The “preliminary” radiology reports from these imaging studies made available to ED clinicians at the time of diagnosis were adjudicated as positive or negative for PE by the clinical team. The reports were then divided into development, internal validation, and temporal validation cohorts in order to train, test, and validate an natural language processing model that could identify the presence of PE based on unstructured text. For risk stratification, patient- and encounter-level data elements were curated from the electronic health record and used to compute a real-time simplified pulmonary embolism severity (sPESI) score at the time of diagnosis. Chart abstraction was performed on all low-risk PE patients admitted for inpatient management. Results When applied to the internal validation and temporal validation cohorts, the natural language processing model identified the presence of PE from radiology reports with an area under the receiver operating characteristic curve of 0.99, sensitivity of 0.86 to 0.87, and specificity of 0.99. Across cohorts, 10.5% of PE-CTPA studies were positive for PE, of which 22.2% were classified as low-risk by the sPESI score. Of all low-risk PE patients, 74.3% were admitted for inpatient management. Conclusion This study demonstrates that a natural language processing-based model utilizing real-time radiology reports can accurately identify patients with PE. Further, this model, used in combination with a validated risk stratification score (sPESI), provides a clinical decision support tool that accurately identifies patients in the ED with low-risk PE as candidates for outpatient management.",https://doi.org/10.1016/j.annemergmed.2024.01.036,https://www.sciencedirect.com/science/article/pii/S019606442400074X,Annals of Emergency Medicine,Krunal D. Amin;Elizabeth Hope Weissler;William Ratliff;Alexander E. Sullivan;Tara A. Holder;Cathleen Bury;Samuel Francis;Brent Jason Theiling;Bradley Hintze;Michael Gao;Marshall Nichols;Suresh Balu;William Schuyler Jones;Mark Sendak,2024,7,"@article{2-9578,
  title={Development and validation of a natural language processing model to identify low-risk pulmonary embolism in real time to facilitate safe outpatient management},
  author={Amin, Krunal D. and Weissler, Elizabeth Hope and Ratliff, William and Sullivan, Alexander E. and Holder, Tara A. and Bury, Cathleen and Francis, Samuel and Theiling, Brent Jason and Hintze, Bradley and Gao, Michael and Nichols, Marshall and Balu, Suresh and Jones, William Schuyler and Sendak, Mark},
  year={2024},
  journal={Annals of Emergency Medicine},
  doi={10.1016/j.annemergmed.2024.01.036}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-9618,elsevier,Development of a survey-based stacked ensemble predictive model for autonomy preferences in patients with periodontal disease,"Objectives This study aimed to develop a model to predict the autonomy preference (AP) and satisfaction after tooth extraction (STE) in patients with periodontal disease. Understanding of individual AP and STE is essential for improving patient satisfaction and promoting informed decision-making in periodontics. Methods A stacked ensemble machine learning model was used to predict patient AP and STE based on the results of a survey that included demographic information, oral health status, AP index, and STE. Data from 421 patients with periodontal disease were collected from two university dental hospitals and evaluated for ensemble modeling in the following predictive models: random forest, naïve Bayes, gradient boost, adaptive boost, and XGBoost. Results The models demonstrated good predictive performance, with XGBoost demonstrating the highest accuracy for both AP (0.78) and STE (0.80). The results showed that only 7.6 % of patients had high AP, which tended to decrease with age and varied significantly according to education level and severity of treatment, categorized as supportive periodontal treatment, active periodontal treatment, or extraction and/or dental implant procedures. Additionally, the majority of patients (67.7 %) reported high STE levels, highlighting the effectiveness of the model in accurately predicting AP, which was further supported by the significant correlation between accurately predicted AP levels and high STE outcomes. Conclusions The successful utilization of a stacked ensemble model to predict patient AP and STE demonstrates the potential of machine learning to improve patient-centered care in periodontics. Future research should extend to more diverse patient populations and clinical conditions to validate and refine the predictive abilities of such models in broader healthcare settings. Clinical significance The machine learning-based predictive model effectively enhances personalized decision-making and improves patient satisfaction in periodontal treatment.",https://doi.org/10.1016/j.jdent.2024.105467,https://www.sciencedirect.com/science/article/pii/S0300571224006377,Journal of Dentistry,So-Hae Oh;Jae-Hong Lee;Ji-Youn Hong;Ji-Young Jung;Kyung-A Ko;Jung-Seok Lee,2025,1,"@article{2-9618,
  title={Development of a survey-based stacked ensemble predictive model for autonomy preferences in patients with periodontal disease},
  author={So-Hae Oh and Jae-Hong Lee and Ji-Youn Hong and Ji-Young Jung and Kyung-A Ko and Jung-Seok Lee},
  year={2025},
  journal={Journal of Dentistry},
  doi={10.1016/j.jdent.2024.105467}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Advising, Forecasting","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-9630,elsevier,Development of decision support tool for optimizing urban emergency rescue facility locations to improve humanitarian logistics management,"Emergency rescue facility is an essential component of urban emergency logistics system, and selection of their locations is significant for urban public safety. Urban emergency rescue facility locations (UERFLs) problem is essentially a geospatial multi-objective optimization problem (Geospatial-MOP), which presents a challenge for both researchers and managers. In this study, a user-friendly decision support tool was designed and developed for facilitating the process of optimizing UERFLs in large-scale urban areas. We described the design, architecture and implementation of the tool and its core optimization component. Based on a hypothetical case study, we introduced its functionalities as well as the decision making workflow. The results provide evidences that the tool can successfully generate Pareto-optimal frontier and capture a pool of alternative solutions to the decision maker for trade-off. This work offers new insights on promoting future urban emergency logistics management with the use of GIS and emerging artificial intelligence technologies, and makes contributions in integrating multi-objective optimization algorithm with GIS for solving geospatial multi-objective optimization problem.",https://doi.org/10.1016/j.ssci.2017.10.007,https://www.sciencedirect.com/science/article/pii/S0925753517312432,Safety Science,Ming Zhao;Xiang Liu,2018,53,"@article{2-9630,
  title = {Development of decision support tool for optimizing urban emergency rescue facility locations to improve humanitarian logistics management},
  author = {Ming Zhao and Xiang Liu},
  year = {2018},
  journal = {Safety Science},
  doi = {10.1016/j.ssci.2017.10.007}
}",System/Artifact contributions,Defense / Military / Emergency,Organizational,"Advising, Analyzing","Decision-maker, Guardian",NA,NA,NA,NA,NA,Yes,No
2-970,aaai,Real-Time Driver-Request Assignment in Ridesourcing,"Online on-demand ridesourcing service has played a huge role in transforming urban transportation. A central function in most on-demand ridesourcing platforms is to dynamically assign drivers to rider requests that could balance the request waiting times and the driver pick-up distances. To deal with the online nature of this problem, existing literature either divides the time horizon into short windows and applies a static offline assignment algorithm within each window or assumes a fully online setting that makes decisions for each request immediately upon its arrival. In this paper, we propose a more realistic model for the driver-request assignment that bridges the above two settings together. Our model allows the requests to wait after their arrival but assumes that they may leave at any time following a quitting function. Under this model, we design an efficient algorithm for assigning available drivers to requests in real-time. Our algorithm is able to incorporate future estimated driver arrivals into consideration and make strategic waiting and matching decisions that could balance the waiting time and pick-up distance of the assignment. We prove that our algorithm is optimal ex-ante in the single-request setting, and demonstrate its effectiveness in the general multi-request setting through experiments on both synthetic and real-world datasets.",10.1609/aaai.v36i4.20299,https://ojs.aaai.org/index.php/AAAI/article/view/20299,AAAI Conference on Artificial Intelligence,Hao Wang;Xiaohui Bei,2022,63,"@inproceedings{2-970,
  title={Real-Time Driver-Request Assignment in Ridesourcing},
  author={Wang, Hao and Bei, Xiaohui},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2022},
  doi={10.1609/aaai.v36i4.20299}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,"Analyzing, Executing, Monitoring","Developer, Guardian, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-9748,elsevier,Divide and conquer: a granular concept-cognitive computing system for dynamic classification decision making,"Dynamic classification decision making is a crucial issue in management decision making and data mining, which is widely applied in different areas such as human-machine collaborative decision making, network intrusion detection, and traffic data stream mining. However, the existing strategies of static classification decision making are always unable to achieve desired outcomes in ill-structured domains, as the standard machine learning approaches mainly focus on static learning, which is not suitable to mine evolving dynamic data to support decision making. In addition, the main factors regarding incorrect classification predictions are also important for knowledge management and decision making, which is often ignored in many standard learning systems. Therefore, inspired by the idea of divide and conquer, we in this article propose a novel dynamic concept learning framework, namely granular concept-cognitive computing system (gC3S), for dynamic classification decision making by transforming instances into concepts. More specifically, to better characterize the process of dynamic classification decision making, we give the objective function of gC3S via mathematical programming theory. For management decision making, gC3S emphasizes tracing the corresponding approximate concepts via the incorrect classification predictions. Finally, we also apply gC3S to traffic data stream mining, and the experimental results on the different real-world situations further demonstrate that our approach is very effective for dynamic classification decision making.",https://doi.org/10.1016/j.ejor.2022.12.018,https://www.sciencedirect.com/science/article/pii/S0377221722009663,European Journal of Operational Research,Yunlong Mi;Zongrun Wang;Hui Liu;Yi Qu;Gaofeng Yu;Yong Shi,2023,18,"@article{2-9748,
  title = {Divide and Conquer: A Granular Concept-Cognitive Computing System for Dynamic Classification Decision Making},
  author = {Yunlong Mi and Zongrun Wang and Hui Liu and Yi Qu and Gaofeng Yu and Yong Shi},
  year = {2023},
  journal = {European Journal of Operational Research},
  doi = {10.1016/j.ejor.2022.12.018}
}",Algorithmic contributions,"Transportation / Mobility / Planning, Generic / Abstract / Domain-agnostic",Operational,"Forecasting, Analyzing",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-9763,elsevier,Does the use of synchrony and artificial intelligence in video interviews affect interview ratings and applicant attitudes?,"The use of asynchronous video interviews (AVIs) and artificial intelligence (AI)-based decision agents enables more efficient employment screening compared with traditional synchronous video interviews (SVIs). However, the social impacts of using synchrony and AI decision agents in video interviews have not been investigated. Drawing on media richness theory and social interface theory, this study employed a novel experimental design to compare human ratings and job applicants' response behaviours between the SVI and AVI settings and compare job applicants’ fairness perception between the AVI setting and the AVI setting using an AI decision agent (AVI-AI). The results from 180 observations revealed that 1) first impression and physical appearance significantly affected structured interview ratings regardless of whether the video interview was synchronous; 2) compared with SVIs, AVIs lessened the primacy effect on physical appearance and initial impression among human raters; 3) job applicants had indistinguishable fairness perceptions regarding synchrony (SVI vs. AVI) and decision agent (human vs. AI); and 4) applicants exhibited less favourability towards AVIs than towards SVIs. Findings from this experimental comparison, including recommendations for practice and future research on human resource selection, technology education, and social computing, are discussed.",https://doi.org/10.1016/j.chb.2019.04.012,https://www.sciencedirect.com/science/article/pii/S0747563219301529,Computers in Human Behavior,Hung-Yue Suen;Mavis Yi-Ching Chen;Shih-Hao Lu,2019,53,"@article{2-9763,
  title={Does the use of synchrony and artificial intelligence in video interviews affect interview ratings and applicant attitudes?},
  author={Suen, Hung-Yue and Chen, Mavis Yi-Ching and Lu, Shih-Hao},
  year={2019},
  journal={Computers in Human Behavior},
  doi={10.1016/j.chb.2019.04.012}
}",Empirical contributions,Everyday / Employment / Public Service,Operational,"Executing, Analyzing, Advising","Decision-maker, Decision-subject","Change affective-perceptual, Alter decision outcomes",no such info,NA,NA,NA,Yes,Yes
2-9771,elsevier,Dqn-based ethical decision-making for self-driving cars in unavoidable crashes: an applied ethical knob,"Ethical decision-making in complex urban driving scenarios remains a challenge, particularly when human lives are at risk. This paper presents an ethical decision-making model for self-driving cars in critical urban traffic situations, utilizing deep reinforcement learning (DQN algorithm). The primary objective is to minimize injuries resulting from self-driving car crashes. Crash injury severity prediction models are incorporated to design a proper reward function as well as accounting for the age groups of individuals involved in the crash. It explores seven levels from egoism to altruism, investigating the adjustability and sensitivity of the “ethical knob” concept, as well as the responsibilities of manufacturers, potential risks to passengers and other individuals at the crash scene. The model also incorporates constraints on vehicle longitudinal and lateral accelerations to ensure appropriate vehicle movement toward the collision target. Results indicate that the agent attempts to avoid hazardous situations whenever possible, prioritizing minor injuries when escape is impossible. In the most egoistic mode, the agent prioritizes passenger safety, while the highest level of altruism allows for potential harm to passengers to save those outside the vehicle. The results of the uncertainty analysis indicate that the model presented in this study is robust to noise in the environmental input and dropout in the convolutional neural network; although it should be adjusted for more sensitive conditions. The proposed model offers a practical framework for ethical decision-making in self-driving cars. The code is available at: https://github.com/ehsan-vakili/DQN-ethical-decision-making-self-driving-cars.",https://doi.org/10.1016/j.eswa.2024.124569,https://www.sciencedirect.com/science/article/pii/S0957417424014362,Expert Systems with Applications,Ehsan Vakili;Abdollah Amirkhani;Behrooz Mashadi,2024,28,"@article{2-9771,
  title = {Dqn-based ethical decision-making for self-driving cars in unavoidable crashes: an applied ethical knob},
  author = {Vakili, Ehsan and Amirkhani, Abdollah and Mashadi, Behrooz},
  year = {2024},
  doi = {10.1016/j.eswa.2024.124569},
  journal = {Expert Systems with Applications}
}",Algorithmic contributions,Transportation / Mobility / Planning,Individual,"Forecasting, Explaining, Executing","Stakeholder, Decision-subject, Guardian, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-98,aaai,An Autonomous Override System to Prevent Airborne Loss of Control,"Loss of Control( LOC) is the most common precursor to aircraft accidents. This paper presents a Flight Safety Assessment and Management( FSAM) decision system to reduce in-flight LOC risk. FSAM nominally serves as a monitor to detect conditions that pose LOC risk, automatically activating the appropriate control authority if necessary to prevent LOC and restore a safe operational state. This paper contributes an efficient Markov Decision Process( MDP) formulation for FSAM. The state features capture risk associated with aircraft dynamics, configuration, health, pilot behavior and weather. The reward function trades cost of inaction against the cost of overriding the current control authority. A sparse sampling algorithm obtains a near-optimal solution for the MDP online. This approach enables the FSAM MDP to incorporate dynamically changing flight envelope and environment constraints into decision-making. Case studies based on realworld aviation incidents are presented.",10.1609/aaai.v30i2.19074,https://ojs.aaai.org/index.php/AAAI/article/view/19074,AAAI Conference on Artificial Intelligence,Sweewarman Balachandran;Ella M. Atkins,2016,6,"@inproceedings{2-98,
  title = {An Autonomous Override System to Prevent Airborne Loss of Control},
  author = {Sweewarman Balachandran and Ella M. Atkins},
  year = {2016},
  doi = {10.1609/aaai.v30i2.19074},
  booktitle = {AAAI Conference on Artificial Intelligence}
}",Algorithmic contributions,Transportation / Mobility / Planning,Operational,"Executing, Monitoring","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-981,aaai,Regression under Human Assistance,"Decisions are increasingly taken by both humans and machine learning models. However, machine learning models are currently trained for full automation—they are not aware that some of the decisions may still be taken by humans. In this paper, we take a first step towards the development of machine learning models that are optimized to operate under different automation levels. More specifically, we first introduce the problem of ridge regression under human assistance and show that it is NP-hard. Then, we derive an alternative representation of the corresponding objective function as a difference of nondecreasing submodular functions. Building on this representation, we further show that the objective is nondecreasing and satisfies α-submodularity, a recently introduced notion of approximate submodularity. These properties allow a simple and efficient greedy algorithm to enjoy approximation guarantees at solving the problem. Experiments on synthetic and real-world data from two important applications—medical diagnosis and content moderation—demonstrate that the greedy algorithm beats several competitive baselines.",10.1609/aaai.v34i03.5645,https://ojs.aaai.org/index.php/AAAI/article/view/5645,AAAI Conference on Artificial Intelligence,Abir De;Paramita Koley;Niloy Ganguly;Manuel Gomez-Rodriguez,2020,91,"@inproceedings{2-981,
  title     = {Regression under Human Assistance},
  author    = {Abir De and Paramita Koley and Niloy Ganguly and Manuel Gomez-Rodriguez},
  year      = {2020},
  doi       = {10.1609/aaai.v34i03.5645},
  booktitle = {AAAI Conference on Artificial Intelligence}
}",Theoretical contributions,"Generic / Abstract / Domain-agnostic, Healthcare / Medicine / Surgery, Media / Communication / Entertainment",Operational,"Collaborating, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-9832,elsevier,Dynamic prediction of financial distress using malmquist dea,"Creditors such as banks frequently use expert systems to support their decisions when issuing loans and credit assessment has been an important area of application of machine learning techniques for decades. In practice, banks are often required to provide the rationale behind their decisions in addition to being able to predict the performance of companies when assessing corporate applicants for loans. One solution is to use Data Envelopment Analysis (DEA) to evaluate multiple decision-making units (DMUs or companies) which are ranked according to the best practice in their industrial sector. A linear programming algorithm is employed to calculate corporate efficiency as a measure to distinguish healthy companies from those in financial distress. This paper extends the cross-sectional DEA models to time-varying Malmquist DEA, since dynamic predictive models allow one to incorporate changes over time. This decision-support system can adjust the efficiency frontier intelligently over time and make robust predictions. Results based on a sample of 742 Chinese listed companies observed over 10 years suggest that Malmquist DEA offers insights into the competitive position of a company in addition to accurate financial distress predictions based on the DEA efficiency measures.",https://doi.org/10.1016/j.eswa.2017.03.017,https://www.sciencedirect.com/science/article/pii/S095741741730163X,Expert Systems with Applications,Zhiyong Li;Jonathan Crook;Galina Andreeva,2017,6,"@article{2-9832,
  title={Dynamic prediction of financial distress using Malmquist DEA},
  author={Li, Zhiyong and Crook, Jonathan and Andreeva, Galina},
  year={2017},
  journal={Expert Systems with Applications},
  volume={87},
  pages={237--248},
  doi={10.1016/j.eswa.2017.03.017}
}",Algorithmic contributions,Finance / Business / Economy,Operational,"Forecasting, Analyzing, Advising","Decision-maker, Decision-subject",NA,NA,NA,NA,NA,Yes,No
2-9873,elsevier,Early-stage design support combining machine learning and building information modelling,"Global energy concerns necessitate designing energy-efficient buildings. Many important decisions affecting energy performance are made at early stages with little information. Dynamic simulations support informed decision-making; however, uncertainty, high computational time, and expensive modelling efforts impair their use at early stages. This article develops an approach using building information modelling and machine learning that provides quick energy performance information. This approach has been implemented into a web tool, p-energyanalysis.de. It allows design space exploration, assesses the energy performance of design options, compares multiple options, performs sensitivity analysis, and tracks changes. Twenty-one participants (researchers and architects) used it as a support tool for designing an energy-efficient building. Their feedbacks are discussed as part of the tool development. The study found that the tool supports early-stage design decisions by quickly providing relevant information. The limitations, such as the bias in the results towards training data population and implementation issues, are also discussed.",https://doi.org/10.1016/j.autcon.2022.104147,https://www.sciencedirect.com/science/article/pii/S0926580522000206,Automation in Construction,Manav Mahan Singh;Chirag Deb;Philipp Geyer,2022,55,"@article{2-9873,
  title = {Early-stage design support combining machine learning and building information modelling},
  author = {Manav Mahan Singh and Chirag Deb and Philipp Geyer},
  year = {2022},
  doi = {10.1016/j.autcon.2022.104147},
  journal = {Automation in Construction}
}",System/Artifact contributions,Environment / Resources / Energy,Individual,"Forecasting, Advising",Decision-maker,"Alter decision outcomes, Change cognitive demands","Change AI responses, Update AI competence",energy performance information,NA,Interactive interface,Yes,Yes
2-9904,elsevier,"Effect of a comprehensive deep-learning model on the accuracy of chest x-ray interpretation by radiologists: a retrospective, multireader multicase study","Summary Background Chest x-rays are widely used in clinical practice; however, interpretation can be hindered by human error and a lack of experienced thoracic radiologists. Deep learning has the potential to improve the accuracy of chest x-ray interpretation. We therefore aimed to assess the accuracy of radiologists with and without the assistance of a deep-learning model. Methods In this retrospective study, a deep-learning model was trained on 821 681 images (284 649 patients) from five data sets from Australia, Europe, and the USA. 2568 enriched chest x-ray cases from adult patients (≥16 years) who had at least one frontal chest x-ray were included in the test dataset; cases were representative of inpatient, outpatient, and emergency settings. 20 radiologists reviewed cases with and without the assistance of the deep-learning model with a 3-month washout period. We assessed the change in accuracy of chest x-ray interpretation across 127 clinical findings when the deep-learning model was used as a decision support by calculating area under the receiver operating characteristic curve (AUC) for each radiologist with and without the deep-learning model. We also compared AUCs for the model alone with those of unassisted radiologists. If the lower bound of the adjusted 95% CI of the difference in AUC between the model and the unassisted radiologists was more than −0·05, the model was considered to be non-inferior for that finding. If the lower bound exceeded 0, the model was considered to be superior. Findings Unassisted radiologists had a macroaveraged AUC of 0·713 (95% CI 0·645–0·785) across the 127 clinical findings, compared with 0·808 (0·763–0·839) when assisted by the model. The deep-learning model statistically significantly improved the classification accuracy of radiologists for 102 (80%) of 127 clinical findings, was statistically non-inferior for 19 (15%) findings, and no findings showed a decrease in accuracy when radiologists used the deep-learning model. Unassisted radiologists had a macroaveraged mean AUC of 0·713 (0·645–0·785) across all findings, compared with 0·957 (0·954–0·959) for the model alone. Model classification alone was significantly more accurate than unassisted radiologists for 117 (94%) of 124 clinical findings predicted by the model and was non-inferior to unassisted radiologists for all other clinical findings. Interpretation This study shows the potential of a comprehensive deep-learning model to improve chest x-ray interpretation across a large breadth of clinical practice. Funding Annalise.ai.",https://doi.org/10.1016/S2589-7500(21)00106-0,https://www.sciencedirect.com/science/article/pii/S2589750021001060,The Lancet Digital Health,Jarrel C Y Seah;Cyril H M Tang;Quinlan D Buchlak;Xavier G Holt;Jeffrey B Wardman;Anuar Aimoldin;Nazanin Esmaili;Hassan Ahmad;Hung Pham;John F Lambert;Ben Hachey;Stephen J F Hogg;Benjamin P Johnston;Christine Bennett;Luke Oakden-Rayner;Peter Brotchie;Catherine M Jones,2021,3,"@article{2-9904,
  title = {Effect of a comprehensive deep-learning model on the accuracy of chest x-ray interpretation by radiologists: a retrospective, multireader multicase study},
  author = {Jarrel C Y Seah and Cyril H M Tang and Quinlan D Buchlak and Xavier G Holt and Jeffrey B Wardman and Anuar Aimoldin and Nazanin Esmaili and Hassan Ahmad and Hung Pham and John F Lambert and Ben Hachey and Stephen J F Hogg and Benjamin P Johnston and Christine Bennett and Luke Oakden-Rayner and Peter Brotchie and Catherine M Jones},
  year = {2021},
  doi = {10.1016/S2589-7500(21)00106-0},
  journal = {The Lancet Digital Health}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising","Decision-maker, Decision-subject, Knowledge provider",NA,NA,NA,NA,NA,Yes,No
2-9905,elsevier,Effect of an artificial intelligence decision support tool on palliative care referral in hospitalized patients: a randomized clinical trial,"Context Palliative care services are commonly provided to hospitalized patients, but accurately predicting who needs them remains a challenge. Objectives To assess the effectiveness on clinical outcomes of an artificial intelligence (AI)/machine learning (ML) decision support tool for predicting patient need for palliative care services in the hospital. Methods The study design was a pragmatic, cluster-randomized, stepped-wedge clinical trial in 12 nursing units at two hospitals over a 15-month period between August 19, 2019, and November 17, 2020. Eligible patients were randomly assigned to either a medical service consultation recommendation triggered by an AI/ML tool predicting the need for palliative care services or usual care. The primary outcome was palliative care consultation note. Secondary outcomes included: hospital readmissions, length of stay, transfer to intensive care and palliative care consultation note by unit. Results A total of 3183 patient hospitalizations were enrolled. Of eligible patients, A total of 2544 patients were randomized to the decision support tool (1212; 48%) and usual care (1332; 52%). Of these, 1717 patients (67%) were retained for analyses. Patients randomized to the intervention had a statistically significant higher incidence rate of palliative care consultation compared to the control group (IRR, 1.44 [95% CI, 1.11–1.92]). Exploratory evidence suggested that the decision support tool group reduced 60-day and 90-day hospital readmissions (OR, 0.75 [95% CI, 0.57, 0.97]) and (OR, 0.72 [95% CI, 0.55–0.93]) respectively. Conclusion A decision support tool integrated into palliative care practice and leveraging AI/ML demonstrated an increased palliative care consultation rate among hospitalized patients and reductions in hospitalizations.",https://doi.org/10.1016/j.jpainsymman.2023.02.317,https://www.sciencedirect.com/science/article/pii/S0885392423003974,Journal of Pain and Symptom Management,Patrick M. Wilson;Priya Ramar;Lindsey M. Philpot;Jalal Soleimani;Jon O. Ebbert;Curtis B. Storlie;Alisha A. Morgan;Gavin M. Schaeferle;Shusaku W. Asai;Vitaly Herasevich;Brian W. Pickering;Ing C. Tiong;Emily A. Olson;Jordan C. Karow;Yuliya Pinevich;Jacob Strand,2023,0,"@article{2-9905,
  title = {Effect of an artificial intelligence decision support tool on palliative care referral in hospitalized patients: a randomized clinical trial},
  author = {Patrick M. Wilson and Priya Ramar and Lindsey M. Philpot and Jalal Soleimani and Jon O. Ebbert and Curtis B. Storlie and Alisha A. Morgan and Gavin M. Schaeferle and Shusaku W. Asai and Vitaly Herasevich and Brian W. Pickering and Ing C. Tiong and Emily A. Olson and Jordan C. Karow and Yuliya Pinevich and Jacob Strand},
  year = {2023},
  doi = {10.1016/j.jpainsymman.2023.02.317},
  journal = {Journal of Pain and Symptom Management}
}",Empirical contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-9923,elsevier,Effectively fusing clinical knowledge and ai knowledge for reliable lung nodule diagnosis,"Clinicians typically use semantic features to judge the malignant status of nodules, while artificial intelligence systems (AI) tend to extract unknown features to diagnose nodules. The former relies on clinical knowledge, while the latter explores AI knowledge. Although many studies indicate that fusing clinical and AI knowledge can help computer-aided diagnosis (CAD) systems improve diagnostic accuracy and gain clinician approval, how to effectively fuse them is still an open question. This paper proposes a simple and effective pipeline (abbreviated as CKAK), which fuses clinical and AI knowledge at both feature and decision levels for accurate lung nodule malignancy classification and semantic attributes characterization. The feature-level fusion can retain rich information in high-dimensional features and improve the model’s accuracy; the decision-level fusion can provide some interpretability for the model’s decision-making process, which is expected in clinical applications. Specifically, the proposed CKAK consists of two sequential stages: (i) the initial prediction stage (IPS); and (ii) the prediction refine stage (PRS). The IPS predicts eight radiologist-interpreted semantic attributes and an initial malignancy diagnosis in parallel. Then, these results are fed to the subsequent PRS to refine the diagnosis further by fully fusing them at feature and decision levels. Besides, to enhance the ability of feature learning, we propose a novel scale-aware feature extraction block (SAFE). It integrates multi-scale contextual features with a lightweight Transformer rather than adding or concatenating them roughly. Extensive experiments at the LIDC-IDRI data set show that the proposed CKAK can achieve superior benign-malignant classification accuracy with minor radiologist-interpreted semantic scores error, meeting the need for a reliable CAD system.",https://doi.org/10.1016/j.eswa.2023.120634,https://www.sciencedirect.com/science/article/pii/S0957417423011363,Expert Systems with Applications,Duwei Dai;Yongheng Sun;Caixia Dong;Qingsen Yan;Zongfang Li;Songhua Xu,2023,8,"@article{2-9923,
  title = {Effectively fusing clinical knowledge and AI knowledge for reliable lung nodule diagnosis},
  author = {Dai, Duwei and Sun, Yongheng and Dong, Caixia and Yan, Qingsen and Li, Zongfang and Xu, Songhua},
  year = {2023},
  doi = {10.1016/j.eswa.2023.120634},
  journal = {Expert Systems with Applications}
}",Algorithmic contributions,Healthcare / Medicine / Surgery,Operational,"Forecasting, Advising",Decision-maker,NA,NA,NA,NA,NA,Yes,No
2-9927,elsevier,Effects of explainable artificial intelligence on trust and human behavior in a high-risk decision task,"Understanding the recommendations of an artificial intelligence (AI) based assistant for decision-making is especially important in high-risk tasks, such as deciding whether a mushroom is edible or poisonous. To foster user understanding and appropriate trust in such systems, we assessed the effects of explainable artificial intelligence (XAI) methods and an educational intervention on AI-assisted decision-making behavior in a 2 × 2 between subjects online experiment with N=410 participants. We developed a novel use case in which users go on a virtual mushroom hunt and are tasked with picking edible and leaving poisonous mushrooms. Users were provided with an AI-based app that showed classification results of mushroom images. To manipulate explainability, one subgroup additionally received attribution-based and example-based explanations of the AI’s predictions; for the educational intervention one subgroup received additional information on how the AI worked. We found that the group that received explanations outperformed that which did not and showed better calibrated trust levels. Contrary to our expectations, we found that the educational intervention, domain-specific (i.e., mushroom) knowledge, and AI knowledge had no effect on performance. We discuss practical implications and introduce the mushroom-picking task as a promising use case for XAI research.",https://doi.org/10.1016/j.chb.2022.107539,https://www.sciencedirect.com/science/article/pii/S0747563222003594,Computers in Human Behavior,Benedikt Leichtmann;Christina Humer;Andreas Hinterreiter;Marc Streit;Martina Mara,2023,191,"@article{2-9927,
  title = {Effects of explainable artificial intelligence on trust and human behavior in a high-risk decision task},
  author = {Leichtmann, Benedikt and Humer, Christina and Hinterreiter, Andreas and Streit, Marc and Mara, Martina},
  year = {2023},
  doi = {10.1016/j.chb.2022.107539},
  journal = {Computers in Human Behavior}
}",Empirical contributions,Generic / Abstract / Domain-agnostic,Individual,"Explaining, Forecasting, Advising",Decision-maker,"Change trust, Alter decision outcomes",no such info,"classification, prediction of alternative, example-based explanations, attribution-based explanations","domain knowledge, AI knowledge","Visual, Interactive interface",Yes,Yes
2-9998,elsevier,Empathic voice assistants: enhancing consumer responses in voice commerce,"Artificial intelligence (AI)-enabled voice assistants (VAs) are transforming firm-customer interactions but often come across as lacking empathy. This challenge may cause business managers to question the overall effectiveness of VAs in shopping contexts. Recognizing empathy as a core design element in the next generation of VAs and the limits of scenario-based studies in voice commerce, this article investigates how empathy exhibited by an existing AI agent (Alexa) may alter consumer shopping responses. AI empathy moderates the original structural model bridging functional, relational, and social-emotional dimensions. Findings of an individual-session online experiment show higher intentions to delegate tasks, seek decision assistance, and trust recommendations from AI agents perceived as empathic. In contrast to individual shoppers, families respond better to functional VA attributes such as ease of use when AI empathy is present. The results contribute to the literature on AI empathy and conversational commerce while informing managerial AI design decisions.",https://doi.org/10.1016/j.jbusres.2024.114566,https://www.sciencedirect.com/science/article/pii/S0148296324000705,Journal of Business Research,Alex Mari;Andreina Mandelli;René Algesheimer,2024,63,"@article{2-9998,
  title = {Empathic voice assistants: enhancing consumer responses in voice commerce},
  author = {Alex Mari and Andreina Mandelli and René Algesheimer},
  year = {2024},
  journal = {Journal of Business Research},
  doi = {10.1016/j.jbusres.2024.114566}
}",Empirical contributions,Finance / Business / Economy,Individual,"Advising, Collaborating",Decision-maker,"Change trust, Change affective-perceptual, Alter decision outcomes",no such info,"delegation, recommendations",emotion expression,Auditory,Yes,Yes
